{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62105484-6c94-4122-b06a-2d7175aeb258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting script/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile script/main.py\n",
    "\n",
    "import torch\n",
    "\n",
    "def acquire_data():\n",
    "    from data_acquisition import get_pdf_paths, open_documents, get_text, get_metadata\n",
    "    pdf_paths = get_pdf_paths()\n",
    "    docs = open_documents(pdf_paths)\n",
    "    pages = get_text(docs)\n",
    "    pages_and_metadata = get_metadata(pages)\n",
    "    \n",
    "    from preprocessing_text import preprocessing_raw_text, convert_paragraphs_to_sentences\n",
    "    pages_and_metadata = preprocessing_raw_text(pages_and_metadata)\n",
    "    pages_and_metadata = convert_paragraphs_to_sentences(pages_and_metadata)\n",
    "\n",
    "    from embeddings import load_embedding_model, convert_data_to_embeddings, get_data_embeddings, convert_embeddings_to_same_dimensions, flatten, save_embeddings, save_data\n",
    "    embedding_model = load_embedding_model(embedding_model_name, device)\n",
    "    pages_and_metadata = convert_data_to_embeddings(embedding_model, pages_and_metadata, device)\n",
    "    pages_and_metadata_embeddings = get_data_embeddings(pages_and_metadata, device)\n",
    "    pages_and_metadata_embeddings = convert_embeddings_to_same_dimensions(pages_and_metadata_embeddings, device)\n",
    "    flat_embeddings, flat_data = flatten(pages_and_metadata_embeddings, pages_and_metadata)\n",
    "    save_embeddings(flat_embeddings)\n",
    "    save_data(flat_data)\n",
    "\n",
    "def ask(query):\n",
    "    from embeddings import load_embeddings, load_data\n",
    "    pages_and_metadata_embeddings = load_embeddings().to(device)\n",
    "    flat_data = load_data()\n",
    "    \n",
    "    from similarity import get_similarity_score_by_query, get_top_k_scores, get_top_k_content\n",
    "    dot_scores = get_similarity_score_by_query(query, embedding_model_name, pages_and_metadata_embeddings, device)\n",
    "    top_scores, top_indices = get_top_k_scores(dot_scores, 15)\n",
    "    context = get_top_k_content(top_indices, flat_data)\n",
    "    \n",
    "    from augmentation import load_llm_model, prompt_augmentation, get_answer, clean_answer\n",
    "    llm_model = load_llm_model(llm_model_name, device)\n",
    "    prompt = prompt_augmentation(llm_model_name, context, query)\n",
    "    output_text = get_answer(llm_model, prompt, llm_model_name, device)\n",
    "    processed_answer = clean_answer(output_text)\n",
    "    return processed_answer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    embedding_model_name = 'all-MiniLM-L12-v2'\n",
    "    llm_model_name = 'google/gemma-2b-it'\n",
    "    query = \"What are machine learning? Explain in 500 words\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    acquire_data()\n",
    "    print(\"Data has been acquired\")\n",
    "    answer = ask(query)\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "751a0259-d456-4148-afa1-c147a02b43b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting script/data_acquisition.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile script/data_acquisition.py\n",
    "\n",
    "from glob import glob\n",
    "import fitz\n",
    "\n",
    "def get_pdf_paths():\n",
    "    pdf_paths = glob('.\\\\Dataset\\\\*.pdf')\n",
    "    return pdf_paths\n",
    "\n",
    "def open_documents(pdf_paths):\n",
    "    docs = list()\n",
    "    for doc_path in pdf_paths:\n",
    "        doc = fitz.open(doc_path)\n",
    "        docs.append(doc)\n",
    "    return docs\n",
    "\n",
    "def get_text(documents):\n",
    "    pages = dict()\n",
    "    for doc in documents:\n",
    "        for page_number, page in enumerate(doc):\n",
    "            if (page_number<15):\n",
    "                continue\n",
    "            else:\n",
    "                page_number = len(pages)\n",
    "                pages[page_number] = page.get_text()\n",
    "    return pages\n",
    "\n",
    "def get_metadata(pages):\n",
    "    pages_and_metadata = list()\n",
    "    for page_number, page in pages.items():\n",
    "        metadata = dict()\n",
    "        metadata['page_number'] = page_number\n",
    "        metadata['raw_text'] = page\n",
    "        metadata['number_of_characters'] = len(page)\n",
    "        metadata['number_of_tokens'] = len(page)/4\n",
    "        metadata['number_of_words'] = len(page.split())\n",
    "        pages_and_metadata.append(metadata)\n",
    "    return pages_and_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c36c85fd-4b0a-4018-baf4-b0b1f13e2f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting script/preprocessing_text.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile script/preprocessing_text.py\n",
    "\n",
    "from spacy.lang.en import STOP_WORDS\n",
    "import re\n",
    "from spacy.lang.en import English\n",
    "\n",
    "def convert_to_lowercase(text):\n",
    "    new_text = text.lower()\n",
    "    return new_text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    new_text = []\n",
    "    for word in text.split():\n",
    "        if word not in STOP_WORDS:\n",
    "            new_text.append(word)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    new_text = re.sub(r\"<!--.*?-->\", \"\", text)\n",
    "    return new_text\n",
    "\n",
    "def remove_newlines(text):\n",
    "    new_text = re.sub(r\"\\n+\", \" \", text)\n",
    "    return new_text\n",
    "\n",
    "def remove_multiple_spaces(text):\n",
    "    new_text = text.replace(\"  \", \" \")\n",
    "    return new_text\n",
    "\n",
    "def remove_comments(text):\n",
    "    new_text = re.sub(r\"<!--.*?-->\", \"\", text)\n",
    "    return new_text\n",
    "\n",
    "def remove_unnecessary_text(text):\n",
    "    new_text = text.replace(\"answer:\",\"\").replace(\"question\", \"\").replace(\":\",\"\").replace(\"  \",\" \")\n",
    "    return new_text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = convert_to_lowercase(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = remove_html_tags(text)\n",
    "    text = remove_newlines(text)\n",
    "    text = remove_multiple_spaces(text)\n",
    "    text = remove_comments(text)\n",
    "    text = remove_unnecessary_text(text)\n",
    "    return text\n",
    "\n",
    "def preprocessing_raw_text(pages_and_metadata):\n",
    "    for page in pages_and_metadata:\n",
    "        page[\"formatted_text\"] = preprocess_text(page[\"raw_text\"])\n",
    "    return pages_and_metadata\n",
    "\n",
    "def convert_paragraphs_to_sentences(pages_and_metadata):\n",
    "    nlp = English()\n",
    "    nlp.add_pipe(\"sentencizer\")\n",
    "    for page in pages_and_metadata:\n",
    "        sentences = nlp(page[\"formatted_text\"]).sents\n",
    "        sentences = list(set([str(sentence).strip() for sentence in sentences if len(str(sentence).split())>10]))\n",
    "        pages_and_metadata[page[\"page_number\"]][\"sentences\"] = sentences\n",
    "        pages_and_metadata[page[\"page_number\"]][\"number_of_sentences\"] = len(sentences)\n",
    "    return pages_and_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52e96613-f7ea-4b3f-9c14-c1430ebd6f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting script/embeddings.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile script/embeddings.py\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "def load_embedding_model(embedding_model_name, device=\"cpu\"):\n",
    "    model = SentenceTransformer(embedding_model_name).to(device)\n",
    "    return model\n",
    "\n",
    "def convert_data_to_embeddings(embedding_model,pages_and_metadata, device=\"cpu\"):\n",
    "    for page in pages_and_metadata:\n",
    "        embeddings = list()\n",
    "        for sentence in page[\"sentences\"]:\n",
    "            # sentence = sentence.to(device)\n",
    "            embedding = embedding_model.encode(sentence, batch_size=1024, convert_to_tensor=True, show_progress_bar=False, device=device)\n",
    "            embedding = np.stack(embedding.tolist(), axis=0)\n",
    "            embedding = torch.tensor(embedding)\n",
    "            embedding = embedding.type(torch.float32)\n",
    "            embeddings.append(embedding)\n",
    "        sentence_embeddings = [np.array(embedding) for embedding in embeddings]\n",
    "        pages_and_metadata[page[\"page_number\"]][\"embeddings\"] = sentence_embeddings\n",
    "    return pages_and_metadata\n",
    "\n",
    "\n",
    "def get_data_embeddings(pages_and_metadata, device=\"cpu\"):\n",
    "    pages_and_metadata_embeddings = []\n",
    "    for page in pages_and_metadata:\n",
    "        page_embeddings = []\n",
    "        for sentence_embedding in pages_and_metadata[page[\"page_number\"]][\"embeddings\"]:\n",
    "            if isinstance(sentence_embedding, torch.Tensor):\n",
    "                sentence_embedding = sentence_embedding.tolist()\n",
    "            page_embeddings.append(sentence_embedding)\n",
    "        pages_and_metadata_embeddings.append(page_embeddings)\n",
    "    return pages_and_metadata_embeddings\n",
    "\n",
    "def convert_embeddings_to_same_dimensions(pages_and_metadata_embeddings, device=\"cpu\"):\n",
    "    if pages_and_metadata_embeddings:\n",
    "        embedding_dim = len(pages_and_metadata_embeddings[0][0])\n",
    "        pages_and_metadata_embeddings = [\n",
    "                [np.pad(chunk, (0, max(0, embedding_dim - len(chunk))), mode='constant')[:embedding_dim]\n",
    "                 for chunk in page]\n",
    "                for page in pages_and_metadata_embeddings\n",
    "            ]\n",
    "    return pages_and_metadata_embeddings\n",
    "\n",
    "def flatten(pages_and_metadata_embeddings, pages_and_metadata):\n",
    "    flat_embeddings = [chunk for page in pages_and_metadata_embeddings for chunk in page]\n",
    "    flat_data = [sentence for page in pages_and_metadata for sentence in page[\"sentences\"]]\n",
    "    return flat_embeddings, flat_data\n",
    "\n",
    "def save_embeddings(flat_embeddings, name=\"SaveFile/embeddings.csv\"):\n",
    "    df = pd.DataFrame(flat_embeddings)\n",
    "    df.to_csv(name, index=False)\n",
    "\n",
    "def save_data(flat_data, name=\"SaveFile/data.csv\"):\n",
    "    df = pd.DataFrame(flat_data)\n",
    "    df.to_csv(name, index=False)\n",
    "\n",
    "def load_embeddings(name=\"SaveFile/embeddings.csv\", device=\"cpu\"):\n",
    "    flat_embeddings = pd.read_csv(name).to_numpy()\n",
    "    pages_and_metadata_embeddings = np.array(flat_embeddings, dtype=np.float32)\n",
    "    pages_and_metadata_embeddings = torch.tensor(pages_and_metadata_embeddings, dtype=torch.float32).to(device)\n",
    "    return pages_and_metadata_embeddings\n",
    "\n",
    "def load_data(name=\"SaveFile/data.csv\", device=\"cpu\"):\n",
    "    flat_data = pd.read_csv(name)[\"0\"].tolist()\n",
    "    return flat_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "230dcad3-deae-4d23-8e35-a548ffc213cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting script/similarity.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile script/similarity.py\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import util\n",
    "import torch\n",
    "\n",
    "def get_similarity_score_by_query(query, embedding_model_name, pages_and_metadata_embeddings, device=\"cpu\"):\n",
    "    embedding_model = SentenceTransformer(embedding_model_name)\n",
    "    query_embeddings = embedding_model.encode(query, convert_to_tensor=True).to(device)\n",
    "    dot_scores = util.dot_score(query_embeddings, pages_and_metadata_embeddings)[0]\n",
    "    return dot_scores\n",
    "\n",
    "def get_top_k_scores(dot_scores, k=10):\n",
    "    top_scores, top_indices = torch.topk(dot_scores, k=k)\n",
    "    return top_scores, top_indices\n",
    "\n",
    "def get_top_k_content(top_indices, flat_data):\n",
    "    context = list()\n",
    "    for idx in top_indices:\n",
    "        context.append(flat_data[idx.item()])\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "344669e1-4d86-4748-a431-3b1343d24c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting script/augmentation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile script/augmentation.py\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import re\n",
    "\n",
    "def load_llm_model(llm_model_name, device=\"cpu\"):\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        pretrained_model_name_or_path=llm_model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        low_cpu_mem_usage=False,\n",
    "    ).to(device)\n",
    "    return model\n",
    "\n",
    "def prompt_augmentation(llm_model_name, context, query):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
    "    context = \"\\n -\".join(context)\n",
    "    base_prompt = f'''Based on the following context items, please answer the query\n",
    "    Context Items:\n",
    "    {context}\n",
    "    Query:\n",
    "    {query}\n",
    "    Answer:'''\n",
    "    base_prompt = base_prompt.format(context=context, query=query)\n",
    "    dialogue_template = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": base_prompt,\n",
    "    }]\n",
    "    prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
    "                                       tokenize=False,\n",
    "                                       add_generation_prompt=True)\n",
    "    return prompt\n",
    "\n",
    "def get_answer(llm_model, prompt, llm_model_name, device=\"cpu\", temperature=0.2, max_new_tokens=512):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = llm_model.generate(**input_ids, temperature=temperature, do_sample=True, max_new_tokens=max_new_tokens)\n",
    "    output_text = tokenizer.decode(outputs[0])\n",
    "    return output_text\n",
    "\n",
    "def clean_answer(output_text):\n",
    "    idx = output_text.find(\"Answer\")\n",
    "    answer = output_text[idx+7:]\n",
    "    answer = answer.replace(\"**\", \"\")\n",
    "    answer = answer.replace(\"<start_of_turn>model\",\"\")\n",
    "    answer = re.sub(\"<.*?>\", \"\", answer)\n",
    "    return (f\"The cleaned answer is: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6899e3b9-b8f2-4a20-b8e5-deff32a59eb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc16927d-5f29-4b37-8ca3-e7fd4e5e041e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569b057b-18e4-4d5f-a0a9-51d5e15aab54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f686159-5a90-4961-bc0d-df75ce1b4bcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
