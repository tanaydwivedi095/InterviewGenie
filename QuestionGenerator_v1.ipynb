{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4a09aeb1-124f-4b0a-a0b3-20ab78d5ee03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22725c5765a64f70b34646906144f479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1 DONE\n",
      "STEP 2 DONE\n",
      "\n",
      "    Generate 15 questions for an Interview for the position of Machine Learning Engineer.\n",
      "    Each question should be of type as mentioned below:\n",
      "    1. probability and statistics\n",
      "2. algorithmic theory\n",
      "3. definition\n",
      "4. algorithmic theory\n",
      "5. mathematics\n",
      "6. definition\n",
      "7. algorithmic theory\n",
      "8. case-study\n",
      "9. case-study\n",
      "10. definition\n",
      "11. formula\n",
      "12. mathematics\n",
      "13. probability and statistics\n",
      "14. formula\n",
      "15. scenario\n",
      "    The output provided should be in format as given below:\n",
      "    Question 1: Question Type 1 ....\n",
      "    Question 2: Question Type 2 ....\n",
      "    Question 3: Question Type 3 .....\n",
      "    and so on.\n",
      "    No extra text should be generated as answer.\n",
      "    The case study questions should be well defined.\n",
      "    The questions should only be of the specified type.\n",
      "    \n",
      "STEP 3 DONE\n",
      "INPUT IDS\n",
      "OUTPUTS\n",
      "STEP 4 DONE\n",
      "ENDING\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "\n",
    "def load_llm_model(llm_model_name, device=\"cpu\"):\n",
    "    llm_model = AutoModelForCausalLM.from_pretrained(llm_model_name, \n",
    "                                                     torch_dtype=torch.float16).to(device)\n",
    "    return llm_model\n",
    "\n",
    "def load_tokenizer(llm_model_name, device=\"cpu\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
    "    return tokenizer\n",
    "    \n",
    "def generate_prompt():\n",
    "    question_type = {\n",
    "        0: \"definition\",\n",
    "        1: \"formula\",\n",
    "        2: \"scenario\",\n",
    "        3: \"case-study\",\n",
    "        4: \"algorithmic theory\",\n",
    "        5: \"mathematics\",\n",
    "        6: \"probability and statistics\",\n",
    "    }\n",
    "    question_types = list()\n",
    "    number_of_questions = 15\n",
    "    for i in range(number_of_questions):\n",
    "        random_idx = random.randint(0, len(question_type)-1)\n",
    "        question_types.append(f\"{i+1}. {question_type[random_idx]}\")\n",
    "    question_types = \"\\n\".join(question_types)\n",
    "    base_prompt = f\"\"\"\n",
    "    Generate {number_of_questions} questions for an Interview for the position of Machine Learning Engineer.\n",
    "    Each question should be of type as mentioned below:\n",
    "    {question_types}\n",
    "    The output provided should be in format as given below:\n",
    "    Question 1: Question Type 1 ....\n",
    "    Question 2: Question Type 2 ....\n",
    "    Question 3: Question Type 3 .....\n",
    "    and so on.\n",
    "    No extra text should be generated as answer.\n",
    "    The case study questions should be well defined.\n",
    "    The questions should only be of the specified type.\n",
    "    \"\"\"\n",
    "    \n",
    "    base_prompt = base_prompt.format(question_types = question_types, \n",
    "                                     number_of_questions = number_of_questions)\n",
    "    dialogue_template = [{\n",
    "        \"role\":\"user\",\n",
    "        \"message\":base_prompt\n",
    "    }]\n",
    "    prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
    "                                           tokenize=False,\n",
    "                                           add_generation_prompt=True)\n",
    "    return base_prompt\n",
    "\n",
    "def generate_question(llm_model, tokenizer, prompt, temperature=0.3, max_new_tokens=2048, device=\"cpu\"):\n",
    "    input_ids = tokenizer([prompt], \n",
    "                          return_tensors=\"pt\").to(device)\n",
    "    print(\"INPUT IDS\")\n",
    "    outputs = llm_model.generate(**input_ids, \n",
    "                                 temperature=temperature, \n",
    "                                 do_sample=True, \n",
    "                                 max_new_tokens=max_new_tokens)\n",
    "    print(\"OUTPUTS\")\n",
    "    output_text = tokenizer.batch_decode(outputs)[0]\n",
    "    return output_text\n",
    "\n",
    "def post_process_questions(questions):\n",
    "    questions = questions[questions.find(\"\\n\\n**\")+4:].replace(\"**\",\"\").replace(\"\\n\",\"\")\n",
    "    questions = questions.split(\"Question\")\n",
    "    questions = [question for question in questions if question]\n",
    "    questions[-1] = questions[-1][:-5]\n",
    "    return questions\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"STARTING\")\n",
    "    # llm_model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "    llm_model_name = \"google/gemma-2b-it\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    llm_model = load_llm_model(llm_model_name, device)\n",
    "    print(\"STEP 1 DONE\")\n",
    "    tokenizer = load_tokenizer(llm_model_name, device)\n",
    "    print(\"STEP 2 DONE\")\n",
    "    prompt = generate_prompt()\n",
    "    print(prompt)\n",
    "    print(\"STEP 3 DONE\")\n",
    "    questions = generate_question(llm_model=llm_model, tokenizer=tokenizer, prompt=prompt, device=device)\n",
    "    print(\"STEP 4 DONE\")\n",
    "    questionLst = post_process_questions(questions)\n",
    "    print(\"ENDING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d42b1c66-8ca4-41f7-9dc5-08f7aba406b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1:What is the difference between probability and statistics?\n",
      " 2:Explain the concept of a decision boundary in the context of machine learning.\n",
      " 3:Define the term \"overfitting\" in the context of machine learning.\n",
      " 4:Describe the difference between supervised and unsupervised learning.\n",
      " 5:What is the difference between linear regression and logistic regression?\n",
      " 6:Explain the concept of dimensionality reduction in the context of machine learning.\n",
      " 7:Define the term \"ensemble learning\" in the context of machine learning.\n",
      " 8:Provide an example of a common algorithm used for time series analysis.\n",
      " 9:Describe the difference between supervised and unsupervised learning.\n",
      " 10:What is the difference between a hypothesis and a model in machine learning?\n",
      " 11:Define the term \"regularization\" in the context of machine learning.\n",
      " 12:What is the difference between a supervised and an unsupervised learning algorithm?\n",
      " 13:What is the concept of \"weak learning\"?\n",
      " 14:Describe the difference between a supervised and an unsupervised learning algorithm.\n",
      " 15:What is the difference between a feature and a target variable in machine learning?\n"
     ]
    }
   ],
   "source": [
    "for question in questionLst:\n",
    "    print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4050afa-9599-466f-bb17-59e1ef038211",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
