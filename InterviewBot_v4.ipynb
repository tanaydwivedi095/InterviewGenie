{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f2d85b35e1e945f",
   "metadata": {},
   "source": [
    "# 1. IMPORTING LIBRARIES, FUNCTION AND DEFINING GLOBAL VARIABLES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3e95c360721358",
   "metadata": {},
   "source": [
    "## 1.1 Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1be9ae60d3bc106",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:38:29.057193Z",
     "start_time": "2025-02-10T16:38:24.848738Z"
    }
   },
   "outputs": [],
   "source": [
    "# Library needed to load the pdf\n",
    "import fitz\n",
    "\n",
    "# Library needed to process the text using regular expressions\n",
    "import re\n",
    "\n",
    "# Library needed to display or process the data in forms of dataframes\n",
    "import pandas as pd\n",
    "\n",
    "# Library needed to handle the operations in deep learning\n",
    "import torch\n",
    "\n",
    "# Library needed to convert the data into arrays for faster processing\n",
    "import numpy as np\n",
    "\n",
    "# Library to handle operating system related operations\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b18b422d1d839a",
   "metadata": {},
   "source": [
    "## 1.2 Importing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d138f8cdda61885",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:39:37.411274Z",
     "start_time": "2025-02-10T16:39:32.176313Z"
    }
   },
   "outputs": [],
   "source": [
    "# (OPTIONAL) Function to beautify the waiting process with a loading bar\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "\n",
    "# Function to process the text in English\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# Function to convert paragraphs to sentences\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Function to provide utility services to process the text such as tokenization, sentencizer\n",
    "from sentence_transformers import util\n",
    "\n",
    "# Functions for loading the LLM model\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# Function for fetching the paths to pdfs\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d989d11d53db12e8",
   "metadata": {},
   "source": [
    "## 1.3 Defining Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d9391505ec14fa5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:39:42.151208Z",
     "start_time": "2025-02-10T16:39:42.144311Z"
    }
   },
   "outputs": [],
   "source": [
    "# Global variable consisting of all the stop words\n",
    "from spacy.lang.en import STOP_WORDS\n",
    "\n",
    "# Global variable telling about the number of sentences in each chunk stored in the dictionary\n",
    "SENTENCE_CHUNKS = 10\n",
    "\n",
    "# Global variable storing the name of the model that is used for the embedding\n",
    "EMBEDDING_MODEL = 'all-mpnet-base-v2'\n",
    "\n",
    "# Global variable storing the names of the pdfs that are to be loaded to be fed into the RAG model\n",
    "PDF_PATHS = list()\n",
    "\n",
    "# Global variable storing the integer telling to fetch the top k similar records for further processing\n",
    "K = 50\n",
    "\n",
    "# Global variable storing the name of the LLM model that will be used for augmenting the similar data\n",
    "LLM_MODEL = 'google/gemma-2b-it'\n",
    "\n",
    "# (FOR TESTING) Global variable storing the query that user wants to ask\n",
    "QUERY = \"What is overfitting in machine learning? Explain in 500 words\"\n",
    "\n",
    "# Setting up the device agnostic code\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Global variable for setting the temperature of the LLM model i.e how much data should LLM generate\n",
    "TEMPERATURE = 0.3\n",
    "\n",
    "## Global variable defining the length of tokens that the LLM has to generate\n",
    "MAX_NEW_TOKENS = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd579bf352b46f64",
   "metadata": {},
   "source": [
    "# 2. DATA ACQUISITION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ac37ccdc34c608",
   "metadata": {},
   "source": [
    "## 2.1 Getting the paths to all the pdfs in the `Dataset` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144a542f97a7fd0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:39:44.681217Z",
     "start_time": "2025-02-10T16:39:44.671241Z"
    }
   },
   "outputs": [],
   "source": [
    "PDF_PATHS = glob('.\\\\Dataset\\\\*.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c7124cd29610ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:39:46.384156Z",
     "start_time": "2025-02-10T16:39:46.366334Z"
    }
   },
   "outputs": [],
   "source": [
    "for idx, path in tqdm(enumerate(PDF_PATHS), total=len(PDF_PATHS)):\n",
    "    print(f\"{idx+1}. {path[10:-4]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dd0b9bf343f79b",
   "metadata": {},
   "source": [
    "## 2.2 Opening all the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84de47a6ce3e068a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:39:48.621570Z",
     "start_time": "2025-02-10T16:39:48.539419Z"
    }
   },
   "outputs": [],
   "source": [
    "documents = list()\n",
    "for path in tqdm(PDF_PATHS, total=len(PDF_PATHS)):\n",
    "    doc = fitz.open(path)\n",
    "    documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fee0989615356e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:39:49.354274Z",
     "start_time": "2025-02-10T16:39:49.339845Z"
    }
   },
   "outputs": [],
   "source": [
    "for doc in tqdm(documents, total=len(documents)):\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e1e996c74b7c79",
   "metadata": {},
   "source": [
    "## 2.3 Getting the text from all the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f67dd2a9810cc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:39:55.297068Z",
     "start_time": "2025-02-10T16:39:54.009306Z"
    }
   },
   "outputs": [],
   "source": [
    "pages = dict()\n",
    "for doc in tqdm(documents, total=len(documents)):\n",
    "    for page_number, page in tqdm(enumerate(doc), total=len(doc)):\n",
    "        page_number = len(pages)\n",
    "        pages[page_number] = page.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826c83e4f4dbda7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:40:00.931923Z",
     "start_time": "2025-02-10T16:40:00.896512Z"
    }
   },
   "outputs": [],
   "source": [
    "for page_number, page in tqdm(pages.items(), total=len(documents)):\n",
    "    print(f\"{page_number}. {pages[page_number]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bb2c1ba808daed",
   "metadata": {},
   "source": [
    "## 2.4 Getting the metadata of each page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7753b375ca7e8f31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:40:03.757120Z",
     "start_time": "2025-02-10T16:40:03.738343Z"
    }
   },
   "outputs": [],
   "source": [
    "pages_and_metadata = list()\n",
    "for page_number, page in tqdm(pages.items(), total=len(pages)):\n",
    "    metadata = dict()\n",
    "    metadata['page_number'] = page_number\n",
    "    metadata['raw_text'] = page\n",
    "    metadata['number_of_characters'] = len(page)\n",
    "    metadata['number_of_tokens'] = len(page)/4\n",
    "    metadata['number_of_words'] = len(page.split())\n",
    "    pages_and_metadata.append(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf53efcf6db14a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:40:04.656908Z",
     "start_time": "2025-02-10T16:40:04.643066Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pages_and_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fda5bbeb07d871b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:40:06.939020Z",
     "start_time": "2025-02-10T16:40:06.915569Z"
    }
   },
   "outputs": [],
   "source": [
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6f59c1ce38342",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:40:08.516641Z",
     "start_time": "2025-02-10T16:40:08.511511Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"The number of tokens this model is being trained on are: {df[\"number_of_tokens\"].sum()/1000000:.2f} million tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea25bc7daf4f49bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:40:09.361819Z",
     "start_time": "2025-02-10T16:40:09.356518Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"The number of pages in the database are: {len(pages_and_metadata)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb4a2f984a78d18",
   "metadata": {},
   "source": [
    "## 2.4 Preprocessing the `raw_text` from metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8103461a031b8c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:41:53.870665Z",
     "start_time": "2025-02-10T16:41:53.856349Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_to_lowercase(text):\n",
    "    new_text = text.lower()\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522849a9bcea61aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:41:54.463014Z",
     "start_time": "2025-02-10T16:41:54.459723Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    new_text = []\n",
    "    for word in text.split():\n",
    "        if word not in STOP_WORDS:\n",
    "            new_text.append(word)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2f5226953fb7ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:41:54.848133Z",
     "start_time": "2025-02-10T16:41:54.842801Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "    new_text = re.sub(r\"<!--.*?-->\", \"\", text)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d03d45cc290e34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:41:55.205639Z",
     "start_time": "2025-02-10T16:41:55.202443Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_newlines(text):\n",
    "    new_text = re.sub(r\"\\n+\", \" \", text)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be43849bb32f99de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:41:55.666352Z",
     "start_time": "2025-02-10T16:41:55.662230Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_multiple_spaces(text):\n",
    "    new_text = text.replace(\"  \", \" \")\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d5cde9e9b9d9ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:41:56.144254Z",
     "start_time": "2025-02-10T16:41:56.139212Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_comments(text):\n",
    "    new_text = re.sub(r\"<!--.*?-->\", \"\", text)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed85a9aa-2a62-472e-8f54-b32283309103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unnecessary_text(text):\n",
    "    new_text = text.replace(\"answer:\",\"\").replace(\"question\", \"\").replace(\":\",\"\").replace(\"  \",\" \")\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce97b989463a7c01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:41:56.936299Z",
     "start_time": "2025-02-10T16:41:56.922968Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = convert_to_lowercase(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = remove_html_tags(text)\n",
    "    text = remove_newlines(text)\n",
    "    text = remove_multiple_spaces(text)\n",
    "    text = remove_comments(text)\n",
    "    text = remove_unnecessary_text(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2241967daa9316b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:41:57.858803Z",
     "start_time": "2025-02-10T16:41:57.814634Z"
    }
   },
   "outputs": [],
   "source": [
    "for page in tqdm(pages_and_metadata, total=len(pages_and_metadata)):\n",
    "    page[\"formatted_text\"] = preprocess_text(page[\"raw_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92a1fdb3f006122",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:41:59.743703Z",
     "start_time": "2025-02-10T16:41:59.720627Z"
    }
   },
   "outputs": [],
   "source": [
    "for page in tqdm(pages_and_metadata, total=len(pages_and_metadata)):\n",
    "    print(page[\"formatted_text\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d1c3bd529dadde",
   "metadata": {},
   "source": [
    "## 2.5 Converting the paragraphs to sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3247973ed0de93c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:42:03.889149Z",
     "start_time": "2025-02-10T16:42:03.168549Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = English()\n",
    "nlp.add_pipe('sentencizer')\n",
    "for page in tqdm(pages_and_metadata, total=len(pages_and_metadata)):\n",
    "    sentences = nlp(page[\"formatted_text\"]).sents\n",
    "    sentences = list(set([str(sentence).strip() for sentence in sentences if len(str(sentence).split())>10]))\n",
    "    pages_and_metadata[page[\"page_number\"]][\"sentences\"] = sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1e7e6e7b96e2e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:42:05.307900Z",
     "start_time": "2025-02-10T16:42:05.277901Z"
    }
   },
   "outputs": [],
   "source": [
    "for page in tqdm(pages_and_metadata, total=len(pages_and_metadata)):\n",
    "    print(page[\"sentences\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3244522b7b680f87",
   "metadata": {},
   "source": [
    "## 2.6 Update the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8943b0ff4b313750",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:42:07.860161Z",
     "start_time": "2025-02-10T16:42:07.846917Z"
    }
   },
   "outputs": [],
   "source": [
    "for page in tqdm(pages_and_metadata, total=len(pages_and_metadata)):\n",
    "    pages_and_metadata[page[\"page_number\"]][\"number_of_sentences\"] = len(page[\"sentences\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9813d9d02986948",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:42:08.918920Z",
     "start_time": "2025-02-10T16:42:08.912408Z"
    }
   },
   "outputs": [],
   "source": [
    "for key in pages_and_metadata[0].keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2d4bf7c69a5ea0",
   "metadata": {},
   "source": [
    "## 2.7 Converting sentences to sentence_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ed0b39d46cf55e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:42:11.689414Z",
     "start_time": "2025-02-10T16:42:11.669184Z"
    }
   },
   "outputs": [],
   "source": [
    "for page in tqdm(pages_and_metadata, total=len(pages_and_metadata)):\n",
    "    sentences = pages_and_metadata[page[\"page_number\"]][\"sentences\"]\n",
    "    sentence_chunk = [sentences[i : i+SENTENCE_CHUNKS] for i in range(0, len(sentences), SENTENCE_CHUNKS)]\n",
    "    page[\"sentence_chunk\"] = sentence_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b03d9f534a696c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:42:12.995127Z",
     "start_time": "2025-02-10T16:42:12.968943Z"
    }
   },
   "outputs": [],
   "source": [
    "for page in tqdm(pages_and_metadata, total=len(pages_and_metadata)):\n",
    "    print(page[\"sentence_chunk\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedc481bbe1a825b",
   "metadata": {},
   "source": [
    "## 2.8 Converting sentence_chunks into sentence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f450c90f591d29ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:42:25.651367Z",
     "start_time": "2025-02-10T16:42:21.601242Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer(model_name_or_path=EMBEDDING_MODEL).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8820fa929ef41f04",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-02-10T16:42:47.633183Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "for page in pages_and_metadata:\n",
    "    embeddings = list()\n",
    "    print(f\"Processing Page {page['page_number']}\")\n",
    "    for sentence in page[\"sentences\"]:\n",
    "        # sentence = sentence.to(device)\n",
    "        embedding = embedding_model.encode(sentence, batch_size=512, convert_to_tensor=True, show_progress_bar=False, device=device)\n",
    "        embedding = np.stack(embedding.tolist(), axis=0)\n",
    "        embedding = torch.tensor(embedding)\n",
    "        embedding = embedding.type(torch.float32)\n",
    "        embeddings.append(embedding)\n",
    "    sentence_embeddings = [np.array(embedding) for embedding in embeddings]\n",
    "    pages_and_metadata[page[\"page_number\"]][\"embeddings\"] = sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3567e35e0f369ed6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T03:59:45.693927Z",
     "start_time": "2025-02-11T03:59:45.300342Z"
    }
   },
   "outputs": [],
   "source": [
    "for page in tqdm(pages_and_metadata, total=len(pages_and_metadata)):\n",
    "    print(np.array(page[\"embeddings\"]).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8360bc8d64e45d79",
   "metadata": {},
   "source": [
    "## 2.9 Checking the metadata present for use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5023c654f155fd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in pages_and_metadata[0].keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bd19f71651b712",
   "metadata": {},
   "source": [
    "# 3. FETCHING SIMILAR CONTENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184fd946401c91fd",
   "metadata": {},
   "source": [
    "## 3.1 Getting the data embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c81dcf0f0426027",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_and_metadata_embeddings = []\n",
    "\n",
    "for page in tqdm(pages_and_metadata, total=len(pages_and_metadata)):\n",
    "    page_embeddings = []\n",
    "    for chunk_embedding in pages_and_metadata[page[\"page_number\"]][\"embeddings\"]:\n",
    "        if isinstance(chunk_embedding, torch.Tensor):\n",
    "            chunk_embedding = chunk_embedding.tolist()\n",
    "        page_embeddings.append(chunk_embedding)\n",
    "    pages_and_metadata_embeddings.append(page_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78792dfaf1127f6",
   "metadata": {},
   "source": [
    "## 3.2 Converting each embedding into the same dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59618f1fd119517a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pages_and_metadata_embeddings:\n",
    "    embedding_dim = len(pages_and_metadata_embeddings[0][0])\n",
    "    pages_and_metadata_embeddings = [\n",
    "            [np.pad(chunk, (0, max(0, embedding_dim - len(chunk))), mode='constant')[:embedding_dim]\n",
    "             for chunk in page]\n",
    "            for page in pages_and_metadata_embeddings\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2300c8743f62a23a",
   "metadata": {},
   "source": [
    "## 3.3 Flattening the nested list of embeddings and the sentence to fetch by index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e07bdbecbd85c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_embeddings = [chunk for page in pages_and_metadata_embeddings for chunk in page]\n",
    "flat_data = [sentence for page in pages_and_metadata for sentence in page[\"sentences\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9a9684b1287496",
   "metadata": {},
   "source": [
    "## 3.4 Saving the flattened embeddings and the flattened data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4533ad448b48e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(flat_embeddings)\n",
    "df.to_csv(\"embeddings_v4.csv\", index=False)\n",
    "\n",
    "df = pd.DataFrame(flat_data)\n",
    "df.to_csv(\"data_v4.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9c5f925a05048b",
   "metadata": {},
   "source": [
    "## 3.5 Loading the flattened embeddings and flattened data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80f723dee61c4c4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:38:13.460239Z",
     "start_time": "2025-02-09T11:38:13.128443Z"
    }
   },
   "outputs": [],
   "source": [
    "flat_embeddings = pd.read_csv(\"embeddings_v4.csv\").to_numpy()\n",
    "flat_data = pd.read_csv(\"data_v4.csv\")[\"0\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d04b8ed2-21c5-4bd2-92e2-8d016e4c7079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the flat_embeddings is 39293\n",
      "The size of the flat_data is 39293\n"
     ]
    }
   ],
   "source": [
    "print(f\"The size of the flat_embeddings is {len(flat_embeddings)}\")\n",
    "print(f\"The size of the flat_data is {len(flat_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd66e23aff7a901",
   "metadata": {},
   "source": [
    "## 3.6 Converting embeddings to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3f43ce0368e6c95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:38:14.661823Z",
     "start_time": "2025-02-09T11:38:14.644537Z"
    }
   },
   "outputs": [],
   "source": [
    "pages_and_metadata_embeddings = np.array(flat_embeddings, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61118042f2f3a0c1",
   "metadata": {},
   "source": [
    "## 3.7 Converting the numpy array embeddings to torch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "476ae4b96e92b843",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:38:16.402973Z",
     "start_time": "2025-02-09T11:38:15.539250Z"
    }
   },
   "outputs": [],
   "source": [
    "pages_and_metadata_embeddings = torch.tensor(pages_and_metadata_embeddings, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7691ac5f8be5636",
   "metadata": {},
   "source": [
    "## 3.8 Getting the similarity score by query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6745dc59ef034840",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:38:24.424800Z",
     "start_time": "2025-02-09T11:38:17.617912Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "query_embeddings = embedding_model.encode(QUERY, convert_to_tensor=True).to(device)\n",
    "dot_score = util.dot_score(query_embeddings, pages_and_metadata_embeddings)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bea1268563c3129",
   "metadata": {},
   "source": [
    "## 3.9 Getting the top k similar scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dea591cca2af654",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:38:27.505935Z",
     "start_time": "2025-02-09T11:38:27.441640Z"
    }
   },
   "outputs": [],
   "source": [
    "top_scores, top_indices = torch.topk(dot_score, k=K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87a0a2111e42c0cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:38:28.290866Z",
     "start_time": "2025-02-09T11:38:28.283472Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top scores: tensor([0.7010, 0.6388, 0.6295, 0.6056, 0.6056, 0.6056, 0.5908, 0.5883, 0.5883,\n",
      "        0.5865, 0.5865, 0.5824, 0.5809, 0.5809, 0.5684, 0.5684, 0.5669, 0.5636,\n",
      "        0.5628, 0.5588, 0.5574, 0.5574, 0.5533, 0.5527, 0.5523, 0.5522, 0.5515,\n",
      "        0.5514, 0.5509, 0.5448, 0.5448, 0.5445, 0.5439, 0.5436, 0.5414, 0.5392,\n",
      "        0.5389, 0.5348, 0.5346, 0.5327, 0.5323, 0.5305, 0.5295, 0.5272, 0.5267,\n",
      "        0.5264, 0.5262, 0.5258, 0.5257, 0.5257], device='cuda:0')\n",
      "Top indices: tensor([ 1444,  3262, 28329, 38917, 28202, 34307, 38980,   669, 35475,  1407,\n",
      "        28328,  1557, 28198, 34303,  7270, 13283,  4531, 38879, 12143, 27897,\n",
      "         7214, 13227, 27921, 28136,  5486, 28000,  8058,  7032,  3345,   641,\n",
      "        35447, 38156,  2654, 37872, 38978,  1560,  1418, 34178, 28609, 12096,\n",
      "        38882, 36657, 32895, 34766, 21797,  7002, 32897,  6190, 37852, 15709],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(f\"Top scores: {top_scores}\")\n",
    "print(f\"Top indices: {top_indices}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a93ccde99a8e5fd",
   "metadata": {},
   "source": [
    "## 3.10 Getting the top k content based on the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "412b8aad1ac4bc28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:38:31.127772Z",
     "start_time": "2025-02-09T11:38:31.124080Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data from page 1444\n",
      "Fetching data from page 3262\n",
      "Fetching data from page 28329\n",
      "Fetching data from page 38917\n",
      "Fetching data from page 28202\n",
      "Fetching data from page 34307\n",
      "Fetching data from page 38980\n",
      "Fetching data from page 669\n",
      "Fetching data from page 35475\n",
      "Fetching data from page 1407\n",
      "Fetching data from page 28328\n",
      "Fetching data from page 1557\n",
      "Fetching data from page 28198\n",
      "Fetching data from page 34303\n",
      "Fetching data from page 7270\n",
      "Fetching data from page 13283\n",
      "Fetching data from page 4531\n",
      "Fetching data from page 38879\n",
      "Fetching data from page 12143\n",
      "Fetching data from page 27897\n",
      "Fetching data from page 7214\n",
      "Fetching data from page 13227\n",
      "Fetching data from page 27921\n",
      "Fetching data from page 28136\n",
      "Fetching data from page 5486\n",
      "Fetching data from page 28000\n",
      "Fetching data from page 8058\n",
      "Fetching data from page 7032\n",
      "Fetching data from page 3345\n",
      "Fetching data from page 641\n",
      "Fetching data from page 35447\n",
      "Fetching data from page 38156\n",
      "Fetching data from page 2654\n",
      "Fetching data from page 37872\n",
      "Fetching data from page 38978\n",
      "Fetching data from page 1560\n",
      "Fetching data from page 1418\n",
      "Fetching data from page 34178\n",
      "Fetching data from page 28609\n",
      "Fetching data from page 12096\n",
      "Fetching data from page 38882\n",
      "Fetching data from page 36657\n",
      "Fetching data from page 32895\n",
      "Fetching data from page 34766\n",
      "Fetching data from page 21797\n",
      "Fetching data from page 7002\n",
      "Fetching data from page 32897\n",
      "Fetching data from page 6190\n",
      "Fetching data from page 37852\n",
      "Fetching data from page 15709\n"
     ]
    }
   ],
   "source": [
    "context = list()\n",
    "for index in top_indices:\n",
    "    print(f\"Fetching data from page {index}\")\n",
    "    context.append(flat_data[index.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "777d6213b100c8ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:38:31.789353Z",
     "start_time": "2025-02-09T11:38:31.783885Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['overfitting comes light data associated complexity, means associated parameters relative number observations.', 'main reasons long time understood, introduce concepts terms utilized extensively machine learning topics.', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x overfitting, statistical model describes random error noise instead underlying relationship.', 'simple restatement fundamental problem machine learning possibility overfitting training data carrying noise data test set, providing inaccurate generalizations.', 'lot data overﬁtting avoided, overﬁtting happens relatively small dataset, try learn it.', 'lot data overﬁtting avoided, overﬁtting happens relatively small dataset, try learn it.', 'decision trees prone overfitting, pruning tree helps reduce size minimizes chances overfitting.', 'overfitting evaluation 83 (from weiss, s., kulikowski, c., computer systems learn, morgan kaufmann, 1991) training errors validation errors 1 2 3 4 5 6 7 8 9 0.2 0.4 0.6 0.8 1.0 0 0 error rate number terminal nodes iris data decision tree figure 6.8 determining overﬁtting begins stopping growth decision tree, grow size prune away leaf nodes ancestors cross- validation accuracy longer increases.', 'overfitting evaluation 83 (from weiss, s., kulikowski, c., computer systems learn, morgan kaufmann, 1991) training errors validation errors 1 2 3 4 5 6 7 8 9 0.2 0.4 0.6 0.8 1.0 0 0 error rate number terminal nodes iris data decision tree figure 6.8 determining overﬁtting begins stopping growth decision tree, grow size prune away leaf nodes ancestors cross- validation accuracy longer increases.', 'underfitting occurs statistical model machine learning algorithm capture underlying trend data.', 'underfitting occurs statistical model machine learning algorithm capture underlying trend data.', 'poor predictive performance – overfitting underfitting yield poor predictive performance, way different.', 'machine learning, statistical model describes random error noise instead underlying relationship ‘overﬁtting’ occurs.', 'machine learning, statistical model describes random error noise instead underlying relationship ‘overﬁtting’ occurs.', 'quora) simple restatement fundamental problem machine learning possibility overfitting training data carrying noise data test set, providing inaccu- rate generalizations.', 'quora) simple restatement fundamental problem machine learning possibility overfitting training data carrying noise data test set, providing inaccu- rate generalizations.', 'basics machine learning (a) large underfitting error (b) large overfitting error underfitting error (c) best trade-off over- target curve noise approximated curve training examples (with noise) figure 2.13 trade-off underﬁtting overﬁtting shown.', 'overfitting situation occurs model learns training set well, taking random fluctuations training data concepts.', 'i’ve mentioned times, overﬁtting major problem neural networks, especially computers powerful, ability train larger networks.', 'principles effects machine learning previous sections, definition machine learning optimization categorization mentioned, section,', 'reading bias-variance tradeoff (wikipedia) bias error erroneous overly simplistic assumptions learning algorithm you’re using.', 'reading bias-variance tradeoff (wikipedia) bias error erroneous overly simplistic assumptions learning algorithm you’re using.', 'given training set, too-easy model learning, ( ) e g ( ) e g high, called “under-fitting”.', 'conclusion tutorial, broad overview machine learning containing theoretical practical aspects presented.', 'overﬁtting (high variance model selection) occurs, ﬁtting selected model noise training data.', 'reason complicated hypothesis model higher ability fit training data noise, penalty term hypothesis complexity guide learning algorithm find final hypothesis g possibility over-fit noise.', '¯enew ¯etrain generalizarion error overﬁt underﬁt model complexity error figure 5.3 behavior ¯etrain ¯enew supervised machine learning methods, function model complexity.', 'consequence, strict minimization training error, example, counter-productive lead overtraining overﬁtting eﬀects.', 'concept introducing, penalizing large weights, example what’s known regularization, it’s come diﬀerent machine learning methods.', '6.4 overﬁtting evaluation 6.4.1 overﬁtting supervised learning, choose function ﬁt training set set hypotheses.', '6.4 overﬁtting evaluation 6.4.1 overﬁtting supervised learning, choose function ﬁt training set set hypotheses.', 'illustrates summarizes important issue machine learning lines opening quote chapter “accuracy enough.”', 'notably, model’s over-fit available data extremely detrimental system, underlying model better data immediately available it.', 'bias variance, over- underfitting 159 model b target ﬁts model model parameter 2 adaptive parameter 1 prediction error test set performance ←high bias high variance → training set performance measure model complexity figure 7.2 left panel illustration bias-variance dilemma.', 'goals model training identify signal ignore noise model given free rein minimize error, possibility suffering overfitting.', 'validation set training set parameter selection avoiding overfitting machine learning model developed.', 'prediction rate provides low prediction training error test error leads high business problem, error rate training set high error rate test set high, conclude overfitting model.', 'believe need better explain non-experts types problems formulated machine learning problems.', 'cover wide variety important topics machine learning, chosen omit important ones, including graphical models neural networks, sake brevity current lack solid theoretical guarantees methods.', 'overﬁtting regularization 89 particular, suppose training input x corresponding desired output y. ordi- narily, we’d train forward-propagating x network, backpropagating determine contribution gradient.', 'predictive models tradeoff bias (how model fits data) variance (how model changes based changes inputs).', '◦it serves prototypical model system provides theoretical, mathe- matical intuitive insights basic mechanisms machine learn- ing.', 'explanation usually relates feature values instance model prediction humanly understandable way.', 'terms machine learning, algorithm aims understand example particular concept generalizations form concepts training examples.', 'additionally, we’ll discuss biases perpetuated machine learning algorithms, consider kept mind prevent biases building algorithms.', 'evaluation validation prediction error test set performance ←high bias high variance → training set performance model complexity, e.g. k figure 6.2 schematic illustration underﬁtting overﬁtting (after [17]) ex- pected error respect test set (generalization error) training set per- formance function model complexity.', 'translucency describes explanation method relies looking machine learning model, like parameters.', 'perceptron discussed great detail main chapter provides valuable insights basic concepts machine learning.', 'bias variance, over- underfitting 157 represent function f(x) course unknown learning system.', 'therefore, dealing big data, caution taken bias introduced, compromising ability generalize.']\n"
     ]
    }
   ],
   "source": [
    "print(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8877aa473dfa93b2",
   "metadata": {},
   "source": [
    "# 4. Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8fe840bd679626",
   "metadata": {},
   "source": [
    "## 4.1 Login to HuggingFace CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7d432027efb7be0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:38:36.750390Z",
     "start_time": "2025-02-09T11:38:36.738802Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5354caf510c45579308e169dc814a34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75492f83d44a73d",
   "metadata": {},
   "source": [
    "## 4.2 Loading the LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e064d9014c81b7b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:38:47.266956Z",
     "start_time": "2025-02-09T11:38:37.962878Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4a6a7724e9540f399c1cfa89a553f2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=LLM_MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=False,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d6e6ed3c6d3949",
   "metadata": {},
   "source": [
    "## 4.3 Augmenting the prompt for instructing the LLM in a better way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1042c149cef8b04a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:38:48.422916Z",
     "start_time": "2025-02-09T11:38:47.273510Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL)\n",
    "context = \"\\n -\".join(context)\n",
    "base_prompt = f'''Bases on the following context items, please answer the query\n",
    "Context Items:\n",
    "{context}\n",
    "Query:\n",
    "{QUERY}\n",
    "Answer:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9329f053bc5df59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:38:51.178431Z",
     "start_time": "2025-02-09T11:38:51.173961Z"
    }
   },
   "outputs": [],
   "source": [
    "base_prompt = base_prompt.format(context=context, query=QUERY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ae612c14837c95",
   "metadata": {},
   "source": [
    "## 4.4 Creating the dialogue template for the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6eadc67098c5cc4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:38:55.854374Z",
     "start_time": "2025-02-09T11:38:55.847614Z"
    }
   },
   "outputs": [],
   "source": [
    "dialogue_template = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": base_prompt,\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafb0016557c0272",
   "metadata": {},
   "source": [
    "## 4.5 Applying the prompt to the dialogue template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "277dde89685a2245",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:38:57.127131Z",
     "start_time": "2025-02-09T11:38:57.105583Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
    "                                       tokenize=False,\n",
    "                                       add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c2a05e96737f1",
   "metadata": {},
   "source": [
    "## 4.6 Providing the prompt and retrieving the answer from the LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b57262112415464",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:42:29.646183Z",
     "start_time": "2025-02-09T11:42:27.227166Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL)\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**input_ids, temperature=TEMPERATURE, do_sample=True, max_new_tokens=MAX_NEW_TOKENS)\n",
    "output_text = tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7816da98e6a7a3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:42:29.658075Z",
     "start_time": "2025-02-09T11:42:29.653897Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><bos><start_of_turn>user\n",
      "Bases on the following context items, please answer the query\n",
      "Context Items:\n",
      "overfitting comes light data associated complexity, means associated parameters relative number observations.\n",
      " -main reasons long time understood, introduce concepts terms utilized extensively machine learning topics.\n",
      " -follow steve nouri ai data science posts https//lnkd.in/gzu463x overfitting, statistical model describes random error noise instead underlying relationship.\n",
      " -simple restatement fundamental problem machine learning possibility overfitting training data carrying noise data test set, providing inaccurate generalizations.\n",
      " -lot data overﬁtting avoided, overﬁtting happens relatively small dataset, try learn it.\n",
      " -lot data overﬁtting avoided, overﬁtting happens relatively small dataset, try learn it.\n",
      " -decision trees prone overfitting, pruning tree helps reduce size minimizes chances overfitting.\n",
      " -overfitting evaluation 83 (from weiss, s., kulikowski, c., computer systems learn, morgan kaufmann, 1991) training errors validation errors 1 2 3 4 5 6 7 8 9 0.2 0.4 0.6 0.8 1.0 0 0 error rate number terminal nodes iris data decision tree figure 6.8 determining overﬁtting begins stopping growth decision tree, grow size prune away leaf nodes ancestors cross- validation accuracy longer increases.\n",
      " -overfitting evaluation 83 (from weiss, s., kulikowski, c., computer systems learn, morgan kaufmann, 1991) training errors validation errors 1 2 3 4 5 6 7 8 9 0.2 0.4 0.6 0.8 1.0 0 0 error rate number terminal nodes iris data decision tree figure 6.8 determining overﬁtting begins stopping growth decision tree, grow size prune away leaf nodes ancestors cross- validation accuracy longer increases.\n",
      " -underfitting occurs statistical model machine learning algorithm capture underlying trend data.\n",
      " -underfitting occurs statistical model machine learning algorithm capture underlying trend data.\n",
      " -poor predictive performance – overfitting underfitting yield poor predictive performance, way different.\n",
      " -machine learning, statistical model describes random error noise instead underlying relationship ‘overﬁtting’ occurs.\n",
      " -machine learning, statistical model describes random error noise instead underlying relationship ‘overﬁtting’ occurs.\n",
      " -quora) simple restatement fundamental problem machine learning possibility overfitting training data carrying noise data test set, providing inaccu- rate generalizations.\n",
      " -quora) simple restatement fundamental problem machine learning possibility overfitting training data carrying noise data test set, providing inaccu- rate generalizations.\n",
      " -basics machine learning (a) large underfitting error (b) large overfitting error underfitting error (c) best trade-off over- target curve noise approximated curve training examples (with noise) figure 2.13 trade-off underﬁtting overﬁtting shown.\n",
      " -overfitting situation occurs model learns training set well, taking random fluctuations training data concepts.\n",
      " -i’ve mentioned times, overﬁtting major problem neural networks, especially computers powerful, ability train larger networks.\n",
      " -principles effects machine learning previous sections, definition machine learning optimization categorization mentioned, section,\n",
      " -reading bias-variance tradeoff (wikipedia) bias error erroneous overly simplistic assumptions learning algorithm you’re using.\n",
      " -reading bias-variance tradeoff (wikipedia) bias error erroneous overly simplistic assumptions learning algorithm you’re using.\n",
      " -given training set, too-easy model learning, ( ) e g ( ) e g high, called “under-fitting”.\n",
      " -conclusion tutorial, broad overview machine learning containing theoretical practical aspects presented.\n",
      " -overﬁtting (high variance model selection) occurs, ﬁtting selected model noise training data.\n",
      " -reason complicated hypothesis model higher ability fit training data noise, penalty term hypothesis complexity guide learning algorithm find final hypothesis g possibility over-fit noise.\n",
      " -¯enew ¯etrain generalizarion error overﬁt underﬁt model complexity error figure 5.3 behavior ¯etrain ¯enew supervised machine learning methods, function model complexity.\n",
      " -consequence, strict minimization training error, example, counter-productive lead overtraining overﬁtting eﬀects.\n",
      " -concept introducing, penalizing large weights, example what’s known regularization, it’s come diﬀerent machine learning methods.\n",
      " -6.4 overﬁtting evaluation 6.4.1 overﬁtting supervised learning, choose function ﬁt training set set hypotheses.\n",
      " -6.4 overﬁtting evaluation 6.4.1 overﬁtting supervised learning, choose function ﬁt training set set hypotheses.\n",
      " -illustrates summarizes important issue machine learning lines opening quote chapter “accuracy enough.”\n",
      " -notably, model’s over-fit available data extremely detrimental system, underlying model better data immediately available it.\n",
      " -bias variance, over- underfitting 159 model b target ﬁts model model parameter 2 adaptive parameter 1 prediction error test set performance ←high bias high variance → training set performance measure model complexity figure 7.2 left panel illustration bias-variance dilemma.\n",
      " -goals model training identify signal ignore noise model given free rein minimize error, possibility suffering overfitting.\n",
      " -validation set training set parameter selection avoiding overfitting machine learning model developed.\n",
      " -prediction rate provides low prediction training error test error leads high business problem, error rate training set high error rate test set high, conclude overfitting model.\n",
      " -believe need better explain non-experts types problems formulated machine learning problems.\n",
      " -cover wide variety important topics machine learning, chosen omit important ones, including graphical models neural networks, sake brevity current lack solid theoretical guarantees methods.\n",
      " -overﬁtting regularization 89 particular, suppose training input x corresponding desired output y. ordi- narily, we’d train forward-propagating x network, backpropagating determine contribution gradient.\n",
      " -predictive models tradeoff bias (how model fits data) variance (how model changes based changes inputs).\n",
      " -◦it serves prototypical model system provides theoretical, mathe- matical intuitive insights basic mechanisms machine learn- ing.\n",
      " -explanation usually relates feature values instance model prediction humanly understandable way.\n",
      " -terms machine learning, algorithm aims understand example particular concept generalizations form concepts training examples.\n",
      " -additionally, we’ll discuss biases perpetuated machine learning algorithms, consider kept mind prevent biases building algorithms.\n",
      " -evaluation validation prediction error test set performance ←high bias high variance → training set performance model complexity, e.g. k figure 6.2 schematic illustration underﬁtting overﬁtting (after [17]) ex- pected error respect test set (generalization error) training set per- formance function model complexity.\n",
      " -translucency describes explanation method relies looking machine learning model, like parameters.\n",
      " -perceptron discussed great detail main chapter provides valuable insights basic concepts machine learning.\n",
      " -bias variance, over- underfitting 157 represent function f(x) course unknown learning system.\n",
      " -therefore, dealing big data, caution taken bias introduced, compromising ability generalize.\n",
      "Query:\n",
      "What is overfitting in machine learning? Explain in 500 words\n",
      "Answer:<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Sure, here's a 500-word explanation of overfitting in machine learning:\n",
      "\n",
      "Overfitting is a major problem in machine learning (ML) that occurs when a model becomes too closely fit to the training data and fails to generalize well to new, unseen data. This can result in poor predictive performance, even on the same data that the model was trained on.\n",
      "\n",
      "**Key characteristics of overfitting:**\n",
      "\n",
      "* The model performs well on the training data but performs poorly on unseen data.\n",
      "* The model becomes too complex and captures irrelevant features, leading to overfitting.\n",
      "* The model's predictions become too specific to the training data and fail to capture the underlying patterns in the data.\n",
      "* The model becomes insensitive to changes in the data, resulting in poor performance on unseen data.\n",
      "\n",
      "**Causes of overfitting:**\n",
      "\n",
      "* **High training data quality:** Overfitting can be exacerbated when the training data is of high quality, as it provides the model with more data points to learn from.\n",
      "* **Complex models:** Models with many parameters are more likely to overfit.\n",
      "* **High variance in the training data:** Models with high variance in the training data are more likely to overfit.\n",
      "* **Non-linear relationships:** Overfitting can occur when the underlying relationship between the features and target variable is complex.\n",
      "\n",
      "**Consequences of overfitting:**\n",
      "\n",
      "* Poor predictive performance on unseen data.\n",
      "* Increased variance in the model's predictions.\n",
      "* Difficulty in interpreting the model's results.\n",
      "\n",
      "**Strategies for preventing overfitting:**\n",
      "\n",
      "* **Data sampling:** Selecting a representative subset of the training data for training.\n",
      "* **Regularization:** Penalizing model complexity to prevent overfitting.\n",
      "* **Early stopping:** Stopping the training process when the model starts overfitting.\n",
      "* **Cross-validation:** Using cross-validation to evaluate the model's performance on unseen data.\n",
      "* **Feature selection:** Identifying and selecting the most relevant features for the task.\n",
      "\n",
      "**Consequences of underfitting:**\n",
      "\n",
      "* Poor predictive performance on the training data.\n",
      "* High bias in the model's predictions.\n",
      "* Difficulty in interpreting the model's results.\n",
      "\n",
      "**Comparison:**\n",
      "\n",
      "* Underfitting occurs when the model is too simple and cannot capture the underlying patterns in the data.\n",
      "* Overfitting occurs when the model is too complex and captures irrelevant features, leading to overfitting.\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "Overfitting is a major challenge in machine learning that can significantly impact model performance\n"
     ]
    }
   ],
   "source": [
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b739bcea21ecd35a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:42:29.775358Z",
     "start_time": "2025-02-09T11:42:29.772028Z"
    }
   },
   "outputs": [],
   "source": [
    "idx = output_text.find(\"Answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8a743149d967b4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:42:29.840167Z",
     "start_time": "2025-02-09T11:42:29.836521Z"
    }
   },
   "outputs": [],
   "source": [
    "answer = output_text[idx+7:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3acff9162acb0b8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:42:29.974904Z",
     "start_time": "2025-02-09T11:42:29.970946Z"
    }
   },
   "outputs": [],
   "source": [
    "answer = answer.replace(\"**\", \"\")\n",
    "answer = answer.replace(\"<start_of_turn>model\",\"\")\n",
    "answer = re.sub(\"<.*?>\", \"\", answer)\n",
    "# answer = answer[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea26bd933ddca3e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:42:37.796573Z",
     "start_time": "2025-02-09T11:42:37.787565Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cleaned answer is: \n",
      "\n",
      "Sure, here's a 500-word explanation of overfitting in machine learning:\n",
      "\n",
      "Overfitting is a major problem in machine learning (ML) that occurs when a model becomes too closely fit to the training data and fails to generalize well to new, unseen data. This can result in poor predictive performance, even on the same data that the model was trained on.\n",
      "\n",
      "Key characteristics of overfitting:\n",
      "\n",
      "* The model performs well on the training data but performs poorly on unseen data.\n",
      "* The model becomes too complex and captures irrelevant features, leading to overfitting.\n",
      "* The model's predictions become too specific to the training data and fail to capture the underlying patterns in the data.\n",
      "* The model becomes insensitive to changes in the data, resulting in poor performance on unseen data.\n",
      "\n",
      "Causes of overfitting:\n",
      "\n",
      "* High training data quality: Overfitting can be exacerbated when the training data is of high quality, as it provides the model with more data points to learn from.\n",
      "* Complex models: Models with many parameters are more likely to overfit.\n",
      "* High variance in the training data: Models with high variance in the training data are more likely to overfit.\n",
      "* Non-linear relationships: Overfitting can occur when the underlying relationship between the features and target variable is complex.\n",
      "\n",
      "Consequences of overfitting:\n",
      "\n",
      "* Poor predictive performance on unseen data.\n",
      "* Increased variance in the model's predictions.\n",
      "* Difficulty in interpreting the model's results.\n",
      "\n",
      "Strategies for preventing overfitting:\n",
      "\n",
      "* Data sampling: Selecting a representative subset of the training data for training.\n",
      "* Regularization: Penalizing model complexity to prevent overfitting.\n",
      "* Early stopping: Stopping the training process when the model starts overfitting.\n",
      "* Cross-validation: Using cross-validation to evaluate the model's performance on unseen data.\n",
      "* Feature selection: Identifying and selecting the most relevant features for the task.\n",
      "\n",
      "Consequences of underfitting:\n",
      "\n",
      "* Poor predictive performance on the training data.\n",
      "* High bias in the model's predictions.\n",
      "* Difficulty in interpreting the model's results.\n",
      "\n",
      "Comparison:\n",
      "\n",
      "* Underfitting occurs when the model is too simple and cannot capture the underlying patterns in the data.\n",
      "* Overfitting occurs when the model is too complex and captures irrelevant features, leading to overfitting.\n",
      "\n",
      "Conclusion:\n",
      "\n",
      "Overfitting is a major challenge in machine learning that can significantly impact model performance\n"
     ]
    }
   ],
   "source": [
    "print(f\"The cleaned answer is: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687ee293-bfa9-4715-a7d7-77644fec5bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
