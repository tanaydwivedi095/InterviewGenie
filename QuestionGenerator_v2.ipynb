{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "eb050f3d-0c9f-4099-b0b0-ec36a67e3f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: Explain the concept of overfitting in machine learning algorithms and discuss methods to mitigate it in high-dimensional datasets.\n",
      "Question 2: Derive the formula for the gradient descent optimization algorithm and explain the significance of the learning rate in the context of training machine learning models.\n",
      "Question 3: What is the definition of the bias-variance tradeoff in machine learning, and how does it influence model selection and evaluation?\n",
      "Question 4: Describe a real-world scenario where a recommendation system using collaborative filtering can significantly enhance user experience. Discuss the underlying machine learning principles that facilitate this process.\n",
      "Question 5: Given a dataset containing 1000 samples with 10 features, calculate the dimensionality of the feature space and discuss the implications of high dimensionality on model training and performance.\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "import torch\n",
    "from openai import OpenAI\n",
    "\n",
    "def load_llm_model(openai_api_key):\n",
    "    llm_model = OpenAI(api_key=openai_api_key)\n",
    "    return llm_model\n",
    "    \n",
    "def generate_prompt(difficulty):\n",
    "    question_type = {\n",
    "        0: \"definition\",\n",
    "        1: \"formula\",\n",
    "        2: \"scenario\",\n",
    "        3: \"case-study\",\n",
    "        4: \"algorithmic theory\",\n",
    "        5: \"mathematics\",\n",
    "        6: \"probability and statistics\",\n",
    "        7: \"real life application\",\n",
    "    }\n",
    "    question_types = list()\n",
    "    number_of_questions = 5\n",
    "    for i in range(number_of_questions):\n",
    "        random_idx = random.randint(0, len(question_type)-1)\n",
    "        question_types.append(f\"{i+1}. {question_type[random_idx]}\")\n",
    "    question_types = \"\\n\".join(question_types)\n",
    "    base_prompt = f\"\"\"\n",
    "    Generate {number_of_questions} questions for an Interview for the position of Machine Learning Engineer.\n",
    "    Each question should be of type as mentioned below:\n",
    "    {question_types}\n",
    "    The output provided should be in format as given below:\n",
    "    Question 1: Question Type 1 ....\n",
    "    Question 2: Question Type 2 ....\n",
    "    Question 3: Question Type 3 .....\n",
    "    and so on.\n",
    "    No extra text should be generated as answer.\n",
    "    The case study questions should be well defined.\n",
    "    The questions should only be of the specified type.\n",
    "    Do not mention the type of question in the generated question.\n",
    "    Make the questions hard on a scale of {difficulty} out of 10.\n",
    "    \"\"\"\n",
    "    \n",
    "    base_prompt = base_prompt.format(question_types = question_types, \n",
    "                                     number_of_questions = number_of_questions,\n",
    "                                     difficulty=difficulty)\n",
    "    dialogue_template = [{\n",
    "        \"role\":\"user\",\n",
    "        \"content\":base_prompt\n",
    "    }]\n",
    "    return dialogue_template\n",
    "\n",
    "def generate_question(llm_model, prompt):\n",
    "    questions = llm_model.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        store=False,\n",
    "        messages=prompt\n",
    "    )\n",
    "    return questions\n",
    "    \n",
    "def post_process_questions(questions):\n",
    "    questions = questions.choices[0].message.content.split(\"Question\")\n",
    "    questions = [f\"{question.strip().replace('\\n', \"\")[2:]}\" for question in questions if question]\n",
    "    return questions\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    openai_api_key = \"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\"\n",
    "    llm_model = load_llm_model(openai_api_key)\n",
    "    difficulty = 10\n",
    "    prompt = generate_prompt(difficulty)\n",
    "    questions = generate_question(llm_model=llm_model, prompt=prompt)\n",
    "    questionLst = post_process_questions(questions)\n",
    "    for question in questionLst:\n",
    "        print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265b664c-2e56-47d4-b38f-4d341203bef1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
