{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f2d85b35e1e945f",
   "metadata": {},
   "source": [
    "# 1. IMPORTING LIBRARIES, FUNCTION AND DEFINING GLOBAL VARIABLES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3e95c360721358",
   "metadata": {},
   "source": [
    "## 1.1 Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1be9ae60d3bc106",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:38:29.057193Z",
     "start_time": "2025-02-10T16:38:24.848738Z"
    }
   },
   "outputs": [],
   "source": [
    "# Library needed to load the pdf\n",
    "import fitz\n",
    "\n",
    "# Library needed to process the text using regular expressions\n",
    "import re\n",
    "\n",
    "# Library needed to display or process the data in forms of dataframes\n",
    "import pandas as pd\n",
    "\n",
    "# Library needed to handle the operations in deep learning\n",
    "import torch\n",
    "\n",
    "# Library needed to convert the data into arrays for faster processing\n",
    "import numpy as np\n",
    "\n",
    "# Library to handle operating system related operations\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b18b422d1d839a",
   "metadata": {},
   "source": [
    "## 1.2 Importing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d138f8cdda61885",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:39:37.411274Z",
     "start_time": "2025-02-10T16:39:32.176313Z"
    }
   },
   "outputs": [],
   "source": [
    "# (OPTIONAL) Function to beautify the waiting process with a loading bar\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "\n",
    "# Function to process the text in English\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# Function to convert paragraphs to sentences\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Function to provide utility services to process the text such as tokenization, sentencizer\n",
    "from sentence_transformers import util\n",
    "\n",
    "# Functions for loading the LLM model\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# Function for fetching the paths to pdfs\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d989d11d53db12e8",
   "metadata": {},
   "source": [
    "## 1.3 Defining Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5d9391505ec14fa5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:39:42.151208Z",
     "start_time": "2025-02-10T16:39:42.144311Z"
    }
   },
   "outputs": [],
   "source": [
    "# Global variable consisting of all the stop words\n",
    "from spacy.lang.en import STOP_WORDS\n",
    "\n",
    "# Global variable telling about the number of sentences in each chunk stored in the dictionary\n",
    "SENTENCE_CHUNKS = 10\n",
    "\n",
    "# Global variable storing the name of the model that is used for the embedding\n",
    "EMBEDDING_MODEL = 'all-mpnet-base-v2'\n",
    "\n",
    "# Global variable storing the names of the pdfs that are to be loaded to be fed into the RAG model\n",
    "PDF_PATHS = list()\n",
    "\n",
    "# Global variable storing the integer telling to fetch the top k similar records for further processing\n",
    "K = 50\n",
    "\n",
    "# Global variable storing the name of the LLM model that will be used for augmenting the similar data\n",
    "LLM_MODEL = 'google/gemma-2b-it'\n",
    "\n",
    "# (FOR TESTING) Global variable storing the query that user wants to ask\n",
    "QUERY = \"What is overfitting in machine learning? Explain in 200 words\"\n",
    "\n",
    "# Setting up the device agnostic code\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Global variable for setting the temperature of the LLM model i.e how much data should LLM generate\n",
    "TEMPERATURE = 0.3\n",
    "\n",
    "## Global variable defining the length of tokens that the LLM has to generate\n",
    "MAX_NEW_TOKENS = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd579bf352b46f64",
   "metadata": {},
   "source": [
    "# 2. DATA ACQUISITION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ac37ccdc34c608",
   "metadata": {},
   "source": [
    "## 2.1 Getting the paths to all the pdfs in the `Dataset` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "144a542f97a7fd0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:39:44.681217Z",
     "start_time": "2025-02-10T16:39:44.671241Z"
    }
   },
   "outputs": [],
   "source": [
    "PDF_PATHS = glob('.\\\\Dataset\\\\*.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26c7124cd29610ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:39:46.384156Z",
     "start_time": "2025-02-10T16:39:46.366334Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d866dccd319c4dea995c3ea3eb1dc1b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 120_24pgs_mlinterviewquestions\n",
      "2. 4.41_faq_dl_8pgs\n",
      "3. 40 Artificial Intelligence Interview Questions & Answers \n",
      "4. 41 Essential Machine Learning Interview Questions\n",
      "5. 5.AI_40Questions\n",
      "6. 6.45_ml_important_questions_21pgs\n",
      "7. 7.beginner_nn_deep_learning\n",
      "8. 8.Data Science Interview Questions\n",
      "9. 9.interview_preparation_50\n",
      "10. bagging_ensemblelearning\n",
      "11. Data Science Interview Questions\n",
      "12. Deep Learning Interview Questions\n",
      "13. Interview Questions about Python Programming\n",
      "14. Machine Learning Interview\n",
      "15. ML FINAL (1)\n",
      "16. MLBOOK\n",
      "17. Top 100 Machine Learning Questions & Answers\n",
      "18. Top 100 NLP Questions\n",
      "19. Top 100 Python Interview Questions You Must Prepare In 2019\n",
      "20. Top 40 Python Interview Questions & Answers\n"
     ]
    }
   ],
   "source": [
    "for idx, path in tqdm(enumerate(PDF_PATHS), total=len(PDF_PATHS)):\n",
    "    print(f\"{idx+1}. {path[10:-4]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dd0b9bf343f79b",
   "metadata": {},
   "source": [
    "## 2.2 Opening all the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84de47a6ce3e068a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:39:48.621570Z",
     "start_time": "2025-02-10T16:39:48.539419Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1ea37769bd94866b97e75a77cc4ba78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "documents = list()\n",
    "for path in tqdm(PDF_PATHS, total=len(PDF_PATHS)):\n",
    "    doc = fitz.open(path)\n",
    "    documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fee0989615356e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:39:49.354274Z",
     "start_time": "2025-02-10T16:39:49.339845Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f55700cbcc7947fab939f12cc7720700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document('.\\Dataset\\120_24pgs_mlinterviewquestions.pdf')\n",
      "Document('.\\Dataset\\4.41_faq_dl_8pgs.pdf')\n",
      "Document('.\\Dataset\\40 Artificial Intelligence Interview Questions & Answers .pdf')\n",
      "Document('.\\Dataset\\41 Essential Machine Learning Interview Questions.pdf')\n",
      "Document('.\\Dataset\\5.AI_40Questions.pdf')\n",
      "Document('.\\Dataset\\6.45_ml_important_questions_21pgs.pdf')\n",
      "Document('.\\Dataset\\7.beginner_nn_deep_learning.pdf')\n",
      "Document('.\\Dataset\\8.Data Science Interview Questions.pdf')\n",
      "Document('.\\Dataset\\9.interview_preparation_50.pdf')\n",
      "Document('.\\Dataset\\bagging_ensemblelearning.pdf')\n",
      "Document('.\\Dataset\\Data Science Interview Questions.pdf')\n",
      "Document('.\\Dataset\\Deep Learning Interview Questions.pdf')\n",
      "Document('.\\Dataset\\Interview Questions about Python Programming.pdf')\n",
      "Document('.\\Dataset\\Machine Learning Interview.pdf')\n",
      "Document('.\\Dataset\\ML FINAL (1).pdf')\n",
      "Document('.\\Dataset\\MLBOOK.pdf')\n",
      "Document('.\\Dataset\\Top 100 Machine Learning Questions & Answers.pdf')\n",
      "Document('.\\Dataset\\Top 100 NLP Questions.pdf')\n",
      "Document('.\\Dataset\\Top 100 Python Interview Questions You Must Prepare In 2019.pdf')\n",
      "Document('.\\Dataset\\Top 40 Python Interview Questions & Answers.pdf')\n"
     ]
    }
   ],
   "source": [
    "for doc in tqdm(documents, total=len(documents)):\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e1e996c74b7c79",
   "metadata": {},
   "source": [
    "## 2.3 Getting the text from all the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9f67dd2a9810cc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:39:55.297068Z",
     "start_time": "2025-02-10T16:39:54.009306Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "907b006272324e8eb4b52f8d41ac2df2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3c6727ace044772956b48a15e17de07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c66ccc2f7554267b1039880383f0b72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4fdee996c574d8e9fed707113642587",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "070c064048f347bfba2b92e2fdf7dec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dade54503ff04e6eb61f712480f7ef40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1925f1a9e3e1422c9a10e4b823cc74f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "784ef5ad118245ce969431be23dc2735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65790b275f83423198e22268ffccf9d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb9258c3d04241c7b87921ed7b662772",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c451ee8221d44e79ede9483006f1192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f738b2bfa3de49af94fff681819b2195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90f23ab70edc4f5a8c7315fc6c2546a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "113a0ce177104e11adaceb84218d3444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7627b3127c4497dbe9116bb5245807a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f22eac18de9146db9030ef1bb07cea89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/95 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "220424d37d304595bded6e0a4b488ba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3e47c0e9345402ca2537916c84a9ef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce05a16a18cb423a98309427e47b2c1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6a8db2032934efb9235354237b56997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6fbac74fca945fb94f27f8125a461e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pages = dict()\n",
    "for doc in tqdm(documents, total=len(documents)):\n",
    "    for page_number, page in tqdm(enumerate(doc), total=len(doc)):\n",
    "        page_number = len(pages)\n",
    "        pages[page_number] = page.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "826c83e4f4dbda7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:40:00.931923Z",
     "start_time": "2025-02-10T16:40:00.896512Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e352951e2bf44df815163491f0224a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. Data Science Interview Questions Pdf \n",
      "1. What is meant by selection bias? \n",
      "Answer: Selection bias is a type of error that arises when the researcher decides on \n",
      "whom he is going to conduct the study. It happens when the selection of participants \n",
      "takes place not randomly. Selection bias is also sometimes referred to as a selection \n",
      "effect. It works more effectively and sometimes if the selection bias is not taken into \n",
      "account, the conclusions of the study may go wrong. \n",
      "2. What is a Boltzmann machine? \n",
      "Answer: Boltzmann developed with simple learning algorithms that allow them to find \n",
      "the important information that was presented in the complex regularities in the data. \n",
      "These machines are generally used to optimize the quantity and the weights of the \n",
      "given problem. The learning program works very slow in networks due to many layers of \n",
      "feature detectors. When we consider Restricted Boltzmann Machines, this has a single \n",
      "algorithm feature detectors that make it faster compared to others. \n",
      "3. What is the difference between Cluster and Systematic Sampling? \n",
      "Answer: Cluster sampling is a technique used when it becomes difficult to study the \n",
      "target population spread across a wide area and simple random sampling cannot be \n",
      "applied. Cluster Sample is a probability sample where each sampling unit is a collection \n",
      "or cluster of elements. Systematic sampling is a statistical technique where elements \n",
      "are selected from an ordered sampling frame. In systematic sampling, the list is \n",
      "progressed in a circular manner so once you reach the end of the list, it is progressed \n",
      "from the top again. The best example of systematic sampling is equal probability \n",
      "method. \n",
      "4. What is the Law of Large Numbers? \n",
      "Answer: It is a theorem that describes the result of performing the same experiment a \n",
      "large number of times. This theorem forms the basis of frequency-style thinking. It says \n",
      "that the sample means, the sample variance and the sample standard deviation \n",
      "converge to what they are trying to estimate. \n",
      "5. What are Eigenvectors and Eigenvalues? \n",
      "Answer: Eigenvectors are used for understanding linear transformations. In data \n",
      "analysis, we usually calculate the eigenvectors for a correlation or covariance matrix. \n",
      "Eigenvectors are the directions along which a particular linear transformation acts by \n",
      "flipping, compressing or stretching. \n",
      "Eigenvalue can be referred to as the strength of the transformation in the direction of \n",
      "eigenvector or the factor by which the compression occurs. \n",
      "6. Can you cite some examples where both false positive and false negatives are \n",
      "equally important? \n",
      "Answer: In the Banking industry giving loans is the primary source of making money \n",
      "but at the same time if your repayment rate is not good you will not make any profit, \n",
      "rather you will risk huge losses. \n",
      "Banks don’t want to lose good customers and at the same point in time, they don’t want \n",
      "to acquire bad customers. In this scenario, both the false positives and false negatives \n",
      "become very important to measure. \n",
      "\n",
      "\n",
      "1. 7. What is logistic regression? State an example when you have used logistic \n",
      "regression recently. \n",
      "Answer: Logistic Regression often referred to as the logit model is a technique to \n",
      "predict the binary outcome from a linear combination of predictor variables. \n",
      "For example, if you want to predict whether a particular political leader will win the \n",
      "election or not. In this case, the outcome of prediction is binary i.e. 0 or 1 (Win/Lose). \n",
      "The predictor variables here would be the amount of money spent on election \n",
      "campaigning of a particular candidate, the amount of time spent in campaigning, etc. \n",
      "8. What is the role of the Activation Function? \n",
      "Answer: The Activation function is used to introduce non-linearity into the neural \n",
      "network helping it to learn more complex function. Without which the neural network \n",
      "would be only able to learn linear function which is a linear combination of its input data. \n",
      "An activation function is a function in an artificial neuron that delivers an output based \n",
      "on inputs. \n",
      "9. What do you mean by cluster sampling and systematic sampling? \n",
      "Answer: When studying the target population spread throughout a wide area becomes \n",
      "difficult and applying simple random sampling becomes ineffective, the technique of \n",
      "cluster sampling is used. A cluster sample is a probability sample, in which each of the \n",
      "sampling units is a collection or cluster of elements. \n",
      "Following the technique of systematic sampling, elements are chosen from an ordered \n",
      "sampling frame. The list is advanced in a circular fashion. This is done in such a way so \n",
      "that once the end of the list is reached, the same is progressed from the start, or top, \n",
      "again. \n",
      "10. Please explain Gradient Descent? \n",
      "Answer: The degree of change in the output of a function relating to the changes made \n",
      "to the inputs is known as a gradient. It measures the change in all weights with respect \n",
      "to the change in error. A gradient can also be comprehended as the slope of a function. \n",
      "Gradient Descent refers to escalating down to the bottom of a valley. Simply, consider \n",
      "this something as opposed to climbing up a hill. It is a minimization algorithm meant for \n",
      "minimizing a given activation function. \n",
      "11. What do you know about Autoencoders? \n",
      "Answer: Autoencoders are simplistic learning networks used for transforming inputs \n",
      "into outputs with minimum possible error. It means that the outputs resulted are very \n",
      "close to the inputs. \n",
      "A couple of layers are added between the input and the output with the size of each \n",
      "layer smaller than the size pertaining to the input layer. An autoencoder receives \n",
      "unlabeled input that is encoded for reconstructing the output. \n",
      "12. How and by what methods data visualizations can be effectively used? \n",
      "Answer: In addition to giving insights in a very effective and efficient manner, data \n",
      "visualization can also be used in such a way that it is not only restricted to bar, line or \n",
      "some stereotypic graphs. Data can be represented in a much more visually pleasing \n",
      "manner. \n",
      "One thing has to be taken care of is to convey the intended insight or finding correctly to \n",
      "the audience. Once the baseline is set. Innovative and creative part can help you come \n",
      "\n",
      "\n",
      "2. up with better looking and functional dashboards. There is a fine line between the \n",
      "simple insightful dashboard and awesome looking 0 fruitful insight dashboards. \n",
      "13. What is the common perception of visualization? \n",
      "Answer: People think visualization as just charts and summary information. But they \n",
      "are beyond that and drive business with a lot of underlying principles. Learning design \n",
      "principles can help anyone build effective and efficient visualizations and this Tableau \n",
      "prep tool can drastically increase our time on focusing more important part. The only \n",
      "issue with Tableau is, it is paid and companies need to pay for leveraging that awesome \n",
      "tool. \n",
      "14. Where to seek help in case of discrepancies in Tableau? \n",
      "Answer: When you face any issue regarding Tableau, try searching in the Tableau \n",
      "community forum. It is one of the best places to get your queries answered. You can \n",
      "always write your question and get the query answered with an hour or a day. You can \n",
      "always post on LinkedIn and follow people. \n",
      "15. Why is data cleaning essential in Data Science? \n",
      "Answer: Data cleaning is more important in Data Science because the end results or \n",
      "the outcomes of the data analysis come from the existing data where useless or \n",
      "unimportant need to be cleaned periodically as of when not required. This ensures the \n",
      "data reliability & accuracy and also memory is freed up. \n",
      "Data cleaning reduces the data redundancy and gives good results in data analysis \n",
      "where some large customer information exists and that should be cleaned periodically. \n",
      "In businesses like e-commerce, retail, government organizations contain large customer \n",
      "transaction information which is outdated and needs to be cleaned. \n",
      "Depending on the amount or size of data, suitable tools or methods should be used to \n",
      "clean the data from the database or big data environment. There are different types of \n",
      "data existing in a data source such as dirty data, clean data, mixed clean and dirty data \n",
      "and sample clean data. \n",
      "Modern data science applications rely on machine learning model where the learner \n",
      "learns from the existing data. So, the existing data should always be cleanly and well \n",
      "maintained to get sophisticated and good outcomes during the optimization of the \n",
      "system. \n",
      "16. What is A/B testing in Data Science? \n",
      "Answers: A/B testing is also called Bucket Testing or Split Testing. This is the method \n",
      "of comparing and testing two versions of systems or applications against each other to \n",
      "determine which version of application performs better. This is important in the cases \n",
      "where multiple versions are shown to the customers or end-users in order to achieve \n",
      "the goals. \n",
      "In the area of Data Science, this A/B testing is used to know which variable out of the \n",
      "existing two variables in order to optimize or increase the outcome of the goal. A/B \n",
      "testing is also called Design of Experiment. This testing helps in establishing a cause \n",
      "and effect relationship between the independent and dependent variables. \n",
      "This testing is also simply a combination of design experimentation or statistical \n",
      "inference. Significance, Randomization and Multiple Comparisons are the key elements \n",
      "of the A/B testing. \n",
      "\n",
      "\n",
      "3. The significance is the term for the significance of statistical tests conducted. \n",
      "Randomization is the core component of the experimental design where the variables \n",
      "will be balanced. Multiple comparisons are the way of comparing more variables in the \n",
      "case of customer interests that causes more false positives resulting in the requirement \n",
      "of correction in the confidence level of a seller in the area of e-commerce. \n",
      "17. How Machine Learning Is Deployed In Real World Scenarios? \n",
      "Answer: Here are some of the scenarios in which machine learning finds applications in \n",
      "the real world: \n",
      "Ecommerce: Understanding customer churn, deploying targeted advertising, \n",
      "remarketing. \n",
      "Search engine: Ranking pages depending on the personal preferences of the searcher \n",
      "Finance: Evaluating investment opportunities & risks, detecting fraudulent transactions \n",
      "Medicare: Designing drugs depending on the patient’s history and needs \n",
      "Robotics: Machine learning for handling situations that are out of the ordinary \n",
      "Social media: Understanding relationships and recommending connections \n",
      "Extraction of information: framing questions for getting answers from databases over \n",
      "the web. \n",
      "18. What Is Power Analysis? \n",
      "Answer: power analysis is a vital part of the experimental design. It is involved with the \n",
      "process of determining the sample size needed for detecting an effect of a given size \n",
      "from a cause with a certain degree of assurance. It lets you deploy a specific probability \n",
      "in a sample size constraint.  \n",
      "The various techniques of statistical power analysis and sample size estimation are \n",
      "widely deployed for making the statistical judgment that is accurate and evaluates the \n",
      "size needed for experimental effects in practice. \n",
      "Power analysis lets you understand the sample size estimate so that they are neither \n",
      "high nor low. A low sample size there will be no authentication to provide reliable \n",
      "answers and if it is large there will be wastage of resources. \n",
      "19. What Is K-means? How Can You Select K For K-means? \n",
      "Answer: K-means clustering can be termed as the basic unsupervised learning \n",
      "algorithm. It is the method of classifying data using a certain set of clusters called K \n",
      "clusters. It is deployed for grouping data in order to find similarity in the data. \n",
      "It includes defining the K centers, one each in a cluster. The clusters are defined into K \n",
      "groups with K being predefined. The K points are selected at random as cluster centers. \n",
      "The objects are assigned to their nearest cluster center. The objects within a cluster are \n",
      "as closely related to one another as possible and differ as much as possible to the \n",
      "objects in other clusters. K-means clustering works very well for large sets of data. \n",
      "20. Why is resampling done? \n",
      "Answer: Resampling is done in any of these cases: \n",
      "Estimating the accuracy of sample statistics by using subsets of accessible data or \n",
      "drawing randomly with replacement from a set of data points \n",
      "\n",
      "\n",
      "4. Substituting labels on data points when performing significance tests \n",
      "Validating models by using random subsets (bootstrapping, cross-validation) \n",
      "21. What tools or devices help you succeed in your role as a data scientist? \n",
      "Answer: This question’s purpose is to learn the programming languages and \n",
      "applications the candidate knows and has experience using. The answer will show the \n",
      "candidate’s need for additional training of basic programming languages and platforms \n",
      "or any transferable skills. This is vital to understand as it can cost more time and money \n",
      "to train if the candidate is not knowledgeable in all of the languages and applications \n",
      "required for the position. \n",
      "22. Why do you want to work at this company as a data scientist? \n",
      "Answer: The purpose of this question is to determine the motivation behind the \n",
      "candidate’s choice of applying and interviewing for the position. Their answer should \n",
      "reveal their inspiration for working for the company and their drive for being a data \n",
      "scientist. It should show the candidate is pursuing the position because they are \n",
      "passionate about data and believe in the company, two elements that can determine the \n",
      "candidate’s performance. Answers to look for include: \n",
      "Interest in data mining \n",
      "Respect for the company’s innovative practices \n",
      "Desire to apply analytical skills to solve real-world issues with data \n",
      "“I have a passion for working for data-driven, innovative companies. Your firm uses \n",
      "advanced technology to address everyday problems for consumers and businesses \n",
      "alike, which I admire. I also enjoy solving issues using an analytical approach and am \n",
      "passionate about incorporating technology into my work. I believe that my skills and \n",
      "passion match the company’s drive and capabilities.” \n",
      "23. What are the differences between overfitting and underfitting? \n",
      "Answer: In statistics and machine learning, one of the most common tasks is to fit a \n",
      "model to a set of training data, so as to be able to make reliable predictions on general \n",
      "untrained data. \n",
      "In overfitting, a statistical model describes random error or noise instead of the \n",
      "underlying relationship. Overfitting occurs when a model is excessively complex, such \n",
      "as having too many parameters relative to the number of observations. A model that \n",
      "has been overfitting has poor predictive performance, as it overreacts to minor \n",
      "fluctuations in the training data. \n",
      "Underfitting occurs when a statistical model or machine learning algorithm cannot \n",
      "capture the underlying trend of the data. Underfitting would occur, for example, when \n",
      "fitting a linear model to non-linear data. Such a model too would have poor predictive \n",
      "performance. \n",
      "24. What is Machine Learning? \n",
      "Answer: Machine Learning explores the study and construction of algorithms that can \n",
      "learn from and make predictions on data. Closely related to computational statistics. \n",
      "Used to devise complex models and algorithms that lend themselves to a prediction \n",
      "which in commercial use is known as predictive analytics. \n",
      "25. Can you enumerate the various differences between Supervised and \n",
      "Unsupervised Learning? \n",
      "\n",
      "\n",
      "5. Answer: Supervised learning is a type of machine learning where a function is inferred \n",
      "from labeled training data. The training data contains a set of training examples. \n",
      "Unsupervised learning, on the other hand, is a type of machine learning where \n",
      "inferences are drawn from datasets containing input data without labeled responses. \n",
      "Following are the various other differences between the two types of machine learning: \n",
      "Algorithms Used – Supervised learning makes use of Decision Trees, K-nearest \n",
      "Neighbor algorithm, Neural Networks, Regression, and Support Vector Machines. \n",
      "Unsupervised learning uses Anomaly Detection, Clustering, Latent Variable Models, \n",
      "and Neural Networks. \n",
      "Enables – Supervised learning enables classification and regression, whereas \n",
      "unsupervised learning enables classification, dimension reduction, and density \n",
      "estimation \n",
      "Use – While supervised learning is used for prediction, unsupervised learning finds use \n",
      "in analysis \n",
      "26. What is underfitting? \n",
      "Answer: Any prediction rate which has provides low prediction in the training error and \n",
      "the test error leads to a high business problem, if the error rate in training set is high \n",
      "and the error rate in the test set is also high, then we can conclude it as overfitting \n",
      "model. \n",
      "27. How to understand the problems faced during data analysis? \n",
      "Answer: Most of the problem faced during hands-on analysis or data science is \n",
      "because of poor understanding of the problem in hand and concentrating more on tools, \n",
      "end results and other aspects of the project. \n",
      "Breaking the problem down to a granular level and understanding takes a lot of time \n",
      "and practice to master. Coming back to square one in data science projects can be \n",
      "seen in a lot of companies and even in your own project or kaggle problems. \n",
      "28. What does SAS stand out to be the best over other data analytics tools? \n",
      "Answer: Ease to understand: The provisions included in SAS are remarkably easy to \n",
      "learn. Further, it offers the most suitable option for those who already are aware of the \n",
      "SQL. On the other hand, R comes with a steep training cover which is supposed to be a \n",
      "low-level programming style. \n",
      "Data Handling Capacities: it is at par the most leading tool which also includes the R& \n",
      "Python. \n",
      "If it advances before handling the huge data, it is the best platform to engage Graphical \n",
      "Capacities: it comes with functional graphical capacities and has a limited knowledge \n",
      "field. \n",
      "It is useful to customize the plots Better tool management: It benefits in a release the \n",
      "updates with regards to the controlled conditions. \n",
      "This is the main reason why it is well tested. Whereas if you considered R&Python, it \n",
      "has open contribution also the risk of errors in the current development is also high. \n",
      "\n",
      "\n",
      "6. 29. What is the best Programming Language to use in Data Science? \n",
      "Answer: Data Science can be handled by using programming languages like Python or \n",
      "R programming language. These two are the two most popular languages being used \n",
      "by the Data Scientists or Data Analysts. R and Python are open source and are free to \n",
      "use and came into existence during the 1990s. \n",
      "Python and R have different advantages depending on the applications and required a \n",
      "business goal. Python is better to be used in the cases of repeated tasks or jobs and for \n",
      "data manipulations whereas R programming can be used for querying or retrieving \n",
      "datasets and customized data analysis. \n",
      "Mostly Python is preferred for all types of data science applications where some time R \n",
      "programming is preferred in the cases of high or complex data applications. Python is \n",
      "easier to learn and has less learning curve whereas R has a deep learning curve. \n",
      "Python is mostly preferred in all the cases which is a general-purpose programming \n",
      "language and can be found in many applications other than Data Science too. R is \n",
      "mostly seen in Data Science area only where it is used for data analysis in standalone \n",
      "servers or computing separately. \n",
      "30. What is a Linear Regression in Data Science? \n",
      "Answer: This is the frequently asked Data Science Interview Questions in an interview. \n",
      "Linear Regression is a technique used in supervised machine learning the algorithmic \n",
      "process in the area of Data Science. This method is used for predictive analysis. \n",
      "Predictive analytics is an area within Statistical Sciences where the existing information \n",
      "will be extracted and processed to predict the trends and outcomes pattern. The core of \n",
      "the subject lies in the analysis of existing context to predict an unknown event. \n",
      "The process of Linear Regression method is to predict a variable called target variable \n",
      "by making the best relationship between the dependent variable and an independent \n",
      "variable. Here the dependent variable is the outcome variable and also response \n",
      "variable whereas the independent variable is the predictor variable or explanatory \n",
      "variable. \n",
      "For example in real life, depending on the expenses occurred in this financial year or \n",
      "monthly expenses, the predictions happen by calculating the approximate upcoming \n",
      "months or financial years expenses. \n",
      "In this method, the implementation can be done by using Python programming \n",
      "technique where this is the most important method used in Machine Learning technique \n",
      "under the area of Data Science. \n",
      "Linear regression is also called Regression analysis that comes under the area of \n",
      "Statistical Sciences which is integrated together with Data Science. \n",
      "31. What Is A Recommender System? \n",
      "Answer: A recommender system is a today widely deployed in multiple fields like movie \n",
      "recommendations, music preferences, social tags, research articles, search queries and \n",
      "so on. The recommender systems work as per collaborative and content-based filtering \n",
      "or by deploying a personality-based approach. This type of system works based on a \n",
      "person’s past behavior in order to build a model for the future. This will predict future \n",
      "product buying, movie viewing or book reading by people. It also creates a filtering \n",
      "\n",
      "\n",
      "7. approach using the discrete characteristics of items while recommending additional \n",
      "items. \n",
      "32. How Do Data Scientists Use Statistics? \n",
      "Answer: Statistics help Data Scientists to look into the data for patterns, hidden insights \n",
      "and convert Big Data into Big insights. It helps to get a better idea of what the \n",
      "customers are expecting. Data Scientists can learn about consumer behavior, interest, \n",
      "engagement, retention and finally conversion all through the power of insightful \n",
      "statistics. It helps them to build powerful data models in order to validate certain \n",
      "inferences and predictions. All this can be converted into a powerful business \n",
      "proposition by giving users what they want at precisely when they want it. \n",
      "33. What Do You Understand By The Term Normal Distribution? \n",
      "Answer: It is a set of a continuous variable spread across a normal curve or in the \n",
      "shape of a bell curve. It can be considered as a continuous probability distribution and is \n",
      "useful in statistics. It is the most common distribution curve and it becomes very useful \n",
      "to analyze the variables and their relationships when we have the normal distribution \n",
      "curve. \n",
      "The normal distribution curve is symmetrical. The non-normal distribution approaches \n",
      "the normal distribution as the size of the samples increases. It is also very easy to \n",
      "deploy the Central Limit Theorem. This method helps to make sense of data that is \n",
      "random by creating an order and interpreting the results using a bell-shaped graph. \n",
      "34. What is collaborative filtering? \n",
      "Answer: Filtering is a process used by recommender systems to find patterns and \n",
      "information from numerous data sources, several agents, and collaborating \n",
      "perspectives. In other words, the collaborative method is a process of making automatic \n",
      "predictions from human preferences or interests. \n",
      "35. Explain the difference between overfitting and underfitting? \n",
      "Answer: In machine learning as well as in statistics, the common task to undergo is to \n",
      "fit a model to a set of training data. It helps us in making reliable predictions using \n",
      "general untrained data. \n",
      "In overfitting, a statistical model will help us in letting know the random noise or errors \n",
      "instead of the underlying relationship. Overfitting comes into light when the data is \n",
      "associated with too much complexity, which means it is associated with so many \n",
      "parameters relative to the number of observations. A model that is overfitted is always \n",
      "performed poor in predictive performance and acts overly to the minor fluctuations in the \n",
      "training data. \n",
      "Unnderfittinng happens when a machine learning algorithm or statistical model is unable \n",
      "to focus on the underlying insights of the data. The case when you are trying to fix a \n",
      "linear model to a nonlinear one. This kind of model would result in poor predictive \n",
      "performance. \n",
      "36. What is systematic sampling? \n",
      "Answer: Systematic sampling is a technique, and the name resembles that it follows \n",
      "some systematic way, and the samples are selected from an ordered sampling frame. \n",
      "In systematic sampling, the list is actually in a circular manner and the selection starts \n",
      "from one end and reaches the final, and the cycle goes on. Equal probability method \n",
      "would be the best example for the systematic sampling. \n",
      "\n",
      "\n",
      "8. 37. What are recommender systems? \n",
      "Answer: Recommender systems are also treated as information filtering systems that \n",
      "work to predict or likeness of a user for a product. These recommender systems are \n",
      "widely used in areas like news, movies, social tags, music, products, etc. \n",
      "We can see the movie recommenders in Netflix, IMDB, & bookMyShow, and product \n",
      "recommender e-commerce sites like eBay, Amazon, Flipcart, Youtube video \n",
      "recommendations, and game recommendations. \n",
      "38. What are Artificial Neural Networks? \n",
      "Answer: Artificial neural networks are the main elements which have made the \n",
      "machine learning popular. These neural networks are developed based on the \n",
      "functionality of a human brain. The Artificial neural networks are trained to learn from \n",
      "the examples and experiences without being programmed explicitly. Artificial neural \n",
      "networks work based on nodes called artificial neurons that are connected to one \n",
      "another. Each connection acts similar to synapses in the human brain that helps in \n",
      "transmitting the signals between the artificial neurons. \n",
      "39. Explain the role of Activation function?  \n",
      "Answer: The activation function helps in introducing the nonlinearity into the neural \n",
      "network that enables the neural network to learn the complex functions. Without this, it \n",
      "is challenging for the linear function to analyze complex data. An activation function is a \n",
      "function is an artificial neuron which delivers the output based on the input given.   \n",
      "40. What is the difference between Supervised Learning an Unsupervised \n",
      "Learning? \n",
      "Answer: If an algorithm learns something from the training data so that the knowledge \n",
      "can be applied to the test data, then it is referred to as Supervised Learning. \n",
      "Classification is an example for Supervised Learning. If the algorithm does not learn \n",
      "anything beforehand because there is no response variable or any training data, then it \n",
      "is referred to as unsupervised learning. Clustering is an example of unsupervised \n",
      "learning. \n",
      "41. What is the Central Limit Theorem and why is it important? \n",
      "Answer: “Suppose that we are interested in estimating the average height among all \n",
      "people. Collecting data for every person in the world is impossible. While we can’t \n",
      "obtain a height measurement from everyone in the population, we can still sample some \n",
      "people. The question now becomes, what can we say about the average height of the \n",
      "entire population given a single sample. \n",
      "42. What are the feature vectors? \n",
      "Answer: A feature vector is an n-dimensional vector of numerical features that \n",
      "represent some object. In machine learning, feature vectors are used to represent \n",
      "numeric or symbolic characteristics, called features, of an object in a mathematical, \n",
      "easily analyzable way. \n",
      "43. What is Cluster Sampling? \n",
      "Answer: Cluster sampling is a technique used when it becomes difficult to study the \n",
      "target population spread across a wide area and simple random sampling cannot be \n",
      "applied. Cluster Sample is a probability sample where each sampling unit is a collection \n",
      "or cluster of elements. \n",
      "For eg., A researcher wants to survey the academic performance of high school \n",
      "students in Japan. He can divide the entire population of Japan into different clusters \n",
      "\n",
      "\n",
      "9. (cities). Then the researcher selects a number of clusters depending on his research \n",
      "through simple or systematic random sampling. \n",
      "44. What are the various steps involved in an analytics project? \n",
      "Answer: The following are the various steps involved in an analytics project: \n",
      "Understand the Business problem \n",
      "Explore the data and become familiar with it. \n",
      "Prepare the data for modeling by detecting outliers, treating missing values, \n",
      "transforming variables, etc. \n",
      "After data preparation, start running the model, analyze the result and tweak the \n",
      "approach. This is an iterative step until the best possible outcome is achieved. \n",
      "Validate the model using a new data set. \n",
      "Start implementing the model and track the result to analyze the performance of the \n",
      "model over the period of time. \n",
      "45. Please explain Eigenvectors and Eigenvalues? \n",
      "Answer: Eigenvectors help in understanding linear transformations. They are \n",
      "calculated typically for a correlation or covariance matrix in data analysis. \n",
      "In other words, eigenvectors are those directions along which some particular linear \n",
      "transformation acts by compressing, flipping, or stretching. \n",
      "Eigenvalues can be understood either as the strengths of the transformation in the \n",
      "direction of the eigenvectors or the factors by which the compressions happens. \n",
      "46. What are outlier values and how do you treat them? \n",
      "Answer: Outlier values, or simply outliers, are data points in statistics that don’t belong \n",
      "to a certain population. An outlier value is an abnormal observation that is very much \n",
      "different from other values belonging to the set. \n",
      "Identification of outlier values can be done by using univariate or some other graphical \n",
      "analysis method. Few outlier values can be assessed individually but assessing a large \n",
      "set of outlier values require the substitution of the same with either the 99th or the 1st \n",
      "percentile values. \n",
      "There are two popular ways of treating outlier values: \n",
      "To change the value so that it can be brought within a range \n",
      "To simply remove the value \n",
      "Note:  Not all extreme values are outlier values. \n",
      "47. How to choose the right chart in case of creating a viz? \n",
      "Answer: Using the right chart to represent data is one of the key aspects of data \n",
      "visualization and design principle. You will always have options to choose from when \n",
      "deciding on a chart. But fixing to the right chart comes only by experience, practice and \n",
      "deep understanding of end-user needs. That dictates everything in the dashboard. \n",
      "48. What is the basic responsibility of a Data Scientist? \n",
      "Answer: As a data scientist, we have the responsibility to make complex things simple \n",
      "enough that anyone without context should understand, what we are trying to convey. \n",
      "The moment, we start explaining even the simple things the mission of making the \n",
      "complex simple goes away. This happens a lot when we are doing data visualization. \n",
      "Less is more. Rather than pushing too much information on to readers brain, we need to \n",
      "figure out how easily we can help them consume a dashboard or a chart. \n",
      "\n",
      "\n",
      "10. The process is simple to say but difficult to implement. You must bring the complex \n",
      "business value out of a self-explanatory chart. It’s a skill every data scientist should \n",
      "strive towards and good to have in their arsenal. \n",
      "49. What is the difference between Machine learning Vs Data Mining? \n",
      "Answer: Data mining is about working on unlimited data and then extract it to a level \n",
      "anywhere the unusual and unknown patterns are identified. \n",
      "Machine learning is any method about a study whether it closely relates to design, \n",
      "development concerning the algorithms that provide an ability to certain computers to \n",
      "capacity to learn. \n",
      "50. What are the types of biases that can occur during sampling? \n",
      "Answer: Some simple models of selection bias are described below. Undercoverage \n",
      "occurs when some members of the population live badly represented inside the sample. \n",
      "… The survey relied on a service unit, drawn of telephone directories and car \n",
      "registration lists. \n",
      "• \n",
      "Selection bias \n",
      "• \n",
      "Under coverage bias \n",
      "• \n",
      "Survivorship bias \n",
      "51. Why data cleaning plays a vital role in the analysis? \n",
      "Answer: Cleaning data from multiple sources to transform it into a format that data \n",
      "analysts or data scientists can work with is a cumbersome process because – as the \n",
      "number of data sources increases, the time take to clean the data increases \n",
      "exponentially due to the number of sources and the volume of data generated in these \n",
      "sources. It might take up to 80% of the time for just cleaning data making it a critical part \n",
      "of the analysis task. \n",
      "52. What are an Eigenvalue and Eigenvector? \n",
      "Answer: Eigenvectors are used for understanding linear transformations. In data \n",
      "analysis, we usually calculate the eigenvectors for a correlation or covariance matrix. \n",
      "Eigenvectors are the directions along which a particular linear transformation acts by \n",
      "flipping, compressing or stretching. Eigenvalue can be referred to as the strength of the \n",
      "transformation in the direction of eigenvector or the factor by which the compression \n",
      "occurs. \n",
      "53. Define some key performance indicators for the product \n",
      "Answer: After playing around with the product, think about this: what are some of the \n",
      "key metrics that the product might want to optimize? Part of a data scientist’s role in \n",
      "certain companies involves working closely with the product teams to help define, \n",
      "measure, and report on these metrics. This is an exercise you can go through by \n",
      "yourself at home, and can really help during your interview process \n",
      "54. Why is data cleaning important for analysis? \n",
      "Answer: This is a knowledge-based question with a relatively simple answer. So much \n",
      "of a data scientist’s time goes into cleaning data – and as the data gets bigger, so does \n",
      "the time it takes to clean. Cleaning it right is the foundation of analysis, and the time it \n",
      "takes to clean data, alone, makes it important. \n",
      "55. Do you prefer Python or R for text analytics? \n",
      "Answer: Here, you’re being asked to insert your own opinion. However, most data \n",
      "scientists agree that the right opinion is Python. This is because Python has Pandas \n",
      "\n",
      "\n",
      "11. library which has strong data analysis tools and an easy-to-use structure. What’s more, \n",
      "Python is typically faster for text analytics. \n",
      "56. Explain Star Schema? \n",
      "Answer: It is a traditional database schema with a central table. Satellite tables map \n",
      "ID’s to physical name or description and can be connected to the central fact table using \n",
      "the ID fields; these tables are known as lookup tables, and are principally useful in real-\n",
      "time applications, as they save a lot of memory. Sometimes star schemas involve \n",
      "several layers of summarization to recover information faster. \n",
      "57. What do you mean by word Data Science? \n",
      "Answer: Data Science is the extraction of knowledge from large volumes of data that \n",
      "are structured or unstructured, which is a continuation of the field data mining and \n",
      "predictive analytics, It is also known as knowledge discovery and data mining.  \n",
      "58. What do you understand by term hash table collisions? \n",
      "Answer: Hash table (hash map) is a kind of data structure used to implement an \n",
      "associative array, a structure that can map keys to values. Ideally, the hash function will \n",
      "assign each key to a unique bucket, but sometimes it is possible that two keys will \n",
      "generate an identical hash causing both keys to point to the same bucket. It is known as \n",
      "hash collisions. \n",
      "59. How can you assess a good logistic model? \n",
      "Answer: There are various methods to assess the results of logistic regression \n",
      "analysis- \n",
      "Using Classification Matrix to look at the true negatives and false positives. \n",
      "Concordance that helps identify the ability of the logistic model to differentiate between \n",
      "the event happening and not happening. \n",
      "Lift helps assess the logistic model by comparing it with random selection. \n",
      "60. Why do you want to work as a data scientist? \n",
      "Answer: This question plays off of your definition of data science. However, now \n",
      "recruiters are looking to understand what you’ll contribute and what you’ll gain from this \n",
      "field. Focus on what makes your path to becoming a data scientist unique – whether it \n",
      "be a mentor or a preferred method of data extraction. \n",
      "61. How have you overcome a barrier to finding a solution? \n",
      "Answer: This question directly asks you to draw upon your experiences and your ability \n",
      "to problem-solve. Data scientists are, after all, numbers-based problem-solvers, so, it’s \n",
      "important to determine an example of a problem you’ve solved ahead of time. Whether \n",
      "it’s through re-cleaning data or using a different program, you should be able to explain \n",
      "your process to the recruiter. \n",
      "62. How To Work Towards A Random Forest? \n",
      "Answer: The underlying principle of this technique is that several weak learners \n",
      "combined provide a strong learner. The steps involved are \n",
      "Build several decision trees on bootstrapped training samples of data \n",
      "On each tree, each time a split is considered, a random sample of mm predictors is \n",
      "chosen as split candidates, out of all pp predictors \n",
      "\n",
      "\n",
      "12. Rule of thumb: at each split m=p√m=p \n",
      "Predictions: at the majority rule. \n",
      "63. Explain Cross-validation? \n",
      "Answer: It is a model validation technique for evaluating how the outcomes of a \n",
      "statistical analysis will generalize to an independent data set. Mainly used in \n",
      "backgrounds where the objective is forecast and one wants to estimate how accurately \n",
      "a model will accomplish in practice. \n",
      "The goal of cross-validation is to term a data set to test the model in the training phase \n",
      "(i.e. validation data set) in order to limit problems like overfitting and get an insight on \n",
      "how the model will generalize to an independent data set. \n",
      "64. What is a Linear Regression? \n",
      "Answer: Linear regression is a statistical technique where the score of a variable Y is \n",
      "predicted from the score of a second variable X. X is referred to as the predictor \n",
      "variable and Y as the criterion variable. \n",
      "65. Can you explain the difference between a Test Set and a Validation Set? \n",
      "Answer: Validation set can be considered as a part of the training set as it is used for \n",
      "parameter selection and to avoid Overfitting of the model being built. On the other hand, \n",
      "the test set is used for testing or evaluating the performance of a trained machine \n",
      "learning model. \n",
      "In simple terms, the differences can be summarized as- \n",
      "Training Set is to fit the parameters i.e. weights. \n",
      "Test Set is to assess the performance of the model i.e. evaluating the predictive power \n",
      "and generalization. \n",
      "66. How do you define data science? \n",
      "Answer: This question allows you to show your interviewer who you are. For example, \n",
      "what’s your favorite part of the process, or what’s the most impactful project you’ve \n",
      "worked on? Focus first on what data science is to everyone – a means of extracting \n",
      "insights from numbers – then explain what makes it personal. \n",
      "67. What devices or tools help you most as a data scientist? \n",
      "Answer: By asking this question, recruiters are seeking to learn more about your \n",
      "qualifications. Explain how you utilize every coding language you know, from R to SQL, \n",
      "and how each language helps complete certain tasks. This is also an opportunity to \n",
      "explain more about how your education or methods go above and beyond. \n",
      "68. How often should an algorithm be updated? \n",
      "Answer: This quasi-trick question has no specific time-based answer. This is because \n",
      "an algorithm should be updated whenever the underlying data is changing or when you \n",
      "want the model to evolve over time. Understanding the outcomes of dynamic algorithms \n",
      "is key to answering this question with confidence. \n",
      "69. Python or R – Which one would you prefer for text analytics? \n",
      "Answer: The best possible answer for this would be Python because it has Pandas \n",
      "library that provides easy to use data structures and high-performance data analysis \n",
      "tools. \n",
      "\n",
      "\n",
      "13. 70. What is an Auto-Encoder? \n",
      "Answer: The Auto-Encoders are learning networks that work for transforming the inputs \n",
      "into outputs with no errors or minimized error. It means the output must be very close to \n",
      "the input. We add a few layers between the input and output and the sizes of these \n",
      "layers would be smaller than the input layer. Actually, the Auto-encoder is provided with \n",
      "the unlabelled input then it would be transmitted into reconstructing the input. \n",
      "71. What is back Propagation? \n",
      "Answer: Backpropagation is an algorithm used in Deep Learning to train the multilayer \n",
      "neural network. Using this method, we can move an error form an end of a network to \n",
      "the inside of it, and that brings the efficient computation of gradient. \n",
      "It consists of the below-mentioned steps: \n",
      "Forward data propagation of data that is being used for training \n",
      "Derivatives are computed with the help of output and target. \n",
      "Backpropagation for computing the derivative error. \n",
      "You can use the output that was previously calculated for output. \n",
      "Update the weights. \n",
      "72. How can the outlier values be treated? \n",
      "Answer: We can identify the outlier values by using graphical analysis method or by \n",
      "using Univariate method. It becomes easier and can be assessed individually when the \n",
      "outlier values are few but when the outlier values are more in number then these values \n",
      "required to be substituted either with the 1st or with the 99th percentile values. \n",
      "Below are the common ways to treat outlier values. \n",
      "To bring down and change the value \n",
      "To remove the value \n",
      "73. Explain the difference between Univariate, Bivariate and Multivariate analysis? \n",
      "Answer: Univariate analysis is a descriptive analysis and can be used to differentiate \n",
      "the number of variables involved at a given point of time. For instance, the sales of a \n",
      "particular territory include only one variable, and then the same is treated as a \n",
      "Univariate analysis. \n",
      "Bivariate analysis is used to understand the difference between two variables at a given \n",
      "time on the scatter pilot. The best example for bivariate analysis of the difference \n",
      "between the sale and expenses happens for a particular product. \n",
      "Multivariate analysis is used to understand the more than two variables responses for \n",
      "the variables. \n",
      "74. What makes the difference between “Long” and “Wide” Format data? \n",
      "Answer: In a wide format method, when we take a subject, the repeated responses are \n",
      "recorded in a single row, and each recorded response is in a separate column. When it \n",
      "comes to Long format data, each row acts as a one-time point per subject. In wide \n",
      "format, the columns are generally divided into groups whereas in a long-form the rows \n",
      "are divided into groups. \n",
      "75. Do we have different Selection Biases, if yes, what are they? \n",
      "Answer: Sampling Bias: This bias arises when you select only particular people or \n",
      "when non-random selection of samples happened. In general terms, it is nothing but a \n",
      "selection of the majority of the people belong to one group. \n",
      "\n",
      "\n",
      "14. Time Interval: sometimes a trial may be terminated earlier than actual time (probably \n",
      "due to some ethical reasons) but the extreme value finally taken into consideration is \n",
      "the most significant value even though all other variables have similar Mean. \n",
      "Data: We can name it as a Data bias when a separate set of data is taken to support a \n",
      "conclusion or eliminates terrible data based on the arbitrary grounds, instead of \n",
      "generally relying on generally stated criteria. \n",
      "Attrition bias: Attrition bias is defined as an error that occurs due to Unequal loss of \n",
      "participants from a randomized controlled trial (RCT). \n",
      "76. What is meant by supervised and unsupervised learning in data? \n",
      "Answer: Supervised Learning: Supervised learning is a process of training machines \n",
      "with the labeled or right kind of data. In supervised learning, the machine uses the \n",
      "labeled data as a base to give the next answer. \n",
      "Unsupervised learning: It is another form of training machines using information which is \n",
      "unlabeled or unstructured one. Unlike Supervised learning, there is no special teacher \n",
      "or predefined data for the machine to quickly learn from. \n",
      "77. What is Data Science? \n",
      "Answer: Data science is defined as a multidisciplinary subject used to extract \n",
      "meaningful insights out of different types of data by employing various scientific \n",
      "methods such as scientific processes and algorithms. Data science helps in solving the \n",
      "analytically complex problems in a simplified way. It acts as a stream where you can \n",
      "utilize raw data to generate business value. \n",
      "78. What is Cross-validation? \n",
      "Answer: It is a model validation technique used to evaluate how the statistical analysis \n",
      "would generalize to an independent dataset. This could be helpful in the areas of \n",
      "backgrounds where the objective is exactly forecasted, and the people want to estimate \n",
      "how accurately the model would work in real-time. \n",
      "The main ambition of cross-validation is to test a model that is to test a model which is \n",
      "in the training phase and limit the problems like overfitting and to get insights on how to \n",
      "generalize the to an independent data set. \n",
      "79. How can the outlier values be treated? \n",
      "Answer: We can identify the outlier values by using graphical analysis method or by \n",
      "using Univariate method. It becomes easier and can be assessed individually when the \n",
      "outlier values are few but when the outlier values are more in number then these values \n",
      "required to be substituted either with the 1st or with the 99th percentile values. \n",
      "Below are the common ways to treat outlier values. \n",
      "To bring down and change the value \n",
      "To remove the value \n",
      "80. List the variants of backpropagation? \n",
      "Answer: Below mentioned are the three different variants of backpropagation \n",
      "Stochastic Gradient Descent: In this module, we take the help of the single training as \n",
      "an example for updating the parameters and for calculation of gradient. \n",
      "Batch Gradient Descent: in this backpropagation method, we consider whole data to \n",
      "calculating the gradient and executes the update at each iteration. \n",
      "Mini-batch Gradient Descent: It is considered as a popular optimization algorithm in \n",
      "\n",
      "\n",
      "15. deep learning. In this Mini-batch gradient Descent instead of single training example, \n",
      "mini-batch of samples is used. \n",
      "81. What is a Boltzmann machine? \n",
      "Answer: Boltzmann developed with simple learning algorithms that allow them to find \n",
      "the important information that was presented in the complex regularities in the data. \n",
      "These machines are generally used to optimize the quantity and the weights of the \n",
      "given problem. The learning program works very slow in networks due to many layers of \n",
      "feature detectors. When we consider Restricted Boltzmann Machines, this has a single \n",
      "algorithm feature detectors that make it faster compared to others. \n",
      "82. Do gradient descent methods at all times converge to a similar point? \n",
      "Answer: No, they do not because in some cases they reach a local minimum or a local \n",
      "optima point. You would not reach the global optima point. This is governed by the data \n",
      "and the starting conditions. \n",
      "83. What are Eigenvalue and Eigenvector? \n",
      "Answer: Eigenvectors are for understanding linear transformations. In data analysis, \n",
      "we usually calculate the eigenvectors for a correlation or covariance matrix. \n",
      "Eigenvalues are the directions along which a particular linear transformation acts by \n",
      "flipping, compressing or stretching. \n",
      "84. What is Selection Bias? \n",
      "Answer: Selection bias is a kind of error that occurs when the researcher decides who \n",
      "is going to be studied. It is usually associated with research where the selection of \n",
      "participants isn’t random. It is sometimes referred to as the selection effect. It is the \n",
      "distortion of statistical analysis, resulting from the method of collecting samples. If the \n",
      "selection bias is not taken into account, then some conclusions of the study may not be \n",
      "accurate. \n",
      "The types of selection bias include: \n",
      "Sampling bias: It is a systematic error due to a non-random sample of a population \n",
      "causing some members of the population to be less likely to be included than others \n",
      "resulting in a biased sample. \n",
      "Time interval: A trial may be terminated early at an extreme value (often for ethical \n",
      "reasons), but the extreme value is likely to be reached by the variable with the largest \n",
      "variance, even if all variables have a similar mean. \n",
      "Data: When specific subsets of data are chosen to support a conclusion or rejection of \n",
      "bad data on arbitrary grounds, instead of according to previously stated or generally \n",
      "agreed criteria. \n",
      "Attrition: Attrition bias is a kind of selection bias caused by attrition (loss of participants) \n",
      "discounting trial subjects/tests that did not run to completion. \n",
      "85. How does data cleaning plays a vital role in the analysis? \n",
      "Answer: Data cleaning can help in the analysis because: \n",
      "Cleaning data from multiple sources helps to transform it into a format that data analysts \n",
      "or data scientists can work with. \n",
      "Data Cleaning helps to increase the accuracy of the model in machine learning. \n",
      "It is a cumbersome process because as the number of data sources increases, the time \n",
      "taken to clean the data increases exponentially due to the number of sources and the \n",
      "volume of data generated by these sources. \n",
      "\n",
      "\n",
      "16. It might take up to 80% of the time for just cleaning data making it a critical part of the \n",
      "analysis task. \n",
      "86. Can you explain the difference between a Validation Set and a Test Set? \n",
      "Answer: A Validation set can be considered as a part of the training set as it is used for \n",
      "parameter selection and to avoid overfitting of the model being built. \n",
      "On the other hand, a Test Set is used for testing or evaluating the performance of a \n",
      "trained machine learning model. \n",
      "In simple terms, the differences can be summarized as; training set is to fit the \n",
      "parameters i.e. weights and test set is to assess the performance of the model i.e. \n",
      "evaluating the predictive power and generalization. \n",
      "87. What do you mean by Deep Learning and Why has it become popular now? \n",
      "Answer: Deep Learning is nothing but a paradigm of machine learning which has \n",
      "shown incredible promise in recent years. This is because of the fact that Deep \n",
      "Learning shows a great analogy with the functioning of the human brain. \n",
      "Now although Deep Learning has been around for many years, the major \n",
      "breakthroughs from these techniques came just in recent years. \n",
      "This is because of two main reasons: \n",
      "The increase in the amount of data generated through various sources \n",
      "The growth in hardware resources required to run these models \n",
      "GPUs are multiple times faster and they help us build bigger and deeper deep learning \n",
      "models in comparatively less time than we required previously. \n",
      "88. What are the variants of Back Propagation? \n",
      "Answer: Stochastic Gradient Descent: We use only a single training example for \n",
      "calculation of gradient and update parameters. \n",
      "Batch Gradient Descent: We calculate the gradient for the whole dataset and perform \n",
      "the update at each iteration. \n",
      "Mini-batch Gradient Descent: It’s one of the most popular optimization algorithms. It’s a \n",
      "variant of Stochastic Gradient Descent and here instead of single training example, \n",
      "mini-batch of samples is used. \n",
      "89. Please explain the role of data cleaning in data analysis. \n",
      "Answer: Data cleaning can be a daunting task due to the fact that with the increase in \n",
      "the number of data sources, the time required for cleaning the data increases at an \n",
      "exponential rate. \n",
      "This is due to the vast volume of data generated by additional sources. Also, data \n",
      "cleaning can solely take up to 80% of the total time required for carrying out a data \n",
      "analysis task. \n",
      "Nevertheless, there are several reasons for using data cleaning in data analysis. \n",
      "Two of the most important ones are: \n",
      "Cleaning data from different sources helps in transforming the data into a format that is \n",
      "easy to work with \n",
      "Data cleaning increases the accuracy of a machine learning model \n",
      "\n",
      "\n",
      "17. 90. What do you understand by linear regression and logistic regression? \n",
      "Answer: Linear regression is a form of statistical technique in which the score of some \n",
      "variable Y is predicted on the basis of the score of a second variable X, referred to as \n",
      "the predictor variable. The Y variable is known as the criterion variable. \n",
      "Also known as the logit model, logistic regression is a statistical technique for predicting \n",
      "the binary outcome from a linear combination of predictor variables. \n",
      "91. What do you understand by Deep Learning? \n",
      "Answer: Deep Learning is a paradigm of machine learning that displays a great degree \n",
      "of analogy with the functioning of the human brain. It is a neural network method based \n",
      "on convolutional neural networks (CNN). \n",
      "Deep learning has a wide array of uses, ranging from social network filtering to medical \n",
      "image analysis and speech recognition. Although Deep Learning has been present for a \n",
      "long time, it’s only recently that it has gained worldwide acclaim. This is mainly due to: \n",
      "An increase in the amount of data generation via various sources \n",
      "The growth in hardware resources required for running Deep Learning models \n",
      "Caffe, Chainer, Keras, Microsoft Cognitive Toolkit, Pytorch, and TensorFlow are some \n",
      "of the most popular Deep Learning frameworks as of today. \n",
      "92. What is overfitting? \n",
      "Answer: Any prediction rate which has a high inconsistency between the training error \n",
      "and the test error leads ta a high business problem, if the error rate in training set is low \n",
      "and the error rate ithe n test set is high, then we can conclude it as overfitting model. \n",
      "93. Advantages of Tableau Prep? \n",
      "Answer: Tableau Prep will reduce a lot of time like how its parent software (Tableau) \n",
      "does when creating impressive visualizations. The tool has a lot of potentials in taking \n",
      "professionals from data cleaning, merging step to creating final usable data that can be \n",
      "linked to the Tableau desktop for getting visualization and business insights. A lot of \n",
      "manual tasks will be reduced and the time can be used to make better findings and \n",
      "insights. \n",
      "94. How make you 3D plots/visualizations using NumPy/SciPy? \n",
      "Answer: Like 2D plotting, 3D graphics is beyond the scope of NumPy and SciPy, but \n",
      "just as in this 2D example, packages exist that integrate with NumPy. Matplotlib \n",
      "provides primary 3D plotting in \n",
      "the mplot3d subpackage, whereas Mayavi produces a wide range of high-quality 3D \n",
      "visualization features, utilizing the powerful VTK engine. \n",
      "95. Compare Sas, R, And Python Programming? \n",
      "Answer: \n",
      "SAS: it is one of the most widely used analytics tools used by some of the biggest \n",
      "companies on earth. It has some of the best statistical functions, graphical user \n",
      "interface, but can come with a price tag and hence it cannot be readily adopted by \n",
      "smaller enterprises \n",
      "R: The best part about R is that it is an Open Source tool and hence used generously \n",
      "by academia and the research community. It is a robust tool for statistical computation, \n",
      "graphical representation, and reporting. Due to its open source nature, it is always being \n",
      "updated with the latest features and then readily available to everybody. \n",
      "\n",
      "\n",
      "18. Python: Python is a powerful open source programming language that is easy to learn, \n",
      "works well with most other tools and technologies. The best part about Python is that it \n",
      "has innumerable libraries and community created modules making it very robust. It has \n",
      "functions for statistical operation, \n",
      "96. Describe Univariate, Bivariate And Multivariate Analysis? \n",
      "Answer: As the name suggests these are analysis methodologies having a single, \n",
      "double or multiple variables. \n",
      "So a univariate analysis will have one variable and due to this, there are no \n",
      "relationships, causes. The major aspect of the univariate analysis is to summarize the \n",
      "data and find the patterns within it to make actionable decisions. \n",
      "A Bivariate analysis deals with the relationship between two sets of data. These sets of \n",
      "paired data come from related sources, or samples. There are various tools to analyze \n",
      "such data including the chi-squared tests and t-tests when the data are having a \n",
      "correlation. \n",
      "If the data can be quantified then it can be analyzed using a graph plot or a scatterplot. \n",
      "The strength of the correlation between the two data sets will be tested in a Bivariate \n",
      "analysis. \n",
      "97. What Are Interpolation And Extrapolation? \n",
      "Answer: The terms of interpolation and extrapolation are extremely important in any \n",
      "statistical analysis. Extrapolation is the determination or estimation using a known set of \n",
      "values or facts by extending it and taking it to an area or region that is unknown. It is the \n",
      "technique of inferring something using data that is available. \n",
      "Interpolation, on the other hand, is the method of determining a certain value which falls \n",
      "between a certain set of values or the sequence of values. \n",
      "This is especially useful when you have data at the two extremities of a certain region \n",
      "but you don’t have enough data points at a specific point. This is when you deploy \n",
      "interpolation to determine the value that you need. \n",
      "98. How Is Data Modeling Different From Database Design? \n",
      "Answer: Data Modeling: It can be considered as the first step towards the design of a \n",
      "database. Data modeling creates a conceptual model based on the relationship \n",
      "between various data models. The process involves moving from the conceptual stage \n",
      "to the logical model to the physical schema. It involves the systematic method of \n",
      "applying data modeling techniques. \n",
      "Database Design: This is the process of designing the database. The database design \n",
      "creates an output which is a detailed data model of the database. Strictly speaking, \n",
      "database design includes the detailed logical model of a database but it can also \n",
      "include physical design choices and storage parameters. \n",
      "99. Differentiate between Data modeling and Database design? \n",
      "Answer: Data Modeling – Data modeling (or modeling) in software engineering is the \n",
      "process of creating a data model for an information system by applying formal data \n",
      "modeling techniques. \n",
      "\n",
      "\n",
      "19. Database Design– Database design is the system of producing a detailed data model \n",
      "of a database. The term database design can be used to describe many different parts \n",
      "of the design of an overall database system. \n",
      "100. What is selection bias and why does it matter? \n",
      "Answer: Selection bias is a product of inadequately or improperly randomized data \n",
      "leading to data sets that are not representative of the whole. In an interview, you should \n",
      "express the importance of this in terms of its effect on your solution. If your data is not \n",
      "representative, your solutions likely are not either. \n",
      "101. Differentiate between univariate, bivariate and multivariate analysis? \n",
      "Answer: Univariate analyses are descriptive statistical analysis techniques which can \n",
      "be differentiated based on the number of variables involved at a given point of time. For \n",
      "example, the pie charts of sales based on territory involve only one variable and can the \n",
      "analysis can be referred to as univariate analysis. \n",
      "The bivariate analysis attempts to understand the difference between two variables at a \n",
      "time as in a scatterplot. For example, analyzing the volume of sale and spending can be \n",
      "considered as an example of bivariate analysis. \n",
      "The multivariate analysis deals with the study of more than two variables to understand \n",
      "the effect of variables on the responses. \n",
      "102. Can you cite some examples where a false negative important than a false \n",
      "positive? \n",
      "Answer: 1: Assume there is an airport ‘A’ which has received high-security threats and \n",
      "based on certain characteristics they identify whether a particular passenger can be a \n",
      "threat or not. Due to a shortage of staff, they decide to scan passengers being predicted \n",
      "as risk positives by their predictive model. What will happen if a true threat customer is \n",
      "being flagged as non-threat by airport model? \n",
      "2: What if the Jury or judge decides to make a criminal go free? \n",
      "3: What if you rejected to marry a very good person based on your predictive model and \n",
      "you happen to meet him/her after a few years and realize that you had a false negative? \n",
      "103. Describe the structure of Artificial Neural Networks? \n",
      "Answer: Artificial Neural Networks works on the same principle as a biological Neural \n",
      "Network. It consists of inputs which get processed with weighted sums and Bias, with \n",
      "the help of Activation Functions. \n",
      "104. What do you understand by the Selection Bias? What are its various types? \n",
      "Answer: Selection bias is typically associated with research that doesn’t have a random \n",
      "selection of participants. It is a type of error that occurs when a researcher decides who \n",
      "is going to be studied. On some occasions, selection bias is also referred to as the \n",
      "selection effect. \n",
      "In other words, selection bias is a distortion of statistical analysis that results from the \n",
      "sample collecting method. When selection bias is not taken into account, some \n",
      "conclusions made by a research study might not be accurate. Following are the various \n",
      "types of selection bias: \n",
      "\n",
      "\n",
      "20. Sampling Bias: A systematic error resulting due to a non-random sample of a populace \n",
      "causing certain members of the same to be less likely included than others that results \n",
      "in a biased sample. \n",
      "Time Interval – A trial might be ended at an extreme value, usually due to ethical \n",
      "reasons, but the extreme value is most likely to be reached by the variable with the \n",
      "most variance, even though all variables have a similar mean. \n",
      "Data – Results when specific data subsets are selected for supporting a conclusion or \n",
      "rejection of bad data arbitrarily. \n",
      "Attrition – Caused due to attrition, i.e. loss of participants, discounting trial subjects or \n",
      "tests that didn’t run to completion. \n",
      "105. Please explain Recommender Systems along with an application? \n",
      "Answer: Recommender Systems is a subclass of information filtering systems, meant \n",
      "for predicting the preferences or ratings awarded by a user to some product. \n",
      "An application of a recommender system is the product recommendations section on \n",
      "Amazon. This section contains items based on the user’s search history and past \n",
      "orders. \n",
      "106. Could you explain how to define the number of clusters in a clustering \n",
      "algorithm? \n",
      "Answer: The primary objective of clustering is to group together similar identities in \n",
      "such a way that while entities within a group are similar to each other, the groups \n",
      "remain different from one another. \n",
      "Generally, Within Sum of Squares is used for explaining the homogeneity within a \n",
      "cluster. For defining the number of clusters in a clustering algorithm, WSS is plotted for \n",
      "a range pertaining to a number of clusters. The resultant graph is known as the Elbow \n",
      "Curve. \n",
      "The Elbow Curve graph contains a point that represents the point post which there \n",
      "aren’t any decrements in the WSS. This is known as the bending point and represents K \n",
      "in K–Means. \n",
      "Although the aforementioned is the widely-used approach, another important approach \n",
      "is the Hierarchical clustering. In this approach, dendrograms are created first and then \n",
      "distinct groups are identified from there. \n",
      "106. What are the types of machine learning? \n",
      "Answer: \n",
      "• \n",
      "Supervised learning \n",
      "• \n",
      "Unsupervised learning \n",
      "• \n",
      "Reinforcement Learning \n",
      "107. What is a Random Forest? \n",
      "Answer: Random forest is a versatile method in machine learning that performs both \n",
      "classification and regression tasks. It also helps in areas like treats missing values, \n",
      "dimensionality reduction, and outlier values. It is like gathering the various weak \n",
      "modules comes together to form a robust model \n",
      "108. What is Reinforcement learning? \n",
      "Answer: Reinforcement learning maps the situations to what to do and how to map \n",
      "actions. The end result of this Reinforcement learning is to maximize the numerical \n",
      "reward signal. The learner is not defined with what action to do next but instead must \n",
      "\n",
      "\n",
      "21. discover which actions will give the maximum reward. Reinforcement learning is \n",
      "developed from the learning process of human beings. It works based on the \n",
      "reward/penalty mechanism. \n",
      "109. What does P-value signify about the statistical data? \n",
      "Answer: P-value is used to determine the significance of results after a hypothesis test \n",
      "in statistics. P-value helps the readers to draw conclusions and is always between 0 \n",
      "and 1. \n",
      "P-Value – 0.05 denotes weak evidence against the null hypothesis which means the \n",
      "null hypothesis cannot be rejected. \n",
      "P-value -0.05 denotes strong evidence against the null hypothesis which means the null \n",
      "hypothesis can be rejected. \n",
      "P-value -0.05is the marginal value indicating it is possible to go either way. \n",
      "Get hands-on experience for your interviews with free access to the solved code \n",
      "example. \n",
      "110. What is an example of a data set with a non-Gaussian distribution? \n",
      "Answer:  The Gaussian distribution is part of the Exponential family of distributions, but \n",
      "there are a lot more of them, with the same sort of ease of use, in many cases, and if \n",
      "the person doing the machine learning has a solid grounding in statistics, they can be \n",
      "utilized where appropriate \n",
      "111. How regularly must an algorithm be updated? \n",
      "Answer: \n",
      "You will want to update an algorithm when: \n",
      "You want the model to evolve as data streams through infrastructure \n",
      "The underlying data source is changing \n",
      "There is a case of non-stationarity \n",
      "Planning for Data Science Certification in R – Programming? Here’re 100 Data Science \n",
      "Foundations Questions. Take this free practice test to know where you stand. \n",
      "112. How has your prior experience prepared you for a role in data science? \n",
      "Answer: This question helps determine the candidate’s experience from a holistic \n",
      "perspective and reveals experience in demonstrating interpersonal, communication and \n",
      "technical skills. It is important to understand this because data scientists must be able to \n",
      "communicate their findings, work in a team environment and have the skills to perform \n",
      "the task. \n",
      "113. What is Unsupervised learning? \n",
      "Answer: Unsupervised learning is a type of machine learning algorithm used to draw \n",
      "inferences from datasets consisting of input data without labeled responses. \n",
      "Algorithms: Clustering, Anomaly Detection, Neural Networks, and Latent Variable \n",
      "Models \n",
      "Data Science Mock interviews for you \n",
      "Interviews by Industry ExpertsPersonalized detailed interview feedback access to \n",
      "exclusive and curated content \n",
      "E.g. In the same example, a fruit clustering will categorize as “fruits with soft skin and \n",
      "lots of dimples”, “fruits with shiny hard skin” and “elongated yellow fruits”. \n",
      "\n",
      "\n",
      "22. 114. Could you draw a comparison between overfitting and underfitting? \n",
      "Answer: In order to make reliable predictions on general untrained data in machine \n",
      "learning and statistics, it is required to fit a  model to a set of training data. Overfitting \n",
      "and underfitting are two of the most common modeling errors that occur while doing so. \n",
      "Following are the various differences between overfitting and underfitting: \n",
      "Definition – A statistical model suffering from overfitting describes some random error or \n",
      "noise in place of the underlying relationship. When underfitting occurs, a statistical \n",
      "model or machine learning algorithm fails in capturing the underlying trend of the data. \n",
      "Occurrence – When a statistical model or machine learning algorithm is excessively \n",
      "complex, it can result in overfitting. Example of a complex model is one having too \n",
      "many parameters when compared to the total number of observations. Underfitting \n",
      "occurs when trying to fit a linear model to non-linear data. \n",
      "Poor Predictive Performance – Although both overfitting and underfitting yield poor \n",
      "predictive performance, the way in which each one of them does so is different. While \n",
      "the overfitted model overreacts to minor fluctuations in the training data, the underfit \n",
      "model under-reacts to even bigger fluctuations. \n",
      "115. Can you compare the validation set with the test set? \n",
      "Answer: A validation set is part of the training set used for parameter selection as well \n",
      "as for avoiding overfitting of the machine learning model being developed. On the \n",
      "contrary, a test set is meant for evaluating or testing the performance of a trained \n",
      "machine learning model. \n",
      "116. Please explain the concept of a Boltzmann Machine. \n",
      "Answer: A Boltzmann Machine features a simple learning algorithm that enables the \n",
      "same to discover fascinating features representing complex regularities present in the \n",
      "training data. It is basically used for optimizing the quantity and weight for some given \n",
      "problem. \n",
      "The simple learning algorithm involved in a Boltzmann Machine is very slow in networks \n",
      "that have many layers of feature detectors. \n",
      "117. What are the time series algorithms? \n",
      "Answer: Time series algorithms like ARIMA, ARIMAX, SARIMA, Holts winters are very \n",
      "interesting to learn and use as well to solve a lot of complex problems for businesses. \n",
      "Data preparation for time series analysis plays a vital role. The stationarity, seasonality, \n",
      "cycles, and noises need time and attention. Take as much time as you would like to \n",
      "make the data right. Then you can run any model on top of it. \n",
      "118. Now companies are heavily investing their money and time to make the \n",
      "dashboards. Why? \n",
      "Answer: To make stakeholders more aware of the business through data. Working on \n",
      "visualization projects helps you develop one of the key skills every data scientist should \n",
      "possess i.e. Thinking from the shoes of the end-user. \n",
      "If you’re learning any visualization tool, download a dataset from kaggle. Building charts \n",
      "and graphs for the dashboard should be the last step. Research more about the domain \n",
      "and think about the KPIs you would like to see in the dashboard if you’re going to be the \n",
      "end-user. Then start building the dashboard piece by piece. \n",
      "\n",
      "\n",
      "23. 119. Explain The Various Benefits Of R Language? \n",
      "Answer: The R programming language includes a set of a software suite that is used \n",
      "for graphical representation, statistical computing, data manipulation, and calculation. \n",
      "Some of the highlights of the R programming environment include the following: \n",
      "An extensive collection of tools for data analysis \n",
      "Operators for performing calculations on matrix and array \n",
      "Data analysis technique for graphical representation \n",
      "A highly developed yet simple and effective programming language \n",
      "It extensively supports machine learning applications \n",
      "It acts as a connecting link between various software, tools, and datasets \n",
      "Create high-quality reproducible analysis that is flexible and powerful \n",
      "Provides a robust package ecosystem for diverse needs \n",
      "It is useful when you have to solve a data-oriented problem \n",
      "120. Why Data Cleansing Is Important In Data Analysis? \n",
      "Answer: With data coming in from multiple sources it is important to ensure that data is \n",
      "good enough for analysis. This is where data cleansing becomes extremely vital. Data \n",
      "cleansing extensively deals with the process of detecting and correcting data records, \n",
      "ensuring that data is complete and accurate and the components of data that are \n",
      "irrelevant are deleted or modified as per the needs. This process can be deployed in \n",
      "concurrence with data wrangling or batch processing. \n",
      "Once the data is cleaned it confirms with the rules of the data sets in the system. Data \n",
      "cleansing is an essential part of the data science because the data can be prone to \n",
      "error due to human negligence, corruption during transmission or storage among other \n",
      "things. Data cleansing takes a huge chunk of time and effort of a Data Scientist \n",
      "because of the multiple sources from which data emanates and the speed at which it \n",
      "comes. \n",
      " \n",
      "\n",
      "\n",
      "24. Top Deep Learning Interview Questions You Must Know\n",
      "1.3K Views\n",
      "Kurt\n",
      "Last updated on May 22,2019\n",
      "Deep Learning is one of the Hottest topics of 2018-19 and for a good reason. There have been so many advancements in the Industry wherein the time has come when machines or Computer\n",
      "Programs are actually replacing Humans. Arti+cial Intelligence is going to create 2.3 million Jobs by 2020 and to crack those job interview I have come up with a set of Deep Learning Interview\n",
      "Questions. I have divided this article into two sections:\n",
      "Basic Deep Learning Interview Questions\n",
      "Advance Deep Learning Interview Questions\n",
      "Basics Deep Learning Interview Questions\n",
      "Q1. Differentiate between AI, Machine Learning and Deep Learning.\n",
      "Artificial Intelligence is a technique which enables machines to mimic human behavior.\n",
      "Machine Learning is a subset of AI technique which uses statistical methods to enable machines to improve with experience.\n",
      "Deep learning is a subset of ML which make the computation of multi-layer neural network feasible. It uses Neural networks to simulate human-like decision making.\n",
      "Q2. Do you think Deep Learning is Better than Machine Learning? If so, why?\n",
      "Though traditional ML algorithms solve a lot of our cases, they are not useful while working with high dimensional data, that is where we have a large number of inputs and outputs. For example, in\n",
      "the case of handwriting recognition, we have a large amount of input where we will have a different type of inputs associated with different type of handwriting.\n",
      "The second major challenge is to tell the computer what are the features it should look for that will play an important role in predicting the outcome as well as to achieve better accuracy while\n",
      "doing so.\n",
      "Q3. What is Perceptron? And How does it Work?\n",
      "If we focus on the structure of a biological neuron, it has dendrites which are used to receive inputs. These inputs are summed in the cell body and using the Axon it is passed on to the next\n",
      "biological neuron as shown below.\n",
      "Dendrite: Receives signals from other neurons\n",
      "Cell Body: Sums all the inputs\n",
      "Axon: It is used to transmit signals to the other cells\n",
      "Similarly, a perceptron receives multiple inputs, applies various transformations and functions and provides an output. A Perceptron is a linear model used for binary classi+cation. It models a\n",
      "neuron which has a set of inputs, each of which is given a specific weight. The neuron computes some function on these weighted inputs and gives the output.\n",
      "\n",
      "Subscribe\n",
      "\n",
      "\n",
      "\n",
      "25. Q4. What is the role of weights and bias?\n",
      "For a perceptron, there can be one more input called bias. While the weights determine the slope of the classifier line, bias allows us to shift the line towards left or right. Normally bias is treated\n",
      "as another weighted input with the input value x\n",
      "Q5. What are the activation functions?\n",
      "Activation function translates the inputs into outputs. Activation function decides whether a neuron should be activated or not by calculating the weighted sum and further adding bias with it. The\n",
      "purpose of the activation function is to introduce non-linearity into the output of a neuron.\n",
      "There can be many Activation functions like:\n",
      "Linear or Identity\n",
      "Unit or Binary Step\n",
      "Sigmoid or Logistic\n",
      "Tanh\n",
      "ReLU\n",
      "Softmax\n",
      "Q6. Explain Learning of a Perceptron.\n",
      "1. Initializing the weights and threshold.\n",
      "2. Provide the input and calculate the output.\n",
      "3. Update the weights.\n",
      "4. Repeat Steps 2 and 3\n",
      "Wj (t+1) – Updated Weight\n",
      "Wj (t) – Old Weight\n",
      "d – Desired Output\n",
      "y – Actual Output\n",
      "x – Input\n",
      "Q7. What is the significance of a Cost/Loss function?\n",
      "A cost function is a measure of the accuracy of the neural network with respect to a given training sample and expected output. It provides the performance of a neural network as a whole. In\n",
      "deep learning, the goal is to minimize the cost function. For that, we use the concept of gradient descent.\n",
      "Q8. What is gradient descent?\n",
      "Gradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient.\n",
      "Stochastic Gradient Descent: Uses only a single training example to calculate the gradient and update parameters.\n",
      "Batch Gradient Descent: Calculate the gradients for the whole dataset and perform just one update at each iteration.\n",
      "Mini-batch Gradient Descent: Mini-batch gradient is a variation of stochastic gradient descent where instead of single training example, mini-batch of samples is used. It’s one of the most\n",
      "popular optimization algorithms.\n",
      "Q9. What are the benefits of mini-batch gradient descent?\n",
      "This is more efficient compared to stochastic gradient descent.\n",
      "The generalization by finding the flat minima.\n",
      "Mini-batches allows help to approximate the gradient of the entire training set which helps us to avoid local minima.\n",
      "0.\n",
      "\n",
      "\n",
      "26. Q10.What are the steps for using a gradient descent algorithm?\n",
      "Initialize random weight and bias.\n",
      "Pass an input through the network and get values from the output layer.\n",
      "Calculate the error between the actual value and the predicted value.\n",
      "Go to each neuron which contributes to the error and then change its respective values to reduce the error.\n",
      "Reiterate until you find the best weights of the network.\n",
      "Q11. Create a Gradient Descent in python.\n",
      "Q12. What are the shortcomings of a single layer perceptron?\n",
      "Well, there are two major problems:\n",
      "Single-Layer Perceptrons cannot classify non-linearly separable data points.\n",
      "Complex problems, that involve a lot of parameters cannot be solved by Single-Layer Perceptrons\n",
      "Q13. What is a Multi-Layer-Perceptron\n",
      "A multilayer perceptron (MLP) is a deep, arti+cial neural network. It is composed of more than one perceptron. They are composed of an input layer to receive the signal, an output layer that makes\n",
      "a decision or prediction about the input, and in between those two, an arbitrary number of hidden layers that are the true computational engine of the MLP.\n",
      "Q14. What are the different parts of a multi-layer perceptron?\n",
      "Input Nodes: The Input nodes provide information from the outside world to the network and are together referred to as the “Input Layer”.  No computation is performed in any of the Input\n",
      "nodes – they just pass on the information to the hidden nodes.\n",
      "Hidden Nodes: The Hidden nodes perform computations and transfer information from the input nodes to the output nodes. A collection of hidden nodes forms a “Hidden Layer”. While a network\n",
      "will only have a single input layer and a single output layer, it can have zero or multiple Hidden Layers.\n",
      "Output Nodes: The Output nodes are collectively referred to as the “Output Layer” and are responsible for computations and transferring information from the network to the outside world.\n",
      "Q15. What Is Data Normalization And Why Do We Need It?\n",
      "Data normalization is very important preprocessing step, used to rescale values to +t in a speci+c range to assure better convergence during backpropagation. In general, it boils down to\n",
      "subtracting the mean of each data point and dividing by its standard deviation.\n",
      "These were some basic Deep Learning Interview Questions. Now, let’s move on to some advanced ones.\n",
      "Advance Interview Questions\n",
      "Q16. Which is Better Deep Networks or Shallow ones? and Why?\n",
      "Both the Networks, be it shallow or Deep are capable of approximating any function. But what matters is how precise that network is in terms of getting the results. A shallow network works with\n",
      "only a few features, as it can’t extract more. But a deep network goes deep by computing efficiently and working on more features/parameters.\n",
      "Q17. Why is Weight Initialization important in Neural Networks?\n",
      "Weight initialization is one of the very important steps. A bad weight initialization can prevent a network from learning but good weight initialization helps in giving a quicker convergence and a\n",
      "better overall error.\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "params = [weights_hidden, weights_output, bias_hidden, bias_output]\n",
      " \n",
      "def sgd(cost, params, lr=0.05):\n",
      " \n",
      "grads = T.grad(cost=cost, wrt=params)\n",
      "updates = []\n",
      " \n",
      "for p, g in zip(params, grads):\n",
      "updates.append([p, p - g * lr])\n",
      " \n",
      "return updates\n",
      " \n",
      "updates = sgd(cost, params)\n",
      "\n",
      "\n",
      "27. Biases can be generally initialized to zero. The rule for setting the weights is to be close to zero without being too small.\n",
      "Q18. What’s the difference between a feed-forward and a backpropagation neural network?\n",
      "A Feed-Forward Neural Network is a type of Neural Network architecture where the connections are “fed forward”, i.e. do not form cycles.  The term “Feed-Forward” is also used when you input\n",
      "something at the input layer and it travels from input to hidden and from hidden to the output layer.\n",
      "Backpropagation is a training algorithm consisting of 2 steps:\n",
      "Feed-Forward the values.\n",
      "Calculate the error and propagate it back to the earlier layers.\n",
      "So to be precise, forward-propagation is part of the backpropagation algorithm but comes before back-propagating.\n",
      "Q19. What are the Hperparameteres? Name a few used in any Neural Network.\n",
      "Hyperparameters are the variables which determine the network structure(Eg: Number of Hidden Units) and the variables which determine how the network is trained(Eg: Learning Rate).\n",
      "Hyperparameters are set before training.\n",
      "Number of Hidden Layers\n",
      "Network Weight Initialization\n",
      "Activation Function\n",
      "Learning Rate\n",
      "Momentum\n",
      "Number of Epochs\n",
      "Batch Size\n",
      "Q20. Explain the different Hyperparameters related to Network and Training.\n",
      "Network Hyperparameters\n",
      "The number of Hidden Layers: Many hidden units within a layer with regularization techniques can increase accuracy. Smaller number of units may cause underfitting.\n",
      "Network Weight Initialization: Ideally, it may be better to use different weight initialization schemes according to the activation function used on each layer. Mostly uniform distribution is used.\n",
      "Activation function: Activation functions are used to introduce nonlinearity to models, which allows deep learning models to learn nonlinear prediction boundaries.\n",
      "Training Hyperparameters\n",
      "Learning Rate: The learning rate de+nes how quickly a network updates its parameters. Low learning rate slows down the learning process but converges smoothly. Larger learning rate speeds up\n",
      "the learning but may not converge.\n",
      "Momentum: Momentum helps to know the direction of the next step with the knowledge of the previous steps. It helps to prevent oscillations. A typical choice of momentum is between 0.5 to 0.9.\n",
      "The number of epochs: Number of epochs is the number of times the whole training data is shown to the network while training. Increase the number of epochs until the validation accuracy\n",
      "starts decreasing even when training accuracy is increasing(overfitting).\n",
      "Batch size: Mini batch size is the number of sub-samples given to the network after which parameter update happens. A good default for batch size might be 32. Also try 32, 64, 128, 256, and so\n",
      "on.\n",
      "Q21. What is Dropout?\n",
      "Dropout is a regularization technique to avoid over+tting thus increasing the generalizing power. Generally, we should use a small dropout value of 20%-50% of neurons with 20% providing a good\n",
      "starting point. A probability too low has minimal effect and a value too high results in under-learning by the network.\n",
      "Use a larger network. You are likely to get better performance when dropout is used on a larger network, giving the model more of an opportunity to learn independent representations.\n",
      "Q22. In training a neural network, you notice that the loss does not decrease in the few starting epochs. What could be the reason?\n",
      "The reasons for this could be:\n",
      "The learning is rate is low\n",
      "Regularization parameter is high\n",
      "Stuck at local minima\n",
      "\n",
      "\n",
      "28. Q23. Name a few deep learning frameworks\n",
      "TensorFlow\n",
      "Caffe\n",
      "The Microsoft Cognitive Toolkit/CNTK\n",
      "Torch/PyTorch\n",
      "MXNet\n",
      "Chainer\n",
      "Keras\n",
      "Q24. What are Tensors?\n",
      "Tensors are nothing but a de facto for representing the data in deep learning. They are just multidimensional arrays, that allows you to represent data having higher dimensions. In general, Deep\n",
      "Learning you deal with high dimensional data sets where dimensions refer to different features present in the data set.\n",
      "Q25. List a few advantages of TensorFlow?\n",
      "It has platform flexibility\n",
      "It is easily trainable on CPU as well as GPU for distributed computing.\n",
      "TensorFlow has auto differentiation capabilities\n",
      "It has advanced support for threads, asynchronous computation, and queue es.\n",
      "It is a customizable and open source.\n",
      "Q26. What is Computational Graph?\n",
      "A computational graph is a series of TensorFlow operations arranged as nodes in the graph. Each node takes zero or more tensors as input and produces a tensor as output.\n",
      "Basically, one can think of a Computational Graph as an alternative way of conceptualizing mathematical calculations that takes place in a TensorFlow program. The operations assigned to different\n",
      "nodes of a Computational Graph can be performed in parallel, thus, providing better performance in terms of computations.\n",
      "Q27. What is a CNN?\n",
      "Convolutional neural network (CNN, or ConvNet) is a class of deep neural networks, most commonly applied to analyzing visual imagery. Unlike neural networks, where the input is a vector, here\n",
      "the input is a multi-channeled image. CNNs use a variation of multilayer perceptrons designed to require minimal preprocessing.\n",
      "Q28. Explain the different Layers of CNN.\n",
      "There are four layered concepts we should understand in Convolutional Neural Networks:\n",
      "Convolution: The convolution layer comprises of a set of independent +lters. All these +lters are initialized randomly and become our parameters which will be learned by the network\n",
      "subsequently.\n",
      "ReLu: This layer is used with the convolutional layer.\n",
      "\n",
      "\n",
      "29. Pooling: Its function is to progressively reduce the spatial size of the representation to reduce the number of parameters and computation in the network. Pooling layer operates on each feature\n",
      "map independently.\n",
      "Full Connectedness: Neurons in a fully connected layer have full connections to all activations in the previous layer, as seen in regular Neural Networks. Their activations can hence be computed\n",
      "with a matrix multiplication followed by a bias offset.\n",
      "Q29. What is an RNN?\n",
      "Recurrent Networks are a type of arti+cial neural network designed to recognize patterns in sequences of data, such as text, genomes, handwriting, the spoken word, numerical times series data.\n",
      "Recurrent Neural Networks use backpropagation algorithm for training  Because of their internal memory, RNN’s are able to remember important things about the input they received, which\n",
      "enables them to be very precise in predicting what’s coming next.\n",
      "Q30. What are some issues faced while training an RNN?\n",
      "Recurrent Neural Networks use backpropagation algorithm for training, but it is applied for every timestamp. It is commonly known as Back-propagation Through Time (BTT).\n",
      "There are some issues with Back-propagation such as:\n",
      "Vanishing Gradient\n",
      "Exploding Gradient\n",
      "Q31. What is Vanishing Gradient? And how is this harmful?\n",
      "When we do Back-propagation, the gradients tend to get smaller and smaller as we keep on moving backward in the Network. This means that the neurons in the Earlier layers learn very slowly as\n",
      "compared to the neurons in the later layers in the Hierarchy.\n",
      "Earlier layers in the Network are important because they are responsible to learn and detecting the simple patterns and are actually the building blocks of our Network.\n",
      "Obviously, if they give improper and inaccurate results, then how can we expect the next layers and the complete Network to perform nicely and produce accurate results. The Training process\n",
      "takes too long and the Prediction Accuracy of the Model will decrease.\n",
      "Q32. What is Exploding Gradient Descent?\n",
      "Exploding gradients are a problem when large error gradients accumulate and result in very large updates to neural network model weights during training.\n",
      "Gradient Descent process works best when these updates are small and controlled. When the magnitudes of the gradients accumulate, an unstable network is likely to occur, which can cause poor\n",
      "prediction of results or even a model that reports nothing useful what so ever.\n",
      "Q33. Explain the importance of LSTM.\n",
      "Long short-term memory(LSTM) is an arti+cial recurrent neural network architecture used in the +eld of deep learning. Unlike standard feedforward neural networks, LSTM has feedback\n",
      "connections that make it a “general purpose computer”. It can not only process single data points, but also entire sequences of data.\n",
      "They are a special kind of Recurrent Neural Networks which are capable of learning long-term dependencies.\n",
      "Q34. What are capsules in Capsule Neural Network?\n",
      "Capsules are a vector specifying the features of the object and its likelihood. These features can be any of the instantiation parameters like position, size, orientation, deformation, velocity, hue,\n",
      "texture and much more.\n",
      "\n",
      "\n",
      "30. A capsule can also specify its attributes like angle and size so that it can represent the same generic information. Now, just like a neural network has layers of neurons, a capsule network can have\n",
      "layers of capsules.\n",
      "Now, let’s continue this Deep Learning Interview Questions and move to the section of autoencoders and RBMs.\n",
      "Q35. Explain Autoencoders and it’s uses.\n",
      "An autoencoder neural network is an Unsupervised Machine learning algorithm that applies backpropagation, setting the target values to be equal to the inputs. Autoencoders are used to reduce\n",
      "the size of our inputs into a smaller representation. If anyone needs the original data, they can reconstruct it from the compressed data.\n",
      "Q36. In terms of Dimensionality Reduction, How does Autoencoder differ from PCAs?\n",
      "An autoencoder can learn non-linear transformations with a non-linear activation function and multiple layers.\n",
      "It doesn’t have to learn dense layers. It can use convolutional layers to learn which is better for video, image and series data.\n",
      "It is more efficient to learn several layers with an autoencoder rather than learn one huge transformation with PCA.\n",
      "An autoencoder provides a representation of each layer as the output.\n",
      "It can make use of pre-trained layers from another model to apply transfer learning to enhance the encoder/decoder.\n",
      "Q37. Give some real-life examples where autoencoders can be applied.\n",
      "Image Coloring: Autoencoders are used for converting any black and white picture into a colored image. Depending on what is in the picture, it is possible to tell what the color should be.\n",
      "Feature variation: It extracts only the required features of an image and generates the output by removing any noise or unnecessary interruption.\n",
      "Dimensionality Reduction: The reconstructed image is the same as our input but with reduced dimensions. It helps in providing a similar image with a reduced pixel value.\n",
      "Denoising Image: The input seen by the autoencoder is not the raw input but a stochastically corrupted version. A denoising autoencoder is thus trained to reconstruct the original input from the\n",
      "noisy version.\n",
      "Q38. what are the different layers of Autoencoders?\n",
      "An Autoencoder consist of three layers:\n",
      "Encoder\n",
      "Code\n",
      "Decoder\n",
      "Q39. Explain the architecture of an Autoencoder.\n",
      "Encoder: This part of the network compresses the input into a latent space representation. The encoder layer encodes the input image as a compressed representation in a reduced dimension.\n",
      "The compressed image is the distorted version of the original image.\n",
      "\n",
      "\n",
      "31. Code: This part of the network represents the compressed input which is fed to the decoder.\n",
      "Decoder: This layer decodes the encoded image back to the original dimension. The decoded image is a lossy reconstruction of the original image and it is reconstructed from the latent space\n",
      "representation.\n",
      "Q40. What is a Bottleneck in autoencoder and why is it used?\n",
      "The layer between the encoder and decoder, ie. the code is also known as Bottleneck. This is a well-designed approach to decide which aspects of observed data are relevant information and what\n",
      "aspects can be discarded.\n",
      "It does this by balancing two criteria:\n",
      "Compactness of representation, measured as the compressibility.\n",
      "It retains some behaviourally relevant variables from the input.\n",
      "Q41. Is there any variation of Autoencoders?\n",
      "Convolution Autoencoders\n",
      "Sparse Autoencoders\n",
      "Deep Autoencoders\n",
      "Contractive Autoencoders\n",
      "Q42. What are Deep Autoencoders?\n",
      "The extension of the simple Autoencoder is the Deep Autoencoder. The +rst layer of the Deep Autoencoder is used for +rst-order features in the raw input. The second layer is used for second-\n",
      "order features corresponding to patterns in the appearance of first-order features. Deeper layers of the Deep Autoencoder tend to learn even higher-order features.\n",
      "A deep autoencoder is composed of two, symmetrical deep-belief networks:\n",
      "First four or five shallow layers representing the encoding half of the net.\n",
      "The second set of four or five layers that make up the decoding half.\n",
      "Q43. What is a Restricted Boltzmann Machine?\n",
      "Restricted Boltzmann Machine is an undirected graphical model that plays a major role in Deep Learning Framework in recent times.\n",
      "It is an algorithm which is useful for dimensionality reduction, classification, regression, collaborative filtering, feature learning, and topic modeling.\n",
      "Q44. How Does RBM differ from Autoencoders?\n",
      "Autoencoder is a simple 3-layer neural network where output units are directly connected back to input units. Typically, the number of hidden units is much less than the number of visible ones.\n",
      "The task of training is to minimize an error or reconstruction, i.e. find the most efficient compact representation for input data.\n",
      "RBM shares a similar idea, but it uses stochastic units with particular distribution instead of deterministic distribution. The task of training is to +nd out how these two sets of variables are actually\n",
      "\n",
      "\n",
      "32. O\n",
      "Interview Prep: 40 Artificial \n",
      "Intelligence Questions\n",
      "ver the last decade, artificial intelligence (AI) has grown \n",
      "from a pipe dream into the driving force behind the fourth \n",
      "industrial revolution. If you browse through the world’s \n",
      "leading job boards, you’ll find that it’s at the heart of some of the \n",
      "most in-demand tech careers today.\n",
      "“Everyone’s trying to figure out ways to optimize their businesses and \n",
      "their practices, and how to automate and make their day-to-day lives \n",
      "a little bit easier, or a little bit more productive and functional,” \n",
      "www.springboard.com\n",
      "20 mins read\n",
      "\n",
      "\n",
      "33. notes Stephen Zafarino, vice president of national recruiting at \n",
      "Mondo.\n",
      "So, whether your next job interview is related to data science, \n",
      "machine learning (ML), or deep learning (DL), you can bet that artifi-\n",
      "cial intelligence questions will come up.\n",
      "Artificial Intelligence Questions: \n",
      "Categories\n",
      "Because it’s a broad area of computer science, AI questions will keep \n",
      "popping up in various job interview scenarios. To make it easier for \n",
      "you to navigate through this space, we have curated a list of questions \n",
      "about artificial intelligence and divided them into multiple categories. \n",
      "So whether you’re hoping to move up the data science career ladder \n",
      "or looking to start your first machine learning internship, make sure \n",
      "that you brush up on these AI interview questions and answers so you \n",
      "can walk into your next interview oozing confidence.\n",
      "Artificial Intelligence Questions: \n",
      "Introduction to AI\n",
      "If your AI interview is for an internship, there’s a good chance that \n",
      "the interviewer will try to break the ice and make you feel more com-\n",
      "fortable by asking some “simple” general interest questions. \n",
      "These types of questions usually cover the basics, so even if they \n",
      "sound straightforward, you have to make sure that you don’t get \n",
      "stumped (seemingly simple questions require that your answer be \n",
      "\n",
      "\n",
      "34. delivered easily and flawlessly). However, it can quickly get more \n",
      "involved, so you have to be ready for whatever they throw at you.\n",
      "Related: The Most Common Machine Learning Terms, Explained\n",
      "1. What is artificial intelligence?\n",
      "AI can be described as an area of computer science that simulates \n",
      "human intelligence in machines. It’s about smart algorithms making \n",
      "decisions based on the available data. \n",
      "Whether it’s Amazon’s Alexa or a self-driving car, the goal is to mimic \n",
      "human intelligence at lightning speed (and with a reduced rate of \n",
      "error).\n",
      "More reading: What is AI? Everything you need to know about Artifi-\n",
      "cial Intelligence\n",
      "2. What are intelligent agents?\n",
      "An intelligent agent is an autonomous entity that leverages sensors to \n",
      "understand a situation and make decisions. It can also use actuators \n",
      "to perform both simple and complex tasks.\n",
      "In the beginning, it might not be so great at performing a task, but it \n",
      "will improve over time. The Roomba vacuum cleaner is an excellent \n",
      "example of this. \n",
      "More reading: Intelligent agents defending for an IoT world\n",
      "3. What’s the most popular programming language \n",
      "used in AI?\n",
      "The open-source modular programming language Python leads the AI \n",
      "industry because of its simplicity and predictable coding behavior. \n",
      "\n",
      "\n",
      "35. Its popularity can be attributed to open-source libraries like Mat-\n",
      "plotlib and NumPy, efficient frameworks such as Scikit-learn, and \n",
      "practical version libraries like Tensorflow and VTK.\n",
      "There’s a chance that the interviewer might keep the conversation \n",
      "going and ask you for more examples. If that happens, you can men-\n",
      "tion the following:\n",
      "• Java\n",
      "• Julia\n",
      "• Haskell\n",
      "• Lisp\n",
      "More reading: The 5 Best Programming Languages for AI\n",
      "4. What are AI neural networks?\n",
      "Neural networks in AI mathematically model how the human brain \n",
      "works. This approach enables the machine to think and learn as \n",
      "humans do. This is how smart technology today recognizes speech, \n",
      "objects, and more.  \n",
      "More reading: Exploring Neural Networks with Activation Atlases\n",
      "5. What’s the difference between strong AI and weak \n",
      "AI?\n",
      "The difference between the two is just like the terms sound. Strong AI \n",
      "can successfully imitate human intelligence and is at the core of \n",
      "advanced robotics. \n",
      "Weak AI can only predict specific characteristics that resemble human \n",
      "intelligence. Alexa and Siri are excellent examples of weak AI.\n",
      "Strong AI \n",
      "• Can be applied widely \n",
      "• Extensive scope\n",
      "\n",
      "\n",
      "36. • Human-level intelligence\n",
      "• Processes data by using clustering and association\n",
      "Weak AI\n",
      "• Can be great at performing some simple tasks\n",
      "• Uses both supervised and unsupervised learning\n",
      "• The scope can be minimal\n",
      "More reading: What’s the Difference Between Weak and Strong AI?\n",
      "6. What’s the difference between AI and ML?\n",
      "(Source)\n",
      "AI and ML are closely related, but these terms aren’t interchangeable. \n",
      "ML actually falls under the umbrella of AI. It demands that machines \n",
      "carry out tasks in the same way that humans do. \n",
      "The current application of ML in AI is based around the idea that we \n",
      "should enable access to data so machines can observe and learn for \n",
      "themselves.\n",
      "More reading: What’s the Difference between Machine Learning and \n",
      "AI?\n",
      "\n",
      "\n",
      "37. 7. How would you describe ML to a non-technical \n",
      "person?\n",
      "ML is geared toward pattern recognition. A great example of this is \n",
      "your Facebook newsfeed and Netflix’s recommendation engine. \n",
      "In this scenario, ML algorithms observe patterns and learn from \n",
      "them. When you deploy an ML program, it will keep learning and \n",
      "improving with each attempt.\n",
      "If the interviewer prods you to provide more real-world examples, \n",
      "you can list the following:\n",
      "• Amazon product recommendations\n",
      "• Fraud detection\n",
      "• Search ranking\n",
      "• Spam detection\n",
      "• Spell correction\n",
      "More reading: How do you explain Machine Learning and Data Mining \n",
      "to Non-Computer Science people?\n",
      "8. What are some examples of AI in use?\n",
      "Some compelling examples of AI applications are:\n",
      "• Chatbots\n",
      "• Facial recognition\n",
      "• Image tagging\n",
      "• Natural language processing\n",
      "• Sales prediction\n",
      "• Self-driving cars\n",
      "• Sentiment analysis\n",
      "More reading: Ask the AI experts: What are the applications of AI?\n",
      "\n",
      "\n",
      "38. 9. What’s a Turing test?\n",
      "The Turing test, named after Alan Turing, is a method of testing a \n",
      "machine’s human-level intelligence. For example, in a human-versus-\n",
      "machine scenario, a judge will be tasked with identifying which ter-\n",
      "minal was occupied by a human and which was occupied by a \n",
      "computer based on individual performance. \n",
      "Whenever a computer can pass off as a human, it’s deemed intelli-\n",
      "gent. The game has since evolved, but the premise remains the same. \n",
      "More reading: The Turing Test\n",
      "10. What’s TensorFlow?\n",
      "TensorFlow is an open-source framework dedicated to ML. It’s a com-\n",
      "prehensive and highly adaptable ecosystem of libraries, tools, and \n",
      "community resources that help developers build and deploy ML-pow-\n",
      "ered applications. Both AlphaGo and Google Cloud Vision were built \n",
      "on the Tensorflow platform.\n",
      "More reading: TensorFlow Tutorial from Scratch: Building a Deep \n",
      "Learning Model on Fashion MNIST Dataset (Part 1)\n",
      "11. Why is game theory important to AI?\n",
      "Game theory, developed by American mathematician Josh Nash, is \n",
      "essential to AI because it plays an underlying role in how these smart \n",
      "algorithms improve over time. \n",
      "At its most basic, AI is about algorithms that are deployed to find \n",
      "solutions to problems. Game theory is about players in opposition try-\n",
      "ing to achieve specific goals. As most aspects of life are about compe-\n",
      "tition, game theory has many meaningful real-world applications. \n",
      "These problems tend to be dynamic. Some game theory problems are \n",
      "natural candidates for AI algorithms. So, whenever game theory is \n",
      "\n",
      "\n",
      "39. applied, multiple AI agents that interact with each other will only \n",
      "care about utility to itself. \n",
      "Data scientists within this space should be aware of the following \n",
      "games:\n",
      "• Symmetric vs. asymmetric\n",
      "• Perfect vs. imperfect information\n",
      "• Cooperative vs. non-cooperative\n",
      "• Simultaneous vs. sequential\n",
      "• Zero-sum vs. non-zero-sum\n",
      "More reading: What is the connection between Game Theory and AI?\n",
      "12. In your opinion, how will AI impact application \n",
      "development?\n",
      "These types of questions help the interviewer ascertain your level of \n",
      "interest in the field. If you’re naturally passionate about AI and every-\n",
      "thing related to it, you should have some knowledge about current \n",
      "industry trends. \n",
      "So, if you have been actively following this space, you’ll know all \n",
      "about AIOps. In the coming months, you can expect AI to be more \n",
      "involved in how we build applications. It has the potential to trans-\n",
      "form how we use and manage the infrastructure at a micro and macro \n",
      "level. \n",
      "Some say that DevOps will be replaced by what they are calling AIOps \n",
      "because it allows developers to engage in accurate root cause analysis \n",
      "by combining big data, ML, and visualization. \n",
      "AIOps can be described as a multilayered platform that can be used to \n",
      "automate and improve IT operations. In this scenario, developers can \n",
      "leverage analytics and ML to collect and process data from a variety \n",
      "of sources. This information can then be analyzed in real time to \n",
      "identify and rectify problems.\n",
      "\n",
      "\n",
      "40. (Source)\n",
      "More reading: AIOps: Is DevOps Ready for an Infusion of Artificial \n",
      "Intelligence?\n",
      "13. What would you say are common misconceptions \n",
      "about AI?\n",
      "Many AI-related misconceptions are making the rounds in the age of \n",
      "“fake news.” The most common ones are:\n",
      "• AI will replace humans\n",
      "• AI systems aren’t safe\n",
      "• AI will lead to significant unemployment\n",
      "\n",
      "\n",
      "41. While these types of stories are common, they’re far from the truth. \n",
      "Even though some AI-based technology is able to complete some \n",
      "tasks—for example, analyzing zettabytes of data in less than a \n",
      "second—it still needs humans to gather the data and define the pat-\n",
      "terns for identification. \n",
      "So we aren’t near a reality where technology has the potential to \n",
      "replace us or our jobs. \n",
      "More reading: What’s Behind the Hype About Artificial Intelligence?\n",
      "14. Can you name the properties of a good knowledge \n",
      "representation system?\n",
      "From the perspective of systems theory, a good knowledge represen-\n",
      "tation system will have the following:\n",
      "• Acquisition efficiency to acquire and incorporate new data\n",
      "• Inferential adequacy to derive knowledge representation \n",
      "structures like symbols when new knowledge is learned \n",
      "from old knowledge\n",
      "• Inferential efficiency to enable the addition of data into \n",
      "existing knowledge structures to help the inference process\n",
      "• Representation adequacy to represent all the knowledge \n",
      "required in a specific domain\n",
      "More reading: Knowledge representation in AI\n",
      "15. What are the different types of keys in a \n",
      "relational database?\n",
      "There are a variety of keys in a relational database, including:\n",
      "• Alternate keys are candidate keys that exclude all primary \n",
      "keys.\n",
      "\n",
      "\n",
      "42. • Artificial keys are created by assigning a unique number to \n",
      "each occurrence or record when there aren’t any compound \n",
      "or standalone keys.\n",
      "• Compound keys are made by combining multiple elements \n",
      "to develop a unique identifier for a construct when there \n",
      "isn’t a single data element that uniquely identifies \n",
      "occurrences within a construct. Also known as a composite \n",
      "key or a concatenated key, compound keys consist of two or \n",
      "more attributes.\n",
      "• Foreign keys are groups of fields in a database record that \n",
      "point to a key field or a group of fields that create a key of \n",
      "another database record that’s usually in a different table. \n",
      "Often, foreign keys in one table refer to primary keys in \n",
      "another. As the referenced data can be linked together quite \n",
      "quickly, it can be critical to database normalization.\n",
      "• Natural keys are data elements that are stored within \n",
      "constructs and utilized as primary keys.\n",
      "• Primary keys are values that can be used to identify unique \n",
      "rows in a table and the attributes associated with them. For \n",
      "example, these can take the form of a Social Security \n",
      "number that’s related to a specific person. In a relational \n",
      "model of data, the primary key is the candidate key. It’s also \n",
      "the primary method used to identify a tuple in each possible \n",
      "relation.\n",
      "• Super keys are defined in the relational model as a set of \n",
      "attributes of a relation variable. It holds that all relations \n",
      "assigned to that variable don’t have any distinct tuples. \n",
      "They also don’t have the same values for the attributes in \n",
      "the set. Super keys also are defined as a set of attributes of \n",
      "a relational variable upon which all of the functionality \n",
      "depends.\n",
      "More reading: What are the different types of keys in RDBMS?\n",
      "\n",
      "\n",
      "43. Artificial Intelligence Questions: \n",
      "Statistics\n",
      "AI, ML, and data science all have a great deal of overlap, so it’s crucial \n",
      "to cover all bases before your AI interview. However, it’s important to \n",
      "note that these fields aren’t interchangeable. Although everything is \n",
      "relative, AI produces actions, ML produces predictions, and data sci-\n",
      "ence produces insights.\n",
      "So what kind of potential data science-related AI questions should you \n",
      "be prepared for? Let’s take a look. \n",
      "16. In Python’s standard library, what packages \n",
      "would you say are the most useful for data \n",
      "scientists? \n",
      "Python wasn’t built for data science. However, in recent years it has \n",
      "grown to become the go-to programming language for the following:\n",
      "• Machine learning\n",
      "• Predictive analytics\n",
      "• Simple data analytics \n",
      "• Statistics\n",
      "For data science projects, the following packages in the Python stand-\n",
      "ard library will make life easier and accelerate deliveries:\n",
      "• NumPy (to process large multidimensional arrays, \n",
      "extensive collections of high-level mathematical functions, \n",
      "and matrices)\n",
      "• Pandas (to leverage built-in methods for rapidly combining, \n",
      "filtering, and grouping data)\n",
      "• SciPy (to extend NumPy’s capabilities and solve tasks \n",
      "related to integral calculus, linear algebra, and probability \n",
      "theory)\n",
      "\n",
      "\n",
      "44. More reading: 20 Python Interview Questions and Answers—Start Pre-\n",
      "paring for Your Ideal Job\n",
      "17. What is collaborative filtering?\n",
      "Collaborative filtering can be described as a process of finding pat-\n",
      "terns from available information to build personalized recommenda-\n",
      "tions. You can find collaborative filtering in action when you visit \n",
      "websites like Amazon and IMDB.\n",
      "Also known as social filtering, this approach essentially makes sug-\n",
      "gestions based on the recommendations and preferences of other peo-\n",
      "ple who share similar interests. \n",
      "More reading: Collaborative Filtering\n",
      "18. Can you list some disadvantages related to linear \n",
      "models?\n",
      "There are many disadvantages to using linear models, but the main \n",
      "ones are:\n",
      "• Errors in linearity assumptions \n",
      "• Lacks autocorrelation\n",
      "• It can’t solve overfitting problems \n",
      "• You can’t use it to calculate outcomes or binary outcomes\n",
      "More reading: What are the limitations of linear regression modeling \n",
      "in data analysis?\n",
      "19. What’s a feature vector?\n",
      "A feature vector is an n-dimensional vector that contains essential \n",
      "information that describes the characteristics of an object. For exam-\n",
      "ple, it can be an object’s numerical features or a list of numbers taken \n",
      "from the output of a neural network layer. \n",
      "\n",
      "\n",
      "45. In AI and data science, feature vectors can be used to represent \n",
      "numeric or symbolic characteristics of an object in mathematical \n",
      "terms for seamless analysis.\n",
      "Let’s break this down. A data set is usually organized into multiple \n",
      "examples where each example will have several features. However, a \n",
      "feature vector won’t have the same feature for numerous examples. \n",
      "Instead, each example will correspond to one feature vector that will \n",
      "contain all the numerical values for that example object. \n",
      "Feature vectors are often stacked into a design matrix. In this sce-\n",
      "nario, each row will be a feature vector for one example. Each column \n",
      "will feature all the examples that correspond to that particular fea-\n",
      "ture. This means that it will be like a matrix, but with just one row \n",
      "and multiple columns (or a single column and multiple rows) like \n",
      "[1,2,3,5,6,3,2,0].\n",
      "More reading: Extract a feature vector for any image with PyTorch\n",
      "20. What are the typical characteristics of elements \n",
      "in a list and a dictionary?\n",
      "In lists, elements maintain their order unless they are explicitly com-\n",
      "manded to re-order. These can be made up of any data type that can \n",
      "be all the same or mixed. However, elements in lists can only be \n",
      "accessed via numeric, zero-based indices.\n",
      "In a dictionary, the order isn’t guaranteed. However, each entry will \n",
      "be assigned a key and a value. As a result, elements within a diction-\n",
      "ary can be accessed by using their individual key.\n",
      "So whenever you have a set of unique keys, you have to use a diction-\n",
      "ary. Whenever a collection of items are in order, you can use a list.\n",
      "It’s difficult to predict how an AI interview will unfold, so if they fol-\n",
      "low up by asking you how to get a list of all the keys in a dictionary, \n",
      "respond with the following:\n",
      "\n",
      "\n",
      "46. To obtain a list of keys in a dictionary, you’ll have to use the following \n",
      "function keys():\n",
      "mydict={‘a’:1,’b’:2,’c’:3,’e’:5}\n",
      "mydict.keys()\n",
      "dict_keys([‘a’, ‘b’, ‘c’, ‘e’])\n",
      "More reading: How to Sort Python Dictionaries by Key or Value\n",
      "21. What’s selection bias? What other types of biases \n",
      "could you encounter during sampling?\n",
      "When you’re dealing with a non-random sample, selection bias will \n",
      "occur due to flaws in the selection process. This happens when a sub-\n",
      "set of the data is consistently excluded because of a particular attrib-\n",
      "ute. This exclusion will distort results and influence the statistical \n",
      "significance of the test.\n",
      "Other types of biases include survivorship bias and undercoverage \n",
      "bias. It’s important to always consider and reduce such biases \n",
      "because you’ll want your smart algorithms to make accurate predic-\n",
      "tions based on the data.\n",
      "More reading: Mitigating Bias in AI Models\n",
      "22. What’s a random forest? Could you explain its \n",
      "role in AI?\n",
      "A random forest is a data construct that’s applied to ML projects to \n",
      "develop a large number of random decision trees while analyzing var-\n",
      "iables. \n",
      "\n",
      "\n",
      "47. (Source)\n",
      "These algorithms can be leveraged to improve the way technologies \n",
      "analyze complex data sets. The basic premise here is that multiple \n",
      "weak learners can be combined to build one strong learner. \n",
      "This is an excellent tool for AI and ML projects because it can work \n",
      "with large labeled and unlabeled data sets with a large number of \n",
      "attributes. It can also maintain accuracy when some data is missing. \n",
      "As it can model the importance of attributes, it can be used for dimen-\n",
      "sionality reduction.\n",
      "More reading: Machine Learning With Random Forest\n",
      "23. What’s an eigenvalue? What about an \n",
      "eigenvector?\n",
      "The directions along which a particular linear transformation com-\n",
      "presses, flips, or stretches is called eigenvalue. Eigenvectors are used \n",
      "to understand these linear transformations. \n",
      "\n",
      "\n",
      "48. For example, to make better sense of the covariance of the covariance \n",
      "matrix, the eigenvector will help identify the direction in which the \n",
      "covariances are going. The eigenvalues will express the importance of \n",
      "each feature. \n",
      "Eigenvalues and eigenvectors are both critical to computer vision and \n",
      "ML applications. The most popular of these is known as principal \n",
      "component analysis for dimensionality reduction (e.g., eigenfaces for \n",
      "face recognition).\n",
      "More reading: What are eigenvectors and eigenvalues?\n",
      "24. Would you use batch normalization? If so, can \n",
      "you explain why?\n",
      "The idea here is to standardize the data before sending it to another \n",
      "layer. This approach helps reduce the impact of previous layers by \n",
      "keeping the mean and variance constant. It also makes the layers \n",
      "independent of each other to achieve rapid convergence. For example, \n",
      "when we normalize features from 0 to 1 or from 1 to 100, it helps \n",
      "accelerate the learning cycle.\n",
      "Check out this video: Why Does Batch Normalization Work?\n",
      "Artificial Intelligence Questions: \n",
      "Programming\n",
      "AI interview questions are bound to enter the sphere of programming \n",
      "sooner rather than later. So let’s dive right into it with the following \n",
      "AI questions and answers.\n",
      "\n",
      "\n",
      "49. 25. What’s a hash table?\n",
      "There are two parts to a hash table. The first is an array, or the actual \n",
      "table where the data is stored, and the other is a mapping function \n",
      "that’s known as the hash function. \n",
      "It’s a data structure that implements an associative array abstract \n",
      "data type that can map key values. It can also compute an index into \n",
      "an array of slots or buckets where the desired value can be found.\n",
      "(Source)\n",
      "More reading: Basics of Hash Tables\n",
      "26. What are the different algorithm techniques you \n",
      "can use in AI and ML?\n",
      "Some algorithm techniques that can be leveraged are:\n",
      "• Learning to learn\n",
      "• Reinforcement learning (deep adversarial networks, \n",
      "q-learning, and temporal difference)\n",
      "• Semi-supervised learning\n",
      "\n",
      "\n",
      "50. • Supervised learning (decision trees, linear regression, naive \n",
      "bayes, nearest neighbor, neural networks, and support \n",
      "vector machines)\n",
      "• Transduction\n",
      "• Unsupervised learning (association rules and k-means \n",
      "clustering)\n",
      "More reading: Types of Machine Learning Algorithms You Should \n",
      "Know\n",
      "27. How would you go about choosing an algorithm \n",
      "to solve a business problem?\n",
      "First, you have to develop a “problem statement” that’s based on the \n",
      "problem provided by the business. This step is essential because it’ll \n",
      "help ensure that you fully understand the type of problem and the \n",
      "input and the output of the problem you want to solve.\n",
      "The problem statement should be simple and no more than a single \n",
      "sentence. For example, let’s consider enterprise spam that requires \n",
      "an algorithm to identify it. \n",
      "The problem statement would be: “Is the email fake/spam or not?”\n",
      "In this scenario, the identification of whether it’s fake/spam will be \n",
      "the output.\n",
      "Once you have defined the problem statement, you have to identify \n",
      "the appropriate algorithm from the following:\n",
      "• Any classification algorithm\n",
      "• Any clustering algorithm\n",
      "• Any regression algorithm\n",
      "• Any recommendation algorithm\n",
      "Which algorithm you use will depend on the specific problem you’re \n",
      "trying to solve. In this scenario, you can move forward with a cluster-\n",
      "\n",
      "\n",
      "51. ing algorithm and choose a k-means algorithm to achieve your goal of \n",
      "filtering spam from the email system.\n",
      "While examples aren’t always necessary when answering questions \n",
      "about artificial intelligence, sometimes it will help make it easier for \n",
      "you to get your point across.\n",
      "More reading: How to Choose ML Algorithm: Machine Learning Ques-\n",
      "tions & Answers Part – III\n",
      "28. When is it necessary to update an algorithm?\n",
      "You should update an algorithm when the underlying data source has \n",
      "been changed or whenever there’s a case of non-stationarity. The \n",
      "algorithm should also be updated when you want the model to evolve \n",
      "as data streams through the infrastructure.\n",
      "More reading: “The data has been changed” error when stepping from \n",
      "main form into subform\n",
      "29. What’s regularization?\n",
      "When you have underfitting or overfitting issues in a statistical \n",
      "model, you can use the regularization technique to resolve it. Regular-\n",
      "ization techniques like LASSO help penalize some model parameters if \n",
      "they are likely to lead to overfitting.\n",
      "If the interviewer follows up with a question about other methods \n",
      "that can be used to avoid overfitting, you can mention cross-valida-\n",
      "tion techniques such as k-folds cross-validation. \n",
      "Another approach is to keep the model simple by taking into account \n",
      "fewer variables and parameters. Doing this helps remove some of the \n",
      "noise in the training data.\n",
      "More reading: Machine Learning Explained: Regularization\n",
      "\n",
      "\n",
      "52. 30. What’s the difference between inductive, \n",
      "deductive, and abductive learning?\n",
      "Inductive learning describes smart algorithms that learn from a set \n",
      "of instances to draw conclusions. In statistical ML, k-nearest neighbor \n",
      "and support vector machine are good examples of inductive learning.\n",
      "There are three literals in (top-down) inductive learning: \n",
      "• Arithmetic literals\n",
      "• Equality and inequality\n",
      "• Predicates\n",
      "In deductive learning, the smart algorithms draw conclusions by fol-\n",
      "lowing a truth-generating structure (major premise, minor premise, \n",
      "and conclusion) and then improve them based on previous decisions. \n",
      "In this scenario, the ML algorithm engages in deductive reasoning \n",
      "using a decision tree.\n",
      "Abductive learning is a DL technique where conclusions are made \n",
      "based on various instances. With this approach, inductive reasoning \n",
      "is applied to causal relationships in deep neural networks.\n",
      "More reading: What’s the difference between “inductive, “deductive” \n",
      "and “abductive” reasoning?\n",
      "31. What steps would you take to evaluate the \n",
      "effectiveness of your ML model?\n",
      "You have to first split the data set into training and test sets. You also \n",
      "have the option of using a cross-validation technique to further seg-\n",
      "ment the data set into a composite of training and test sets within the \n",
      "data.\n",
      "Then you have to implement a choice selection of the performance \n",
      "metrics like the following:\n",
      "\n",
      "\n",
      "53. • Confusion matrix\n",
      "• Accuracy\n",
      "• Precision\n",
      "• Recall or sensitivity\n",
      "• Specificity\n",
      "• F1 score\n",
      "For the most part, you can use measures such as accuracy, confusion \n",
      "matrix, or F1 score. However, it’ll be critical for you to demonstrate \n",
      "that you understand the nuances of how each model can be measured \n",
      "by choosing the right performance measure to match the problem.\n",
      "More reading: Performance Metrics for Classification problems in \n",
      "Machine Learning\n",
      "32. What would you do if data in a data set were \n",
      "missing or corrupted?\n",
      "Whenever data is missing or corrupted, you either replace it with \n",
      "another value or drop those rows and columns altogether. In Pandas, \n",
      "both isNull() and dropNA() are handy tools to find missing or cor-\n",
      "rupted data and drop those values. You can also use the fillna()\n",
      "method to fill the invalid values in a placeholder—for example, “0.”\n",
      "More reading: 5 Ways To Handle Missing Values In Machine Learning \n",
      "Datasets\n",
      "33. Do you know how to build a simple neural \n",
      "network? Can you demonstrate how you would do it \n",
      "with Python code?\n",
      "These types of questions are designed to ascertain your programming \n",
      "skills. If you’re a coding ninja, you should be able to achieve this with \n",
      "nine lines of Python code.\n",
      "\n",
      "\n",
      "54. (Source)\n",
      "More reading: How to build a simple neural network in 9 lines of \n",
      "Python code\n",
      "34. Can you write a Python program to draw the \n",
      "flower shown below?\n",
      "(Source)\n",
      "\n",
      "\n",
      "55. Again, the interviewer is trying to test your coding skills. It’s always \n",
      "good to perceive these types of questions as an opportunity to show a \n",
      "potential employer what you can do. So demonstrate your program-\n",
      "ming skills without hesitation and with confidence. \n",
      "(Source)\n",
      "More reading: Drawing a flower with python turtle\n",
      "Artificial Intelligence Questions: General \n",
      "AI Interest\n",
      "The tech talent shortage has created fierce demand for your skills, but \n",
      "to land your “dream job” it’ll help if you can demonstrate your pas-\n",
      "sion for the field. Whether your next scheduled AI interview is with a \n",
      "startup or an established tech giant, be ready for wide-ranging ques-\n",
      "tions like the ones listed below.\n",
      "\n",
      "\n",
      "56. 35. Do you have research experience in AI?\n",
      "At present, a lot of work within the AI space is research-based. As a \n",
      "result, many organizations will be digging into your background to \n",
      "ascertain what kind of experience you have in this area. If you \n",
      "authored or co-authored research papers or have been supervised by \n",
      "industry leaders, make sure to share that information.\n",
      "In fact, take it a step further and have a summary of your research \n",
      "experience along with your research papers ready to share with the \n",
      "interviewing panel. \n",
      "However, if you don’t have any formal research experience, have an \n",
      "explanation ready. For example, you can talk about how your AI jour-\n",
      "ney started as a weekend hobby and grew into so much more within a \n",
      "space of two or three years.\n",
      "36. What’s the last AI-related research paper you \n",
      "read? What were your conclusions?\n",
      "If you’re passionate about AI, you have to keep up with scientific \n",
      "research within the field. An excellent place to start is by following \n",
      "ScienceDirect to keep track of published research papers along with \n",
      "what’s in the pipeline.\n",
      "\n",
      "\n",
      "57. (Source)\n",
      "37. What’s your favorite use case?\n",
      "Just like research, you should be up to date on what’s going on in the \n",
      "industry. As such, if you’re asked about use cases, make sure that you \n",
      "have a few examples in mind that you can share. Whenever possible, \n",
      "bring up your personal experiences. \n",
      "You can also share what’s happening in the industry. For example, if \n",
      "you’re interested in the use of AI in medical images, Health IT Analyt-\n",
      "ics has some interesting use cases:\n",
      "• Detecting Fractures And Other Musculoskeletal Injuries\n",
      "• Aiding In The Diagnosis Neurological Diseases\n",
      "• Flagging Thoracic Complications And Conditions\n",
      "• Screening For Common Cancers\n",
      "38. What conferences are you hoping to attend this \n",
      "year? Any keynote speeches you’re hoping to catch?\n",
      "Conferences are great places to network, attend workshops, learn, \n",
      "and grow. So if you’re planning to stick to a career in artificial intelli-\n",
      "gence, you should be going to some of these. For example, Deep \n",
      "Learning World has a great one every summer. \n",
      "This year’s event in Las Vegas will feature keynote speakers like Dr. \n",
      "Dyann Daley (founder and CEO of Predict Align Prevent), Siddha \n",
      "Ganju (solutions architect at Nvidia), and Dr. Alex Glushkovsky (prin-\n",
      "cipal data scientist at BMO Financial Group, and others). \n",
      "39. How is Google training data for self-driving cars?\n",
      "If you’re interested and heavily involved within this space, this ques-\n",
      "tion should be a no-brainer. If you know the answer, it’ll demonstrate \n",
      "your knowledge about a variety of ML methods and how ML is applied \n",
      "\n",
      "\n",
      "58. to autonomous vehicles. But even if you don’t know the answer, take \n",
      "a stab at it as it will show your creativity and inventive nature. \n",
      "Google has been using reCAPTCHA to source labeled data on store-\n",
      "fronts and traffic signs for many years now. The company also has \n",
      "been using training data collected by Sebastian Thrun, CEO of the \n",
      "Kitty Hawk Corporation and the co-founder (and former CEO) of \n",
      "Udacity. \n",
      "Such information, although it might not seem significant, will show a \n",
      "potential employer that you’re interested and excited about this field.\n",
      "More reading: Google X: Leveraging data and algorithms for self-driv-\n",
      "ing cars\n",
      "40. Where do you usually source your data sets?\n",
      "If you talk about AI projects that you’ve worked on in your free time, \n",
      "the interviewer will probably ask where you sourced your data sets. If \n",
      "you’re genuinely passionate about the field, you would have worked \n",
      "on enough projects to know where you can find free data sets.\n",
      "For example, here are some freely available public data sets that you \n",
      "should know about (without conducting a Google search):\n",
      "• CelebFaces (with 200,000 celebrity images along with 40 \n",
      "attribute annotations)\n",
      "• CIFAR (with 60,000 images that map 10 different classes)\n",
      "• YouTube-8M (with over 4,000 annotated entities taken \n",
      "from an enormous data set of YouTube videos)\n",
      "Researchers have released hundreds of free resources like these along \n",
      "with the actual network architecture and weights used in their exam-\n",
      "ples. So it will serve you well to explore some of these data sets and \n",
      "run some experiments before heading out for an AI interview.\n",
      "More reading: How to find datasets for Artificial Intelligence training\n",
      "\n",
      "\n",
      "59. 41 Essential Machine \n",
      "Learning Interview \n",
      "Questions\n",
      "www.springboard.com\n",
      "18 mins read\n",
      "\n",
      "\n",
      "60. M\n",
      "achine learning interview questions are an integral part \n",
      "of the data science interview and the path to becoming a \n",
      "data scientist, machine learning engineer, or data engi-\n",
      "neer. Springboard created a free guide to data science interviews, so \n",
      "we know exactly how they can trip up candidates! In order to help \n",
      "resolve that, here is a curated and created a list of key questions that \n",
      "you could see in a machine learning interview. There are some \n",
      "answers to go along with them so you don’t get stumped. You’ll be \n",
      "able to do well in any job interview (even for a machine learning \n",
      "internship) with after reading through this piece.\n",
      "Machine Learning Interview Questions: \n",
      "Categories\n",
      "We’ve traditionally seen machine learning interview questions pop up \n",
      "in several categories. The first really has to do with the algorithms \n",
      "and theory behind machine learning. You’ll have to show an under-\n",
      "standing of how algorithms compare with one another and how to \n",
      "measure their efficacy and accuracy in the right way. The second cat-\n",
      "egory has to do with your programming skills and your ability to exe-\n",
      "cute on top of those algorithms and the theory. The third has to do \n",
      "with your general interest in machine learning: you’ll be asked about \n",
      "what’s going on in the industry and how you keep up with the latest \n",
      "machine learning trends. Finally, there are company or industry-spe-\n",
      "cific questions that test your ability to take your general machine \n",
      "\n",
      "\n",
      "61. learning knowledge and turn it into actionable points to drive the bot-\n",
      "tom line forward.\n",
      "We’ve divided this guide to machine learning interview questions into \n",
      "the categories we mentioned above so that you can more easily get to \n",
      "the information you need when it comes to machine learning inter-\n",
      "view questions.\n",
      "Machine Learning Interview Questions: \n",
      "Algorithms/Theory\n",
      "These algorithms questions will test your grasp of the theory behind \n",
      "machine learning.\n",
      "Q1- What’s the trade-off between bias and variance?\n",
      "More reading: Bias-Variance Tradeoff (Wikipedia)\n",
      "Bias is error due to erroneous or overly simplistic assumptions in the \n",
      "learning algorithm you’re using. This can lead to the model underfit-\n",
      "ting your data, making it hard for it to have high predictive accuracy \n",
      "and for you to generalize your knowledge from the training set to the \n",
      "test set.\n",
      "Variance is error due to too much complexity in the learning algo-\n",
      "rithm you’re using. This leads to the algorithm being highly sensitive \n",
      "to high degrees of variation in your training data, which can lead your \n",
      "model to overfit the data. You’ll be carrying too much noise from your \n",
      "training data for your model to be very useful for your test data.\n",
      "The bias-variance decomposition essentially decomposes the learning \n",
      "error from any algorithm by adding the bias, the variance and a bit of \n",
      "irreducible error due to noise in the underlying dataset. Essentially, if \n",
      "you make the model more complex and add more variables, you’ll lose \n",
      "bias but gain some variance — in order to get the optimally reduced \n",
      "amount of error, you’ll have to tradeoff bias and variance. You don’t \n",
      "want either high bias or high variance in your model.\n",
      "\n",
      "\n",
      "62. Q2- What is the difference between supervised and unsu-\n",
      "pervised machine learning?\n",
      "More reading: What is the difference between supervised and unsuper-\n",
      "vised machine learning? (Quora)\n",
      "Supervised learning requires training labeled data. For example, in \n",
      "order to do classification (a supervised learning task), you’ll need to \n",
      "first label the data you’ll use to train the model to classify data into \n",
      "your labeled groups. Unsupervised learning, in contrast, does not \n",
      "require labeling data explicitly.\n",
      "Q3- How is KNN different from k-means clustering?\n",
      "More reading: How is the k-nearest neighbor algorithm different from \n",
      "k-means clustering? (Quora)\n",
      "K-Nearest Neighbors is a supervised classification algorithm, while \n",
      "k-means clustering is an unsupervised clustering algorithm. While the \n",
      "mechanisms may seem similar at first, what this really means is that \n",
      "in order for K-Nearest Neighbors to work, you need labeled data you \n",
      "want to classify an unlabeled point into (thus the nearest neighbor \n",
      "part). K-means clustering requires only a set of unlabeled points and \n",
      "a threshold: the algorithm will take unlabeled points and gradually \n",
      "learn how to cluster them into groups by computing the mean of the \n",
      "distance between different points.\n",
      "The critical difference here is that KNN needs labeled points and is \n",
      "thus supervised learning, while k-means doesn’t — and is thus unsu-\n",
      "pervised learning.\n",
      "Q4- Explain how a ROC curve works.\n",
      "More reading: Receiver operating characteristic (Wikipedia)\n",
      "The ROC curve is a graphical representation of the contrast between \n",
      "true positive rates and the false positive rate at various thresholds. \n",
      "It’s often used as a proxy for the trade-off between the sensitivity of \n",
      "\n",
      "\n",
      "63. the model (true positives) vs the fall-out or the probability it will trig-\n",
      "ger a false alarm (false positives).\n",
      "Q5- Define precision and recall.\n",
      "More reading: Precision and recall (Wikipedia)\n",
      "Recall is also known as the true positive rate: the amount of positives \n",
      "your model claims compared to the actual number of positives there \n",
      "are throughout the data. Precision is also known as the positive pre-\n",
      "dictive value, and it is a measure of the amount of accurate positives \n",
      "your model claims compared to the number of positives it actually \n",
      "claims. It can be easier to think of recall and precision in the context \n",
      "of a case where you’ve predicted that there were 10 apples and 5 \n",
      "oranges in a case of 10 apples. You’d have perfect recall (there are \n",
      "actually 10 apples, and you predicted there would be 10) but 66.7% \n",
      "precision because out of the 15 events you predicted, only 10 (the \n",
      "apples) are correct.\n",
      "\n",
      "\n",
      "64. Q6- What is Bayes’ Theorem? How is it useful in a machine \n",
      "learning context?\n",
      "More reading: An Intuitive (and Short) Explanation of Bayes’ Theorem \n",
      "(BetterExplained)\n",
      "Bayes’ Theorem gives you the posterior probability of an event given \n",
      "what is known as prior knowledge.\n",
      "Mathematically, it’s expressed as the true positive rate of a condition \n",
      "sample divided by the sum of the false positive rate of the population \n",
      "and the true positive rate of a condition. Say you had a 60% chance of \n",
      "actually having the flu after a flu test, but out of people who had the \n",
      "flu, the test will be false 50% of the time, and the overall population \n",
      "only has a 5% chance of having the flu. Would you actually have a \n",
      "60% chance of having the flu after having a positive test?\n",
      "Bayes’ Theorem says no. It says that you have a (.6 * 0.05) (True Pos-\n",
      "itive Rate of a Condition Sample) / (.6*0.05)(True Positive Rate of a \n",
      "Condition Sample) + (.5*0.95) (False Positive Rate of a Population)  = \n",
      "0.0594 or 5.94% chance of getting a flu.\n",
      "Bayes’ Theorem is the basis behind a branch of machine learning that \n",
      "most notably includes the Naive Bayes classifier. That’s something \n",
      "\n",
      "\n",
      "65. important to consider when you’re faced with machine learning inter-\n",
      "view questions.\n",
      "Q7- Why is “Naive” Bayes naive?\n",
      "More reading: Why is “naive Bayes” naive? (Quora)\n",
      "Despite its practical applications, especially in text mining, Naive \n",
      "Bayes is considered “Naive” because it makes an assumption that is \n",
      "virtually impossible to see in real-life data: the conditional probabil-\n",
      "ity is calculated as the pure product of the individual probabilities of \n",
      "components. This implies the absolute independence of features — a \n",
      "condition probably never met in real life.\n",
      "As a Quora commenter put it whimsically, a Naive Bayes classifier \n",
      "that figured out that you liked pickles and ice cream would probably \n",
      "naively recommend you a pickle ice cream.\n",
      "Q8- Explain the difference between L1 and L2 regulariza-\n",
      "tion.\n",
      "More reading: What is the difference between L1 and L2 regulariza-\n",
      "tion? (Quora)\n",
      "L2 regularization tends to spread error among all the terms, while L1 \n",
      "is more binary/sparse, with many variables either being assigned a 1 \n",
      "or 0 in weighting. L1 corresponds to setting a Laplacean prior on the \n",
      "terms, while L2 corresponds to a Gaussian prior.\n",
      "\n",
      "\n",
      "66. Q9- What’s your favorite algorithm, and can you explain it \n",
      "to me in less than a minute?\n",
      "This type of question tests your understanding of how to communi-\n",
      "cate complex and technical nuances with poise and the ability to sum-\n",
      "marize quickly and efficiently. Make sure you have a choice and make \n",
      "sure you can explain different algorithms so simply and effectively \n",
      "that a five-year-old could grasp the basics!\n",
      "Q10- What’s the difference between Type I and Type II \n",
      "error?\n",
      "More reading: Type I and type II errors (Wikipedia)\n",
      "Don’t think that this is a trick question! Many machine learning inter-\n",
      "view questions will be an attempt to lob basic questions at you just to \n",
      "make sure you’re on top of your game and you’ve prepared all of your \n",
      "bases.\n",
      "Type I error is a false positive, while Type II error is a false negative. \n",
      "Briefly stated, Type I error means claiming something has happened \n",
      "when it hasn’t, while Type II error means that you claim nothing is \n",
      "happening when in fact something is.\n",
      "A clever way to think about this is to think of Type I error as telling a \n",
      "man he is pregnant, while Type II error means you tell a pregnant \n",
      "woman she isn’t carrying a baby.\n",
      "Q11- What’s a Fourier transform?\n",
      "More reading: Fourier transform (Wikipedia)\n",
      "\n",
      "\n",
      "67. A Fourier transform is a generic method to decompose generic func-\n",
      "tions into a superposition of symmetric functions. Or as this more \n",
      "intuitive tutorial puts it, given a smoothie, it’s how we find the rec-\n",
      "ipe. The Fourier transform finds the set of cycle speeds, amplitudes \n",
      "and phases to match any time signal. A Fourier transform converts a \n",
      "signal from time to frequency domain — it’s a very common way to \n",
      "extract features from audio signals or other time series such as sen-\n",
      "sor data.\n",
      "Q12- What’s the difference between probability and likeli-\n",
      "hood?\n",
      "More reading: What is the difference between “likelihood” and “proba-\n",
      "bility”? (Cross Validated)\n",
      "Q13- What is deep learning, and how does it contrast with \n",
      "other machine learning algorithms?\n",
      "More reading: Deep learning (Wikipedia)\n",
      "Deep learning is a subset of machine learning that is concerned with \n",
      "neural networks: how to use backpropagation and certain principles \n",
      "\n",
      "\n",
      "68. from neuroscience to more accurately model large sets of unlabelled \n",
      "or semi-structured data. In that sense, deep learning represents an \n",
      "unsupervised learning algorithm that learns representations of data \n",
      "through the use of neural nets.\n",
      "Q14- What’s the difference between a generative and dis-\n",
      "criminative model?\n",
      "More reading: What is the difference between a Generative and Dis-\n",
      "criminative Algorithm? (Stack Overflow)\n",
      "A generative model will learn categories of data while a discrimina-\n",
      "tive model will simply learn the distinction between different catego-\n",
      "ries of data. Discriminative models will generally outperform \n",
      "generative models on classification tasks.\n",
      "Q15- What cross-validation technique would you use on a \n",
      "time series dataset?\n",
      "More reading: Using k-fold cross-validation for time-series model \n",
      "selection (CrossValidated)\n",
      "Instead of using standard k-folds cross-validation, you have to pay \n",
      "attention to the fact that a time series is not randomly distributed \n",
      "data — it is inherently ordered by chronological order. If a pattern \n",
      "emerges in later time periods for example, your model may still pick \n",
      "up on it even if that effect doesn’t hold in earlier years!\n",
      "You’ll want to do something like forward chaining where you’ll be \n",
      "able to model on past data then look at forward-facing data.\n",
      "• fold 1 : training [1], test [2]\n",
      "• fold 2 : training [1 2], test [3]\n",
      "• fold 3 : training [1 2 3], test [4]\n",
      "• fold 4 : training [1 2 3 4], test [5]\n",
      "• fold 5 : training [1 2 3 4 5], test [6]\n",
      "Q16- How is a decision tree pruned?\n",
      "\n",
      "\n",
      "69. More reading: Pruning (decision trees)\n",
      "Pruning is what happens in decision trees when branches that have \n",
      "weak predictive power are removed in order to reduce the complexity \n",
      "of the model and increase the predictive accuracy of a decision tree \n",
      "model. Pruning can happen bottom-up and top-down, with \n",
      "approaches such as reduced error pruning and cost complexity prun-\n",
      "ing.\n",
      "Reduced error pruning is perhaps the simplest version: replace each \n",
      "node. If it doesn’t decrease predictive accuracy, keep it pruned. While \n",
      "simple, this heuristic actually comes pretty close to an approach that \n",
      "would optimize for maximum accuracy.\n",
      "Q17- Which is more important to you– model accuracy, or \n",
      "model performance?\n",
      "More reading: Accuracy paradox (Wikipedia)\n",
      "This question tests your grasp of the nuances of machine learning \n",
      "model performance! Machine learning interview questions often look \n",
      "towards the details. There are models with higher accuracy that can \n",
      "perform worse in predictive power — how does that make sense?\n",
      "Well, it has everything to do with how model accuracy is only a sub-\n",
      "set of model performance, and at that, a sometimes misleading one. \n",
      "For example, if you wanted to detect fraud in a massive dataset with \n",
      "a sample of millions, a more accurate model would most likely predict \n",
      "no fraud at all if only a vast minority of cases were fraud. However, \n",
      "this would be useless for a predictive model — a model designed to \n",
      "find fraud that asserted there was no fraud at all! Questions like this \n",
      "help you demonstrate that you understand model accuracy isn’t the \n",
      "be-all and end-all of model performance.\n",
      "Q18- What’s the F1 score? How would you use it?\n",
      "More reading: F1 score (Wikipedia)\n",
      "\n",
      "\n",
      "70. The F1 score is a measure of a model’s performance. It is a weighted \n",
      "average of the precision and recall of a model, with results tending to \n",
      "1 being the best, and those tending to 0 being the worst. You would \n",
      "use it in classification tests where true negatives don’t matter much.\n",
      "Q19- How would you handle an imbalanced dataset?\n",
      "More reading: 8 Tactics to Combat Imbalanced Classes in Your \n",
      "Machine Learning Dataset (Machine Learning Mastery)\n",
      "An imbalanced dataset is when you have, for example, a classification \n",
      "test and 90% of the data is in one class. That leads to problems: an \n",
      "accuracy of 90% can be skewed if you have no predictive power on \n",
      "the other category of data! Here are a few tactics to get over the \n",
      "hump:\n",
      "1- Collect more data to even the imbalances in the dataset.\n",
      "2- Resample the dataset to correct for imbalances.\n",
      "3- Try a different algorithm altogether on your dataset.\n",
      "What’s important here is that you have a keen sense for what damage \n",
      "an unbalanced dataset can cause, and how to balance that.\n",
      "Q20- When should you use classification over regression?\n",
      "More reading: Regression vs Classification (Math StackExchange)\n",
      "Classification produces discrete values and dataset to strict catego-\n",
      "ries, while regression gives you continuous results that allow you to \n",
      "better distinguish differences between individual points. You would \n",
      "use classification over regression if you wanted your results to reflect \n",
      "the belongingness of data points in your dataset to certain explicit \n",
      "categories (ex: If you wanted to know whether a name was male or \n",
      "female rather than just how correlated they were with male and \n",
      "female names.)\n",
      "\n",
      "\n",
      "71. Q21- Name an example where ensemble techniques might \n",
      "be useful.\n",
      "More reading: Ensemble learning (Wikipedia)\n",
      "Ensemble techniques use a combination of learning algorithms to \n",
      "optimize better predictive performance. They typically reduce overfit-\n",
      "ting in models and make the model more robust (unlikely to be influ-\n",
      "enced by small changes in the training data). \n",
      "You could list some examples of ensemble methods, from bagging to \n",
      "boosting to a “bucket of models” method and demonstrate how they \n",
      "could increase predictive power.\n",
      "Q22- How do you ensure you’re not overfitting with a \n",
      "model? \n",
      "More reading: How can I avoid overfitting? (Quora)\n",
      "This is a simple restatement of a fundamental problem in machine \n",
      "learning: the possibility of overfitting training data and carrying the \n",
      "noise of that data through to the test set, thereby providing inaccu-\n",
      "rate generalizations.\n",
      "There are three main methods to avoid overfitting:\n",
      "1- Keep the model simpler: reduce variance by taking into account \n",
      "fewer variables and parameters, thereby removing some of the noise \n",
      "in the training data.\n",
      "2- Use cross-validation techniques such as k-folds cross-validation.\n",
      "3- Use regularization techniques such as LASSO that penalize certain \n",
      "model parameters if they’re likely to cause overfitting.\n",
      "Q23- What evaluation approaches would you work to \n",
      "gauge the effectiveness of a machine learning model?\n",
      "More reading: How to Evaluate Machine Learning Algorithms (Machine \n",
      "Learning Mastery)\n",
      "\n",
      "\n",
      "72. You would first split the dataset into training and test sets, or per-\n",
      "haps use cross-validation techniques to further segment the dataset \n",
      "into composite sets of training and test sets within the data. You \n",
      "should then implement a choice selection of performance metrics: \n",
      "here is a fairly comprehensive list. You could use measures such as \n",
      "the F1 score, the accuracy, and the confusion matrix. What’s \n",
      "important here is to demonstrate that you understand the nuances of \n",
      "how a model is measured and how to choose the right performance \n",
      "measures for the right situations.\n",
      "Q24- How would you evaluate a logistic regression model?\n",
      "More reading: Evaluating a logistic regression (CrossValidated), Lo-\n",
      "gistic Regression in Plain English\n",
      "A subsection of the question above. You have to demonstrate an \n",
      "understanding of what the typical goals of a logistic regression are \n",
      "(classification, prediction, etc.) and bring up a few examples and use \n",
      "cases.\n",
      "Q25- What’s the “kernel trick” and how is it useful?\n",
      "More reading: Kernel method (Wikipedia)\n",
      "The Kernel trick involves kernel functions that can enable in higher-\n",
      "dimension spaces without explicitly calculating the coordinates of \n",
      "points within that dimension: instead, kernel functions compute the \n",
      "inner products between the images of all pairs of data in a feature \n",
      "space. This allows them the very useful attribute of calculating the \n",
      "coordinates of higher dimensions while being computationally \n",
      "cheaper than the explicit calculation of said coordinates. Many algo-\n",
      "rithms can be expressed in terms of inner products. Using the kernel \n",
      "trick enables us effectively run algorithms in a high-dimensional \n",
      "space with lower-dimensional data.\n",
      "(Learn about Springboard’s AI / Machine Learning \n",
      "Bootcamp, the first of its kind to come with a job guaran-\n",
      "tee.)\n",
      "\n",
      "\n",
      "73. Machine Learning Interview Questions: \n",
      "Programming\n",
      "These machine learning interview questions test your knowledge of \n",
      "programming principles you need to implement machine learning \n",
      "principles in practice. Machine learning interview questions tend to \n",
      "be technical questions that test your logic and programming skills: \n",
      "this section focuses more on the latter.\n",
      "Q26- How do you handle missing or corrupted data in a \n",
      "dataset?\n",
      "More reading: Handling missing data (O’Reilly)\n",
      "You could find missing/corrupted data in a dataset and either drop \n",
      "those rows or columns, or decide to replace them with another value.\n",
      "In Pandas, there are two very useful methods: isnull() and dropna() \n",
      "that will help you find columns of data with missing or corrupted \n",
      "data and drop those values. If you want to fill the invalid values with \n",
      "a placeholder value (for example, 0), you could use the fillna() \n",
      "method.\n",
      "Q27- Do you have experience with Spark or big data tools \n",
      "for machine learning?\n",
      "More reading: 50 Top Open Source Tools for Big Data (Datamation)\n",
      "You’ll want to get familiar with the meaning of big data for different \n",
      "companies and the different tools they’ll want. Spark is the big data \n",
      "tool most in demand now, able to handle immense datasets with \n",
      "speed. Be honest if you don’t have experience with the tools \n",
      "\n",
      "\n",
      "74. demanded, but also take a look at job descriptions and see what tools \n",
      "pop up: you’ll want to invest in familiarizing yourself with them.\n",
      "Q28- Pick an algorithm. Write the psuedo-code for a paral-\n",
      "lel implementation.\n",
      "More reading: Writing pseudocode for parallel programming (Stack \n",
      "Overflow)\n",
      "This kind of question demonstrates your ability to think in parallel-\n",
      "ism and how you could handle concurrency in programming imple-\n",
      "mentations dealing with big data. Take a look at pseudocode \n",
      "frameworks such as Peril-L and visualization tools such as Web \n",
      "Sequence Diagrams to help you demonstrate your ability to write code \n",
      "that reflects parallelism.\n",
      "Q29- What are some differences between a linked list and \n",
      "an array?\n",
      "More reading: Array versus linked list (Stack Overflow)\n",
      "An array is an ordered collection of objects. A linked list is a series of \n",
      "objects with pointers that direct how to process them sequentially. An \n",
      "array assumes that every element has the same size, unlike the linked \n",
      "list. A linked list can more easily grow organically: an array has to be \n",
      "pre-defined or re-defined for organic growth. Shuffling a linked list \n",
      "involves changing which points direct where — meanwhile, shuffling \n",
      "an array is more complex and takes more memory.\n",
      "Q30- Describe a hash table.\n",
      "More reading: Hash table (Wikipedia)\n",
      "A hash table is a data structure that produces an associative array. A \n",
      "key is mapped to certain values through the use of a hash function. \n",
      "They are often used for tasks such as database indexing.\n",
      "\n",
      "\n",
      "75. Q31- Which data visualization libraries do you use? What \n",
      "are your thoughts on the best data visualization tools?\n",
      "More reading: 31 Free Data Visualization Tools (Springboard)\n",
      "What’s important here is to define your views on how to properly vis-\n",
      "ualize data and your personal preferences when it comes to tools. \n",
      "Popular tools include R’s ggplot, Python’s seaborn and matplotlib, and \n",
      "tools such as Plot.ly and Tableau.\n",
      "Related: 20 Python Interview Questions\n",
      "Machine Learning Interview Questions: \n",
      "Company/Industry Specific\n",
      "These machine learning interview questions deal with how to imple-\n",
      "ment your general machine learning knowledge to a specific compa-\n",
      "ny’s requirements. You’ll be asked to create case studies and extend \n",
      "your knowledge of the company and industry you’re applying for with \n",
      "your machine learning skills.\n",
      "Q32- How would you implement a recommendation system \n",
      "for our company’s users?\n",
      "More reading: How to Implement A Recommendation System? (Stack \n",
      "Overflow)\n",
      "A lot of machine learning interview questions of this type will involve \n",
      "implementation of machine learning models to a company’s problems. \n",
      "You’ll have to research the company and its industry in-depth, espe-\n",
      "\n",
      "\n",
      "76. cially the revenue drivers the company has, and the types of users the \n",
      "company takes on in the context of the industry it’s in.\n",
      "Q33- How can we use your machine learning skills to gen-\n",
      "erate revenue?\n",
      "More reading: Startup Metrics for Startups (500 Startups)\n",
      "This is a tricky question. The ideal answer would demonstrate \n",
      "knowledge of what drives the business and how your skills could \n",
      "relate. For example, if you were interviewing for music-streaming \n",
      "startup Spotify, you could remark that your skills at developing a bet-\n",
      "ter recommendation model would increase user retention, which \n",
      "would then increase revenue in the long run.\n",
      "The startup metrics Slideshare linked above will help you understand \n",
      "exactly what performance indicators are important for startups and \n",
      "tech companies as they think about revenue and growth.\n",
      "Q34- What do you think of our current data process?\n",
      "More reading: The Data Science Process Email Course – Springboard\n",
      "\n",
      "\n",
      "77. This kind of question requires you to listen carefully and impart feed-\n",
      "back in a manner that is constructive and insightful. Your interviewer \n",
      "is trying to gauge if you’d be a valuable member of their team and \n",
      "whether you grasp the nuances of why certain things are set the way \n",
      "they are in the company’s data process based on company- or indus-\n",
      "try-specific conditions. They’re trying to see if you can be an intellec-\n",
      "tual peer. Act accordingly.\n",
      "Machine Learning Interview Questions: General \n",
      "Machine Learning Interest\n",
      "This series of machine learning interview questions attempts to gauge \n",
      "your passion and interest in machine learning. The right answers will \n",
      "serve as a testament for your commitment to being a lifelong learner \n",
      "in machine learning.\n",
      "Q35- What are the last machine learning papers you’ve \n",
      "read?\n",
      "More reading: What are some of the best research papers/books for \n",
      "machine learning?\n",
      "Keeping up with the latest scientific literature on machine learning is \n",
      "a must if you want to demonstrate interest in a machine learning \n",
      "position. This overview of deep learning in Nature by the scions of \n",
      "deep learning themselves (from Hinton to Bengio to LeCun) can be a \n",
      "good reference paper and an overview of what’s happening in deep \n",
      "learning — and the kind of paper you might want to cite.\n",
      "Q36- Do you have research experience in machine learn-\n",
      "ing?\n",
      "\n",
      "\n",
      "78. Related to the last point, most organizations hiring for machine learn-\n",
      "ing positions will look for your formal experience in the field. \n",
      "Research papers, co-authored or supervised by leaders in the field, \n",
      "can make the difference between you being hired and not. Make sure \n",
      "you have a summary of your research experience and papers ready — \n",
      "and an explanation for your background and lack of formal research \n",
      "experience if you don’t.\n",
      "Q37- What are your favorite use cases of machine learning \n",
      "models?\n",
      "More reading: What are the typical use cases for different machine \n",
      "learning algorithms? (Quora)\n",
      "The Quora thread above contains some examples, such as decision \n",
      "trees that categorize people into different tiers of intelligence based \n",
      "on IQ scores. Make sure that you have a few examples in mind and \n",
      "describe what resonated with you. It’s important that you demon-\n",
      "strate an interest in how machine learning is implemented.\n",
      "Q38- How would you approach the “Netflix Prize” competi-\n",
      "tion?\n",
      "More reading: Netflix Prize (Wikipedia)\n",
      "The Netflix Prize was a famed competition where Netflix offered \n",
      "$1,000,000 for a better collaborative filtering algorithm. The team \n",
      "that won called BellKor had a 10% improvement and used an ensem-\n",
      "ble of different methods to win. Some familiarity with the case and its \n",
      "solution will help demonstrate you’ve paid attention to machine \n",
      "learning for a while.\n",
      "Q39- Where do you usually source datasets?\n",
      "More reading: 19 Free Public Data Sets For Your First Data Science \n",
      "Project (Springboard)\n",
      "Machine learning interview questions like these try to get at the heart \n",
      "of your machine learning interest. Somebody who is truly passionate \n",
      "\n",
      "\n",
      "79. about machine learning will have gone off and done side projects on \n",
      "their own, and have a good idea of what great datasets are out there. \n",
      "If you’re missing any, check out Quandl for economic and financial \n",
      "data, and Kaggle’s Datasets collection for another great list.\n",
      "Q40- How do you think Google is training data for self-\n",
      "driving cars?\n",
      "More reading: Waymo Tech\n",
      "Machine learning interview questions like this one really test your \n",
      "knowledge of different machine learning methods, and your inven-\n",
      "tiveness if you don’t know the answer. Google is currently using \n",
      "recaptcha to source labeled data on storefronts and traffic signs. They \n",
      "are also building on training data collected by Sebastian Thrun at \n",
      "GoogleX — some of which was obtained by his grad students driving \n",
      "buggies on desert dunes!\n",
      "Q41- How would you simulate the approach AlphaGo took \n",
      "to beat Lee Sidol at Go?\n",
      "More reading: Mastering the game of Go with deep neural networks \n",
      "and tree search (Nature)\n",
      "AlphaGo beating Lee Sidol, the best human player at Go, in a best-of-\n",
      "five series was a truly seminal event in the history of machine learn-\n",
      "ing and deep learning. The Nature paper above describes how this \n",
      "was accomplished with “Monte-Carlo tree search with deep neural \n",
      "networks that have been trained by supervised learning, from human \n",
      "expert games, and by reinforcement learning from games of self-\n",
      "play.”\n",
      "Related: 40 Artificial Intelligence Interview Questions\n",
      "Looking to land a role as a machine learning engineer? \n",
      "Find out about Springboard’s Machine Learning Engineer-\n",
      "ing Career Track, the first of its kind to come with a job \n",
      "guarantee.\n",
      "\n",
      "\n",
      "80. O\n",
      "Interview Prep: 40 Artificial \n",
      "Intelligence Questions\n",
      "ver the last decade, artificial intelligence (AI) has grown \n",
      "from a pipe dream into the driving force behind the fourth \n",
      "industrial revolution. If you browse through the world’s \n",
      "leading job boards, you’ll find that it’s at the heart of some of the \n",
      "most in-demand tech careers today.\n",
      "“Everyone’s trying to figure out ways to optimize their businesses and \n",
      "their practices, and how to automate and make their day-to-day lives \n",
      "a little bit easier, or a little bit more productive and functional,” \n",
      "www.springboard.com\n",
      "20 mins read\n",
      "\n",
      "\n",
      "81. notes Stephen Zafarino, vice president of national recruiting at \n",
      "Mondo.\n",
      "So, whether your next job interview is related to data science, \n",
      "machine learning (ML), or deep learning (DL), you can bet that artifi-\n",
      "cial intelligence questions will come up.\n",
      "Artificial Intelligence Questions: \n",
      "Categories\n",
      "Because it’s a broad area of computer science, AI questions will keep \n",
      "popping up in various job interview scenarios. To make it easier for \n",
      "you to navigate through this space, we have curated a list of questions \n",
      "about artificial intelligence and divided them into multiple categories. \n",
      "So whether you’re hoping to move up the data science career ladder \n",
      "or looking to start your first machine learning internship, make sure \n",
      "that you brush up on these AI interview questions and answers so you \n",
      "can walk into your next interview oozing confidence.\n",
      "Artificial Intelligence Questions: \n",
      "Introduction to AI\n",
      "If your AI interview is for an internship, there’s a good chance that \n",
      "the interviewer will try to break the ice and make you feel more com-\n",
      "fortable by asking some “simple” general interest questions. \n",
      "These types of questions usually cover the basics, so even if they \n",
      "sound straightforward, you have to make sure that you don’t get \n",
      "stumped (seemingly simple questions require that your answer be \n",
      "\n",
      "\n",
      "82. delivered easily and flawlessly). However, it can quickly get more \n",
      "involved, so you have to be ready for whatever they throw at you.\n",
      "Related: The Most Common Machine Learning Terms, Explained\n",
      "1. What is artificial intelligence?\n",
      "AI can be described as an area of computer science that simulates \n",
      "human intelligence in machines. It’s about smart algorithms making \n",
      "decisions based on the available data. \n",
      "Whether it’s Amazon’s Alexa or a self-driving car, the goal is to mimic \n",
      "human intelligence at lightning speed (and with a reduced rate of \n",
      "error).\n",
      "More reading: What is AI? Everything you need to know about Artifi-\n",
      "cial Intelligence\n",
      "2. What are intelligent agents?\n",
      "An intelligent agent is an autonomous entity that leverages sensors to \n",
      "understand a situation and make decisions. It can also use actuators \n",
      "to perform both simple and complex tasks.\n",
      "In the beginning, it might not be so great at performing a task, but it \n",
      "will improve over time. The Roomba vacuum cleaner is an excellent \n",
      "example of this. \n",
      "More reading: Intelligent agents defending for an IoT world\n",
      "3. What’s the most popular programming language \n",
      "used in AI?\n",
      "The open-source modular programming language Python leads the AI \n",
      "industry because of its simplicity and predictable coding behavior. \n",
      "\n",
      "\n",
      "83. Its popularity can be attributed to open-source libraries like Mat-\n",
      "plotlib and NumPy, efficient frameworks such as Scikit-learn, and \n",
      "practical version libraries like Tensorflow and VTK.\n",
      "There’s a chance that the interviewer might keep the conversation \n",
      "going and ask you for more examples. If that happens, you can men-\n",
      "tion the following:\n",
      "• Java\n",
      "• Julia\n",
      "• Haskell\n",
      "• Lisp\n",
      "More reading: The 5 Best Programming Languages for AI\n",
      "4. What are AI neural networks?\n",
      "Neural networks in AI mathematically model how the human brain \n",
      "works. This approach enables the machine to think and learn as \n",
      "humans do. This is how smart technology today recognizes speech, \n",
      "objects, and more.  \n",
      "More reading: Exploring Neural Networks with Activation Atlases\n",
      "5. What’s the difference between strong AI and weak \n",
      "AI?\n",
      "The difference between the two is just like the terms sound. Strong AI \n",
      "can successfully imitate human intelligence and is at the core of \n",
      "advanced robotics. \n",
      "Weak AI can only predict specific characteristics that resemble human \n",
      "intelligence. Alexa and Siri are excellent examples of weak AI.\n",
      "Strong AI \n",
      "• Can be applied widely \n",
      "• Extensive scope\n",
      "\n",
      "\n",
      "84. • Human-level intelligence\n",
      "• Processes data by using clustering and association\n",
      "Weak AI\n",
      "• Can be great at performing some simple tasks\n",
      "• Uses both supervised and unsupervised learning\n",
      "• The scope can be minimal\n",
      "More reading: What’s the Difference Between Weak and Strong AI?\n",
      "6. What’s the difference between AI and ML?\n",
      "(Source)\n",
      "AI and ML are closely related, but these terms aren’t interchangeable. \n",
      "ML actually falls under the umbrella of AI. It demands that machines \n",
      "carry out tasks in the same way that humans do. \n",
      "The current application of ML in AI is based around the idea that we \n",
      "should enable access to data so machines can observe and learn for \n",
      "themselves.\n",
      "More reading: What’s the Difference between Machine Learning and \n",
      "AI?\n",
      "\n",
      "\n",
      "85. 7. How would you describe ML to a non-technical \n",
      "person?\n",
      "ML is geared toward pattern recognition. A great example of this is \n",
      "your Facebook newsfeed and Netflix’s recommendation engine. \n",
      "In this scenario, ML algorithms observe patterns and learn from \n",
      "them. When you deploy an ML program, it will keep learning and \n",
      "improving with each attempt.\n",
      "If the interviewer prods you to provide more real-world examples, \n",
      "you can list the following:\n",
      "• Amazon product recommendations\n",
      "• Fraud detection\n",
      "• Search ranking\n",
      "• Spam detection\n",
      "• Spell correction\n",
      "More reading: How do you explain Machine Learning and Data Mining \n",
      "to Non-Computer Science people?\n",
      "8. What are some examples of AI in use?\n",
      "Some compelling examples of AI applications are:\n",
      "• Chatbots\n",
      "• Facial recognition\n",
      "• Image tagging\n",
      "• Natural language processing\n",
      "• Sales prediction\n",
      "• Self-driving cars\n",
      "• Sentiment analysis\n",
      "More reading: Ask the AI experts: What are the applications of AI?\n",
      "\n",
      "\n",
      "86. 9. What’s a Turing test?\n",
      "The Turing test, named after Alan Turing, is a method of testing a \n",
      "machine’s human-level intelligence. For example, in a human-versus-\n",
      "machine scenario, a judge will be tasked with identifying which ter-\n",
      "minal was occupied by a human and which was occupied by a \n",
      "computer based on individual performance. \n",
      "Whenever a computer can pass off as a human, it’s deemed intelli-\n",
      "gent. The game has since evolved, but the premise remains the same. \n",
      "More reading: The Turing Test\n",
      "10. What’s TensorFlow?\n",
      "TensorFlow is an open-source framework dedicated to ML. It’s a com-\n",
      "prehensive and highly adaptable ecosystem of libraries, tools, and \n",
      "community resources that help developers build and deploy ML-pow-\n",
      "ered applications. Both AlphaGo and Google Cloud Vision were built \n",
      "on the Tensorflow platform.\n",
      "More reading: TensorFlow Tutorial from Scratch: Building a Deep \n",
      "Learning Model on Fashion MNIST Dataset (Part 1)\n",
      "11. Why is game theory important to AI?\n",
      "Game theory, developed by American mathematician Josh Nash, is \n",
      "essential to AI because it plays an underlying role in how these smart \n",
      "algorithms improve over time. \n",
      "At its most basic, AI is about algorithms that are deployed to find \n",
      "solutions to problems. Game theory is about players in opposition try-\n",
      "ing to achieve specific goals. As most aspects of life are about compe-\n",
      "tition, game theory has many meaningful real-world applications. \n",
      "These problems tend to be dynamic. Some game theory problems are \n",
      "natural candidates for AI algorithms. So, whenever game theory is \n",
      "\n",
      "\n",
      "87. applied, multiple AI agents that interact with each other will only \n",
      "care about utility to itself. \n",
      "Data scientists within this space should be aware of the following \n",
      "games:\n",
      "• Symmetric vs. asymmetric\n",
      "• Perfect vs. imperfect information\n",
      "• Cooperative vs. non-cooperative\n",
      "• Simultaneous vs. sequential\n",
      "• Zero-sum vs. non-zero-sum\n",
      "More reading: What is the connection between Game Theory and AI?\n",
      "12. In your opinion, how will AI impact application \n",
      "development?\n",
      "These types of questions help the interviewer ascertain your level of \n",
      "interest in the field. If you’re naturally passionate about AI and every-\n",
      "thing related to it, you should have some knowledge about current \n",
      "industry trends. \n",
      "So, if you have been actively following this space, you’ll know all \n",
      "about AIOps. In the coming months, you can expect AI to be more \n",
      "involved in how we build applications. It has the potential to trans-\n",
      "form how we use and manage the infrastructure at a micro and macro \n",
      "level. \n",
      "Some say that DevOps will be replaced by what they are calling AIOps \n",
      "because it allows developers to engage in accurate root cause analysis \n",
      "by combining big data, ML, and visualization. \n",
      "AIOps can be described as a multilayered platform that can be used to \n",
      "automate and improve IT operations. In this scenario, developers can \n",
      "leverage analytics and ML to collect and process data from a variety \n",
      "of sources. This information can then be analyzed in real time to \n",
      "identify and rectify problems.\n",
      "\n",
      "\n",
      "88. (Source)\n",
      "More reading: AIOps: Is DevOps Ready for an Infusion of Artificial \n",
      "Intelligence?\n",
      "13. What would you say are common misconceptions \n",
      "about AI?\n",
      "Many AI-related misconceptions are making the rounds in the age of \n",
      "“fake news.” The most common ones are:\n",
      "• AI will replace humans\n",
      "• AI systems aren’t safe\n",
      "• AI will lead to significant unemployment\n",
      "\n",
      "\n",
      "89. While these types of stories are common, they’re far from the truth. \n",
      "Even though some AI-based technology is able to complete some \n",
      "tasks—for example, analyzing zettabytes of data in less than a \n",
      "second—it still needs humans to gather the data and define the pat-\n",
      "terns for identification. \n",
      "So we aren’t near a reality where technology has the potential to \n",
      "replace us or our jobs. \n",
      "More reading: What’s Behind the Hype About Artificial Intelligence?\n",
      "14. Can you name the properties of a good knowledge \n",
      "representation system?\n",
      "From the perspective of systems theory, a good knowledge represen-\n",
      "tation system will have the following:\n",
      "• Acquisition efficiency to acquire and incorporate new data\n",
      "• Inferential adequacy to derive knowledge representation \n",
      "structures like symbols when new knowledge is learned \n",
      "from old knowledge\n",
      "• Inferential efficiency to enable the addition of data into \n",
      "existing knowledge structures to help the inference process\n",
      "• Representation adequacy to represent all the knowledge \n",
      "required in a specific domain\n",
      "More reading: Knowledge representation in AI\n",
      "15. What are the different types of keys in a \n",
      "relational database?\n",
      "There are a variety of keys in a relational database, including:\n",
      "• Alternate keys are candidate keys that exclude all primary \n",
      "keys.\n",
      "\n",
      "\n",
      "90. • Artificial keys are created by assigning a unique number to \n",
      "each occurrence or record when there aren’t any compound \n",
      "or standalone keys.\n",
      "• Compound keys are made by combining multiple elements \n",
      "to develop a unique identifier for a construct when there \n",
      "isn’t a single data element that uniquely identifies \n",
      "occurrences within a construct. Also known as a composite \n",
      "key or a concatenated key, compound keys consist of two or \n",
      "more attributes.\n",
      "• Foreign keys are groups of fields in a database record that \n",
      "point to a key field or a group of fields that create a key of \n",
      "another database record that’s usually in a different table. \n",
      "Often, foreign keys in one table refer to primary keys in \n",
      "another. As the referenced data can be linked together quite \n",
      "quickly, it can be critical to database normalization.\n",
      "• Natural keys are data elements that are stored within \n",
      "constructs and utilized as primary keys.\n",
      "• Primary keys are values that can be used to identify unique \n",
      "rows in a table and the attributes associated with them. For \n",
      "example, these can take the form of a Social Security \n",
      "number that’s related to a specific person. In a relational \n",
      "model of data, the primary key is the candidate key. It’s also \n",
      "the primary method used to identify a tuple in each possible \n",
      "relation.\n",
      "• Super keys are defined in the relational model as a set of \n",
      "attributes of a relation variable. It holds that all relations \n",
      "assigned to that variable don’t have any distinct tuples. \n",
      "They also don’t have the same values for the attributes in \n",
      "the set. Super keys also are defined as a set of attributes of \n",
      "a relational variable upon which all of the functionality \n",
      "depends.\n",
      "More reading: What are the different types of keys in RDBMS?\n",
      "\n",
      "\n",
      "91. Artificial Intelligence Questions: \n",
      "Statistics\n",
      "AI, ML, and data science all have a great deal of overlap, so it’s crucial \n",
      "to cover all bases before your AI interview. However, it’s important to \n",
      "note that these fields aren’t interchangeable. Although everything is \n",
      "relative, AI produces actions, ML produces predictions, and data sci-\n",
      "ence produces insights.\n",
      "So what kind of potential data science-related AI questions should you \n",
      "be prepared for? Let’s take a look. \n",
      "16. In Python’s standard library, what packages \n",
      "would you say are the most useful for data \n",
      "scientists? \n",
      "Python wasn’t built for data science. However, in recent years it has \n",
      "grown to become the go-to programming language for the following:\n",
      "• Machine learning\n",
      "• Predictive analytics\n",
      "• Simple data analytics \n",
      "• Statistics\n",
      "For data science projects, the following packages in the Python stand-\n",
      "ard library will make life easier and accelerate deliveries:\n",
      "• NumPy (to process large multidimensional arrays, \n",
      "extensive collections of high-level mathematical functions, \n",
      "and matrices)\n",
      "• Pandas (to leverage built-in methods for rapidly combining, \n",
      "filtering, and grouping data)\n",
      "• SciPy (to extend NumPy’s capabilities and solve tasks \n",
      "related to integral calculus, linear algebra, and probability \n",
      "theory)\n",
      "\n",
      "\n",
      "92. More reading: 20 Python Interview Questions and Answers—Start Pre-\n",
      "paring for Your Ideal Job\n",
      "17. What is collaborative filtering?\n",
      "Collaborative filtering can be described as a process of finding pat-\n",
      "terns from available information to build personalized recommenda-\n",
      "tions. You can find collaborative filtering in action when you visit \n",
      "websites like Amazon and IMDB.\n",
      "Also known as social filtering, this approach essentially makes sug-\n",
      "gestions based on the recommendations and preferences of other peo-\n",
      "ple who share similar interests. \n",
      "More reading: Collaborative Filtering\n",
      "18. Can you list some disadvantages related to linear \n",
      "models?\n",
      "There are many disadvantages to using linear models, but the main \n",
      "ones are:\n",
      "• Errors in linearity assumptions \n",
      "• Lacks autocorrelation\n",
      "• It can’t solve overfitting problems \n",
      "• You can’t use it to calculate outcomes or binary outcomes\n",
      "More reading: What are the limitations of linear regression modeling \n",
      "in data analysis?\n",
      "19. What’s a feature vector?\n",
      "A feature vector is an n-dimensional vector that contains essential \n",
      "information that describes the characteristics of an object. For exam-\n",
      "ple, it can be an object’s numerical features or a list of numbers taken \n",
      "from the output of a neural network layer. \n",
      "\n",
      "\n",
      "93. In AI and data science, feature vectors can be used to represent \n",
      "numeric or symbolic characteristics of an object in mathematical \n",
      "terms for seamless analysis.\n",
      "Let’s break this down. A data set is usually organized into multiple \n",
      "examples where each example will have several features. However, a \n",
      "feature vector won’t have the same feature for numerous examples. \n",
      "Instead, each example will correspond to one feature vector that will \n",
      "contain all the numerical values for that example object. \n",
      "Feature vectors are often stacked into a design matrix. In this sce-\n",
      "nario, each row will be a feature vector for one example. Each column \n",
      "will feature all the examples that correspond to that particular fea-\n",
      "ture. This means that it will be like a matrix, but with just one row \n",
      "and multiple columns (or a single column and multiple rows) like \n",
      "[1,2,3,5,6,3,2,0].\n",
      "More reading: Extract a feature vector for any image with PyTorch\n",
      "20. What are the typical characteristics of elements \n",
      "in a list and a dictionary?\n",
      "In lists, elements maintain their order unless they are explicitly com-\n",
      "manded to re-order. These can be made up of any data type that can \n",
      "be all the same or mixed. However, elements in lists can only be \n",
      "accessed via numeric, zero-based indices.\n",
      "In a dictionary, the order isn’t guaranteed. However, each entry will \n",
      "be assigned a key and a value. As a result, elements within a diction-\n",
      "ary can be accessed by using their individual key.\n",
      "So whenever you have a set of unique keys, you have to use a diction-\n",
      "ary. Whenever a collection of items are in order, you can use a list.\n",
      "It’s difficult to predict how an AI interview will unfold, so if they fol-\n",
      "low up by asking you how to get a list of all the keys in a dictionary, \n",
      "respond with the following:\n",
      "\n",
      "\n",
      "94. To obtain a list of keys in a dictionary, you’ll have to use the following \n",
      "function keys():\n",
      "mydict={‘a’:1,’b’:2,’c’:3,’e’:5}\n",
      "mydict.keys()\n",
      "dict_keys([‘a’, ‘b’, ‘c’, ‘e’])\n",
      "More reading: How to Sort Python Dictionaries by Key or Value\n",
      "21. What’s selection bias? What other types of biases \n",
      "could you encounter during sampling?\n",
      "When you’re dealing with a non-random sample, selection bias will \n",
      "occur due to flaws in the selection process. This happens when a sub-\n",
      "set of the data is consistently excluded because of a particular attrib-\n",
      "ute. This exclusion will distort results and influence the statistical \n",
      "significance of the test.\n",
      "Other types of biases include survivorship bias and undercoverage \n",
      "bias. It’s important to always consider and reduce such biases \n",
      "because you’ll want your smart algorithms to make accurate predic-\n",
      "tions based on the data.\n",
      "More reading: Mitigating Bias in AI Models\n",
      "22. What’s a random forest? Could you explain its \n",
      "role in AI?\n",
      "A random forest is a data construct that’s applied to ML projects to \n",
      "develop a large number of random decision trees while analyzing var-\n",
      "iables. \n",
      "\n",
      "\n",
      "95. (Source)\n",
      "These algorithms can be leveraged to improve the way technologies \n",
      "analyze complex data sets. The basic premise here is that multiple \n",
      "weak learners can be combined to build one strong learner. \n",
      "This is an excellent tool for AI and ML projects because it can work \n",
      "with large labeled and unlabeled data sets with a large number of \n",
      "attributes. It can also maintain accuracy when some data is missing. \n",
      "As it can model the importance of attributes, it can be used for dimen-\n",
      "sionality reduction.\n",
      "More reading: Machine Learning With Random Forest\n",
      "23. What’s an eigenvalue? What about an \n",
      "eigenvector?\n",
      "The directions along which a particular linear transformation com-\n",
      "presses, flips, or stretches is called eigenvalue. Eigenvectors are used \n",
      "to understand these linear transformations. \n",
      "\n",
      "\n",
      "96. For example, to make better sense of the covariance of the covariance \n",
      "matrix, the eigenvector will help identify the direction in which the \n",
      "covariances are going. The eigenvalues will express the importance of \n",
      "each feature. \n",
      "Eigenvalues and eigenvectors are both critical to computer vision and \n",
      "ML applications. The most popular of these is known as principal \n",
      "component analysis for dimensionality reduction (e.g., eigenfaces for \n",
      "face recognition).\n",
      "More reading: What are eigenvectors and eigenvalues?\n",
      "24. Would you use batch normalization? If so, can \n",
      "you explain why?\n",
      "The idea here is to standardize the data before sending it to another \n",
      "layer. This approach helps reduce the impact of previous layers by \n",
      "keeping the mean and variance constant. It also makes the layers \n",
      "independent of each other to achieve rapid convergence. For example, \n",
      "when we normalize features from 0 to 1 or from 1 to 100, it helps \n",
      "accelerate the learning cycle.\n",
      "Check out this video: Why Does Batch Normalization Work?\n",
      "Artificial Intelligence Questions: \n",
      "Programming\n",
      "AI interview questions are bound to enter the sphere of programming \n",
      "sooner rather than later. So let’s dive right into it with the following \n",
      "AI questions and answers.\n",
      "\n",
      "\n",
      "97. 25. What’s a hash table?\n",
      "There are two parts to a hash table. The first is an array, or the actual \n",
      "table where the data is stored, and the other is a mapping function \n",
      "that’s known as the hash function. \n",
      "It’s a data structure that implements an associative array abstract \n",
      "data type that can map key values. It can also compute an index into \n",
      "an array of slots or buckets where the desired value can be found.\n",
      "(Source)\n",
      "More reading: Basics of Hash Tables\n",
      "26. What are the different algorithm techniques you \n",
      "can use in AI and ML?\n",
      "Some algorithm techniques that can be leveraged are:\n",
      "• Learning to learn\n",
      "• Reinforcement learning (deep adversarial networks, \n",
      "q-learning, and temporal difference)\n",
      "• Semi-supervised learning\n",
      "\n",
      "\n",
      "98. • Supervised learning (decision trees, linear regression, naive \n",
      "bayes, nearest neighbor, neural networks, and support \n",
      "vector machines)\n",
      "• Transduction\n",
      "• Unsupervised learning (association rules and k-means \n",
      "clustering)\n",
      "More reading: Types of Machine Learning Algorithms You Should \n",
      "Know\n",
      "27. How would you go about choosing an algorithm \n",
      "to solve a business problem?\n",
      "First, you have to develop a “problem statement” that’s based on the \n",
      "problem provided by the business. This step is essential because it’ll \n",
      "help ensure that you fully understand the type of problem and the \n",
      "input and the output of the problem you want to solve.\n",
      "The problem statement should be simple and no more than a single \n",
      "sentence. For example, let’s consider enterprise spam that requires \n",
      "an algorithm to identify it. \n",
      "The problem statement would be: “Is the email fake/spam or not?”\n",
      "In this scenario, the identification of whether it’s fake/spam will be \n",
      "the output.\n",
      "Once you have defined the problem statement, you have to identify \n",
      "the appropriate algorithm from the following:\n",
      "• Any classification algorithm\n",
      "• Any clustering algorithm\n",
      "• Any regression algorithm\n",
      "• Any recommendation algorithm\n",
      "Which algorithm you use will depend on the specific problem you’re \n",
      "trying to solve. In this scenario, you can move forward with a cluster-\n",
      "\n",
      "\n",
      "99. ing algorithm and choose a k-means algorithm to achieve your goal of \n",
      "filtering spam from the email system.\n",
      "While examples aren’t always necessary when answering questions \n",
      "about artificial intelligence, sometimes it will help make it easier for \n",
      "you to get your point across.\n",
      "More reading: How to Choose ML Algorithm: Machine Learning Ques-\n",
      "tions & Answers Part – III\n",
      "28. When is it necessary to update an algorithm?\n",
      "You should update an algorithm when the underlying data source has \n",
      "been changed or whenever there’s a case of non-stationarity. The \n",
      "algorithm should also be updated when you want the model to evolve \n",
      "as data streams through the infrastructure.\n",
      "More reading: “The data has been changed” error when stepping from \n",
      "main form into subform\n",
      "29. What’s regularization?\n",
      "When you have underfitting or overfitting issues in a statistical \n",
      "model, you can use the regularization technique to resolve it. Regular-\n",
      "ization techniques like LASSO help penalize some model parameters if \n",
      "they are likely to lead to overfitting.\n",
      "If the interviewer follows up with a question about other methods \n",
      "that can be used to avoid overfitting, you can mention cross-valida-\n",
      "tion techniques such as k-folds cross-validation. \n",
      "Another approach is to keep the model simple by taking into account \n",
      "fewer variables and parameters. Doing this helps remove some of the \n",
      "noise in the training data.\n",
      "More reading: Machine Learning Explained: Regularization\n",
      "\n",
      "\n",
      "100. 30. What’s the difference between inductive, \n",
      "deductive, and abductive learning?\n",
      "Inductive learning describes smart algorithms that learn from a set \n",
      "of instances to draw conclusions. In statistical ML, k-nearest neighbor \n",
      "and support vector machine are good examples of inductive learning.\n",
      "There are three literals in (top-down) inductive learning: \n",
      "• Arithmetic literals\n",
      "• Equality and inequality\n",
      "• Predicates\n",
      "In deductive learning, the smart algorithms draw conclusions by fol-\n",
      "lowing a truth-generating structure (major premise, minor premise, \n",
      "and conclusion) and then improve them based on previous decisions. \n",
      "In this scenario, the ML algorithm engages in deductive reasoning \n",
      "using a decision tree.\n",
      "Abductive learning is a DL technique where conclusions are made \n",
      "based on various instances. With this approach, inductive reasoning \n",
      "is applied to causal relationships in deep neural networks.\n",
      "More reading: What’s the difference between “inductive, “deductive” \n",
      "and “abductive” reasoning?\n",
      "31. What steps would you take to evaluate the \n",
      "effectiveness of your ML model?\n",
      "You have to first split the data set into training and test sets. You also \n",
      "have the option of using a cross-validation technique to further seg-\n",
      "ment the data set into a composite of training and test sets within the \n",
      "data.\n",
      "Then you have to implement a choice selection of the performance \n",
      "metrics like the following:\n",
      "\n",
      "\n",
      "101. • Confusion matrix\n",
      "• Accuracy\n",
      "• Precision\n",
      "• Recall or sensitivity\n",
      "• Specificity\n",
      "• F1 score\n",
      "For the most part, you can use measures such as accuracy, confusion \n",
      "matrix, or F1 score. However, it’ll be critical for you to demonstrate \n",
      "that you understand the nuances of how each model can be measured \n",
      "by choosing the right performance measure to match the problem.\n",
      "More reading: Performance Metrics for Classification problems in \n",
      "Machine Learning\n",
      "32. What would you do if data in a data set were \n",
      "missing or corrupted?\n",
      "Whenever data is missing or corrupted, you either replace it with \n",
      "another value or drop those rows and columns altogether. In Pandas, \n",
      "both isNull() and dropNA() are handy tools to find missing or cor-\n",
      "rupted data and drop those values. You can also use the fillna()\n",
      "method to fill the invalid values in a placeholder—for example, “0.”\n",
      "More reading: 5 Ways To Handle Missing Values In Machine Learning \n",
      "Datasets\n",
      "33. Do you know how to build a simple neural \n",
      "network? Can you demonstrate how you would do it \n",
      "with Python code?\n",
      "These types of questions are designed to ascertain your programming \n",
      "skills. If you’re a coding ninja, you should be able to achieve this with \n",
      "nine lines of Python code.\n",
      "\n",
      "\n",
      "102. (Source)\n",
      "More reading: How to build a simple neural network in 9 lines of \n",
      "Python code\n",
      "34. Can you write a Python program to draw the \n",
      "flower shown below?\n",
      "(Source)\n",
      "\n",
      "\n",
      "103. Again, the interviewer is trying to test your coding skills. It’s always \n",
      "good to perceive these types of questions as an opportunity to show a \n",
      "potential employer what you can do. So demonstrate your program-\n",
      "ming skills without hesitation and with confidence. \n",
      "(Source)\n",
      "More reading: Drawing a flower with python turtle\n",
      "Artificial Intelligence Questions: General \n",
      "AI Interest\n",
      "The tech talent shortage has created fierce demand for your skills, but \n",
      "to land your “dream job” it’ll help if you can demonstrate your pas-\n",
      "sion for the field. Whether your next scheduled AI interview is with a \n",
      "startup or an established tech giant, be ready for wide-ranging ques-\n",
      "tions like the ones listed below.\n",
      "\n",
      "\n",
      "104. 35. Do you have research experience in AI?\n",
      "At present, a lot of work within the AI space is research-based. As a \n",
      "result, many organizations will be digging into your background to \n",
      "ascertain what kind of experience you have in this area. If you \n",
      "authored or co-authored research papers or have been supervised by \n",
      "industry leaders, make sure to share that information.\n",
      "In fact, take it a step further and have a summary of your research \n",
      "experience along with your research papers ready to share with the \n",
      "interviewing panel. \n",
      "However, if you don’t have any formal research experience, have an \n",
      "explanation ready. For example, you can talk about how your AI jour-\n",
      "ney started as a weekend hobby and grew into so much more within a \n",
      "space of two or three years.\n",
      "36. What’s the last AI-related research paper you \n",
      "read? What were your conclusions?\n",
      "If you’re passionate about AI, you have to keep up with scientific \n",
      "research within the field. An excellent place to start is by following \n",
      "ScienceDirect to keep track of published research papers along with \n",
      "what’s in the pipeline.\n",
      "\n",
      "\n",
      "105. (Source)\n",
      "37. What’s your favorite use case?\n",
      "Just like research, you should be up to date on what’s going on in the \n",
      "industry. As such, if you’re asked about use cases, make sure that you \n",
      "have a few examples in mind that you can share. Whenever possible, \n",
      "bring up your personal experiences. \n",
      "You can also share what’s happening in the industry. For example, if \n",
      "you’re interested in the use of AI in medical images, Health IT Analyt-\n",
      "ics has some interesting use cases:\n",
      "• Detecting Fractures And Other Musculoskeletal Injuries\n",
      "• Aiding In The Diagnosis Neurological Diseases\n",
      "• Flagging Thoracic Complications And Conditions\n",
      "• Screening For Common Cancers\n",
      "38. What conferences are you hoping to attend this \n",
      "year? Any keynote speeches you’re hoping to catch?\n",
      "Conferences are great places to network, attend workshops, learn, \n",
      "and grow. So if you’re planning to stick to a career in artificial intelli-\n",
      "gence, you should be going to some of these. For example, Deep \n",
      "Learning World has a great one every summer. \n",
      "This year’s event in Las Vegas will feature keynote speakers like Dr. \n",
      "Dyann Daley (founder and CEO of Predict Align Prevent), Siddha \n",
      "Ganju (solutions architect at Nvidia), and Dr. Alex Glushkovsky (prin-\n",
      "cipal data scientist at BMO Financial Group, and others). \n",
      "39. How is Google training data for self-driving cars?\n",
      "If you’re interested and heavily involved within this space, this ques-\n",
      "tion should be a no-brainer. If you know the answer, it’ll demonstrate \n",
      "your knowledge about a variety of ML methods and how ML is applied \n",
      "\n",
      "\n",
      "106. to autonomous vehicles. But even if you don’t know the answer, take \n",
      "a stab at it as it will show your creativity and inventive nature. \n",
      "Google has been using reCAPTCHA to source labeled data on store-\n",
      "fronts and traffic signs for many years now. The company also has \n",
      "been using training data collected by Sebastian Thrun, CEO of the \n",
      "Kitty Hawk Corporation and the co-founder (and former CEO) of \n",
      "Udacity. \n",
      "Such information, although it might not seem significant, will show a \n",
      "potential employer that you’re interested and excited about this field.\n",
      "More reading: Google X: Leveraging data and algorithms for self-driv-\n",
      "ing cars\n",
      "40. Where do you usually source your data sets?\n",
      "If you talk about AI projects that you’ve worked on in your free time, \n",
      "the interviewer will probably ask where you sourced your data sets. If \n",
      "you’re genuinely passionate about the field, you would have worked \n",
      "on enough projects to know where you can find free data sets.\n",
      "For example, here are some freely available public data sets that you \n",
      "should know about (without conducting a Google search):\n",
      "• CelebFaces (with 200,000 celebrity images along with 40 \n",
      "attribute annotations)\n",
      "• CIFAR (with 60,000 images that map 10 different classes)\n",
      "• YouTube-8M (with over 4,000 annotated entities taken \n",
      "from an enormous data set of YouTube videos)\n",
      "Researchers have released hundreds of free resources like these along \n",
      "with the actual network architecture and weights used in their exam-\n",
      "ples. So it will serve you well to explore some of these data sets and \n",
      "run some experiments before heading out for an AI interview.\n",
      "More reading: How to find datasets for Artificial Intelligence training\n",
      "\n",
      "\n",
      "107. 41 Essential Machine \n",
      "Learning Interview \n",
      "Questions\n",
      "www.springboard.com\n",
      "18 mins read\n",
      "\n",
      "\n",
      "108. M\n",
      "achine learning interview questions are an integral part \n",
      "of the data science interview and the path to becoming a \n",
      "data scientist, machine learning engineer, or data engi-\n",
      "neer. Springboard created a free guide to data science interviews, so \n",
      "we know exactly how they can trip up candidates! In order to help \n",
      "resolve that, here is a curated and created a list of key questions that \n",
      "you could see in a machine learning interview. There are some \n",
      "answers to go along with them so you don’t get stumped. You’ll be \n",
      "able to do well in any job interview (even for a machine learning \n",
      "internship) with after reading through this piece.\n",
      "Machine Learning Interview Questions: \n",
      "Categories\n",
      "We’ve traditionally seen machine learning interview questions pop up \n",
      "in several categories. The first really has to do with the algorithms \n",
      "and theory behind machine learning. You’ll have to show an under-\n",
      "standing of how algorithms compare with one another and how to \n",
      "measure their efficacy and accuracy in the right way. The second cat-\n",
      "egory has to do with your programming skills and your ability to exe-\n",
      "cute on top of those algorithms and the theory. The third has to do \n",
      "with your general interest in machine learning: you’ll be asked about \n",
      "what’s going on in the industry and how you keep up with the latest \n",
      "machine learning trends. Finally, there are company or industry-spe-\n",
      "cific questions that test your ability to take your general machine \n",
      "\n",
      "\n",
      "109. learning knowledge and turn it into actionable points to drive the bot-\n",
      "tom line forward.\n",
      "We’ve divided this guide to machine learning interview questions into \n",
      "the categories we mentioned above so that you can more easily get to \n",
      "the information you need when it comes to machine learning inter-\n",
      "view questions.\n",
      "Machine Learning Interview Questions: \n",
      "Algorithms/Theory\n",
      "These algorithms questions will test your grasp of the theory behind \n",
      "machine learning.\n",
      "Q1- What’s the trade-off between bias and variance?\n",
      "More reading: Bias-Variance Tradeoff (Wikipedia)\n",
      "Bias is error due to erroneous or overly simplistic assumptions in the \n",
      "learning algorithm you’re using. This can lead to the model underfit-\n",
      "ting your data, making it hard for it to have high predictive accuracy \n",
      "and for you to generalize your knowledge from the training set to the \n",
      "test set.\n",
      "Variance is error due to too much complexity in the learning algo-\n",
      "rithm you’re using. This leads to the algorithm being highly sensitive \n",
      "to high degrees of variation in your training data, which can lead your \n",
      "model to overfit the data. You’ll be carrying too much noise from your \n",
      "training data for your model to be very useful for your test data.\n",
      "The bias-variance decomposition essentially decomposes the learning \n",
      "error from any algorithm by adding the bias, the variance and a bit of \n",
      "irreducible error due to noise in the underlying dataset. Essentially, if \n",
      "you make the model more complex and add more variables, you’ll lose \n",
      "bias but gain some variance — in order to get the optimally reduced \n",
      "amount of error, you’ll have to tradeoff bias and variance. You don’t \n",
      "want either high bias or high variance in your model.\n",
      "\n",
      "\n",
      "110. Q2- What is the difference between supervised and unsu-\n",
      "pervised machine learning?\n",
      "More reading: What is the difference between supervised and unsuper-\n",
      "vised machine learning? (Quora)\n",
      "Supervised learning requires training labeled data. For example, in \n",
      "order to do classification (a supervised learning task), you’ll need to \n",
      "first label the data you’ll use to train the model to classify data into \n",
      "your labeled groups. Unsupervised learning, in contrast, does not \n",
      "require labeling data explicitly.\n",
      "Q3- How is KNN different from k-means clustering?\n",
      "More reading: How is the k-nearest neighbor algorithm different from \n",
      "k-means clustering? (Quora)\n",
      "K-Nearest Neighbors is a supervised classification algorithm, while \n",
      "k-means clustering is an unsupervised clustering algorithm. While the \n",
      "mechanisms may seem similar at first, what this really means is that \n",
      "in order for K-Nearest Neighbors to work, you need labeled data you \n",
      "want to classify an unlabeled point into (thus the nearest neighbor \n",
      "part). K-means clustering requires only a set of unlabeled points and \n",
      "a threshold: the algorithm will take unlabeled points and gradually \n",
      "learn how to cluster them into groups by computing the mean of the \n",
      "distance between different points.\n",
      "The critical difference here is that KNN needs labeled points and is \n",
      "thus supervised learning, while k-means doesn’t — and is thus unsu-\n",
      "pervised learning.\n",
      "Q4- Explain how a ROC curve works.\n",
      "More reading: Receiver operating characteristic (Wikipedia)\n",
      "The ROC curve is a graphical representation of the contrast between \n",
      "true positive rates and the false positive rate at various thresholds. \n",
      "It’s often used as a proxy for the trade-off between the sensitivity of \n",
      "\n",
      "\n",
      "111. the model (true positives) vs the fall-out or the probability it will trig-\n",
      "ger a false alarm (false positives).\n",
      "Q5- Define precision and recall.\n",
      "More reading: Precision and recall (Wikipedia)\n",
      "Recall is also known as the true positive rate: the amount of positives \n",
      "your model claims compared to the actual number of positives there \n",
      "are throughout the data. Precision is also known as the positive pre-\n",
      "dictive value, and it is a measure of the amount of accurate positives \n",
      "your model claims compared to the number of positives it actually \n",
      "claims. It can be easier to think of recall and precision in the context \n",
      "of a case where you’ve predicted that there were 10 apples and 5 \n",
      "oranges in a case of 10 apples. You’d have perfect recall (there are \n",
      "actually 10 apples, and you predicted there would be 10) but 66.7% \n",
      "precision because out of the 15 events you predicted, only 10 (the \n",
      "apples) are correct.\n",
      "\n",
      "\n",
      "112. Q6- What is Bayes’ Theorem? How is it useful in a machine \n",
      "learning context?\n",
      "More reading: An Intuitive (and Short) Explanation of Bayes’ Theorem \n",
      "(BetterExplained)\n",
      "Bayes’ Theorem gives you the posterior probability of an event given \n",
      "what is known as prior knowledge.\n",
      "Mathematically, it’s expressed as the true positive rate of a condition \n",
      "sample divided by the sum of the false positive rate of the population \n",
      "and the true positive rate of a condition. Say you had a 60% chance of \n",
      "actually having the flu after a flu test, but out of people who had the \n",
      "flu, the test will be false 50% of the time, and the overall population \n",
      "only has a 5% chance of having the flu. Would you actually have a \n",
      "60% chance of having the flu after having a positive test?\n",
      "Bayes’ Theorem says no. It says that you have a (.6 * 0.05) (True Pos-\n",
      "itive Rate of a Condition Sample) / (.6*0.05)(True Positive Rate of a \n",
      "Condition Sample) + (.5*0.95) (False Positive Rate of a Population)  = \n",
      "0.0594 or 5.94% chance of getting a flu.\n",
      "Bayes’ Theorem is the basis behind a branch of machine learning that \n",
      "most notably includes the Naive Bayes classifier. That’s something \n",
      "\n",
      "\n",
      "113. important to consider when you’re faced with machine learning inter-\n",
      "view questions.\n",
      "Q7- Why is “Naive” Bayes naive?\n",
      "More reading: Why is “naive Bayes” naive? (Quora)\n",
      "Despite its practical applications, especially in text mining, Naive \n",
      "Bayes is considered “Naive” because it makes an assumption that is \n",
      "virtually impossible to see in real-life data: the conditional probabil-\n",
      "ity is calculated as the pure product of the individual probabilities of \n",
      "components. This implies the absolute independence of features — a \n",
      "condition probably never met in real life.\n",
      "As a Quora commenter put it whimsically, a Naive Bayes classifier \n",
      "that figured out that you liked pickles and ice cream would probably \n",
      "naively recommend you a pickle ice cream.\n",
      "Q8- Explain the difference between L1 and L2 regulariza-\n",
      "tion.\n",
      "More reading: What is the difference between L1 and L2 regulariza-\n",
      "tion? (Quora)\n",
      "L2 regularization tends to spread error among all the terms, while L1 \n",
      "is more binary/sparse, with many variables either being assigned a 1 \n",
      "or 0 in weighting. L1 corresponds to setting a Laplacean prior on the \n",
      "terms, while L2 corresponds to a Gaussian prior.\n",
      "\n",
      "\n",
      "114. Q9- What’s your favorite algorithm, and can you explain it \n",
      "to me in less than a minute?\n",
      "This type of question tests your understanding of how to communi-\n",
      "cate complex and technical nuances with poise and the ability to sum-\n",
      "marize quickly and efficiently. Make sure you have a choice and make \n",
      "sure you can explain different algorithms so simply and effectively \n",
      "that a five-year-old could grasp the basics!\n",
      "Q10- What’s the difference between Type I and Type II \n",
      "error?\n",
      "More reading: Type I and type II errors (Wikipedia)\n",
      "Don’t think that this is a trick question! Many machine learning inter-\n",
      "view questions will be an attempt to lob basic questions at you just to \n",
      "make sure you’re on top of your game and you’ve prepared all of your \n",
      "bases.\n",
      "Type I error is a false positive, while Type II error is a false negative. \n",
      "Briefly stated, Type I error means claiming something has happened \n",
      "when it hasn’t, while Type II error means that you claim nothing is \n",
      "happening when in fact something is.\n",
      "A clever way to think about this is to think of Type I error as telling a \n",
      "man he is pregnant, while Type II error means you tell a pregnant \n",
      "woman she isn’t carrying a baby.\n",
      "Q11- What’s a Fourier transform?\n",
      "More reading: Fourier transform (Wikipedia)\n",
      "\n",
      "\n",
      "115. A Fourier transform is a generic method to decompose generic func-\n",
      "tions into a superposition of symmetric functions. Or as this more \n",
      "intuitive tutorial puts it, given a smoothie, it’s how we find the rec-\n",
      "ipe. The Fourier transform finds the set of cycle speeds, amplitudes \n",
      "and phases to match any time signal. A Fourier transform converts a \n",
      "signal from time to frequency domain — it’s a very common way to \n",
      "extract features from audio signals or other time series such as sen-\n",
      "sor data.\n",
      "Q12- What’s the difference between probability and likeli-\n",
      "hood?\n",
      "More reading: What is the difference between “likelihood” and “proba-\n",
      "bility”? (Cross Validated)\n",
      "Q13- What is deep learning, and how does it contrast with \n",
      "other machine learning algorithms?\n",
      "More reading: Deep learning (Wikipedia)\n",
      "Deep learning is a subset of machine learning that is concerned with \n",
      "neural networks: how to use backpropagation and certain principles \n",
      "\n",
      "\n",
      "116. from neuroscience to more accurately model large sets of unlabelled \n",
      "or semi-structured data. In that sense, deep learning represents an \n",
      "unsupervised learning algorithm that learns representations of data \n",
      "through the use of neural nets.\n",
      "Q14- What’s the difference between a generative and dis-\n",
      "criminative model?\n",
      "More reading: What is the difference between a Generative and Dis-\n",
      "criminative Algorithm? (Stack Overflow)\n",
      "A generative model will learn categories of data while a discrimina-\n",
      "tive model will simply learn the distinction between different catego-\n",
      "ries of data. Discriminative models will generally outperform \n",
      "generative models on classification tasks.\n",
      "Q15- What cross-validation technique would you use on a \n",
      "time series dataset?\n",
      "More reading: Using k-fold cross-validation for time-series model \n",
      "selection (CrossValidated)\n",
      "Instead of using standard k-folds cross-validation, you have to pay \n",
      "attention to the fact that a time series is not randomly distributed \n",
      "data — it is inherently ordered by chronological order. If a pattern \n",
      "emerges in later time periods for example, your model may still pick \n",
      "up on it even if that effect doesn’t hold in earlier years!\n",
      "You’ll want to do something like forward chaining where you’ll be \n",
      "able to model on past data then look at forward-facing data.\n",
      "• fold 1 : training [1], test [2]\n",
      "• fold 2 : training [1 2], test [3]\n",
      "• fold 3 : training [1 2 3], test [4]\n",
      "• fold 4 : training [1 2 3 4], test [5]\n",
      "• fold 5 : training [1 2 3 4 5], test [6]\n",
      "Q16- How is a decision tree pruned?\n",
      "\n",
      "\n",
      "117. More reading: Pruning (decision trees)\n",
      "Pruning is what happens in decision trees when branches that have \n",
      "weak predictive power are removed in order to reduce the complexity \n",
      "of the model and increase the predictive accuracy of a decision tree \n",
      "model. Pruning can happen bottom-up and top-down, with \n",
      "approaches such as reduced error pruning and cost complexity prun-\n",
      "ing.\n",
      "Reduced error pruning is perhaps the simplest version: replace each \n",
      "node. If it doesn’t decrease predictive accuracy, keep it pruned. While \n",
      "simple, this heuristic actually comes pretty close to an approach that \n",
      "would optimize for maximum accuracy.\n",
      "Q17- Which is more important to you– model accuracy, or \n",
      "model performance?\n",
      "More reading: Accuracy paradox (Wikipedia)\n",
      "This question tests your grasp of the nuances of machine learning \n",
      "model performance! Machine learning interview questions often look \n",
      "towards the details. There are models with higher accuracy that can \n",
      "perform worse in predictive power — how does that make sense?\n",
      "Well, it has everything to do with how model accuracy is only a sub-\n",
      "set of model performance, and at that, a sometimes misleading one. \n",
      "For example, if you wanted to detect fraud in a massive dataset with \n",
      "a sample of millions, a more accurate model would most likely predict \n",
      "no fraud at all if only a vast minority of cases were fraud. However, \n",
      "this would be useless for a predictive model — a model designed to \n",
      "find fraud that asserted there was no fraud at all! Questions like this \n",
      "help you demonstrate that you understand model accuracy isn’t the \n",
      "be-all and end-all of model performance.\n",
      "Q18- What’s the F1 score? How would you use it?\n",
      "More reading: F1 score (Wikipedia)\n",
      "\n",
      "\n",
      "118. The F1 score is a measure of a model’s performance. It is a weighted \n",
      "average of the precision and recall of a model, with results tending to \n",
      "1 being the best, and those tending to 0 being the worst. You would \n",
      "use it in classification tests where true negatives don’t matter much.\n",
      "Q19- How would you handle an imbalanced dataset?\n",
      "More reading: 8 Tactics to Combat Imbalanced Classes in Your \n",
      "Machine Learning Dataset (Machine Learning Mastery)\n",
      "An imbalanced dataset is when you have, for example, a classification \n",
      "test and 90% of the data is in one class. That leads to problems: an \n",
      "accuracy of 90% can be skewed if you have no predictive power on \n",
      "the other category of data! Here are a few tactics to get over the \n",
      "hump:\n",
      "1- Collect more data to even the imbalances in the dataset.\n",
      "2- Resample the dataset to correct for imbalances.\n",
      "3- Try a different algorithm altogether on your dataset.\n",
      "What’s important here is that you have a keen sense for what damage \n",
      "an unbalanced dataset can cause, and how to balance that.\n",
      "Q20- When should you use classification over regression?\n",
      "More reading: Regression vs Classification (Math StackExchange)\n",
      "Classification produces discrete values and dataset to strict catego-\n",
      "ries, while regression gives you continuous results that allow you to \n",
      "better distinguish differences between individual points. You would \n",
      "use classification over regression if you wanted your results to reflect \n",
      "the belongingness of data points in your dataset to certain explicit \n",
      "categories (ex: If you wanted to know whether a name was male or \n",
      "female rather than just how correlated they were with male and \n",
      "female names.)\n",
      "\n",
      "\n",
      "119. Q21- Name an example where ensemble techniques might \n",
      "be useful.\n",
      "More reading: Ensemble learning (Wikipedia)\n",
      "Ensemble techniques use a combination of learning algorithms to \n",
      "optimize better predictive performance. They typically reduce overfit-\n",
      "ting in models and make the model more robust (unlikely to be influ-\n",
      "enced by small changes in the training data). \n",
      "You could list some examples of ensemble methods, from bagging to \n",
      "boosting to a “bucket of models” method and demonstrate how they \n",
      "could increase predictive power.\n",
      "Q22- How do you ensure you’re not overfitting with a \n",
      "model? \n",
      "More reading: How can I avoid overfitting? (Quora)\n",
      "This is a simple restatement of a fundamental problem in machine \n",
      "learning: the possibility of overfitting training data and carrying the \n",
      "noise of that data through to the test set, thereby providing inaccu-\n",
      "rate generalizations.\n",
      "There are three main methods to avoid overfitting:\n",
      "1- Keep the model simpler: reduce variance by taking into account \n",
      "fewer variables and parameters, thereby removing some of the noise \n",
      "in the training data.\n",
      "2- Use cross-validation techniques such as k-folds cross-validation.\n",
      "3- Use regularization techniques such as LASSO that penalize certain \n",
      "model parameters if they’re likely to cause overfitting.\n",
      "Q23- What evaluation approaches would you work to \n",
      "gauge the effectiveness of a machine learning model?\n",
      "More reading: How to Evaluate Machine Learning Algorithms (Machine \n",
      "Learning Mastery)\n",
      "\n",
      "\n",
      "120. You would first split the dataset into training and test sets, or per-\n",
      "haps use cross-validation techniques to further segment the dataset \n",
      "into composite sets of training and test sets within the data. You \n",
      "should then implement a choice selection of performance metrics: \n",
      "here is a fairly comprehensive list. You could use measures such as \n",
      "the F1 score, the accuracy, and the confusion matrix. What’s \n",
      "important here is to demonstrate that you understand the nuances of \n",
      "how a model is measured and how to choose the right performance \n",
      "measures for the right situations.\n",
      "Q24- How would you evaluate a logistic regression model?\n",
      "More reading: Evaluating a logistic regression (CrossValidated), Lo-\n",
      "gistic Regression in Plain English\n",
      "A subsection of the question above. You have to demonstrate an \n",
      "understanding of what the typical goals of a logistic regression are \n",
      "(classification, prediction, etc.) and bring up a few examples and use \n",
      "cases.\n",
      "Q25- What’s the “kernel trick” and how is it useful?\n",
      "More reading: Kernel method (Wikipedia)\n",
      "The Kernel trick involves kernel functions that can enable in higher-\n",
      "dimension spaces without explicitly calculating the coordinates of \n",
      "points within that dimension: instead, kernel functions compute the \n",
      "inner products between the images of all pairs of data in a feature \n",
      "space. This allows them the very useful attribute of calculating the \n",
      "coordinates of higher dimensions while being computationally \n",
      "cheaper than the explicit calculation of said coordinates. Many algo-\n",
      "rithms can be expressed in terms of inner products. Using the kernel \n",
      "trick enables us effectively run algorithms in a high-dimensional \n",
      "space with lower-dimensional data.\n",
      "(Learn about Springboard’s AI / Machine Learning \n",
      "Bootcamp, the first of its kind to come with a job guaran-\n",
      "tee.)\n",
      "\n",
      "\n",
      "121. Machine Learning Interview Questions: \n",
      "Programming\n",
      "These machine learning interview questions test your knowledge of \n",
      "programming principles you need to implement machine learning \n",
      "principles in practice. Machine learning interview questions tend to \n",
      "be technical questions that test your logic and programming skills: \n",
      "this section focuses more on the latter.\n",
      "Q26- How do you handle missing or corrupted data in a \n",
      "dataset?\n",
      "More reading: Handling missing data (O’Reilly)\n",
      "You could find missing/corrupted data in a dataset and either drop \n",
      "those rows or columns, or decide to replace them with another value.\n",
      "In Pandas, there are two very useful methods: isnull() and dropna() \n",
      "that will help you find columns of data with missing or corrupted \n",
      "data and drop those values. If you want to fill the invalid values with \n",
      "a placeholder value (for example, 0), you could use the fillna() \n",
      "method.\n",
      "Q27- Do you have experience with Spark or big data tools \n",
      "for machine learning?\n",
      "More reading: 50 Top Open Source Tools for Big Data (Datamation)\n",
      "You’ll want to get familiar with the meaning of big data for different \n",
      "companies and the different tools they’ll want. Spark is the big data \n",
      "tool most in demand now, able to handle immense datasets with \n",
      "speed. Be honest if you don’t have experience with the tools \n",
      "\n",
      "\n",
      "122. demanded, but also take a look at job descriptions and see what tools \n",
      "pop up: you’ll want to invest in familiarizing yourself with them.\n",
      "Q28- Pick an algorithm. Write the psuedo-code for a paral-\n",
      "lel implementation.\n",
      "More reading: Writing pseudocode for parallel programming (Stack \n",
      "Overflow)\n",
      "This kind of question demonstrates your ability to think in parallel-\n",
      "ism and how you could handle concurrency in programming imple-\n",
      "mentations dealing with big data. Take a look at pseudocode \n",
      "frameworks such as Peril-L and visualization tools such as Web \n",
      "Sequence Diagrams to help you demonstrate your ability to write code \n",
      "that reflects parallelism.\n",
      "Q29- What are some differences between a linked list and \n",
      "an array?\n",
      "More reading: Array versus linked list (Stack Overflow)\n",
      "An array is an ordered collection of objects. A linked list is a series of \n",
      "objects with pointers that direct how to process them sequentially. An \n",
      "array assumes that every element has the same size, unlike the linked \n",
      "list. A linked list can more easily grow organically: an array has to be \n",
      "pre-defined or re-defined for organic growth. Shuffling a linked list \n",
      "involves changing which points direct where — meanwhile, shuffling \n",
      "an array is more complex and takes more memory.\n",
      "Q30- Describe a hash table.\n",
      "More reading: Hash table (Wikipedia)\n",
      "A hash table is a data structure that produces an associative array. A \n",
      "key is mapped to certain values through the use of a hash function. \n",
      "They are often used for tasks such as database indexing.\n",
      "\n",
      "\n",
      "123. Q31- Which data visualization libraries do you use? What \n",
      "are your thoughts on the best data visualization tools?\n",
      "More reading: 31 Free Data Visualization Tools (Springboard)\n",
      "What’s important here is to define your views on how to properly vis-\n",
      "ualize data and your personal preferences when it comes to tools. \n",
      "Popular tools include R’s ggplot, Python’s seaborn and matplotlib, and \n",
      "tools such as Plot.ly and Tableau.\n",
      "Related: 20 Python Interview Questions\n",
      "Machine Learning Interview Questions: \n",
      "Company/Industry Specific\n",
      "These machine learning interview questions deal with how to imple-\n",
      "ment your general machine learning knowledge to a specific compa-\n",
      "ny’s requirements. You’ll be asked to create case studies and extend \n",
      "your knowledge of the company and industry you’re applying for with \n",
      "your machine learning skills.\n",
      "Q32- How would you implement a recommendation system \n",
      "for our company’s users?\n",
      "More reading: How to Implement A Recommendation System? (Stack \n",
      "Overflow)\n",
      "A lot of machine learning interview questions of this type will involve \n",
      "implementation of machine learning models to a company’s problems. \n",
      "You’ll have to research the company and its industry in-depth, espe-\n",
      "\n",
      "\n",
      "124. cially the revenue drivers the company has, and the types of users the \n",
      "company takes on in the context of the industry it’s in.\n",
      "Q33- How can we use your machine learning skills to gen-\n",
      "erate revenue?\n",
      "More reading: Startup Metrics for Startups (500 Startups)\n",
      "This is a tricky question. The ideal answer would demonstrate \n",
      "knowledge of what drives the business and how your skills could \n",
      "relate. For example, if you were interviewing for music-streaming \n",
      "startup Spotify, you could remark that your skills at developing a bet-\n",
      "ter recommendation model would increase user retention, which \n",
      "would then increase revenue in the long run.\n",
      "The startup metrics Slideshare linked above will help you understand \n",
      "exactly what performance indicators are important for startups and \n",
      "tech companies as they think about revenue and growth.\n",
      "Q34- What do you think of our current data process?\n",
      "More reading: The Data Science Process Email Course – Springboard\n",
      "\n",
      "\n",
      "125. This kind of question requires you to listen carefully and impart feed-\n",
      "back in a manner that is constructive and insightful. Your interviewer \n",
      "is trying to gauge if you’d be a valuable member of their team and \n",
      "whether you grasp the nuances of why certain things are set the way \n",
      "they are in the company’s data process based on company- or indus-\n",
      "try-specific conditions. They’re trying to see if you can be an intellec-\n",
      "tual peer. Act accordingly.\n",
      "Machine Learning Interview Questions: General \n",
      "Machine Learning Interest\n",
      "This series of machine learning interview questions attempts to gauge \n",
      "your passion and interest in machine learning. The right answers will \n",
      "serve as a testament for your commitment to being a lifelong learner \n",
      "in machine learning.\n",
      "Q35- What are the last machine learning papers you’ve \n",
      "read?\n",
      "More reading: What are some of the best research papers/books for \n",
      "machine learning?\n",
      "Keeping up with the latest scientific literature on machine learning is \n",
      "a must if you want to demonstrate interest in a machine learning \n",
      "position. This overview of deep learning in Nature by the scions of \n",
      "deep learning themselves (from Hinton to Bengio to LeCun) can be a \n",
      "good reference paper and an overview of what’s happening in deep \n",
      "learning — and the kind of paper you might want to cite.\n",
      "Q36- Do you have research experience in machine learn-\n",
      "ing?\n",
      "\n",
      "\n",
      "126. Related to the last point, most organizations hiring for machine learn-\n",
      "ing positions will look for your formal experience in the field. \n",
      "Research papers, co-authored or supervised by leaders in the field, \n",
      "can make the difference between you being hired and not. Make sure \n",
      "you have a summary of your research experience and papers ready — \n",
      "and an explanation for your background and lack of formal research \n",
      "experience if you don’t.\n",
      "Q37- What are your favorite use cases of machine learning \n",
      "models?\n",
      "More reading: What are the typical use cases for different machine \n",
      "learning algorithms? (Quora)\n",
      "The Quora thread above contains some examples, such as decision \n",
      "trees that categorize people into different tiers of intelligence based \n",
      "on IQ scores. Make sure that you have a few examples in mind and \n",
      "describe what resonated with you. It’s important that you demon-\n",
      "strate an interest in how machine learning is implemented.\n",
      "Q38- How would you approach the “Netflix Prize” competi-\n",
      "tion?\n",
      "More reading: Netflix Prize (Wikipedia)\n",
      "The Netflix Prize was a famed competition where Netflix offered \n",
      "$1,000,000 for a better collaborative filtering algorithm. The team \n",
      "that won called BellKor had a 10% improvement and used an ensem-\n",
      "ble of different methods to win. Some familiarity with the case and its \n",
      "solution will help demonstrate you’ve paid attention to machine \n",
      "learning for a while.\n",
      "Q39- Where do you usually source datasets?\n",
      "More reading: 19 Free Public Data Sets For Your First Data Science \n",
      "Project (Springboard)\n",
      "Machine learning interview questions like these try to get at the heart \n",
      "of your machine learning interest. Somebody who is truly passionate \n",
      "\n",
      "\n",
      "127. about machine learning will have gone off and done side projects on \n",
      "their own, and have a good idea of what great datasets are out there. \n",
      "If you’re missing any, check out Quandl for economic and financial \n",
      "data, and Kaggle’s Datasets collection for another great list.\n",
      "Q40- How do you think Google is training data for self-\n",
      "driving cars?\n",
      "More reading: Waymo Tech\n",
      "Machine learning interview questions like this one really test your \n",
      "knowledge of different machine learning methods, and your inven-\n",
      "tiveness if you don’t know the answer. Google is currently using \n",
      "recaptcha to source labeled data on storefronts and traffic signs. They \n",
      "are also building on training data collected by Sebastian Thrun at \n",
      "GoogleX — some of which was obtained by his grad students driving \n",
      "buggies on desert dunes!\n",
      "Q41- How would you simulate the approach AlphaGo took \n",
      "to beat Lee Sidol at Go?\n",
      "More reading: Mastering the game of Go with deep neural networks \n",
      "and tree search (Nature)\n",
      "AlphaGo beating Lee Sidol, the best human player at Go, in a best-of-\n",
      "five series was a truly seminal event in the history of machine learn-\n",
      "ing and deep learning. The Nature paper above describes how this \n",
      "was accomplished with “Monte-Carlo tree search with deep neural \n",
      "networks that have been trained by supervised learning, from human \n",
      "expert games, and by reinforcement learning from games of self-\n",
      "play.”\n",
      "Related: 40 Artificial Intelligence Interview Questions\n",
      "Looking to land a role as a machine learning engineer? \n",
      "Find out about Springboard’s Machine Learning Engineer-\n",
      "ing Career Track, the first of its kind to come with a job \n",
      "guarantee.\n",
      "\n",
      "\n",
      "128. A.I. Wiki\n",
      "Interested in AI? Get tips & stories in your inbox:\n",
      " Directory\n",
      "A Beginner's Guide to Neural\n",
      "Networks and Deep Learning\n",
      "Contents\n",
      "Neural Network Definition\n",
      "A Few Concrete Examples\n",
      "Neural Network Elements\n",
      "Key Concepts of Deep Neural Networks\n",
      "Example: Feedforward Networks & Backprop\n",
      "Multiple Linear Regression\n",
      "Updaters\n",
      "Custom Layers, activation functions and loss functions\n",
      "Logistic Regression & Classifiers\n",
      "Loss Functions in DeepLearning4J\n",
      "Neural Networks & Artificial Intelligence\n",
      "Neural Network Definition\n",
      "Neural networks are a set of algorithms, modeled loosely after the human brain,\n",
      "that are designed to recognize patterns. They interpret sensory data through a\n",
      "kind of machine perception, labeling or clustering raw input. The patterns they\n",
      "recognize are numerical, contained in vectors, into which all real-world data, be it\n",
      "images, sound, text or time series, must be translated.\n",
      "Neural networks help us cluster and classify. You can think of them as a clustering\n",
      "and classification layer on top of the data you store and manage. They help to\n",
      "group unlabeled data according to similarities among the example inputs, and they\n",
      "classify data when they have a labeled dataset to train on. (Neural networks can\n",
      "also extract features that are fed to other algorithms for clustering and\n",
      "classification; so you can think of deep neural networks as components of larger\n",
      "machine-learning applications involving algorithms for reinforcement learning,\n",
      "classification and regression.)\n",
      "\n",
      "\n",
      "\n",
      "129. What kind of problems does deep learning solve, and more importantly, can it\n",
      "solve yours? To know the answer, you need to ask questions:\n",
      "What outcomes do I care about? Those outcomes are labels that could be\n",
      "applied to data: for example, spam or not_spam in an email filter, good_guy or\n",
      "bad_guy in fraud detection, angry_customer or happy_customer in customer\n",
      "relationship management.\n",
      "Do I have the data to accompany those labels? That is, can I find labeled\n",
      "data, or can I create a labeled dataset (with a service like AWS Mechanical\n",
      "Turk or Figure Eight or Mighty.ai) where spam has been labeled as spam, in\n",
      "order to teach an algorithm the correlation between labels and inputs?\n",
      "A Few Concrete Examples\n",
      "Deep learning maps inputs to outputs. It finds correlations. It is known as a\n",
      "“universal approximator”, because it can learn to approximate an unknown\n",
      "function f(x) = y between any input x and any output y, assuming they are\n",
      "related at all (by correlation or causation, for example). In the process of learning,\n",
      "a neural network finds the right f, or the correct manner of transforming x into y,\n",
      "whether that be f(x) = 3x + 12 or f(x) = 9x - 0.1. Here are a few examples of\n",
      "what deep learning can do.\n",
      "Classification\n",
      "All classification tasks depend upon labeled datasets; that is, humans must\n",
      "transfer their knowledge to the dataset in order for a neural network to learn the\n",
      "correlation between labels and data. This is known as supervised learning.\n",
      "Detect faces, identify people in images, recognize facial expressions (angry,\n",
      "joyful)\n",
      "Identify objects in images (stop signs, pedestrians, lane markers…)\n",
      "Recognize gestures in video\n",
      "Detect voices, identify speakers, transcribe speech to text, recognize\n",
      "sentiment in voices\n",
      "Classify text as spam (in emails), or fraudulent (in insurance claims);\n",
      "recognize sentiment in text (customer feedback)\n",
      "Any labels that humans can generate, any outcomes that you care about and\n",
      "which correlate to data, can be used to train a neural network.\n",
      "Clustering\n",
      "Clustering or grouping is the detection of similarities. Deep learning does not\n",
      "require labels to detect similarities. Learning without labels is called unsupervised\n",
      "learning. Unlabeled data is the majority of data in the world. One law of machine\n",
      "learning is: the more data an algorithm can train on, the more accurate it will be.\n",
      "Learn to build AI apps now »\n",
      "\n",
      "\n",
      "130. Therefore, unsupervised learning has the potential to produce highly accurate\n",
      "models.\n",
      "Search: Comparing documents, images or sounds to surface similar items.\n",
      "Anomaly detection: The flipside of detecting similarities is detecting\n",
      "anomalies, or unusual behavior. In many cases, unusual behavior correlates\n",
      "highly with things you want to detect and prevent, such as fraud.\n",
      "Predictive Analytics: Regressions\n",
      "With classification, deep learning is able to establish correlations between, say,\n",
      "pixels in an image and the name of a person. You might call this a static\n",
      "prediction. By the same token, exposed to enough of the right data, deep learning\n",
      "is able to establish correlations between present events and future events. It can\n",
      "run regression between the past and the future. The future event is like the label\n",
      "in a sense. Deep learning doesn’t necessarily care about time, or the fact that\n",
      "something hasn’t happened yet. Given a time series, deep learning may read a\n",
      "string of number and predict the number most likely to occur next.\n",
      "Hardware breakdowns (data centers, manufacturing, transport)\n",
      "Health breakdowns (strokes, heart attacks based on vital stats and data from\n",
      "wearables)\n",
      "Customer churn (predicting the likelihood that a customer will leave, based\n",
      "on web activity and metadata)\n",
      "Employee turnover (ditto, but for employees)\n",
      "The better we can predict, the better we can prevent and pre-empt. As you can\n",
      "see, with neural networks, we’re moving towards a world of fewer surprises. Not\n",
      "zero surprises, just marginally fewer. We’re also moving toward a world of smarter\n",
      "agents that combine neural networks with other algorithms like reinforcement\n",
      "learning to attain goals.\n",
      "With that brief overview of deep learning use cases, let’s look at what neural nets\n",
      "are made of.\n",
      "Neural Network Elements\n",
      "Deep learning is the name we use for “stacked neural networks”; that is, networks\n",
      "composed of several layers.\n",
      "Are you using Machine Learning for enterprise applications? The Skymind Platform\n",
      "can help you ship faster. Read the platform overview or request a demo.\n",
      "The layers are made of nodes. A node is just a place where computation happens,\n",
      "loosely patterned on a neuron in the human brain, which fires when it encounters\n",
      "sufficient stimuli. A node combines input from the data with a set of coefficients,\n",
      "or weights, that either amplify or dampen that input, thereby assigning\n",
      "significance to inputs with regard to the task the algorithm is trying to learn; e.g.\n",
      "which input is most helpful is classifying data without error? These input-weight\n",
      "\n",
      "\n",
      "131. products are summed and then the sum is passed through a node’s so-called\n",
      "activation function, to determine whether and to what extent that signal should\n",
      "progress further through the network to affect the ultimate outcome, say, an act\n",
      "of classification. If the signals passes through, the neuron has been “activated.”\n",
      "Here’s a diagram of what one node might look like.\n",
      "A node layer is a row of those neuron-like switches that turn on or off as the input\n",
      "is fed through the net. Each layer’s output is simultaneously the subsequent\n",
      "layer’s input, starting from an initial input layer receiving your data.\n",
      "Pairing the model’s adjustable weights with input features is how we assign\n",
      "significance to those features with regard to how the neural network classifies and\n",
      "clusters input.\n",
      "Key Concepts of Deep Neural Networks\n",
      "Deep-learning networks are distinguished from the more commonplace single-\n",
      "hidden-layer neural networks by their depth; that is, the number of node layers\n",
      "through which data must pass in a multistep process of pattern recognition.\n",
      "Earlier versions of neural networks such as the first perceptrons were shallow,\n",
      "composed of one input and one output layer, and at most one hidden layer in\n",
      "between. More than three layers (including input and output) qualifies as “deep”\n",
      "learning. So deep is not just a buzzword to make algorithms seem like they read\n",
      "Sartre and listen to bands you haven’t heard of yet. It is a strictly defined term\n",
      "that means more than one hidden layer.\n",
      "In deep-learning networks, each layer of nodes trains on a distinct set of features\n",
      "based on the previous layer’s output. The further you advance into the neural net,\n",
      "the more complex the features your nodes can recognize, since they aggregate\n",
      "and recombine features from the previous layer.\n",
      "\n",
      "\n",
      "132. This is known as feature hierarchy, and it is a hierarchy of increasing complexity\n",
      "and abstraction. It makes deep-learning networks capable of handling very large,\n",
      "high-dimensional data sets with billions of parameters that pass through\n",
      "nonlinear functions.\n",
      "Above all, these neural nets are capable of discovering latent structures within\n",
      "unlabeled, unstructured data, which is the vast majority of data in the world.\n",
      "Another word for unstructured data is raw media; i.e. pictures, texts, video and\n",
      "audio recordings. Therefore, one of the problems deep learning solves best is in\n",
      "processing and clustering the world’s raw, unlabeled media, discerning similarities\n",
      "and anomalies in data that no human has organized in a relational database or\n",
      "ever put a name to.\n",
      "For example, deep learning can take a million images, and cluster them according\n",
      "to their similarities: cats in one corner, ice breakers in another, and in a third all\n",
      "the photos of your grandmother. This is the basis of so-called smart photo\n",
      "albums.\n",
      "Now apply that same idea to other data types: Deep learning might cluster raw\n",
      "text such as emails or news articles. Emails full of angry complaints might cluster\n",
      "in one corner of the vector space, while satisfied customers, or spambot\n",
      "messages, might cluster in others. This is the basis of various messaging filters,\n",
      "and can be used in customer-relationship management (CRM). The same applies\n",
      "to voice messages.\n",
      "With time series, data might cluster around normal/healthy behavior and\n",
      "anomalous/dangerous behavior. If the time series data is being generated by a\n",
      "smart phone, it will provide insight into users’ health and habits; if it is being\n",
      "generated by an autopart, it might be used to prevent catastrophic breakdowns.\n",
      "Deep-learning networks perform automatic feature extraction without human\n",
      "intervention, unlike most traditional machine-learning algorithms. Given that\n",
      "feature extraction is a task that can take teams of data scientists years to\n",
      "accomplish, deep learning is a way to circumvent the chokepoint of limited\n",
      "experts. It augments the powers of small data science teams, which by their\n",
      "nature do not scale.\n",
      "\n",
      "\n",
      "133. When training on unlabeled data, each node layer in a deep network learns\n",
      "features automatically by repeatedly trying to reconstruct the input from which it\n",
      "draws its samples, attempting to minimize the difference between the network’s\n",
      "guesses and the probability distribution of the input data itself. Restricted\n",
      "Boltzmann machines, for examples, create so-called reconstructions in this\n",
      "manner.\n",
      "In the process, these neural networks learn to recognize correlations between\n",
      "certain relevant features and optimal results – they draw connections between\n",
      "feature signals and what those features represent, whether it be a full\n",
      "reconstruction, or with labeled data.\n",
      "A deep-learning network trained on labeled data can then be applied to\n",
      "unstructured data, giving it access to much more input than machine-learning\n",
      "nets. This is a recipe for higher performance: the more data a net can train on, the\n",
      "more accurate it is likely to be. (Bad algorithms trained on lots of data can\n",
      "outperform good algorithms trained on very little.) Deep learning’s ability to\n",
      "process and learn from huge quantities of unlabeled data give it a distinct\n",
      "advantage over previous algorithms.\n",
      "Deep-learning networks end in an output layer: a logistic, or softmax, classifier\n",
      "that assigns a likelihood to a particular outcome or label. We call that predictive,\n",
      "but it is predictive in a broad sense. Given raw data in the form of an image, a\n",
      "deep-learning network may decide, for example, that the input data is 90 percent\n",
      "likely to represent a person.\n",
      "Example: Feedforward Networks\n",
      "Our goal in using a neural net is to arrive at the point of least error as fast as\n",
      "possible. We are running a race, and the race is around a track, so we pass the\n",
      "same points repeatedly in a loop. The starting line for the race is the state in\n",
      "which our weights are initialized, and the finish line is the state of those\n",
      "parameters when they are capable of producing sufficiently accurate\n",
      "classifications and predictions.\n",
      "The race itself involves many steps, and each of those steps resembles the steps\n",
      "before and after. Just like a runner, we will engage in a repetitive act over and\n",
      "over to arrive at the finish. Each step for a neural network involves a guess, an\n",
      "error measurement and a slight update in its weights, an incremental adjustment\n",
      "to the coefficients, as it slowly learns to pay attention to the most important\n",
      "features.\n",
      "A collection of weights, whether they are in their start or end state, is also called\n",
      "a model, because it is an attempt to model data’s relationship to ground-truth\n",
      "labels, to grasp the data’s structure. Models normally start out bad and end up\n",
      "less bad, changing over time as the neural network updates its parameters.\n",
      "This is because a neural network is born in ignorance. It does not know which\n",
      "weights and biases will translate the input best to make the correct guesses. It\n",
      "has to start out with a guess, and then try to make better guesses sequentially as\n",
      "\n",
      "\n",
      "134. it learns from its mistakes. (You can think of a neural network as a miniature\n",
      "enactment of the scientific method, testing hypotheses and trying again – only it\n",
      "is the scientific method with a blindfold on. Or like a child: they are born not\n",
      "knowing much, and through exposure to life experience, they slowly learn to solve\n",
      "problems in the world. For neural networks, data is the only experience.)\n",
      "Here is a simple explanation of what happens during learning with a feedforward\n",
      "neural network, the simplest architecture to explain.\n",
      "Input enters the network. The coefficients, or weights, map that input to a set of\n",
      "guesses the network makes at the end.\n",
      "input * weight = guess\n",
      "Weighted input results in a guess about what that input is. The neural then takes\n",
      "its guess and compares it to a ground-truth about the data, effectively asking an\n",
      "expert “Did I get this right?”\n",
      "ground truth - guess = error\n",
      "The difference between the network’s guess and the ground truth is its error. The\n",
      "network measures that error, and walks the error back over its model, adjusting\n",
      "weights to the extent that they contributed to the error.\n",
      "error * weight's contribution to error = adjustment\n",
      "The three pseudo-mathematical formulas above account for the three key\n",
      "functions of neural networks: scoring input, calculating loss and applying an\n",
      "update to the model – to begin the three-step process over again. A neural\n",
      "network is a corrective feedback loop, rewarding weights that support its correct\n",
      "guesses, and punishing weights that lead it to err.\n",
      "Let’s linger on the first step above.\n",
      "Multiple Linear Regression\n",
      "Despite their biologically inspired name, artificial neural networks are nothing\n",
      "more than math and code, like any other machine-learning algorithm. In fact,\n",
      "anyone who understands linear regression, one of first methods you learn in\n",
      "statistics, can understand how a neural net works. In its simplest form, linear\n",
      "regression is expressed as\n",
      "   Y_hat = bX + a\n",
      "where Y_hat is the estimated output, X is the input, b is the slope and a is the\n",
      "intercept of a line on the vertical axis of a two-dimensional graph. (To make this\n",
      "more concrete: X could be radiation exposure and Y could be the cancer risk; X\n",
      "could be daily pushups and Y_hat could be the total weight you can benchpress; X\n",
      "the amount of fertilizer and Y_hat the size of the crop.) You can imagine that every\n",
      "time you add a unit to X, the dependent variable Y_hat increases proportionally, no\n",
      "matter how far along you are on the X axis. That simple relation between two\n",
      "variables moving up or down together is a starting point.\n",
      "\n",
      "\n",
      "135. The next step is to imagine multiple linear regression, where you have many input\n",
      "variables producing an output variable. It’s typically expressed like this:\n",
      "   Y_hat = b_1*X_1 + b_2*X_2 + b_3*X_3 + a\n",
      "(To extend the crop example above, you might add the amount of sunlight and\n",
      "rainfall in a growing season to the fertilizer variable, with all three affecting Y_hat.)\n",
      "Now, that form of multiple linear regression is happening at every node of a neural\n",
      "network. For each node of a single layer, input from each node of the previous\n",
      "layer is recombined with input from every other node. That is, the inputs are\n",
      "mixed in different proportions, according to their coefficients, which are different\n",
      "leading into each node of the subsequent layer. In this way, a net tests which\n",
      "combination of input is significant as it tries to reduce error.\n",
      "Once you sum your node inputs to arrive at Y_hat, it’s passed through a non-linear\n",
      "function. Here’s why: If every node merely performed multiple linear regression,\n",
      "Y_hat would increase linearly and without limit as the X’s increase, but that\n",
      "doesn’t suit our purposes.\n",
      "What we are trying to build at each node is a switch (like a neuron…) that turns on\n",
      "and off, depending on whether or not it should let the signal of the input pass\n",
      "through to affect the ultimate decisions of the network.\n",
      "When you have a switch, you have a classification problem. Does the input’s signal\n",
      "indicate the node should classify it as enough, or not_enough, on or off? A binary\n",
      "decision can be expressed by 1 and 0, and logistic regression is a non-linear\n",
      "function that squashes input to translate it to a space between 0 and 1.\n",
      "The nonlinear transforms at each node are usually s-shaped functions similar to\n",
      "logistic regression. They go by the names of sigmoid (the Greek word for “S”),\n",
      "tanh, hard tanh, etc., and they shaping the output of each node. The output of all\n",
      "nodes, each squashed into an s-shaped space between 0 and 1, is then passed as\n",
      "input to the next layer in a feed forward neural network, and so on until the signal\n",
      "reaches the final layer of the net, where decisions are made.\n",
      "Gradient Descent\n",
      "The name for one commonly used optimization function that adjusts weights\n",
      "according to the error they caused is called “gradient descent.”\n",
      "Gradient is another word for slope, and slope, in its typical form on an x-y graph,\n",
      "represents how two variables relate to each other: rise over run, the change in\n",
      "money over the change in time, etc. In this particular case, the slope we care\n",
      "about describes the relationship between the network’s error and a single weight;\n",
      "i.e. that is, how does the error vary as the weight is adjusted.\n",
      "To put a finer point on it, which weight will produce the least error? Which one\n",
      "correctly represents the signals contained in the input data, and translates them\n",
      "to a correct classification? Which one can hear “nose” in an input image, and\n",
      "know that should be labeled as a face and not a frying pan?\n",
      "\n",
      "\n",
      "136. As a neural network learns, it slowly adjusts many weights so that they can map\n",
      "signal to meaning correctly. The relationship between network Error and each of\n",
      "those weights is a derivative, dE/dw, that measures the degree to which a slight\n",
      "change in a weight causes a slight change in the error.\n",
      "Each weight is just one factor in a deep network that involves many transforms;\n",
      "the signal of the weight passes through activations and sums over several layers,\n",
      "so we use the chain rule of calculus to march back through the networks\n",
      "activations and outputs and finally arrive at the weight in question, and its\n",
      "relationship to overall error.\n",
      "The chain rule in calculus states that\n",
      "In a feedforward network, the relationship between the net’s error and a single\n",
      "weight will look something like this:\n",
      "That is, given two variables, Error and weight, that are mediated by a third\n",
      "variable, activation, through which the weight is passed, you can calculate how a\n",
      "change in weight affects a change in Error by first calculating how a change in\n",
      "activation affects a change in Error, and how a change in weight affects a change\n",
      "in activation.\n",
      "The essence of learning in deep learning is nothing more than that: adjusting a\n",
      "model’s weights in response to the error it produces, until you can’t reduce the\n",
      "error any more.\n",
      "Optimization Algorithms\n",
      "Some examples of optimization algorithms include:\n",
      "ADADELTA\n",
      "ADAGRAD\n",
      "ADAM\n",
      "NESTEROVS\n",
      "NONE\n",
      "RMSPROP\n",
      "SGD\n",
      "CONJUGATE GRADIENT\n",
      "HESSIAN FREE\n",
      "LBFGS\n",
      "LINE GRADIENT DESCENT\n",
      "Activation Functions\n",
      "The activation function determines the output a node will generate, based upon its\n",
      "input. In Deeplearning4j, the activation function is set at the layer level and\n",
      "\n",
      "\n",
      "137. applies to all neurons in that layer.\n",
      "Some examples include:\n",
      "CUBE\n",
      "ELU\n",
      "HARDSIGMOID\n",
      "HARDTANH\n",
      "IDENTITY\n",
      "LEAKYRELU\n",
      "RATIONALTANH\n",
      "RELU\n",
      "RRELU\n",
      "SIGMOID\n",
      "SOFTMAX\n",
      "SOFTPLUS\n",
      "SOFTSIGN\n",
      "TANH\n",
      "Custom layers, activation functions and loss\n",
      "functions\n",
      "Deeplearning4j, one of the major AI frameworks Skymind supports alongside\n",
      "Keras, includes custom layers, activations and loss functions.\n",
      "Logistic Regression\n",
      "On a deep neural network of many layers, the final layer has a particular role.\n",
      "When dealing with labeled input, the output layer classifies each example,\n",
      "applying the most likely label. Each node on the output layer represents one label,\n",
      "and that node turns on or off according to the strength of the signal it receives\n",
      "from the previous layer’s input and parameters.\n",
      "Each output node produces two possible outcomes, the binary output values 0 or\n",
      "1, because an input variable either deserves a label or it does not . After all, there is\n",
      "no such thing as a little pregnant.\n",
      "While neural networks working with labeled data produce binary output, the input\n",
      "they receive is often continuous. That is, the signals that the network receives as\n",
      "input will span a range of values and include any number of metrics, depending on\n",
      "the problem it seeks to solve.\n",
      "For example, a recommendation engine has to make a binary decision about\n",
      "whether to serve an ad or not. But the input it bases its decision on could include\n",
      "how much a customer has spent on Amazon in the last week, or how often that\n",
      "customer visits the site.\n",
      "So the output layer has to condense signals such as $67.59 spent on diapers, and\n",
      "15 visits to a website, into a range between 0 and 1; i.e. a probability that a given\n",
      "input should be labeled or not.\n",
      "\n",
      "\n",
      "138. The mechanism we use to convert continuous signals into binary output is called\n",
      "logistic regression. The name is unfortunate, since logistic regression is used for\n",
      "classification rather than regression in the linear sense that most people are\n",
      "familiar with. It calculates the probability that a set of inputs match the label.\n",
      "Let’s examine this little formula.\n",
      "For continuous inputs to be expressed as probabilities, they must output positive\n",
      "results, since there is no such thing as a negative probability. That’s why you see\n",
      "input as the exponent of e in the denominator – because exponents force our\n",
      "results to be greater than zero. Now consider the relationship of e’s exponent to\n",
      "the fraction 1/1. One, as we know, is the ceiling of a probability, beyond which our\n",
      "results can’t go without being absurd. (We’re 120% sure of that.)\n",
      "As the input x that triggers a label grows, the expression e to the x shrinks toward\n",
      "zero, leaving us with the fraction 1/1, or 100%, which means we approach (without\n",
      "ever quite reaching) absolute certainty that the label applies. Input that correlates\n",
      "negatively with your output will have its value flipped by the negative sign on e’s\n",
      "exponent, and as that negative signal grows, the quantity e to the x becomes\n",
      "larger, pushing the entire fraction ever closer to zero.\n",
      "Now imagine that, rather than having x as the exponent, you have the sum of the\n",
      "products of all the weights and their corresponding inputs – the total signal\n",
      "passing through your net. That’s what you’re feeding into the logistic regression\n",
      "layer at the output layer of a neural network classifier.\n",
      "With this layer, we can set a decision threshold above which an example is labeled\n",
      "1, and below which it is not. You can set different thresholds as you prefer – a low\n",
      "threshold will increase the number of false positives, and a higher one will\n",
      "increase the number of false negatives – depending on which side you would like\n",
      "to err.\n",
      "Loss Functions in DeepLearning4j\n",
      "DeepLearning4j supports the following loss functions.\n",
      "MSE: Mean Squared Error: Linear Regression\n",
      "EXPLL: Exponential log likelihood: Poisson Regression\n",
      "XENT: Cross Entropy: Binary Classification\n",
      "MCXENT: Multiclass Cross Entropy\n",
      "RMSE_XENT: RMSE Cross Entropy\n",
      "SQUARED_LOSS: Squared Loss\n",
      "NEGATIVELOGLIKELIHOOD: Negative Log Likelihood\n",
      "Neural Networks & Artificial Intelligence\n",
      "In some circles, neural networks are thought of as “brute force” AI, because they\n",
      "\n",
      "\n",
      "139. start with a blank slate and hammer their way through to an accurate model. They\n",
      "are effective, but to some eyes inefficient in their approach to modeling, which\n",
      "can’t make assumptions about functional dependencies between output and input.\n",
      "That said, gradient descent is not recombining every weight with every other to\n",
      "find the best match – its method of pathfinding shrinks the relevant weight space,\n",
      "and therefore the number of updates and required computation, by many orders\n",
      "of magnitude. Moreover, algorithms such as Hinton’s capsule networks require far\n",
      "fewer instances of data to converge on an accurate model; that is, present\n",
      "research has the potential to resolve the brute force nature of deep learning.\n",
      "Further Reading\n",
      "A Recipe for Training Neural Networks, by Andrej Karpathy\n",
      "Interactive Demo\n",
      "Learn to build AI applications using our interactive learning portal.\n",
      "TRY IT NOW\n",
      "Company\n",
      "About\n",
      "Press Kit\n",
      "Contact Us\n",
      "Press\n",
      "Privacy\n",
      "Platform\n",
      "SKIL\n",
      "Subscriptions\n",
      "Documentation\n",
      "Community Support\n",
      "International\n",
      "English\n",
      "Japanese\n",
      "Follow Us\n",
      "Facebook\n",
      "Twitter\n",
      "Linkedin\n",
      "\n",
      "\n",
      "140. Gitter\n",
      "Subscribe to IntegrateAI, our bi-weekly newsletter about AI applications in the real world:\n",
      "Subscribe\n",
      "\n",
      "\n",
      "141. DATA SCIENCE INTERVIEW QUESTIONS\n",
      "\n",
      "\n",
      "142. FAQ at interviews\n",
      "It might calm your nerves to know that\n",
      "almost every job seeker struggles. That’s\n",
      "because data science interview questions\n",
      "cover a bunch of different topics (data\n",
      "science is an interdisciplinary field, after all)\n",
      "and those cheeky interviewers love to throw\n",
      "you the odd curveball. So here are some of\n",
      "the FAQ at interviews…\n",
      "The problem:\n",
      "\n",
      "\n",
      "143. 1.\n",
      "What does data science mean?\n",
      "2. What are the assumptions of a linear regres-\n",
      "sion?\n",
      "3. What is the difference between factor analysis\n",
      "and cluster analysis?\n",
      "4. What is an iterator generator?\n",
      "5. Write down an SQL script to return data from\n",
      "two tables.\n",
      "6. Draw graphs relevant to pay-per-click ad-\n",
      "verts and ticket purchases.\n",
      "7. How would you explain Random Forest to a\n",
      "non-technical person?\n",
      "8. How can you prove an improvement you\n",
      "introduced to a model is actually working?\n",
      "9. What is root cause analysis?\n",
      "FAQ AT \n",
      "INTERVIEWS\n",
      "\n",
      "\n",
      "144. 10. Explain K-means.\n",
      "11. What kind of RDBMS software do you have\n",
      "experience with? What about non-relational\n",
      "databases?\n",
      "12. Supervised learning vs unsupervised learning.\n",
      "13. What is overfitting and how to fix it?\n",
      "14. What is the difference between SQL, MySQL\n",
      "and SQL Server?\n",
      "15. How would you start cleaning a big dataset?\n",
      "16. Give examples where a false negative is more\n",
      "important than a false positive, and vice versa.\n",
      "17. State some biases that you are likely to en-\n",
      "counter when cleaning a database.\n",
      "18. What is a logistic regression?\n",
      "FAQ AT \n",
      "INTERVIEWS\n",
      "\n",
      "\n",
      "145. It might calm your nerves to know that almost\n",
      "every job seeker struggles. That’s because data\n",
      "science interview questions cover a bunch of\n",
      "different topics (data science is an interdisciplinary\n",
      "field, after all) and those cheeky interviewers love\n",
      "to throw you the odd curveball.\n",
      "The first step to hitting those curveballs out of the\n",
      "park is to see them coming, and to see them\n",
      "coming you’ve got to be confident about the rest\n",
      "of your game.\n",
      "So, you must do your homework! An interviewer\n",
      "can spot someone who hasn’t from a mile away,\n",
      "but you wouldn’t be here if you didn’t know that\n",
      "already though, would you?\n",
      "FAQ AT \n",
      "INTERVIEWS\n",
      "\n",
      "\n",
      "146. There are plenty of articles out there that will give\n",
      "you all the example answers you could hope for\n",
      "and yes, technical questions will come up. But to\n",
      "remember one hundred-odd different examples\n",
      "would only serve to confuse you more, plus what\n",
      "if a question comes up you didn’t study for?\n",
      "We want to take you through the interview\n",
      "typology.\n",
      "Show\n",
      "you\n",
      "what\n",
      "data\n",
      "science\n",
      "interview questions are made of and what the\n",
      "interviewers are looking for.\n",
      "ANSWERING \n",
      "DATA SCIENCE \n",
      "QUESTIONS\n",
      "\n",
      "\n",
      "147. 1. Technical questions\n",
      "1.1 Mathematics\n",
      "1.2 Statistics\n",
      "1.3 Coding\n",
      "1.4 Machine Learning\n",
      "2. Practical experience questions\n",
      "3. Behavioral questions\n",
      "4. Scenarios (case study questions)\n",
      "CONTENTS\n",
      "LET’S BREAK THINGS DOWN\n",
      "\n",
      "\n",
      "148. TECHNICAL \n",
      "QUESTIONS\n",
      "A strong grasp of mathematics, statistics,\n",
      "coding, and machine learning is a must for\n",
      "a data scientist.\n",
      "You are likely to be asked to demonstrate\n",
      "your hands-on technical skills but prepare\n",
      "to show off your theoretical techniques,\n",
      "too!\n",
      "\n",
      "\n",
      "149. MATHEMATICS\n",
      "\n",
      "\n",
      "150. Mathematics underpins the study of machine\n",
      "learning,\n",
      "statistics,\n",
      "algorithms,\n",
      "and\n",
      "computer\n",
      "architecture, among others.\n",
      "So, applied maths is at the heart of the matter.\n",
      "Showing a good grasp of mathematics signals to\n",
      "the interviewer that you could quickly adapt to\n",
      "those other fields.\n",
      "Questions like these are to check you have basic\n",
      "maths skills and shouldn’t be too tricky for you.\n",
      "MATHEMATICS\n",
      "\n",
      "\n",
      "151. Be prepared to answer some quick (mental) maths\n",
      "questions, such as:\n",
      "•\n",
      "What is the sum of numbers from 1 to 100?\n",
      "•\n",
      "A snail falls down a well 50ft deep. Each day it\n",
      "climbs up 3ft, and each night slides down 1ft.\n",
      "How many days does it take him to get out?\n",
      "•\n",
      "You have a 10x10x10 cube, made of one\n",
      "thousand 1x1x1 cubes. If you remove the outer\n",
      "layer of this structure, how many cubes will you\n",
      "have left?\n",
      "MATHEMATICS\n",
      "\n",
      "\n",
      "152. Here are some real-life data science interview\n",
      "questions:\n",
      "•\n",
      "A race track has 5 lanes. There are 25 horses\n",
      "and one would like to find out the 3 fastest\n",
      "horses of those 25. What is the minimum\n",
      "number of races one would need to conduct to\n",
      "determine the 3 fastest horses?\n",
      "Things become a little more interesting when\n",
      "encountering\n",
      "puzzle\n",
      "questions\n",
      "that\n",
      "test\n",
      "your\n",
      "lateral thinking.\n",
      "MATHEMATICS\n",
      "\n",
      "\n",
      "153. •\n",
      "Four people need to cross a rickety bridge at\n",
      "night. Unfortunately, they have a single torch\n",
      "and the bridge is too dangerous to cross\n",
      "without one. The bridge is only strong enough\n",
      "to support two people at a time. Not all people\n",
      "take the same time to cross the bridge. Times\n",
      "for each person: 1 min, 2 mins, 7 mins and 10\n",
      "mins. What is the shortest time needed for all\n",
      "four of them to cross the bridge?\n",
      "MATHEMATICS\n",
      "\n",
      "\n",
      "154. Finally, there are those hard maths problems.\n",
      "It is unlikely that you’ll be given an equation to\n",
      "solve, rather you’ll be asked a simply worded\n",
      "question which requires conceptual preparation to\n",
      "answer.\n",
      "Furthermore, it may intertwine with probability\n",
      "theory, even if it seems it doesn’t.\n",
      "MATHEMATICS\n",
      "\n",
      "\n",
      "155. Some examples are:\n",
      "•\n",
      "Consider an extension of rock, paper, scissors\n",
      "where there are N options instead of 3 options.\n",
      "For what values of N is it possible to construct a\n",
      "fair game, whereby ‘fair’ we mean that for any\n",
      "move that a player plays there are an equal\n",
      "number of moves that beat it or lose to it?\n",
      "•\n",
      "In a country in which people only want boys,\n",
      "every family continues to have children until\n",
      "they have a boy. If they have a girl, they have\n",
      "another child. If they have a boy, they stop.\n",
      "What is the proportion of boys to girls in the\n",
      "country?\n",
      "MATHEMATICS\n",
      "\n",
      "\n",
      "156. STATISTICS\n",
      "\n",
      "\n",
      "157. Did you know, data Scientists were once called\n",
      "statisticians? The two professions aren’t one and\n",
      "the same, but many data scientists have finished a\n",
      "statistics degree.\n",
      "And that’s no wonder! Statistics is one of the\n",
      "‘founding fathers’ of data science.\n",
      "Logically, you will be tested on your ability to\n",
      "reason statistically. Even if theoretical knowledge\n",
      "isn’t your strongest suit, you need to use precise\n",
      "technical language.\n",
      "STATISTICS\n",
      "\n",
      "\n",
      "158. Consider the following question:\n",
      "What is the difference between false positive and\n",
      "false negative?\n",
      "It seems that you need to provide some textbook\n",
      "definitions…Got\n",
      "you!\n",
      "Nobody\n",
      "wants\n",
      "to\n",
      "hear\n",
      "generic theory; it’s boring and you will blend in\n",
      "with the crowd.\n",
      "Employers will want you to identify situations\n",
      "where you can implement the theory.\n",
      "STATISTICS\n",
      "\n",
      "\n",
      "159. While still talking statistics, what are some other\n",
      "questions that may pop up?\n",
      "•\n",
      "What is the null hypothesis and how do we\n",
      "state it?\n",
      "•\n",
      "How would you explain a linear regression to a\n",
      "business executive?\n",
      "•\n",
      "Tell me what heteroskedasticity is and how to\n",
      "solve it.\n",
      "STATISTICS\n",
      "\n",
      "\n",
      "160. •\n",
      "What’s the Central Limit Theorem and what are\n",
      "its practical implications?\n",
      "•\n",
      "How do you find the correlation between a\n",
      "categorical variable and a continuous variable?\n",
      "•\n",
      "Explain p-value. Present it as if talking to a\n",
      "client.\n",
      "•\n",
      "What do you understand by statistical power\n",
      "and how do you calculate it?\n",
      "•\n",
      "Please\n",
      "explain\n",
      "the\n",
      "differences\n",
      "between\n",
      "overfitting and underfitting.\n",
      "•\n",
      "Explain what cross-validation is. How and why is\n",
      "it used?\n",
      "STATISTICS\n",
      "\n",
      "\n",
      "161. Did you think those last two are machine learning \n",
      "questions? Well spotted, now we see that ML \n",
      "overlaps with statistical concepts!\n",
      "•\n",
      "Could you give examples of data that does not \n",
      "have a Gaussian distribution, nor log-normal?\n",
      "•\n",
      "Explain bootstrapping as if you’re talking to a \n",
      "non-technical person.\n",
      "•\n",
      "State some biases that you are likely to \n",
      "encounter when cleaning a database.\n",
      "STATISTICS\n",
      "\n",
      "\n",
      "162. CODING\n",
      "\n",
      "\n",
      "163. Every data scientist needs a certain amount of\n",
      "programming knowledge. You don’t have to be\n",
      "a pro, but employers will want to see that you\n",
      "have a decent grip on it and have the potential\n",
      "for rapid improvement.\n",
      "Python, R, and SQL are the bread-and-butter\n",
      "programming\n",
      "languages\n",
      "in\n",
      "data\n",
      "science.\n",
      "Questions about these three staples should not\n",
      "come as surprise.\n",
      "CODING\n",
      "\n",
      "\n",
      "164. R\n",
      "•\n",
      "How are missing values and impossible values\n",
      "represented in R?\n",
      "•\n",
      "What is the difference between lapply and\n",
      "sapply?\n",
      "•\n",
      "How do you merge two data frames in R?\n",
      "•\n",
      "What is the command used to store R objects in\n",
      "a file?\n",
      "•\n",
      "How can you split a continuous variable into\n",
      "different groups/ranks in R?\n",
      "•\n",
      "Please explain three key differences between\n",
      "Python and R.\n",
      "CODING\n",
      "\n",
      "\n",
      "165. Python\n",
      "•\n",
      "Which Python library would you prefer to use\n",
      "for Data wrangling?\n",
      "•\n",
      "How can you build a simple logistic regression\n",
      "in Python?\n",
      "•\n",
      "What’s the shortest way open a text file in\n",
      "Python?\n",
      "•\n",
      "Have you done web scraping in Python? How\n",
      "can you do that?\n",
      "•\n",
      "Please explain what is a ‘pass’ in Python.\n",
      "•\n",
      "Please explain how one can perform pattern\n",
      "matching in Python.\n",
      "•\n",
      "What tool would you use to find bugs?\n",
      "•\n",
      "What’s your preferred library for plotting in\n",
      "Python: Seaborn or Matplotlib?\n",
      "CODING\n",
      "\n",
      "\n",
      "166. SQL\n",
      "•\n",
      "You\n",
      "have\n",
      "a\n",
      "table\n",
      "called\n",
      "with\n",
      "Cust_ID,\n",
      "Order_Date, Order_ID, Tran_Amt. How would\n",
      "you select the top 100 customers with the\n",
      "highest spend over a year-long period?\n",
      "•\n",
      "Describe the different parts of an SQL query.\n",
      "•\n",
      "What is the difference between UNION and\n",
      "UNION ALL?\n",
      "•\n",
      "Write down a SQL script to return data from\n",
      "two tables.\n",
      "•\n",
      "Tell me the difference between a primary key\n",
      "and a unique key.\n",
      "•\n",
      "What is the difference between SQL, MySQL\n",
      "and SQL Server?\n",
      "CODING\n",
      "\n",
      "\n",
      "167. MACHINE \n",
      "LEARNING\n",
      "\n",
      "\n",
      "168. MACHINE LEARNING\n",
      "A familiarity with machine learning\n",
      "methodologies is essential for every\n",
      "aspiring data scientist.\n",
      "You should be prepared to explain\n",
      "key concepts in a nutshell.\n",
      "It’s quite possible that the interviewer\n",
      "will outline a prediction problem and\n",
      "ask you to come up with algorithms.\n",
      "With the algorithms, expect to touch\n",
      "upon commonly observed problems\n",
      "and their fixes.\n",
      "\n",
      "\n",
      "169. Check\n",
      "out\n",
      "the\n",
      "following\n",
      "machine\n",
      "learning questions we’ve picked for\n",
      "you:\n",
      "•\n",
      "What is the difference between\n",
      "supervised\n",
      "and\n",
      "unsupervised\n",
      "machine learning?\n",
      "•\n",
      "How\n",
      "would\n",
      "you\n",
      "deal\n",
      "with\n",
      "an\n",
      "imbalanced dataset?\n",
      "•\n",
      "How do you ensure you are not\n",
      "overfitting with a model?\n",
      "•\n",
      "What approaches would you use\n",
      "to evaluate the prediction accuracy\n",
      "of a logistics regression model?\n",
      "•\n",
      "How do you deal with sparse\n",
      "data?\n",
      "•\n",
      "Could\n",
      "you\n",
      "explain\n",
      "the\n",
      "Bias-\n",
      "Variance trade-off?\n",
      "MACHINE LEARNING\n",
      "\n",
      "\n",
      "170. Additionally, you may stumble upon\n",
      "way too specific or way too vague\n",
      "questions such as:\n",
      "•\n",
      "Explain\n",
      "the\n",
      "difference\n",
      "between\n",
      "Gaussian Mixture Model and K-\n",
      "Means.\n",
      "•\n",
      "Tell me about a machine learning\n",
      "project you admire.\n",
      "MACHINE LEARNING\n",
      "\n",
      "\n",
      "171. PRACTICAL EXPERIENCE \n",
      "QUESTIONS\n",
      "\n",
      "\n",
      "172. Technical questions are important, and a data\n",
      "scientist needs to know the answers and how to put\n",
      "them into practice.\n",
      "There are countless data science questions and an\n",
      "interviewer is not going to waste time asking dozens\n",
      "of\n",
      "questions\n",
      "to\n",
      "gauge\n",
      "whether\n",
      "you\n",
      "are\n",
      "the\n",
      "candidate for them. Instead, why not ask you to give\n",
      "your experience.\n",
      "PRACTICAL \n",
      "EXPERIENCE Qs\n",
      "\n",
      "\n",
      "173. PRACTICAL \n",
      "EXPERIENCE Qs\n",
      "These are practical experience questions, designed\n",
      "to shed light on your pace of work, experiences, and\n",
      "habits.\n",
      "To avoid having to sift through your back catalogue\n",
      "of experiences on the spot, have in mind a few\n",
      "experiences that are versatile – Ones that exemplify\n",
      "different skills based on the question.\n",
      "\n",
      "\n",
      "174. PRACTICAL \n",
      "EXPERIENCE Qs\n",
      "Let’s give you taste of those:\n",
      "•\n",
      "Summarize your experience.\n",
      "•\n",
      "Tell me about your first data science pet project.\n",
      "•\n",
      "How do you keep up with the news about\n",
      "politics, economics, and business? What about\n",
      "data science?\n",
      "•\n",
      "So,\n",
      "Python\n",
      "is\n",
      "your\n",
      "preferred\n",
      "programming\n",
      "language. What experience do you have with R?\n",
      "Tell me what you have done with that.\n",
      "\n",
      "\n",
      "175. PRACTICAL \n",
      "EXPERIENCE Qs\n",
      "Of course, you can get it vice-versa:\n",
      "•\n",
      "So,\n",
      "R\n",
      "is\n",
      "your\n",
      "preferred\n",
      "programming\n",
      "language. What experience do you have\n",
      "with Python? Tell me what you have done\n",
      "with that.\n",
      "•\n",
      "Do you have experience in Tableau?\n",
      "•\n",
      "What kind of RDBMS software do you have\n",
      "experience with?\n",
      "\n",
      "\n",
      "176. BEHAVIORAL \n",
      "QUESTIONS\n",
      "\n",
      "\n",
      "177. Like\n",
      "any\n",
      "other\n",
      "job\n",
      "interview,\n",
      "employers\n",
      "are\n",
      "interested in how you handle workplace situations,\n",
      "how you work in a team and whether you are a\n",
      "good fit for the company.\n",
      "Behavioural questions can be asked indirectly, for\n",
      "example,\n",
      "the\n",
      "interviewer\n",
      "may\n",
      "pose\n",
      "broad\n",
      "questions about your motivation or the tasks you\n",
      "enjoy.\n",
      "Certainly, there is not a right answer here. The\n",
      "intent is to judge your past responses as they can\n",
      "accurately predict future behavior.\n",
      "Let’s see an example: Describe a situation when you\n",
      "faced a conflict while working on a team project.\n",
      "BEHAVIORAL Qs\n",
      "\n",
      "\n",
      "178. Instead of asking hypothetical questions (“How will\n",
      "you deal with…”), the interviewer is hoping to elicit\n",
      "a more meaningful response by pushing you to\n",
      "chat about a real-life past event. The interviewer\n",
      "will be looking for four things in your story:\n",
      "Situation: What was the context? (devote around 10% of\n",
      "the answer time)\n",
      "Task: What needed to be done? (devote around 10% of\n",
      "the answer time)\n",
      "Action: What did you do? (devote around 70% of the\n",
      "answer time)\n",
      "Results:\n",
      "What\n",
      "were\n",
      "the\n",
      "accomplishments?\n",
      "(devote\n",
      "around 10% of the answer time)\n",
      "Also known as the STAR technique, these steps will help you present your\n",
      "answers in a clear and succinct fashion.\n",
      "BEHAVIORAL Qs\n",
      "\n",
      "\n",
      "179. Dying for examples? Here you go:\n",
      "•\n",
      "Please describe a data science project you\n",
      "worked\n",
      "on\n",
      "(Yes!\n",
      "It\n",
      "overlaps\n",
      "with\n",
      "the\n",
      "‘practical experience category!)\n",
      "•\n",
      "Tell me about a situation when you had to\n",
      "balance competing priorities.\n",
      "•\n",
      "Describe a time when you managed to\n",
      "persuade someone to see things your way.\n",
      "•\n",
      "Describe a time when you were bored at\n",
      "work. How did you motivate yourself?\n",
      "•\n",
      "when you failed to meet a deadline.\n",
      "•\n",
      "Our team is brand new and is under-\n",
      "financed. We have no standard procedures\n",
      "or training, and everything is ad-hoc. How\n",
      "would you go about this situation?\n",
      "BEHAVIORAL Qs\n",
      "\n",
      "\n",
      "180. CASE STUDY \n",
      "QUESTIONS\n",
      "\n",
      "\n",
      "181. The purpose of scenarios (case study questions) is\n",
      "to test your experience in various data science\n",
      "fields.\n",
      "Case study questions will likely look for skills\n",
      "outside of the technical toolkit.\n",
      "For instance, they may be looking for logical\n",
      "reasoning\n",
      "or\n",
      "business\n",
      "understanding.\n",
      "It’s\n",
      "important\n",
      "for\n",
      "you\n",
      "to\n",
      "demonstrate\n",
      "structured\n",
      "thinking, reasoning, and problem-solving skills.\n",
      "After all, you can’t be a good data scientist if you\n",
      "cannot identify the underlying problems.\n",
      "CASE STUDY Qs\n",
      "\n",
      "\n",
      "182. Let’s see how this works:\n",
      "•\n",
      "The sales department has increased the selling\n",
      "price of all items by 5%. There are 10 items, all\n",
      "with different price tags. Before the price\n",
      "increase, gross revenue was $500,000 with an\n",
      "average selling price of $1. After the price\n",
      "increase, gross revenue was $505,000, with an\n",
      "average selling price of $0.95. Why hasn’t the\n",
      "price\n",
      "increase\n",
      "had\n",
      "the\n",
      "desired\n",
      "impact\n",
      "of\n",
      "increasing revenue and average selling price?\n",
      "CASE STUDY Qs\n",
      "\n",
      "\n",
      "183. You can be also given market sizing questions,\n",
      "called guestimates by some, a term that sounds\n",
      "like you just need to take a stab in the dark, which\n",
      "is just not the case.\n",
      "While reaching a conclusion does require a degree\n",
      "of guesswork and estimation, the process of how\n",
      "you use them is difficult and requires rigid logic.\n",
      "CASE STUDY Qs\n",
      "\n",
      "\n",
      "184. There is not a single correct answer to questions\n",
      "like these and chances are that the interviewer\n",
      "doesn’t know the exact answer, either. Here is an\n",
      "example:\n",
      "•\n",
      "How many SUV’s in the parking lot downstairs?\n",
      "How many ping-pong balls can fit into this\n",
      "room?\n",
      "CASE STUDY Qs\n",
      "\n",
      "\n",
      "185. An interview is a \n",
      "dialogue, \n",
      "not a written test!\n",
      "Excellent, now consider our typology as the starting\n",
      "point in your interview prep.\n",
      "However, we have only scratched the surface when it\n",
      "comes to examples of data science interview questions\n",
      "you may encounter. The industry is booming and as\n",
      "such, companies are constantly adapting their interview\n",
      "sessions (what may be a common question today may\n",
      "be one hardly asked in 2 years).\n",
      "Data\n",
      "science\n",
      "interview\n",
      "questions\n",
      "vary\n",
      "in\n",
      "their\n",
      "peculiarities, but the types of questions remain the\n",
      "same, so having a base knowledge of these types with a\n",
      "good amount of preparation will allow you to logically\n",
      "tackle any question the interviewer has up her sleeve.\n",
      "\n",
      "\n",
      "186. ABOUT THE AUTHORS\n",
      "365 Data Science is an online\n",
      "educational career website that\n",
      "offers the incredible opportunity to find your way into the data science\n",
      "world no matter your previous knowledge and experience. We have\n",
      "comprehensive programs that suit the needs of aspiring BI analysts,\n",
      "Data analysts, and Data scientists.\n",
      "What we do\n",
      "We, the authors, are committed \n",
      "educators who believe that curio-\n",
      "sity should not be hindered by inability to access good learning \n",
      "resources. This is why we focus all our efforts on creating high-quality \n",
      "educational content which anyone can access online. \n",
      "Our courses cover all the necessary topics to build up data science skills\n",
      "from the ground up, including Mathematics and Statistics, to Python, R,\n",
      "SQL, data visualization, and Machine and Deep learning.\n",
      "Who we are\n",
      "\n",
      "\n",
      "187. THE COMPREHENSIVE DATA SCIENCE CURRICULUM TO\n",
      "GROW YOUR DATA SCIENCE SKILLSET\n",
      "ABOUT OUR TRAINING\n",
      "The 365 Data Science Program is comprehensive set of courses, that\n",
      "work together to help any student learn everything they need to\n",
      "become an expert data scientist in months. The training includes all of\n",
      "the most sought-after skills, including:\n",
      "•\n",
      "The fundamentals of Mathematics\n",
      "•\n",
      "Probability\n",
      "•\n",
      "Intro to Data & Data Science\n",
      "•\n",
      "Tableau\n",
      "•\n",
      "SQL\n",
      "•\n",
      "R\n",
      "•\n",
      "Python\n",
      "•\n",
      "Machine Learning\n",
      "The program consists of 45 hours of on-demand video, split into 12\n",
      "courses, with real-life business examples, and over 300 exercises.\n",
      "\n",
      "\n",
      "188. Good luck!\n",
      "\n",
      "\n",
      "189.                                                                                     https://career.guru99.com/\n",
      "Top 50 Machine Learning Interview\n",
      "Questions & Answers\n",
      "1)      What is Machine learning?\n",
      "Machine learning is a branch of computer science which deals with system programming in order to\n",
      "automatically learn and improve with experience.  For example: Robots are programed so that they\n",
      "can perform the task based on data they gather from sensors. It automatically learns programs from\n",
      "data.\n",
      "2)      Mention the diﬀerence between Data Mining and Machine learning?\n",
      "Machine learning relates with the study, design and development of the algorithms that give\n",
      "computers the capability to learn without being explicitly programmed.  While, data mining can be\n",
      "deﬁned as the process in which the unstructured data tries to extract knowledge or unknown\n",
      "interesting patterns.  During this process machine, learning algorithms are used.\n",
      "3)      What is ‘Overﬁtting’ in Machine learning?\n",
      "In machine learning, when a statistical model describes random error or noise instead of underlying\n",
      "relationship ‘overﬁtting’ occurs.  When a model is excessively complex, overﬁtting is normally\n",
      "observed, because of having too many parameters with respect to the number of training data types.\n",
      "The model exhibits poor performance which has been overﬁt.\n",
      "4)      Why overﬁtting happens?\n",
      "The possibility of overﬁtting exists as the criteria used for training the model is not the same as the\n",
      "criteria used to judge the eﬃcacy of a model.\n",
      "5)      How can you avoid overﬁtting ?\n",
      "By using a lot of data overﬁtting can be avoided, overﬁtting happens relatively as you have a small\n",
      "dataset, and you try to learn from it. But if you have a small database and you are forced to come\n",
      "with a model based on that. In such situation, you can use a technique known as cross validation. In\n",
      "this method the dataset splits into two section, testing and training datasets, the testing dataset will\n",
      "only test the model while, in training dataset, the datapoints will come up with the model.\n",
      "In this technique,  a model is usually given a dataset of a known data on which training (training data\n",
      "set) is run and a dataset of unknown data against which the model is tested. The idea of cross\n",
      "validation is to deﬁne a dataset to “test” the model in the training phase.\n",
      "6)      What is inductive machine learning?\n",
      "The inductive machine learning involves the process of learning by examples, where a system, from a\n",
      "set of observed instances tries to induce a general rule.\n",
      "\n",
      "\n",
      "190. 7)      What are the ﬁve popular algorithms of Machine Learning?\n",
      "a)      Decision Trees\n",
      "b)      Neural Networks (back propagation)\n",
      "c)       Probabilistic networks\n",
      "d)      Nearest Neighbor\n",
      "e)      Support vector machines\n",
      "8)      What are the diﬀerent Algorithm techniques in Machine Learning?\n",
      "The diﬀerent types of techniques in Machine Learning are\n",
      "a)      Supervised Learning\n",
      "b)      Unsupervised Learning\n",
      "c)       Semi-supervised Learning\n",
      "d)      Reinforcement Learning\n",
      "e)      Transduction\n",
      "f)       Learning to Learn\n",
      "9)      What are the three stages to build the hypotheses or model in machine learning?\n",
      "a)      Model building\n",
      "b)      Model testing\n",
      "c)       Applying the model\n",
      "10)   What is the standard approach to supervised learning?\n",
      "The standard approach to supervised learning is to split the set of example into the training set and\n",
      "the test.\n",
      "11)   What is ‘Training set’ and ‘Test set’?\n",
      "In various areas of information science like machine learning, a set of data is used to discover the\n",
      "potentially predictive relationship known as ‘Training Set’. Training set is an examples given to the\n",
      "learner, while Test set is used to test the accuracy of the hypotheses generated by the learner, and it\n",
      "is the set of example held back from the learner. Training set are distinct from Test set.\n",
      "12)   List down various approaches for machine learning?\n",
      "The diﬀerent approaches in Machine Learning are\n",
      "a)      Concept Vs Classiﬁcation Learning\n",
      "b)      Symbolic Vs Statistical Learning\n",
      "\n",
      "\n",
      "191. c)       Inductive Vs Analytical Learning\n",
      "13)   What is not Machine Learning?\n",
      "a)      Artiﬁcial Intelligence\n",
      "b)      Rule based inference\n",
      "14)   Explain what is the function of ‘Unsupervised Learning’?\n",
      "a)      Find clusters of the data\n",
      "b)      Find low-dimensional representations of the data\n",
      "c)       Find interesting directions in data\n",
      "d)      Interesting coordinates and correlations\n",
      "e)      Find novel observations/ database cleaning\n",
      "15)   Explain what is the function of ‘Supervised Learning’?\n",
      "a)      Classiﬁcations\n",
      "b)      Speech recognition\n",
      "c)       Regression\n",
      "d)      Predict time series\n",
      "e)      Annotate strings\n",
      "16)   What is algorithm independent machine learning?\n",
      "Machine learning in where mathematical foundations is independent of any particular classiﬁer or\n",
      "learning algorithm is referred as algorithm independent machine learning?\n",
      "17)   What is the diﬀerence between artiﬁcial learning and machine learning?\n",
      "Designing and developing algorithms according to the behaviours based on empirical data are known\n",
      "as Machine Learning.  While artiﬁcial intelligence in addition to machine learning, it also covers other\n",
      "aspects like knowledge representation, natural language processing, planning, robotics etc.\n",
      "18)   What is classiﬁer in machine learning?\n",
      "A classiﬁer in a Machine Learning is a system that inputs a vector of discrete or continuous feature\n",
      "values and outputs a single discrete value, the class.\n",
      "19)   What are the advantages of Naive Bayes?\n",
      "In Naïve Bayes classiﬁer will converge quicker than discriminative models like logistic regression, so\n",
      "you need less training data.  The main advantage is that it can’t learn interactions between features.\n",
      "20)   In what areas Pattern Recognition is used?\n",
      "Pattern Recognition can be used in\n",
      "\n",
      "\n",
      "192. a)      Computer Vision\n",
      "b)      Speech Recognition\n",
      "c)       Data Mining\n",
      "d)      Statistics\n",
      "e)      Informal Retrieval\n",
      "f)       Bio-Informatics\n",
      "21)   What is Genetic Programming?\n",
      "Genetic programming is one of the two techniques used in machine learning. The model is based on\n",
      "the testing and selecting the best choice among a set of results.\n",
      "22)   What is Inductive Logic Programming in Machine Learning?\n",
      "Inductive Logic Programming (ILP) is a subﬁeld of machine learning which uses logical programming\n",
      "representing background knowledge and examples.\n",
      "23)   What is Model Selection in Machine Learning?\n",
      "The process of selecting models among diﬀerent mathematical models, which are used to describe\n",
      "the same data set is known as Model Selection. Model selection is applied to the ﬁelds of statistics,\n",
      "machine learning and data mining.\n",
      "24)   What are the two methods used for the calibration in Supervised Learning?\n",
      "The two methods used for predicting good probabilities in Supervised Learning are\n",
      "a)      Platt Calibration\n",
      "b)      Isotonic Regression\n",
      "These methods are designed for binary classiﬁcation, and it is not trivial.\n",
      "25)   Which method is frequently used to prevent overﬁtting?\n",
      "When there is suﬃcient data ‘Isotonic Regression’ is used to prevent an overﬁtting issue.\n",
      "26)   What is the diﬀerence between heuristic for rule learning and heuristics for decision\n",
      "trees?\n",
      "The diﬀerence is that the heuristics for decision trees evaluate the average quality of a number of\n",
      "disjointed sets while rule learners only evaluate the quality of the set of instances that is covered with\n",
      "the candidate rule.\n",
      "27)   What is Perceptron in Machine Learning?\n",
      "In Machine Learning, Perceptron is an algorithm for supervised classiﬁcation of the input into one of\n",
      "several possible non-binary outputs.\n",
      "28)   Explain the two components of Bayesian logic program?\n",
      "\n",
      "\n",
      "193. Bayesian logic program consists of two components.  The ﬁrst component is a logical one ; it consists\n",
      "of a set of Bayesian Clauses, which captures the qualitative structure of the domain.  The second\n",
      "component is a quantitative one, it encodes the quantitative information about the domain.\n",
      "29)   What are Bayesian Networks (BN) ?\n",
      "Bayesian Network is used to represent the graphical model for probability relationship among a set of\n",
      "variables .\n",
      "30)   Why instance based learning algorithm sometimes referred as Lazy learning\n",
      "algorithm?\n",
      "Instance based learning algorithm is also referred as Lazy learning algorithm as they delay the\n",
      "induction or generalization process until classiﬁcation is performed.\n",
      "31)   What are the two classiﬁcation methods that SVM ( Support Vector Machine) can\n",
      "handle?\n",
      "a)      Combining binary classiﬁers\n",
      "b)      Modifying binary to incorporate multiclass learning\n",
      "32)   What is ensemble learning?\n",
      "To solve a particular computational program, multiple models such as classiﬁers or experts are\n",
      "strategically generated and combined. This process is known as ensemble learning.\n",
      "33)   Why ensemble learning is used?\n",
      "Ensemble learning is used to improve the classiﬁcation, prediction, function approximation etc of a\n",
      "model.\n",
      "34)   When to use ensemble learning?\n",
      "Ensemble learning is used when you build component classiﬁers that are more accurate and\n",
      "independent from each other.\n",
      "35)   What are the two paradigms of ensemble methods?\n",
      "The two paradigms of ensemble methods are\n",
      "a)      Sequential ensemble methods\n",
      "b)      Parallel ensemble methods\n",
      "36)   What is the general principle of an ensemble method and what is bagging and\n",
      "boosting in ensemble method?\n",
      "The general principle of an ensemble method is to combine the predictions of several models built\n",
      "with a given learning algorithm in order to improve robustness over a single model.  Bagging is a\n",
      "method in ensemble for improving unstable estimation or classiﬁcation schemes.  While boosting\n",
      "method are used sequentially to reduce the bias of the combined model.  Boosting and Bagging both\n",
      "can reduce errors by reducing the variance term.\n",
      "37)   What is bias-variance decomposition of classiﬁcation error in ensemble method?\n",
      "\n",
      "\n",
      "194. The expected error of a learning algorithm can be decomposed into bias and variance. A bias term\n",
      "measures how closely the average classiﬁer produced by the learning algorithm matches the target\n",
      "function.  The variance term measures how much the learning algorithm’s prediction ﬂuctuates for\n",
      "diﬀerent training sets.\n",
      "38)   What is an Incremental Learning algorithm in ensemble?\n",
      "Incremental learning method is the ability of an algorithm to learn from new data that may be\n",
      "available after classiﬁer has already been generated from already available dataset.\n",
      "39)   What is PCA, KPCA and ICA used for?\n",
      "PCA (Principal Components Analysis), KPCA ( Kernel based Principal Component Analysis) and ICA (\n",
      "Independent Component Analysis) are important feature extraction techniques used for\n",
      "dimensionality reduction.\n",
      "40)   What is dimension reduction in Machine Learning?\n",
      "In Machine Learning and statistics, dimension reduction is the process of reducing the number of\n",
      "random variables under considerations and can be divided into feature selection and feature\n",
      "extraction\n",
      "41)   What are support vector machines?\n",
      "Support vector machines are supervised learning algorithms used for classiﬁcation and regression\n",
      "analysis.\n",
      "42)   What are the components of relational evaluation techniques?\n",
      "The important components of relational evaluation techniques are\n",
      "a)      Data Acquisition\n",
      "b)      Ground Truth Acquisition\n",
      "c)       Cross Validation Technique\n",
      "d)      Query Type\n",
      "e)      Scoring Metric\n",
      "f)       Signiﬁcance Test\n",
      "43)   What are the diﬀerent methods for Sequential Supervised Learning?\n",
      "The diﬀerent methods to solve Sequential Supervised Learning problems are\n",
      "a)      Sliding-window methods\n",
      "b)      Recurrent sliding windows\n",
      "c)       Hidden Markow models\n",
      "d)      Maximum entropy Markow models\n",
      "e)      Conditional random ﬁelds\n",
      "\n",
      "\n",
      "195. f)       Graph transformer networks\n",
      "44)   What are the areas in robotics and information processing where sequential\n",
      "prediction problem arises?\n",
      "The areas in robotics and information processing  where sequential prediction problem arises are\n",
      "a)      Imitation Learning\n",
      "b)      Structured prediction\n",
      "c)       Model based reinforcement learning\n",
      "45)   What is batch statistical learning?\n",
      "Statistical learning techniques allow learning a function or predictor from a set of observed data that\n",
      "can make predictions about unseen or future data. These techniques provide guarantees on the\n",
      "performance of the learned predictor on the future unseen data based on a statistical assumption on\n",
      "the data generating process.\n",
      "46)   What is PAC Learning?\n",
      "PAC (Probably Approximately Correct) learning is a learning framework that has been introduced to\n",
      "analyze learning algorithms and their statistical eﬃciency.\n",
      "47)    What are the diﬀerent categories you can categorized the sequence learning\n",
      "process?\n",
      "a)      Sequence prediction\n",
      "b)      Sequence generation\n",
      "c)       Sequence recognition\n",
      "d)      Sequential decision\n",
      "48)   What is sequence learning?\n",
      "Sequence learning is a method of teaching and learning in a logical manner.\n",
      "49)   What are two techniques of Machine Learning ?\n",
      "The two techniques of Machine Learning are\n",
      "a)      Genetic Programming\n",
      "b)      Inductive Learning\n",
      "50)   Give a popular application of machine learning that you see on day to day basis?\n",
      "The recommendation engine implemented by major ecommerce websites uses Machine Learning\n",
      " Guru99  Provides  FREE ONLINE TUTORIAL   on Various courses like\n",
      "\n",
      "\n",
      "196. Java\n",
      "MIS\n",
      "MongoDB\n",
      "BigData\n",
      "Cassandra\n",
      "Web Services\n",
      "SQLite\n",
      "JSP\n",
      "Informatica\n",
      "Accounting\n",
      "SAP Training\n",
      "Python\n",
      "Excel\n",
      "ASP Net\n",
      "HBase\n",
      "Project\n",
      "Management\n",
      "Test Management\n",
      "Business Analyst\n",
      "Ethical Hacking\n",
      "PMP\n",
      "Live Project\n",
      "SoapUI\n",
      "Photoshop\n",
      "Manual Testing\n",
      "Mobile Testing\n",
      "Data Warehouse\n",
      "R Tutorial\n",
      "Tableau\n",
      "DevOps\n",
      "AWS\n",
      "Jenkins\n",
      "Agile Testing\n",
      "RPA\n",
      "JUnit\n",
      "Software\n",
      "Engineering\n",
      "Selenium\n",
      "CCNA\n",
      "AngularJS\n",
      "NodeJS\n",
      "PLSQL\n",
      "                                     \n",
      "\n",
      "\n",
      "197. What is Bagging in Ensemble Learning \n",
      " \n",
      "In general, any of the machine learning problems we try to find the best possible optimal \n",
      "model for a given problem. That means finding the best possible model within the given \n",
      "model family, for example, finding the best possible decision tree or finding the best possible \n",
      "KNN model. And if we have more time then we can try all model families available, and come \n",
      "up with the best possible regression model, best possible KNN model, best possible SVM \n",
      "model etc. And among these again select the best possible model, which will be either KNN, \n",
      "SVM or any other. \n",
      "However Ensemble Learning says, if we can build multiple models then why to select the best \n",
      "one why not top 2, again why not top 3 and why not top 10. Then if you find top 10 deploy all \n",
      "10 models. And when new data comes, make a prediction from all 10 models and combine \n",
      "the predictions and finally make a joint prediction. This is the key idea of ensemble learning. \n",
      " \n",
      "Now there are two questions which might come to your mind. \n",
      "1. What is meant by building/training different models? \n",
      "2. How to combine the predictions? \n",
      "So when we say building/training different models then it means either one of the below: \n",
      "• Select different model families among KNN, Decision Trees, Linear Regression etc. \n",
      "• Select one model family and Train it on different training samples resulting in different \n",
      "models of same ML family. \n",
      "• Also can consider different feature spaces while training to result in different models. \n",
      "• Will talk about the impact of selecting different features in the coming paragraphs. \n",
      " \n",
      " \n",
      "\n",
      "\n",
      "198. And when we say combine the predictions of all models then it means: \n",
      "• Regression: (Weighted) Mean, Median, Max, Min \n",
      "o If it is regression problem then you can take either mean, median or mode of \n",
      "all models outcomes to get the final outcome. \n",
      "o We take weighted mean when we have build models with a different set of \n",
      "features along with different samples then features with high importance \n",
      "generate the more optimal model and hence we need to assign more weight \n",
      "to those models while combining. \n",
      "o Also, we can assign the weight based on the model's performance on the test \n",
      "data. Model performing better will get more weight. \n",
      "• Classification : (Weighted) Majority Voting. \n",
      "o Either normal majority counting or weighted majority counting. (the weighted \n",
      "reason is same as described in above point.) \n",
      " \n",
      " \n",
      "I hope by now the basic idea of ensemble learning is clear. Let’s now discuss one of the most \n",
      "important and widely used techniques of ensemble learning that is Bagging. Another \n",
      "technique is Boosting which will discuss in another post. Here I will focus on complete \n",
      "explanation of Bagging. \n",
      " \n",
      "Bagging \n",
      " \n",
      " \n",
      " \n",
      "Bagging: Bootstrap Aggregation – Basic Idea \n",
      " \n",
      "1. Create a sample data set from the big data set to train the model. \n",
      "2. Select one model family from decision trees, KNN etc. of high variance models. \n",
      "3. Train the model on the sample data. This will be called model 1. \n",
      "\n",
      "\n",
      "199. 4. Now again create a new sample with replacement strategy which means mix the \n",
      "original dataset with the sample created in step 1 and again create a new fresh sample. \n",
      "5. Now again train the same model family with new sample data. This will be called \n",
      "model 2. \n",
      "6. This way we create multiple models by training on different samples data. \n",
      " \n",
      "One of the most important reasons to use Bagging is that it reduces the model variance. As \n",
      "we have already studied, the best model should have low bias and low variance. But when we \n",
      "move towards the more complex machine learning algorithms bias reduces but variance \n",
      "increases. Hence to get the optimal performance we need to reduce variance also and here \n",
      "Bagging technique becomes very helpful. \n",
      " \n",
      "Let’s understand how Bagging reduces the variance of the model. \n",
      " \n",
      "Suppose you have build 5 models with the outcome as y1, y2, y3, y4 and y5 respectively. \n",
      "Then their mean (y1+y2+y3+y4+y5)/5 will be the outcome of the final prediction. Note I am \n",
      "taking mean to explain the concept, you can take median, mode etc based on the problem \n",
      "set. \n",
      " \n",
      "You remember these outcomes y1, y2, y3, y4, y5 are predicted from the models trained on \n",
      "different samples from the big dataset. Now recall Central Limit Theorem, which says if we \n",
      "have large enough population size and a good number (greater than 30 is optimal) of samples \n",
      "then means of all samples mean will follow the normal distribution and will be same as the \n",
      "population mean.  \n",
      " \n",
      "And the important one here is the standard deviation of the samples will get reduced by the \n",
      "square root of n where n is the sample size. And the standard deviation is nothing but the \n",
      "square root of the variance hance we can say variance will also get reduced by a factor of \n",
      "1/n. \n",
      " \n",
      "That is how a combiner in Bagging reduces the model variance. And hance Bagging is used \n",
      "with high variance machine learning algorithms like decision trees, KNN and neural networks. \n",
      " \n",
      " \n",
      "Some Important points regarding Bagging \n",
      " \n",
      "• Algorithm independent: general-purpose technique, can work with any machine \n",
      "learning algorithms. (However preferable is to use with high variance algorithms) \n",
      "• Well suited for high variance algorithms. \n",
      "• Variance Reduction: Averaging a set of observations reduces variance – Central Limit \n",
      "Theorem. (explained above) \n",
      "• Choose # of models to build. (This is hyperparameter, as there is no mathematical \n",
      "formula to determine this) \n",
      " \n",
      "One Important thought, how it is different from the K-fold cross-validation technique. \n",
      "\n",
      "\n",
      "200. In k-fold cross-validation, we use the same training data and do different iteration by \n",
      "choosing different hyperparameter values. It is basically used to find the best possible \n",
      "hyperparameter. However in bagging we use different training data set. \n",
      " \n",
      "• Easy to parallelize. \n",
      "• Limitation: Loss of Interpretability, for example, if we take the decision tree as a single \n",
      "model, then it is very easy to interpret however if we build multiple decision trees with \n",
      "the different sample then it becomes a forest and we decide the outcome by \n",
      "combining the outputs of various models here we lose interpretability. \n",
      "• Limitation: What if one of the features dominates? If one feature dominates more \n",
      "then if you build multiple decision trees then they all will be exactly same (think about \n",
      "the if-else rule in decision trees) and bagging won't work properly. That is where we \n",
      "do different feature selection also along with different sampling and then assign \n",
      "different weight to the models depending upon the dominating features. And when \n",
      "we involve different feature subspace with different sampling then this method is \n",
      "called Random forest. \n",
      " \n",
      "Bagging is described using the below infographics on bagging technique. \n",
      " \n",
      " \n",
      " \n",
      "So let’s understand Random Feature Subspace in detail. \n",
      " \n",
      "Random Feature Subspaces \n",
      " \n",
      "Build Different Models using: \n",
      "• A different subset of training data (Create samples with replacement) \n",
      "• A random subset of the features! (In Bagging every time we take all features) \n",
      "• The same Ml algorithm on each training only above two things gets change. \n",
      "\n",
      "\n",
      "201. Why we need Random Feature Subspaces? \n",
      "• Think “regularization” \n",
      "• If there are one very strong predictor & other moderately strong predictors. \n",
      "• All models will give high importance to the strong predictor which means all models \n",
      "in the ensemble will be similar. \n",
      "• This is the reason we do feature subspace along with different samples. \n",
      "• Choose # of models to build and # of features (m) for each sample from p available \n",
      "features. \n",
      "• Recommended heuristics to select m out of p is m = sqrt(p)  \n",
      "• If m = p: Approach reduces to Bagged Trees. (means we include all the features in \n",
      "every sample training data set) \n",
      "Advantages \n",
      "• De-correlates the models in the ensemble \n",
      "• Improve the accuracy of prediction \n",
      "• Of-course reduces model variance. This comes from bagged trees. \n",
      " \n",
      "Random Feature Spaces in Decision Trees: Which is known as Random Forests Algorithm \n",
      "• Decision Trees have high variance. \n",
      "• The resulting tree (model) depends on the underlying training data. \n",
      "• Bagged Trees can help reduce variance; Random Forests even more so… \n",
      "Random Forests \n",
      "• Sample with replacement (shift from 1 training set to Multiple training sets) \n",
      "• Train model on each training set \n",
      "• Each tree uses a random subset of the feature: A random forest \n",
      "• Each DT predicts \n",
      "• Take Mean / Majority vote prediction for the final prediction \n",
      "• Faster than bagging (fewer splits to evaluate per tree) \n",
      " \n",
      " \n",
      "\n",
      "\n",
      "202. Thank You \n",
      "If you like my Posts on Machine Learning, Please connect with me on \n",
      "Follow my blog: https://ashutoshtripathi.com/ \n",
      "LinkedIn: https://www.linkedin.com/in/ashutoshtripathi1/ \n",
      "Instagram: https://www.instagram.com/ashutosh_ai/ \n",
      "Medium Articles: https://medium.com/@ashutosh.optimistic \n",
      " \n",
      " \n",
      "\n",
      "\n",
      "203. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      "Data Science Interview Questions  \n",
      " \n",
      "Statistics: \n",
      "1. \n",
      "What is the Central Limit Theorem and why is it important? \n",
      "“Suppose that we are interested in estimating the average height among all people. Collecting data for \n",
      "every person in the world is impossible. While we can’t obtain a height measurement from everyone in the \n",
      "population, we can still sample some people. The question now becomes, what can we say about the \n",
      "average height of the entire population given a single sample. The Central Limit Theorem addresses this \n",
      "question exactly.” Read more here. \n",
      "2. \n",
      "What is sampling? How many sampling methods do you know? \n",
      "“Data sampling is a statistical analysis technique used to select, manipulate and analyze a representative \n",
      "subset of data points to identify patterns and trends in the larger data set being examined.” Read the full \n",
      "answer here. \n",
      "3. \n",
      "What is the difference between type I vs type II error? \n",
      "“A type I error occurs when the null hypothesis is true, but is rejected. A type II error occurs when the null \n",
      "hypothesis is false, but erroneously fails to be rejected.” Read the full answer here. \n",
      "4. \n",
      "What is linear regression? What do the terms p-value, coefficient, and r-squared \n",
      "value mean? What is the significance of each of these components? \n",
      "A linear regression is a good tool for quick predictive analysis: for example, the price of a house depends \n",
      "on a myriad of factors, such as its size or its location. In order to see the relationship between these \n",
      "variables, we need to build a linear regression, which predicts the line of best fit between them and can \n",
      "help conclude whether or not these two factors have a positive or negative relationship. Read \n",
      "more here and here. \n",
      "5. \n",
      "What are the assumptions required for linear regression? \n",
      "There are four major assumptions: 1. There is a linear relationship between the dependent variables and \n",
      "the regressors, meaning the model you are creating actually fits the data, 2. The errors or residuals of the \n",
      "data are normally distributed and independent from each other, 3. There is minimal multicollinearity \n",
      "between explanatory variables, and 4. Homoscedasticity. This means the variance around the regression \n",
      "line is the same for all values of the predictor variable. \n",
      "6. \n",
      "What is a statistical interaction? \n",
      "”Basically, an interaction is when the effect of one factor (input variable) on the dependent variable (output \n",
      "variable) differs among levels of another factor.” Read more here. \n",
      "7. \n",
      "What is selection bias? \n",
      "“Selection (or ‘sampling’) bias occurs in an ‘active,’ sense when the sample data that is gathered and \n",
      "prepared for modeling has characteristics that are not representative of the true, future population of cases \n",
      "\n",
      "\n",
      "204. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      "the model will see. That is, active selection bias occurs when a subset of the data are systematically (i.e., \n",
      "non-randomly) excluded from analysis.” Read more here. \n",
      "8. What is an example of a data set with a non-Gaussian distribution? \n",
      "“The Gaussian distribution is part of the Exponential family of distributions, but there are a lot more of \n",
      "them, with the same sort of ease of use, in many cases, and if the person doing the machine learning has \n",
      "a solid grounding in statistics, they can be utilized where appropriate.” Read more here. \n",
      "9. What is the Binomial Probability Formula? \n",
      "“The binomial distribution consists of the probabilities of each of the possible numbers of successes on N \n",
      "trials for independent events that each have a probability of π (the Greek letter pi) of occurring.” Read more  \n",
      "Data Science : \n",
      "Q1. What is Data Science? List the differences between supervised and unsupervised \n",
      "learning. \n",
      "Data Science is a blend of various tools, algorithms, and machine learning principles with the goal to discover \n",
      "hidden patterns from the raw data. How is this different from what statisticians have been doing for years? \n",
      "The answer lies in the difference between explaining and predicting. \n",
      " \n",
      "The differences between supervised and unsupervised learning are as follows; \n",
      "Supervised Learning \n",
      "Unsupervised Learning \n",
      "Input data is labelled. \n",
      "Input data is unlabelled. \n",
      "Uses a training data set. \n",
      "Uses the input data set. \n",
      "Used for prediction. \n",
      "Used for analysis. \n",
      "Enables classification and regression. Enables Classification, Density Estimation, & Dimension Reduction \n",
      "Q2. What is Selection Bias? \n",
      "Selection bias is a kind of error that occurs when the researcher decides who is going to be studied. It is \n",
      "usually associated with research where the selection of participants isn’t random. It is sometimes referred to \n",
      "\n",
      "\n",
      "205. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      "as the selection effect. It is the distortion of statistical analysis, resulting from the method of collecting \n",
      "samples. If the selection bias is not taken into account, then some conclusions of the study may not be \n",
      "accurate. \n",
      "The types of selection bias include: \n",
      "1. Sampling bias: It is a systematic error due to a non-random sample of a population causing some \n",
      "members of the population to be less likely to be included than others resulting in a biased sample. \n",
      "2. Time interval: A trial may be terminated early at an extreme value (often for ethical reasons), but the \n",
      "extreme value is likely to be reached by the variable with the largest variance, even if all variables \n",
      "have a similar mean. \n",
      "3. Data: When specific subsets of data are chosen to support a conclusion or rejection of bad data on \n",
      "arbitrary grounds, instead of according to previously stated or generally agreed criteria. \n",
      "4. Attrition: Attrition bias is a kind of selection bias caused by attrition (loss of participants) discounting \n",
      "trial subjects/tests that did not run to completion. \n",
      "Q3. What is bias-variance trade-off? \n",
      "Bias: Bias is an error introduced in your model due to oversimplification of the machine learning algorithm. \n",
      "It can lead to underfitting. When you train your model at that time model makes simplified assumptions to \n",
      "make the target function easier to understand. \n",
      "Low bias machine learning algorithms — Decision Trees, k-NN and SVM High bias machine learning \n",
      "algorithms — Linear Regression, Logistic Regression \n",
      "Variance: Variance is error introduced in your model due to complex machine learning algorithm, your model \n",
      "learns noise also from the training data set and performs badly on test data set. It can lead to high sensitivity \n",
      "and overfitting. \n",
      "Normally, as you increase the complexity of your model, you will see a reduction in error due to lower bias \n",
      "in the model. However, this only happens until a particular point. As you continue to make your model more \n",
      "complex, you end up over-fitting your model and hence your model will start suffering from high variance. \n",
      " \n",
      "Bias-Variance trade-off: The goal of any supervised machine learning algorithm is to have low bias and \n",
      "low variance to achieve good prediction performance. \n",
      "\n",
      "\n",
      "206. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      "1. The k-nearest neighbour algorithm has low bias and high variance, but the trade-off can be changed \n",
      "by increasing the value of k which increases the number of neighbours that contribute to the prediction \n",
      "and in turn increases the bias of the model. \n",
      "2. The support vector machine algorithm has low bias and high variance, but the trade-off can be \n",
      "changed by increasing the C parameter that influences the number of violations of the margin allowed \n",
      "in the training data which increases the bias but decreases the variance. \n",
      "There is no escaping the relationship between bias and variance in machine learning. Increasing the bias \n",
      "will decrease the variance. Increasing the variance will decrease bias. \n",
      "Q4. What is a confusion matrix? \n",
      "The confusion matrix is a 2X2 table that contains 4 outputs provided by the binary classifier. Various \n",
      "measures, such as error-rate, accuracy, specificity, sensitivity, precision and recall are derived from \n",
      "it. Confusion Matrix \n",
      " \n",
      "A data set used for performance evaluation is called a test data set. It should contain the correct labels and \n",
      "predicted labels. \n",
      " \n",
      "The predicted labels will exactly the same if the performance of a binary classifier is perfect. \n",
      "\n",
      "\n",
      "207. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      " \n",
      "The predicted labels usually match with part of the observed labels in real-world scenarios. \n",
      " \n",
      "A binary classifier predicts all data instances of a test data set as either positive or negative. This produces \n",
      "four outcomes- \n",
      "1. True-positive(TP) — Correct positive prediction \n",
      "2. False-positive(FP) — Incorrect positive prediction \n",
      "3. True-negative(TN) — Correct negative prediction \n",
      "4. False-negative(FN) — Incorrect negative prediction \n",
      " \n",
      "Basic measures derived from the confusion matrix \n",
      "1. Error Rate = (FP+FN)/(P+N) \n",
      "2. Accuracy = (TP+TN)/(P+N) \n",
      "3. Sensitivity(Recall or True positive rate) = TP/P \n",
      "4. Specificity(True negative rate) = TN/N \n",
      "5. Precision(Positive predicted value) = TP/(TP+FP) \n",
      "6. F-Score(Harmonic mean of precision and recall) = (1+b)(PREC.REC)/(b²PREC+REC) where b is \n",
      "commonly 0.5, 1, 2. \n",
      "\n",
      "\n",
      "208. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      "  \n",
      "STATISTICS INTERVIEW QUESTIONS \n",
      "Q5. What is the difference between “long” and “wide” format data? \n",
      "In the wide-format, a subject’s repeated responses will be in a single row, and each response is in a \n",
      "separate column. In the long-format, each row is a one-time point per subject. You can recognize data in \n",
      "wide format by the fact that columns generally represent groups. \n",
      " \n",
      "Q6. What do you understand by the term Normal Distribution? \n",
      "Data is usually distributed in different ways with a bias to the left or to the right or it can all be jumbled up. \n",
      "However, there are chances that data is distributed around a central value without any bias to the left or right \n",
      "and reaches normal distribution in the form of a bell-shaped curve. \n",
      " \n",
      "Figure: Normal distribution in a bell curve \n",
      "The random variables are distributed in the form of a symmetrical, bell-shaped curve. \n",
      "Properties of Normal Distribution are as follows; \n",
      "1. Unimodal -one mode \n",
      "2. Symmetrical -left and right halves are mirror images \n",
      "3. Bell-shaped -maximum height (mode) at the mean \n",
      "4. Mean, Mode, and Median are all located in the center \n",
      "5. Asymptotic \n",
      "Q7. What is correlation and covariance in statistics? \n",
      "\n",
      "\n",
      "209. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      "Covariance and Correlation are two mathematical concepts; these two approaches are widely used in \n",
      "statistics. Both Correlation and Covariance establish the relationship and also measure the dependency \n",
      "between two random variables. Though the work is similar between these two in mathematical terms, they \n",
      "are different from each other. \n",
      "Correlation: \n",
      "Correlation is considered or described as the best technique for measuring and also for estimating the \n",
      "quantitative relationship between two variables. Correlation measures how strongly two variables are \n",
      "related. \n",
      "Covariance: In covariance two items vary together and it’s a measure that indicates the extent to which two \n",
      "random variables change in cycle. It is a statistical term; it explains the systematic relation between a pair of \n",
      "random variables, wherein changes in one variable reciprocal by a corresponding change in another \n",
      "variable. \n",
      "Q8. What is the difference between Point Estimates and Confidence Interval? \n",
      "Point Estimation gives us a particular value as an estimate of a population parameter. Method of Moments \n",
      "and Maximum Likelihood estimator methods are used to derive Point Estimators for population parameters. \n",
      "A confidence interval gives us a range of values which is likely to contain the population parameter. The \n",
      "confidence interval is generally preferred, as it tells us how likely this interval is to contain the population \n",
      "parameter. This likeliness or probability is called Confidence Level or Confidence coefficient and represented \n",
      "by 1 — alpha, where alpha is the level of significance. \n",
      "Q9. What is the goal of A/B Testing? \n",
      "It is a hypothesis testing for a randomized experiment with two variables A and B. \n",
      "The goal of A/B Testing is to identify any changes to the web page to maximize or increase the outcome of \n",
      "interest. A/B testing is a fantastic method for figuring out the best online promotional and marketing strategies \n",
      "for your business. It can be used to test everything from website copy to sales emails to search ads \n",
      "\n",
      "\n",
      "210. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      "An example of this could be identifying the click-through rate for a banner ad. \n",
      "Q10. What is p-value? \n",
      "When you perform a hypothesis test in statistics, a p-value can help you determine the strength of your \n",
      "results. p-value is a number between 0 and 1. Based on the value it will denote the strength of the results. \n",
      "The claim which is on trial is called the Null Hypothesis. \n",
      "Low p-value (≤ 0.05) indicates strength against the null hypothesis which means we can reject the null \n",
      "Hypothesis. High p-value (≥ 0.05) indicates strength for the null hypothesis which means we can accept the \n",
      "null Hypothesis p-value of 0.05 indicates the Hypothesis could go either way. To put it in another way, \n",
      "High P values: your data are likely with a true null. Low P values: your data are unlikely with a true null. \n",
      "Q11. In any 15-minute interval, there is a 20% probability that you will see at least one shooting star. \n",
      "What is the probability that you see at least one shooting star in the period of an hour? \n",
      "Probability of not seeing any shooting star in 15 minutes is \n",
      "=   \n",
      "1 \n",
      "– \n",
      "P( \n",
      "Seeing \n",
      "one \n",
      "shooting \n",
      "star \n",
      ") \n",
      "=   1 – 0.2          =    0.8 \n",
      "Probability of not seeing any shooting star in the period of one hour \n",
      "=   (0.8) ^ 4        =    0.4096 \n",
      "Probability of seeing at least one shooting star in the one hour \n",
      "=   \n",
      "1 \n",
      "– \n",
      "P( \n",
      "Not \n",
      "seeing \n",
      "any \n",
      "star \n",
      ") \n",
      "=   1 – 0.4096     =    0.5904 \n",
      "Q12. How can you generate a random number between 1 – 7 with only a die? \n",
      "• \n",
      "Any die has six sides from 1-6. There is no way to get seven equal outcomes from a single rolling of \n",
      "a die. If we roll the die twice and consider the event of two rolls, we now have 36 different outcomes. \n",
      "• \n",
      "To get our 7 equal outcomes we have to reduce this 36 to a number divisible by 7. We can thus \n",
      "consider only 35 outcomes and exclude the other one. \n",
      "• \n",
      "A simple scenario can be to exclude the combination (6,6), i.e., to roll the die again if 6 appears twice. \n",
      "• \n",
      "All the remaining combinations from (1,1) till (6,5) can be divided into 7 parts of 5 each. This way all \n",
      "the seven sets of outcomes are equally likely. \n",
      "Q13. A certain couple tells you that they have two children, at least one of which is a girl. What is the \n",
      "probability that they have two girls? \n",
      "In the case of two children, there are 4 equally likely possibilities  \n",
      "BB, BG, GB and GG; \n",
      "where B = Boy and G = Girl and the first letter denotes the first child. \n",
      "From the question, we can exclude the first case of BB. Thus from the remaining 3 possibilities \n",
      "of BG, GB & BB, we have to find the probability of the case with two girls. \n",
      "Thus, P(Having two girls given one girl)   =    1 / 3 \n",
      "\n",
      "\n",
      "211. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      "Q14. A jar has 1000 coins, of which 999 are fair and 1 is double headed. Pick a coin at random, and \n",
      "toss it 10 times. Given that you see 10 heads, what is the probability that the next toss of that coin \n",
      "is also a head? \n",
      "There are two ways of choosing the coin. One is to pick a fair coin and the other is to pick the one with two \n",
      "heads. \n",
      "Probability \n",
      "of \n",
      "selecting \n",
      "fair \n",
      "coin \n",
      "= \n",
      "999/1000 \n",
      "= 0.999 \n",
      "Probability of selecting unfair coin = 1/1000 = 0.001 \n",
      "Selecting 10 heads in a row = Selecting fair coin * Getting 10 heads  +  Selecting an unfair coin \n",
      "P \n",
      "(A)  \n",
      "=  \n",
      "0.999 \n",
      "* \n",
      "(1/2)^5  \n",
      "=  \n",
      "0.999 \n",
      "* \n",
      "(1/1024)  \n",
      "=  0.000976 \n",
      "P \n",
      "(B)  \n",
      "=  \n",
      "0.001 \n",
      "* \n",
      "1  \n",
      "=  0.001 \n",
      "P( \n",
      "A \n",
      "/ \n",
      "A \n",
      "+ \n",
      "B \n",
      ")  \n",
      "= 0.000976 \n",
      "/  \n",
      "(0.000976 \n",
      "+ \n",
      "0.001)  \n",
      "=  0.4939 \n",
      "P( B / A + B )  = 0.001 / 0.001976  =  0.5061 \n",
      "Probability of selecting another head = P(A/A+B) * 0.5 + P(B/A+B) * 1 = 0.4939 * 0.5 + 0.5061  =  0.7531 \n",
      "Q15. What do you understand by statistical power of sensitivity and how do you calculate it? \n",
      "Sensitivity is commonly used to validate the accuracy of a classifier (Logistic, SVM, Random Forest etc.). \n",
      "Sensitivity is nothing but “Predicted True events/ Total events”. True events here are the events which were \n",
      "true and model also predicted them as true. \n",
      "Calculation of seasonality is pretty straightforward. \n",
      "Seasonality = ( True Positives ) / ( Positives in Actual Dependent Variable ) \n",
      "Q16. Why Is Re-sampling Done? \n",
      "Resampling is done in any of these cases: \n",
      "• \n",
      "Estimating the accuracy of sample statistics by using subsets of accessible data or drawing randomly \n",
      "with replacement from a set of data points \n",
      "• \n",
      "Substituting labels on data points when performing significance tests \n",
      "• \n",
      "Validating models by using random subsets (bootstrapping, cross-validation) \n",
      "Q17. What are the differences between over-fitting and under-fitting? \n",
      "In statistics and machine learning, one of the most common tasks is to fit a model to a set of training data, \n",
      "so as to be able to make reliable predictions on general untrained data. \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      "\n",
      "\n",
      "212. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      " \n",
      "In overfitting, a statistical model describes random error or noise instead of the underlying relationship. \n",
      "Overfitting occurs when a model is excessively complex, such as having too many parameters relative to \n",
      "the number of observations. A model that has been overfitted, has poor predictive performance, as it \n",
      "overreacts to minor fluctuations in the training data. \n",
      "Underfitting occurs when a statistical model or machine learning algorithm cannot capture the underlying \n",
      "trend of the data. Underfitting would occur, for example, when fitting a linear model to non-linear data. Such \n",
      "a model too would have poor predictive performance. \n",
      "Q18. How to combat Overfitting and Underfitting? \n",
      "To combat overfitting and underfitting, you can resample the data to estimate the model accuracy (k-fold \n",
      "cross-validation) and by having a validation dataset to evaluate the model. \n",
      "Q19. What is regularisation? Why is it useful? \n",
      " \n",
      " \n",
      "Data Scientist Masters Program \n",
      "Explore Curriculum \n",
      " \n",
      "Regularisation is the process of adding tuning parameter to a model to induce smoothness in order to prevent \n",
      "overfitting. This is most often done by adding a constant multiple to an existing weight vector. This constant \n",
      "is often the L1(Lasso) or L2(ridge). The model predictions should then minimize the loss function calculated \n",
      "on the regularized training set. \n",
      "Q20. What Is the Law of Large Numbers? \n",
      "\n",
      "\n",
      "213. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      "It is a theorem that describes the result of performing the same experiment a large number of times. This \n",
      "theorem forms the basis of frequency-style thinking. It says that the sample means, the sample variance \n",
      "and the sample standard deviation converge to what they are trying to estimate. \n",
      "Q21.  What Are Confounding Variables? \n",
      "In statistics, a confounder is a variable that influences both the dependent variable and independent variable. \n",
      "For example, if you are researching whether a lack of exercise leads to weight gain, \n",
      "lack of exercise = independent variable \n",
      "weight gain = dependent variable. \n",
      "A confounding variable here would be any other variable that affects both of these variables, such as the age \n",
      "of the subject. \n",
      "Q22. What Are the Types of Biases That Can Occur During Sampling? \n",
      "• \n",
      "Selection bias \n",
      "• \n",
      "Under coverage bias \n",
      "• \n",
      "Survivorship bias \n",
      "Q23. What is Survivorship Bias? \n",
      "It is the logical error of focusing aspects that support surviving some process and casually overlooking those \n",
      "that did not work because of their lack of prominence. This can lead to wrong conclusions in numerous \n",
      "different means. \n",
      "Q24. What is selection Bias? \n",
      "Selection bias occurs when the sample obtained is not representative of the population intended to be \n",
      "analysed. \n",
      "Q25. Explain how a ROC curve works? \n",
      "The ROC curve is a graphical representation of the contrast between true positive rates and false-positive \n",
      "rates at various thresholds. It is often used as a proxy for the trade-off between the sensitivity(true positive \n",
      "rate) and false-positive rate. \n",
      "\n",
      "\n",
      "214. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      " \n",
      "  \n",
      "Q26. What is TF/IDF vectorization? \n",
      "TF–IDF is short for term frequency-inverse document frequency, is a numerical statistic that is intended to \n",
      "reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor \n",
      "in information retrieval and text mining. \n",
      "The TF–IDF value increases proportionally to the number of times a word appears in the document but is \n",
      "offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear \n",
      "more frequently in general. \n",
      "Q27. Why we generally use Softmax non-linearity function as last operation in-network? \n",
      "It is because it takes in a vector of real numbers and returns a probability distribution. Its definition is as \n",
      "follows. Let x be a vector of real numbers (positive, negative, whatever, there are no constraints). \n",
      "Then the i’th component of Softmax(x) is — \n",
      "\n",
      "\n",
      "215. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      " \n",
      "It should be clear that the output is a probability distribution: each element is non-negative and the sum over \n",
      "all components is 1. \n",
      "  \n",
      "DATA ANALYSIS INTERVIEW QUESTIONS \n",
      "Q28. Python or R – Which one would you prefer for text analytics? \n",
      "We will prefer Python because of the following reasons: \n",
      "• \n",
      "Python would be the best option because it has Pandas library that provides easy to use data \n",
      "structures and high-performance data analysis tools. \n",
      "• \n",
      "R is more suitable for machine learning than just text analysis. \n",
      "• \n",
      "Python performs faster for all types of text analytics. \n",
      "Q29. How does data cleaning plays a vital role in the analysis? \n",
      "Data cleaning can help in analysis because: \n",
      "• \n",
      "Cleaning data from multiple sources helps to transform it into a format that data analysts or data \n",
      "scientists can work with. \n",
      "• \n",
      "Data Cleaning helps to increase the accuracy of the model in machine learning. \n",
      "• \n",
      "It is a cumbersome process because as the number of data sources increases, the time taken to \n",
      "clean the data increases exponentially due to the number of sources and the volume of data \n",
      "generated by these sources. \n",
      "• \n",
      "It might take up to 80% of the time for just cleaning data making it a critical part of the analysis task. \n",
      "Q30. Differentiate between univariate, bivariate and multivariate analysis. \n",
      "Univariate analyses are descriptive statistical analysis techniques which can be differentiated based on the \n",
      "number of variables involved at a given point of time. For example, the pie charts of sales based on territory \n",
      "involve only one variable and can the analysis can be referred to as univariate analysis. \n",
      "The bivariate analysis attempts to understand the difference between two variables at a time as in a \n",
      "scatterplot. For example, analyzing the volume of sale and spending can be considered as an example of \n",
      "bivariate analysis. \n",
      "Multivariate analysis deals with the study of more than two variables to understand the effect of variables \n",
      "on the responses. \n",
      "\n",
      "\n",
      "216. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      "Q31. Explain Star Schema. \n",
      "It is a traditional database schema with a central table. Satellite tables map IDs to physical names or \n",
      "descriptions and can be connected to the central fact table using the ID fields; these tables are known as \n",
      "lookup tables and are principally useful in real-time applications, as they save a lot of memory. Sometimes \n",
      "star schemas involve several layers of summarization to recover information faster. \n",
      "Q32. What is Cluster Sampling? \n",
      "Cluster sampling is a technique used when it becomes difficult to study the target population spread across \n",
      "a wide area and simple random sampling cannot be applied. Cluster Sample is a probability sample where \n",
      "each sampling unit is a collection or cluster of elements. \n",
      "For eg., A researcher wants to survey the academic performance of high school students in Japan. He can \n",
      "divide the entire population of Japan into different clusters (cities). Then the researcher selects a number of \n",
      "clusters depending on his research through simple or systematic random sampling. \n",
      "Let’s continue our Data Science Interview Questions blog with some more statistics questions. \n",
      "Q33. What is Systematic Sampling? \n",
      "Systematic sampling is a statistical technique where elements are selected from an ordered sampling frame. \n",
      "In systematic sampling, the list is progressed in a circular manner so once you reach the end of the list, it is \n",
      "progressed from the top again. The best example of systematic sampling is equal probability method. \n",
      "Q34. What are Eigenvectors and Eigenvalues? \n",
      "Eigenvectors are used for understanding linear transformations. In data analysis, we usually calculate the \n",
      "eigenvectors for a correlation or covariance matrix. Eigenvectors are the directions along which a particular \n",
      "linear transformation acts by flipping, compressing or stretching. \n",
      "Eigenvalue can be referred to as the strength of the transformation in the direction of eigenvector or the \n",
      "factor by which the compression occurs. \n",
      "Q35. Can you cite some examples where a false positive is important than a false negative? \n",
      "Let us first understand what false positives and false negatives are. \n",
      "• \n",
      "False Positives are the cases where you wrongly classified a non-event as an event a.k.a Type I \n",
      "error. \n",
      "• \n",
      "False Negatives are the cases where you wrongly classify events as non-events, a.k.a Type II error. \n",
      "Example 1: In the medical field, assume you have to give chemotherapy to patients. Assume a patient \n",
      "comes to that hospital and he is tested positive for cancer, based on the lab prediction but he actually doesn’t \n",
      "have cancer. This is a case of false positive. Here it is of utmost danger to start chemotherapy on this patient \n",
      "when he actually does not have cancer. In the absence of cancerous cell, chemotherapy will do certain \n",
      "damage to his normal healthy cells and might lead to severe diseases, even cancer. \n",
      "Example 2: Let’s say an e-commerce company decided to give $1000 Gift voucher to the customers whom \n",
      "they assume to purchase at least $10,000 worth of items. They send free voucher mail directly to 100 \n",
      "customers without any minimum purchase condition because they assume to make at least 20% profit on \n",
      "sold items above $10,000. Now the issue is if we send the $1000 gift vouchers to customers who have not \n",
      "actually purchased anything but are marked as having made $10,000 worth of purchase. \n",
      "\n",
      "\n",
      "217. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      "Q36. Can you cite some examples where a false negative important than a false positive? \n",
      "Example 1: Assume there is an airport ‘A’ which has received high-security threats and based on certain \n",
      "characteristics they identify whether a particular passenger can be a threat or not. Due to a shortage of staff, \n",
      "they decide to scan passengers being predicted as risk positives by their predictive model. What will happen \n",
      "if a true threat customer is being flagged as non-threat by airport model? \n",
      "Example 2: What if Jury or judge decides to make a criminal go free? \n",
      "Example 3: What if you rejected to marry a very good person based on your predictive model and you \n",
      "happen to meet him/her after a few years and realize that you had a false negative? \n",
      "Q37. Can you cite some examples where both false positive and false negatives are equally \n",
      "important? \n",
      "In the Banking industry giving loans is the primary source of making money but at the same time if your \n",
      "repayment rate is not good you will not make any profit, rather you will risk huge losses. \n",
      "Banks don’t want to lose good customers and at the same point in time, they don’t want to acquire bad \n",
      "customers. In this scenario, both the false positives and false negatives become very important to measure. \n",
      "Q38. Can you explain the difference between a Validation Set and a Test Set? \n",
      "A Validation set can be considered as a part of the training set as it is used for parameter selection and to \n",
      "avoid overfitting of the model being built. \n",
      "On the other hand, a Test Set is used for testing or evaluating the performance of a trained machine learning \n",
      "model. \n",
      "In simple terms, the differences can be summarized as; training set is to fit the parameters i.e. weights and \n",
      "test set is to assess the performance of the model i.e. evaluating the predictive power and generalization. \n",
      "Q39. Explain cross-validation. \n",
      "Cross-validation is a model validation technique for evaluating how the outcomes of statistical analysis \n",
      "will generalize to an independent dataset. Mainly used in backgrounds where the objective is forecast and \n",
      "one wants to estimate how accurately a model will accomplish in practice. \n",
      "The goal of cross-validation is to term a data set to test the model in the training phase (i.e. validation data \n",
      "set) in order to limit problems like overfitting and get an insight on how the model will generalize to an \n",
      "independent data set. \n",
      "  \n",
      "MACHINE LEARNING INTERVIEW QUESTIONS \n",
      "Q40. What is Machine Learning? \n",
      "Machine Learning explores the study and construction of algorithms that can learn from and make \n",
      "predictions on data. Closely related to computational statistics. Used to devise complex models and \n",
      "algorithms that lend themselves to a prediction which in commercial use is known as predictive analytics. \n",
      "Given below, is an image representing the various domains Machine Learning lends itself to. \n",
      "\n",
      "\n",
      "218. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      " \n",
      " \n",
      "Q41. What is Supervised Learning? \n",
      "Supervised learning is the machine learning task of inferring a function from labeled training data. The \n",
      "training data consist of a set of training examples. \n",
      "Algorithms: Support Vector Machines, Regression, Naive Bayes, Decision Trees, K-nearest \n",
      "Neighbor Algorithm and Neural Networks \n",
      "E.g. If you built a fruit classifier, the labels will be “this is an orange, this is an apple and this is a banana”, \n",
      "based on showing the classifier examples of apples, oranges and bananas. \n",
      "Q42. What is Unsupervised learning? \n",
      "Unsupervised learning is a type of machine learning algorithm used to draw inferences from datasets \n",
      "consisting of input data without labelled responses. \n",
      "Algorithms: Clustering, Anomaly Detection, Neural Networks and Latent Variable Models \n",
      "E.g. In the same example, a fruit clustering will categorize as “fruits with soft skin and lots of dimples”, “fruits \n",
      "with shiny hard skin” and “elongated yellow fruits”. \n",
      "Q43. What are the various classification algorithms? \n",
      "The diagram lists the most important classification algorithms. \n",
      " \n",
      "Q44. What is ‘Naive’ in a Naive Bayes? \n",
      "The Naive Bayes Algorithm is based on the Bayes Theorem. Bayes’ theorem describes the probability of \n",
      "an event, based on prior knowledge of conditions that might be related to the event. \n",
      "The Algorithm is ‘naive’ because it makes assumptions that may or may not turn out to be correct. \n",
      "Q45. Explain SVM algorithm in detail. \n",
      "SVM stands for support vector machine, it is a supervised machine learning algorithm which can be used \n",
      "for both Regression and Classification. If you have n features in your training data set, SVM tries to plot it \n",
      "\n",
      "\n",
      "219. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      "in n-dimensional space with the value of each feature being the value of a particular coordinate. SVM uses \n",
      "hyperplanes to separate out different classes based on the provided kernel function. \n",
      " \n",
      "Q46. What are the support vectors in SVM? \n",
      " \n",
      "In the diagram, we see that the thinner lines mark the distance from the classifier to the closest data points \n",
      "called the support vectors (darkened data points). The distance between the two thin lines is called the \n",
      "margin. \n",
      "Q47. What are the different kernels in SVM? \n",
      "There are four types of kernels in SVM. \n",
      "1. Linear Kernel \n",
      "2. Polynomial kernel \n",
      "3. Radial basis kernel \n",
      "4. Sigmoid kernel \n",
      "Q48. Explain Decision Tree algorithm in detail. \n",
      "\n",
      "\n",
      "220. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      "A decision tree is a supervised machine learning algorithm mainly used for Regression and \n",
      "Classification. It breaks down a data set into smaller and smaller subsets while at the same time an \n",
      "associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf \n",
      "nodes. A decision tree can handle both categorical and numerical data. \n",
      " \n",
      "Q49. What are Entropy and Information gain in Decision tree algorithm? \n",
      "The core algorithm for building a decision tree is called ID3. ID3 uses Entropy and Information Gain  \n",
      "Entropy \n",
      "A decision tree is built top-down from a root node and involve partitioning of data into homogenious \n",
      "subsets. ID3 uses enteropy to check the homogeneity of a sample. If the sample is completely homogenious \n",
      "then entropy is zero and if the sample is an equally divided it has entropy of one. \n",
      " \n",
      "Information Gain \n",
      "The Information Gain is based on the decrease in entropy after a dataset is split on an attribute. \n",
      "Constructing a decision tree is all about finding attributes that return the highest information gain. \n",
      "\n",
      "\n",
      "221. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      " \n",
      " \n",
      "Q50. What is pruning in Decision Tree? \n",
      "Pruning is a technique in machine learning and search algorithms that reduces the size of decision \n",
      "trees by removing sections of the tree that provide little power to classify instances. So, when we remove \n",
      "sub-nodes of a decision node, this process is called pruning or opposite process of splitting. \n",
      "Q51. What is logistic regression? State an example when you have used logistic \n",
      "regression recently. \n",
      "Logistic Regression often referred to as the logit model is a technique to predict the binary outcome from \n",
      "a linear combination of predictor variables.  \n",
      "For example, if you want to predict whether a particular political leader will win the election or not. In this \n",
      "case, the outcome of prediction is binary i.e. 0 or 1 (Win/Lose). The predictor variables here would be the \n",
      "amount of money spent for election campaigning of a particular candidate, the amount of time spent in \n",
      "campaigning, etc. \n",
      "Q52. What is Linear Regression? \n",
      "Linear regression is a statistical technique where the score of a variable Y is predicted from the score of a \n",
      "second variable X. X is referred to as the predictor variable and Y as the criterion variable. \n",
      "\n",
      "\n",
      "222. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      " \n",
      "Follow me for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      "Q53. What Are the Drawbacks of the Linear Model? \n",
      "Some drawbacks of the linear model are: \n",
      "• \n",
      "The assumption of linearity of the errors. \n",
      "• \n",
      "It can’t be used for count outcomes or binary outcomes \n",
      "• \n",
      "There are overfitting problems that it can’t solve \n",
      "Q54. What is the difference between Regression and classification ML techniques? \n",
      "Both Regression and classification machine learning techniques come under Supervised machine \n",
      "learning algorithms. In Supervised machine learning algorithm, we have to train the model using labelled \n",
      "data set, While training we have to explicitly provide the correct labels and algorithm tries to learn the pattern \n",
      "from input to output. If our labels are discrete values then it will a classification problem, e.g A,B etc. but if \n",
      "our labels are continuous values then it will be a regression problem, e.g 1.23, 1.333 etc. \n",
      "Q55. What are Recommender Systems? \n",
      "Recommender Systems are a subclass of information filtering systems that are meant to predict the \n",
      "preferences or ratings that a user would give to a product. Recommender systems are widely used in movies, \n",
      "news, research articles, products, social tags, music, etc. \n",
      "Examples include movie recommenders in IMDB, Netflix & BookMyShow, product recommenders in e-\n",
      "commerce sites like Amazon, eBay & Flipkart, YouTube video recommendations and game \n",
      "recommendations in Xbox. \n",
      "Q56. What is Collaborative filtering? \n",
      "The process of filtering used by most of the recommender systems to find patterns or information by \n",
      "collaborating viewpoints, various data sources and multiple agents. \n",
      "\n",
      "\n",
      "223. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      " \n",
      "An example of collaborative filtering can be to predict the rating of a particular user based on his/her ratings \n",
      "for other movies and others’ ratings for all movies. This concept is widely used in recommending movies in \n",
      "IMDB, Netflix & BookMyShow, product recommenders in e-commerce sites like Amazon, eBay & Flipkart, \n",
      "YouTube video recommendations and game recommendations in Xbox. \n",
      "Q57. How can outlier values be treated? \n",
      "Outlier values can be identified by using univariate or any other graphical analysis method. If the number of \n",
      "outlier values is few then they can be assessed individually but for a large number of outliers, the values can \n",
      "be substituted with either the 99th or the 1st percentile values. \n",
      "All extreme values are not outlier values. The most common ways to treat outlier values \n",
      "1. To change the value and bring it within a range. \n",
      "2. To just remove the value. \n",
      "Q58. What are the various steps involved in an analytics project? \n",
      "The following are the various steps involved in an analytics project: \n",
      "1. Understand the Business problem \n",
      "2. Explore the data and become familiar with it. \n",
      "3. Prepare the data for modelling by detecting outliers, treating missing values, transforming variables, \n",
      "etc. \n",
      "4. After data preparation, start running the model, analyze the result and tweak the approach. This is an \n",
      "iterative step until the best possible outcome is achieved. \n",
      "5. Validate the model using a new data set. \n",
      "6. Start implementing the model and track the result to analyze the performance of the model over the \n",
      "period of time. \n",
      "Q59. During analysis, how do you treat missing values? \n",
      "The extent of the missing values is identified after identifying the variables with missing values. If any \n",
      "patterns are identified the analyst has to concentrate on them as it could lead to interesting and meaningful \n",
      "business insights. \n",
      "If there are no patterns identified, then the missing values can be substituted with mean or median values \n",
      "(imputation) or they can simply be ignored. Assigning a default value which can be mean, minimum or \n",
      "maximum value. Getting into the data is important. \n",
      "If it is a categorical variable, the default value is assigned. The missing value is assigned a default value. If \n",
      "you have a distribution of data coming, for normal distribution give the mean value. \n",
      "If 80% of the values for a variable are missing then you can answer that you would be dropping the variable \n",
      "instead of treating the missing values. \n",
      "\n",
      "\n",
      "224. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      "Q60. How will you define the number of clusters in a clustering algorithm? \n",
      "Though the Clustering Algorithm is not specified, this question is mostly in reference to K-Means \n",
      "clustering where “K” defines the number of clusters. The objective of clustering is to group similar entities \n",
      "in a way that the entities within a group are similar to each other but the groups are different from each other. \n",
      "For example, the following image shows three different groups.  \n",
      "Within \n",
      "Sum \n",
      "of \n",
      "squares is generally used to explain the homogeneity within a cluster. If you plot WSS for a range of number \n",
      "of clusters, you will get the plot shown below. \n",
      " \n",
      "• \n",
      "The Graph is generally known as Elbow Curve. \n",
      "• \n",
      "Red circled a point in above graph i.e. Number of Cluster =6 is the point after which you don’t see \n",
      "any decrement in WSS. \n",
      "• \n",
      "This point is known as the bending point and taken as K in K – Means. \n",
      "This is the widely used approach but few data scientists also use Hierarchical clustering first to create \n",
      "dendrograms and identify the distinct groups from there. \n",
      "Q61. What is Ensemble Learning? \n",
      "Ensemble Learning is basically combining a diverse set of learners(Individual models) together to improvise \n",
      "on the stability and predictive power of the model. \n",
      "Q62. Describe in brief any type of Ensemble Learning? \n",
      "Ensemble learning has many types but two more popular ensemble learning techniques are mentioned \n",
      "below. \n",
      "Bagging \n",
      "Bagging tries to implement similar learners on small sample populations and then takes a mean of all the \n",
      "predictions. In generalised bagging, you can use different learners on different population. As you expect \n",
      "this helps us to reduce the variance error. \n",
      "\n",
      "\n",
      "225. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      " \n",
      "Boosting \n",
      "Boosting is an iterative technique which adjusts the weight of an observation based on the last \n",
      "classification. If an observation was classified incorrectly, it tries to increase the weight of this observation \n",
      "and vice versa. Boosting in general decreases the bias error and builds strong predictive models. However, \n",
      "they may over fit on the training data. \n",
      " \n",
      "Q63. What is a Random Forest? How does it work? \n",
      "Random forest is a versatile machine learning method capable of performing both regression and \n",
      "classification tasks. It is also used for dimensionality reduction, treats missing values, outlier values. It is a \n",
      "type of ensemble learning method, where a group of weak models combine to form a powerful model. \n",
      "\n",
      "\n",
      "226. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      "In Random Forest, we grow multiple trees as opposed to \n",
      "a single tree. To classify a new object based on attributes, each tree gives a classification. The forest \n",
      "chooses the classification having the most votes(Overall the trees in the forest) and in case of regression, \n",
      "it takes the average of outputs by different trees. \n",
      "Q64. How Do You Work Towards a Random Forest? \n",
      "The underlying principle of this technique is that several weak learners combined to provide a keen \n",
      "learner. The steps involved are \n",
      "• \n",
      "Build several decision trees on bootstrapped training samples of data \n",
      "• \n",
      "On each tree, each time a split is considered, a random sample of mm predictors is chosen as split \n",
      "candidates, out of all pp predictors \n",
      "• \n",
      "Rule of thumb: At each split m=p√m=p \n",
      "• \n",
      "Predictions: At the majority rule \n",
      "Q65. What cross-validation technique would you use on a time series data set? \n",
      "Instead of using k-fold cross-validation, you should be aware of the fact that a time series is not randomly \n",
      "distributed data — It is inherently ordered by chronological order. \n",
      "In case of time series data, you should use techniques like forward=chaining — Where you will be model \n",
      "on past data then look at forward-facing data. \n",
      "fold 1: training[1], test[2] \n",
      "fold 1: training[1 2], test[3] \n",
      "fold 1: training[1 2 3], test[4] \n",
      "fold 1: training[1 2 3 4], test[5] \n",
      "Q66. What is a Box-Cox Transformation? \n",
      "The dependent variable for a regression analysis might not satisfy one or more assumptions of an ordinary \n",
      "least squares regression. The residuals could either curve as the prediction increases or follow the skewed \n",
      "distribution. In such scenarios, it is necessary to transform the response variable so that the data meets \n",
      "the required assumptions. A Box cox transformation is a statistical technique to transform non-normal \n",
      "dependent variables into a normal shape. If the given data is not normal then most of the statistical \n",
      "\n",
      "\n",
      "227. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      "techniques assume normality. Applying a box cox transformation means that you can run a broader \n",
      "number of tests. \n",
      " \n",
      "A Box-Cox transformation is a way to transform non-normal dependent variables into a normal shape. \n",
      "Normality is an important assumption for many statistical techniques, if your data isn’t normal, applying a \n",
      "Box-Cox means that you are able to run a broader number of tests. The Box-Cox transformation is named \n",
      "after statisticians George Box and Sir David Roxbee Cox who collaborated on a 1964 paper and \n",
      "developed the technique. \n",
      "Q67. How Regularly Must an Algorithm be Updated? \n",
      "You will want to update an algorithm when: \n",
      "• \n",
      "You want the model to evolve as data streams through infrastructure \n",
      "• \n",
      "The underlying data source is changing \n",
      "• \n",
      "There is a case of non-stationarity \n",
      "• \n",
      "The algorithm underperforms/ results lack accuracy \n",
      "Q68. If you are having 4GB RAM in your machine and you want to train your model on \n",
      "10GB data set. How would you go about this problem? Have you ever faced this kind of \n",
      "problem in your machine learning/data science experience so far? \n",
      "First of all, you have to ask which ML model you want to train. \n",
      "For Neural networks: Batch size with Numpy array will work. \n",
      "Steps: \n",
      "1. Load the whole data in the Numpy array. Numpy array has a property to create a mapping of the \n",
      "complete data set, it doesn’t load complete data set in memory. \n",
      "2. You can pass an index to Numpy array to get required data. \n",
      "3. Use this data to pass to the Neural network. \n",
      "\n",
      "\n",
      "228. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      "4. Have a small batch size. \n",
      "For SVM: Partial fit will work \n",
      "Steps: \n",
      "1. Divide one big data set in small size data sets. \n",
      "2. Use a partial fit method of SVM, it requires a subset of the complete data set. \n",
      "3. Repeat step 2 for other subsets. \n",
      "However, you could actually face such an issue in reality. So, you could check out the best laptop for \n",
      "Machine Learning to prevent that. Having said that, let’s move on to some questions on deep learning. \n",
      "  \n",
      "DEEP LEARNING INTERVIEW QUESTIONS \n",
      "Q69. What do you mean by Deep Learning?  \n",
      "Deep Learning is nothing but a paradigm of machine learning which has shown incredible promise in recent \n",
      "years. This is because of the fact that Deep Learning shows a great analogy with the functioning of the \n",
      "human brain. \n",
      "Q70. What is the difference between machine learning and deep learning? \n",
      "Machine learning is a field of computer science that gives computers the ability to learn without being \n",
      "explicitly programmed. Machine learning can be categorised in the following three categories. \n",
      "1. Supervised machine learning, \n",
      "2. Unsupervised machine learning, \n",
      "3. Reinforcement learning \n",
      "Deep Learning is a \n",
      "subfield of machine learning concerned with algorithms inspired by the structure and function of the brain \n",
      "called artificial neural networks. \n",
      "Q71. What, in your opinion, is the reason for the popularity of Deep Learning in recent \n",
      "times? \n",
      "\n",
      "\n",
      "229. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      "Now although Deep Learning has been around for many years, the major breakthroughs from these \n",
      "techniques came just in recent years. This is because of two main reasons: \n",
      "• \n",
      "The increase in the amount of data generated through various sources \n",
      "• \n",
      "The growth in hardware resources required to run these models \n",
      "GPUs are multiple times faster and they help us build bigger and deeper deep learning models in \n",
      "comparatively less time than we required previously. \n",
      "Q72. What is reinforcement learning? \n",
      "  \n",
      "Reinforcement \n",
      "Learning is \n",
      "learning what to do and how to map situations to actions. The end result is to maximise the numerical reward \n",
      "signal. The learner is not told which action to take but instead must discover which action will yield the \n",
      "maximum reward. Reinforcement learning is inspired by the learning of human beings, it is based on the \n",
      "reward/penalty mechanism. \n",
      "Q73. What are Artificial Neural Networks? \n",
      "Artificial Neural networks are a specific set of algorithms that have revolutionized machine learning. They \n",
      "are inspired by biological neural networks. Neural Networks can adapt to changing the input so the network \n",
      "generates the best possible result without needing to redesign the output criteria. \n",
      "Q74. Describe the structure of Artificial Neural Networks? \n",
      "Artificial Neural Networks works on the same principle as a biological Neural Network. It consists of inputs \n",
      "which get processed with weighted sums and Bias, with the help of Activation Functions. \n",
      "\n",
      "\n",
      "230. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      " \n",
      "Q75. How Are Weights Initialized in a Network? \n",
      "There are two methods here: we can either initialize the weights to zero or assign them randomly. \n",
      "Initializing all weights to 0: This makes your model similar to a linear model. All the neurons and every layer \n",
      "perform the same operation, giving the same output and making the deep net useless. \n",
      "Initializing all weights randomly: Here, the weights are assigned randomly by initializing them very close to \n",
      "0. It gives better accuracy to the model since every neuron performs different computations. This is the most \n",
      "commonly used method. \n",
      "Q76. What Is the Cost Function? \n",
      "Also referred to as “loss” or “error,” cost function is a measure to evaluate how good your model’s \n",
      "performance is. It’s used to compute the error of the output layer during backpropagation. We push that error \n",
      "backwards through the neural network and use that during the different training functions. \n",
      "Q77. What Are Hyperparameters? \n",
      "With neural networks, you’re usually working with hyperparameters once the data is formatted correctly. A \n",
      "hyperparameter is a parameter whose value is set before the learning process begins. It determines how a \n",
      "network is trained and the structure of the network (such as the number of hidden units, the learning rate, \n",
      "epochs, etc.). \n",
      "Q78. What Will Happen If the Learning Rate Is Set inaccurately (Too Low or Too High)? \n",
      "When your learning rate is too low, training of the model will progress very slowly as we are making minimal \n",
      "updates to the weights. It will take many updates before reaching the minimum point. \n",
      "If the learning rate is set too high, this causes undesirable divergent behaviour to the loss function due to \n",
      "drastic updates in weights. It may fail to converge (model can give a good output) or even diverge (data is \n",
      "too chaotic for the network to train). \n",
      "Q79. What Is the Difference Between Epoch, Batch, and Iteration in Deep Learning? \n",
      "• \n",
      "Epoch – Represents one iteration over the entire dataset (everything put into the training model). \n",
      "• \n",
      "Batch – Refers to when we cannot pass the entire dataset into the neural network at once, so we \n",
      "divide the dataset into several batches. \n",
      "\n",
      "\n",
      "231. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      "• \n",
      "Iteration – if we have 10,000 images as data and a batch size of 200. then an epoch should run 50 \n",
      "iterations (10,000 divided by 50). \n",
      "Q80. What Are the Different Layers on CNN? \n",
      "There are four layers in CNN: \n",
      "1. Convolutional Layer –  the layer that performs a convolutional operation, creating several smaller \n",
      "picture windows to go over the data. \n",
      "2. ReLU Layer – it brings non-linearity to the network and converts all the negative pixels to zero. The \n",
      "output is a rectified feature map. \n",
      "3. Pooling Layer – pooling is a down-sampling operation that reduces the dimensionality of the feature \n",
      "map. \n",
      "4. Fully Connected Layer – this layer recognizes and classifies the objects in the image. \n",
      "Q81. What Is \n",
      "Pooling on CNN, and How Does It Work? \n",
      "Pooling is used to reduce the spatial dimensions of a CNN. It performs down-sampling operations to reduce \n",
      "the dimensionality and creates a pooled feature map by sliding a filter matrix over the input matrix. \n",
      "Q82. What are Recurrent Neural Networks(RNNs)? \n",
      "RNNs are a type of artificial neural networks designed to recognise the pattern from the sequence of data \n",
      "such as Time series, stock market and government agencies etc. To understand recurrent nets, first, you \n",
      "have to understand the basics of feedforward nets. \n",
      "Both these networks RNN and feed-forward named after the way they channel information through a series \n",
      "of mathematical orations performed at the nodes of the network. One feeds information through \n",
      "straight(never touching the same node twice), while the other cycles it through a loop, and the latter are \n",
      "called recurrent. \n",
      "\n",
      "\n",
      "232. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      " \n",
      "Recurrent networks, on the other hand, take as their input, not just the current input example they see, but \n",
      "also the what they have perceived previously in time. \n",
      "The decision a recurrent neural network reached at time t-1 affects the decision that it will reach one moment \n",
      "later at time t. So recurrent networks have two sources of input, the present and the recent past, which \n",
      "combine to determine how they respond to new data, much as we do in life. \n",
      "The error they generate will return via backpropagation and be used to adjust their weights until error can’t \n",
      "go any lower. Remember, the purpose of recurrent nets is to accurately classify sequential input. We rely on \n",
      "the backpropagation of error and gradient descent to do so. \n",
      "Q83. How Does an LSTM Network Work? \n",
      "Long-Short-Term Memory (LSTM) is a special kind of recurrent neural network capable of learning long-\n",
      "term dependencies, remembering information for long periods as its default behaviour. There are three \n",
      "steps in an LSTM network: \n",
      "• \n",
      "Step 1: The network decides what to forget and what to remember. \n",
      "• \n",
      "Step 2: It selectively updates cell state values. \n",
      "• \n",
      "Step 3: The network decides what part of the current state makes it to the output. \n",
      "Q84. What Is a Multi-layer Perceptron(MLP)? \n",
      "\n",
      "\n",
      "233. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      "As in Neural Networks, MLPs have an input layer, a hidden layer, and an output layer. It has the \n",
      "same structure as a single layer perceptron with one or more hidden layers. A single layer perceptron can \n",
      "classify only linear separable classes with binary output (0,1), but MLP can classify nonlinear classes. \n",
      "Except for the input layer, each node in the other \n",
      "layers uses a nonlinear activation function. This means the input layers, the data coming in, and the \n",
      "activation function is based upon all nodes and weights being added together, producing the output. MLP \n",
      "uses a supervised learning method called “backpropagation.” In backpropagation, the neural network \n",
      "calculates the error with the help of cost function. It propagates this error backward from where it came \n",
      "(adjusts the weights to train the model more accurately). \n",
      "Q85. Explain Gradient Descent. \n",
      "To Understand Gradient Descent, Let’s understand what is a Gradient first. \n",
      "A gradient measures how much the output of a function changes if you change the inputs a little bit. It \n",
      "simply measures the change in all weights with regard to the change in error. You can also think of a \n",
      "gradient as the slope of a function. \n",
      "Gradient Descent can be thought of climbing down to the bottom of a valley, instead of climbing up a \n",
      "hill.  This is because it is a minimization algorithm that minimizes a given function (Activation Function). \n",
      "Q86. What is \n",
      "exploding gradients? \n",
      "\n",
      "\n",
      "234. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      "While training an RNN, if you see exponentially growing (very large) error gradients which accumulate \n",
      "and result in very large updates to neural network model weights during training, they’re known as \n",
      "exploding gradients. At an extreme, the values of weights can become so large as to overflow and result in \n",
      "NaN values. \n",
      "This has the effect of your model is unstable and unable to learn from your training data. \n",
      "Q87. What is vanishing gradients? \n",
      "While training an RNN, your slope can become either too small; this makes the training difficult. When the \n",
      "slope is too small, the problem is known as a Vanishing Gradient. It leads to long training times, poor \n",
      "performance, and low accuracy. \n",
      "Q89. What is Back Propagation and Explain it’s Working. \n",
      "Backpropagation is a training algorithm used for multilayer neural network. In this method, we move the \n",
      "error from an end of the network to all weights inside the network and thus allowing efficient computation of \n",
      "the gradient. \n",
      "It has the following steps: \n",
      " \n",
      " \n",
      "Data Scientist Masters Program \n",
      "Weekday / Weekend BatchesSee Batch Details \n",
      " \n",
      "• \n",
      "Forward Propagation of Training Data \n",
      "• \n",
      "Derivatives are computed using output and target \n",
      "• \n",
      "Back Propagate for computing derivative of error wrt output activation \n",
      "• \n",
      "Using previously calculated derivatives for output \n",
      "• \n",
      "Update the Weights \n",
      "Q90. What are the variants of Back Propagation? \n",
      "• \n",
      "Stochastic Gradient Descent: We use only a single training example for calculation of gradient and \n",
      "update parameters. \n",
      "• \n",
      "Batch Gradient Descent: We calculate the gradient for the whole dataset and perform the update at \n",
      "each iteration. \n",
      "• \n",
      "Mini-batch Gradient Descent: It’s one of the most popular optimization algorithms. It’s a variant of \n",
      "Stochastic Gradient Descent and here instead of single training example, mini-batch of samples is \n",
      "used. \n",
      "Q91. What are the different Deep Learning Frameworks? \n",
      "• \n",
      "Pytorch \n",
      "• \n",
      "TensorFlow \n",
      "• \n",
      "Microsoft Cognitive Toolkit \n",
      "\n",
      "\n",
      "235. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      "• \n",
      "Keras \n",
      "• \n",
      "Caffe \n",
      "• \n",
      "Chainer \n",
      "Q92. What is the role of the Activation Function? \n",
      "The Activation function is used to introduce non-linearity into the neural network helping it to learn more \n",
      "complex function. Without which the neural network would be only able to learn linear function which is a \n",
      "linear combination of its input data. An activation function is a function in an artificial neuron that delivers an \n",
      "output based on inputs. \n",
      "Q93. Name a few Machine Learning libraries for various purposes. \n",
      "Purpose \n",
      "Libraries \n",
      "Scientific Computation \n",
      "Numpy \n",
      "Tabular Data \n",
      "Pandas \n",
      "Data Modelling & Preprocessing \n",
      "Scikit Learn \n",
      "Time-Series Analysis \n",
      "Statsmodels \n",
      "Text processing \n",
      "Regular Expressions, NLTK \n",
      "Deep Learning \n",
      "Tensorflow, Pytorch \n",
      "Q94. What is an Auto-Encoder?  \n",
      "Auto-encoders are simple learning networks that aim to transform inputs into outputs with the minimum \n",
      "possible error. This means that we want the output to be as close to input as possible. We add a couple of \n",
      "layers between the input and the output, and the sizes of these layers are smaller than the input layer. The \n",
      "auto-encoder receives unlabelled input which is then encoded to reconstruct the input. \n",
      " \n",
      "Q95. What is a Boltzmann Machine?  \n",
      "Boltzmann machines have a simple learning algorithm that allows them to discover interesting features that \n",
      "represent complex regularities in the training data. The Boltzmann machine is basically used to optimise the \n",
      "weights and the quantity for the given problem. The learning algorithm is very slow in networks with many \n",
      "layers of feature detectors. “Restricted Boltzmann Machines” algorithm has a single layer of feature \n",
      "detectors which makes it faster than the rest. \n",
      "\n",
      "\n",
      "236. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      "Q96. What Is Dropout and Batch Normalization? \n",
      "Dropout is a technique of dropping out hidden and visible units of a network randomly to prevent overfitting \n",
      "of data (typically dropping 20 per cent of the nodes). It doubles the number of iterations needed to converge \n",
      "the network. \n",
      " \n",
      "Batch normalization is the technique to improve the performance and stability of neural networks by \n",
      "normalizing the inputs in every layer so that they have mean output activation of zero and standard deviation \n",
      "of one. \n",
      "Q97. What Is the Difference Between Batch Gradient Descent and Stochastic Gradient \n",
      "Descent? \n",
      "Batch Gradient Descent \n",
      "Stochastic Gradient Descent \n",
      "The batch gradient computes the gradient using the \n",
      "entire dataset. \n",
      "The stochastic gradient computes the gradient using \n",
      "a single sample. \n",
      "It takes time to converge because the volume of data \n",
      "is huge, and weights update slowly. \n",
      "It converges much faster than the batch gradient \n",
      "because it updates weight more frequently. \n",
      "Q98. Why Is Tensorflow the Most Preferred Library in Deep Learning? \n",
      "Tensorflow provides both C++ and Python APIs, making it easier to work on and has a faster compilation \n",
      "time compared to other Deep Learning libraries like Keras and Torch. Tensorflow supports both CPU and \n",
      "GPU computing devices. \n",
      "Q99. What Do You Mean by Tensor in Tensorflow? \n",
      "A tensor is a mathematical object represented as arrays of higher dimensions. These arrays of data with \n",
      "different dimensions and ranks fed as input to the neural network are called “Tensors.” \n",
      "\n",
      "\n",
      "237. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      " \n",
      "Q100. What is the Computational Graph? \n",
      "Everything in a tensorflow is based on creating a computational graph. It has a network of nodes where each \n",
      "node operates, Nodes represent mathematical operations, and edges represent tensors. Since data flows \n",
      "in the form of a graph, it is also called a “DataFlow Graph.” \n",
      " \n",
      "Q101. What are the differences between supervised and unsupervised learning? \n",
      "Supervised Learning \n",
      "Unsupervised Learning \n",
      "• \n",
      "Uses known and labeled data as input \n",
      "• \n",
      "Supervised learning has a feedback \n",
      "mechanism  \n",
      "• \n",
      "Most commonly used supervised \n",
      "learning algorithms are decision trees, \n",
      "logistic regression, and support vector \n",
      "machine \n",
      "• \n",
      "Uses unlabeled data as input \n",
      "• \n",
      "Unsupervised learning has no feedback \n",
      "mechanism  \n",
      "• \n",
      "Most commonly used unsupervised \n",
      "learning algorithms are k-means \n",
      "clustering, hierarchical clustering, and \n",
      "apriori algorithm \n",
      "102. How is logistic regression done? \n",
      "Logistic regression measures the relationship between the dependent variable (our label of what we want \n",
      "to predict) and one or more independent variables (our features) by estimating probability using its \n",
      "underlying logistic function (sigmoid). \n",
      "The image shown below depicts how logistic regression works: \n",
      "\n",
      "\n",
      "238. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      " \n",
      "The formula and graph for the sigmoid function are as shown: \n",
      " \n",
      "103. Explain the steps in making a decision tree. \n",
      "1. Take the entire data set as input \n",
      "2. Calculate entropy of the target variable, as well as the predictor attributes \n",
      "3. Calculate your information gain of all attributes (we gain information on sorting different objects from \n",
      "each other) \n",
      "4. Choose the attribute with the highest information gain as the root node  \n",
      "5. Repeat the same procedure on every branch until the decision node of each branch is finalized \n",
      "For example, let's say you want to build a decision tree to decide whether you should accept or decline a \n",
      "job offer. The decision tree for this case is as shown: \n",
      "\n",
      "\n",
      "239. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      " \n",
      "It is clear from the decision tree that an offer is accepted if: \n",
      "• \n",
      "Salary is greater than $50,000 \n",
      "• \n",
      "The commute is less than an hour  \n",
      "• \n",
      "Incentives are offered  \n",
      "104. How do you build a random forest model? \n",
      "A random forest is built up of a number of decision trees. If you split the data into different packages and \n",
      "make a decision tree in each of the different groups of data, the random forest brings all those trees \n",
      "together. \n",
      "Steps to build a random forest model: \n",
      "1. Randomly select 'k' features from a total of'm' features where k << m \n",
      "2. Among the 'k' features, calculate the node D using the best split point \n",
      "3. Split the node into daughter nodes using the best split \n",
      "4. Repeat steps two and three until leaf nodes are finalized  \n",
      "5. Build forest by repeating steps one to four for 'n' times to create 'n' number of trees  \n",
      "105. How can you avoid the overfitting your model? \n",
      "Overfitting refers to a model that is only set for a very small amount of data and ignores the bigger picture. \n",
      "There are three main methods to avoid overfitting: \n",
      "1. Keep the model simple—take fewer variables into account, thereby removing some of the noise in the \n",
      "training data \n",
      "2. Use cross-validation techniques, such as k folds cross-validation  \n",
      "3. Use regularization techniques, such as LASSO, that penalize certain model parameters if they're likely \n",
      "to cause overfitting \n",
      "106. Differentiate between univariate, bivariate, and multivariate analysis. \n",
      "\n",
      "\n",
      "240. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      "Univariate  \n",
      "Univariate data contains only one variable. The purpose of the univariate analysis is to describe the data \n",
      "and find patterns that exist within it.  \n",
      "Example: height of students  \n",
      "Height (in cm) \n",
      "164 \n",
      "167.3 \n",
      "170 \n",
      "174.2 \n",
      "178 \n",
      "180 \n",
      "The patterns can be studied by drawing conclusions using mean, median, mode, dispersion or range, \n",
      "minimum, maximum, etc. \n",
      "Bivariate  \n",
      "Bivariate data involves two different variables. The analysis of this type of data deals with causes and \n",
      "relationships and the analysis is done to determine the relationship between the two variables. \n",
      "Example: temperature and ice cream sales in the summer season \n",
      "\n",
      "\n",
      "241. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      "Temperature (in Celcius) \n",
      "Sales \n",
      "20 \n",
      "2,000 \n",
      "25 \n",
      "2,100 \n",
      "26 \n",
      "2,300 \n",
      "28 \n",
      "2,400 \n",
      "30 \n",
      "2,600 \n",
      "36 \n",
      "3,100 \n",
      "Here, the relationship is visible from the table that temperature and sales are directly proportional to each \n",
      "other. The hotter the temperature, the better the sales. \n",
      "Multivariate  \n",
      "Multivariate data involves three or more variables, it is categorized under multivariate. It is similar to a \n",
      "bivariate, but contains more than one dependent variable. \n",
      "Example: data for house price prediction  \n",
      "No. of rooms \n",
      "Floors \n",
      "Area (sq ft) \n",
      "Price \n",
      "2 \n",
      "0 \n",
      "900 \n",
      "$4000,00 \n",
      "\n",
      "\n",
      "242. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      "3 \n",
      "2 \n",
      "1,100 \n",
      "$600,000 \n",
      "3.5 \n",
      "5 \n",
      "1,500 \n",
      "$900,000 \n",
      "4 \n",
      "3 \n",
      "2,100 \n",
      "$1,200,000 \n",
      "The patterns can be studied by drawing conclusions using mean, median, and mode, dispersion or range, \n",
      "minimum, maximum, etc. You can start describing the data and using it to guess what the price of the \n",
      "house will be. \n",
      "107. What are the feature selection methods used to select the right variables? \n",
      "There are two main methods for feature selection: \n",
      "Filter Methods \n",
      "This involves:  \n",
      "• \n",
      "Linear discrimination analysis \n",
      "• \n",
      "ANOVA \n",
      "• \n",
      "Chi-Square \n",
      "The best analogy for selecting features is \"bad data in, bad answer out.\" When we're limiting or selecting \n",
      "the features, it's all about cleaning up the data coming in.  \n",
      "Wrapper Methods  \n",
      "This involves:  \n",
      "• \n",
      "Forward Selection: We test one feature at a time and keep adding them until we get a good fit \n",
      "• \n",
      "Backward Selection: We test all the features and start removing them to see what works better \n",
      "• \n",
      "Recursive Feature Elimination: Recursively looks through all the different features and how they pair \n",
      "together \n",
      "Wrapper methods are very labor-intensive, and high-end computers are needed if a lot of data analysis is \n",
      "performed with the wrapper method.  \n",
      "108. In your choice of language, write a program that prints the numbers ranging from \n",
      "one to 50. \n",
      "\n",
      "\n",
      "243. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      "But for multiples of three, print \"Fizz\" instead of the number and for the multiples of five, print \"Buzz.\" For \n",
      "numbers which are multiples of both three and five, print \"FizzBuzz\"  \n",
      "The code is shown below: \n",
      " \n",
      "Note that the range mentioned is 51, which means zero to 50. However, the range asked in the question is \n",
      "one to 50. Therefore, in the above code, you can include the range as (1,51). \n",
      "The output of the above code is as shown: \n",
      " \n",
      "109. You are given a data set consisting of variables with more than 30 percent missing \n",
      "values. How will you deal with them? \n",
      "The following are ways to handle missing data values: \n",
      "If the data set is large, we can just simply remove the rows with missing data values. It is the quickest way; \n",
      "we use the rest of the data to predict the values. \n",
      "For smaller data sets, we can substitute missing values with the mean or average of the rest of the data \n",
      "using pandas data frame in python. There are different ways to do so, such as df.mean(), df.fillna(mean). \n",
      "\n",
      "\n",
      "244. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      "110. For the given points, how will you calculate the Euclidean distance in Python? \n",
      "plot1 = [1,3] \n",
      "plot2 = [2,5] \n",
      "The Euclidean distance can be calculated as follows: \n",
      "euclidean_distance = sqrt( (plot1[0]-plot2[0])**2 + (plot1[1]-plot2[1])**2 ) \n",
      "111. What are dimensionality reduction and its benefits? \n",
      "Dimensionality reduction refers to the process of converting a data set with vast dimensions into data \n",
      "with fewer dimensions (fields) to convey similar information concisely.  \n",
      "This reduction helps in compressing data and reducing storage space. It also reduces computation time as \n",
      "fewer dimensions lead to less computing. It removes redundant features; for example, there's no point in \n",
      "storing a value in two different units (meters and inches).  \n",
      "112. How will you calculate eigenvalues and eigenvectors of the following 3x3 matrix? \n",
      "-2 \n",
      "-4 \n",
      "2 \n",
      "-2 \n",
      "1 \n",
      "2 \n",
      "4 \n",
      "2 \n",
      "5 \n",
      "The characteristic equation is as shown: \n",
      "Expanding determinant: \n",
      "(-2 – λ) [(1-λ) (5-λ)-2x2] + 4[(-2) x (5-λ) -4x2] + 2[(-2) x 2-4(1-λ)] =0 \n",
      "- λ3 + 4λ2 + 27λ – 90 = 0, \n",
      "λ3 - 4 λ2 -27 λ + 90 = 0 \n",
      "Here we have an algebraic equation built from the eigenvectors. \n",
      "By hit and trial: \n",
      "\n",
      "\n",
      "245. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      "33 – 4 x 32 - 27 x 3 +90 = 0 \n",
      "Hence, (λ - 3) is a factor: \n",
      "λ3 - 4 λ2 - 27 λ +90 = (λ – 3) (λ2 – λ – 30) \n",
      "Eigenvalues are 3,-5,6: \n",
      "(λ – 3) (λ2 – λ – 30) = (λ – 3) (λ+5) (λ-6), \n",
      "Calculate eigenvector for λ = 3 \n",
      "For X = 1, \n",
      "-5 - 4Y + 2Z =0, \n",
      "-2 - 2Y + 2Z =0 \n",
      "Subtracting the two equations:  \n",
      "3 + 2Y = 0, \n",
      "Subtracting back into second equation: \n",
      "Y = -(3/2)  \n",
      "Z = -(1/2) \n",
      "Similarly, we can calculate the eigenvectors for -5 and 6. \n",
      "113. How should you maintain a deployed model? \n",
      "The steps to maintain a deployed model are: \n",
      "Monitor  \n",
      "Constant monitoring of all models is needed to determine their performance accuracy. When you change \n",
      "something, you want to figure out how your changes are going to affect things. This needs to be monitored \n",
      "to ensure it's doing what it's supposed to do. \n",
      "Evaluate   \n",
      "Evaluation metrics of the current model are calculated to determine if a new algorithm is needed.  \n",
      "Compare  \n",
      "\n",
      "\n",
      "246. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      "The new models are compared to each other to determine which model performs the best.  \n",
      "Rebuild  \n",
      "The best performing model is re-built on the current state of data. \n",
      "114. What are recommender systems? \n",
      "A recommender system predicts what a user would rate a specific product based on their preferences. It \n",
      "can be split into two different areas: \n",
      "Collaborative filtering  \n",
      "As an example, Last.fm recommends tracks that other users with similar interests play often. This is also \n",
      "commonly seen on Amazon after making a purchase; customers may notice the following message \n",
      "accompanied by product recommendations: \"Users who bought this also bought…\" \n",
      "Content-based filtering \n",
      "As an example: Pandora uses the properties of a song to recommend music with similar properties. Here, \n",
      "we look at content, instead of looking at who else is listening to music. \n",
      "115. How do you find RMSE and MSE in a linear regression model? \n",
      "RMSE and MSE are two of the most common measures of accuracy for a linear regression model.  \n",
      "RMSE indicates the Root Mean Square Error.  \n",
      " \n",
      "MSE indicates the Mean Square Error. \n",
      " \n",
      "116. How can you select k for k-means?  \n",
      "We use the elbow method to select k for k-means clustering. The idea of the elbow method is to run k-\n",
      "means clustering on the data set where 'k' is the number of clusters. \n",
      "\n",
      "\n",
      "247. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      "Within the sum of squares (WSS), it is defined as the sum of the squared distance between each member \n",
      "of the cluster and its centroid.  \n",
      "117. What is the significance of p-value? \n",
      "p-value typically ≤ 0.05 \n",
      "This indicates strong evidence against the null hypothesis; so you reject the null hypothesis. \n",
      "p-value typically > 0.05 \n",
      "This indicates weak evidence against the null hypothesis, so you accept the null hypothesis.  \n",
      "p-value at cutoff 0.05  \n",
      "This is considered to be marginal, meaning it could go either way. \n",
      "118. How can outlier values be treated? \n",
      "You can drop outliers only if it is a garbage value.  \n",
      "Example: height of an adult = abc ft. This cannot be true, as the height cannot be a string value. In this \n",
      "case, outliers can be removed. \n",
      "If the outliers have extreme values, they can be removed. For example, if all the data points are clustered \n",
      "between zero to 10, but one point lies at 100, then we can remove this point. \n",
      "If you cannot drop outliers, you can try the following: \n",
      "• \n",
      "Try a different model. Data detected as outliers by linear models can be fit by nonlinear models. \n",
      "Therefore, be sure you are choosing the correct model. \n",
      "• \n",
      "Try normalizing the data. This way, the extreme data points are pulled to a similar range. \n",
      "• \n",
      "You can use algorithms that are less affected by outliers; an example would be random forests.  \n",
      "119. How can a time-series data be declared as stationery? \n",
      "It is stationary when the variance and mean of the series are constant with time.  \n",
      "Here is a visual example:  \n",
      "\n",
      "\n",
      "248. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      " \n",
      "In the first graph, the variance is constant with time. Here, X is the time factor and Y is the variable. The \n",
      "value of Y goes through the same points all the time; in other words, it is stationary. \n",
      "In the second graph, the waves get bigger, which means it is non-stationary and the variance is changing \n",
      "with time. \n",
      "120. How can you calculate accuracy using a confusion matrix? \n",
      "Consider this confusion matrix: \n",
      " \n",
      "You can see the values for total data, actual values, and predicted values. \n",
      "The formula for accuracy is: \n",
      "Accuracy = (True Positive + True Negative) / Total Observations \n",
      "  = (262 + 347) / 650 \n",
      "  = 609 / 650 \n",
      "  = 0.93 \n",
      "As a result, we get an accuracy of 93 percent. \n",
      "121. Write the equation and calculate the precision and recall rate. \n",
      "\n",
      "\n",
      "249. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      "Consider the same confusion matrix used in the previous question. \n",
      " \n",
      "Precision = (True positive) / (True Positive + False Positive) \n",
      "    = 262 / 277 \n",
      "    = 0.94 \n",
      "Recall Rate = (True Positive) / (Total Positive + False Negative) \n",
      "        = 262 / 288 \n",
      "        = 0.90 \n",
      "122. 'People who bought this also bought…' recommendations seen on Amazon are a \n",
      "result of which algorithm? \n",
      "The recommendation engine is accomplished with collaborative filtering. Collaborative filtering explains the \n",
      "behavior of other users and their purchase history in terms of ratings, selection, etc.  \n",
      "The engine makes predictions on what might interest a person based on the preferences of other users. In \n",
      "this algorithm, item features are unknown. \n",
      "  \n",
      "For example, a sales page shows that a certain number of people buy a new phone and also buy \n",
      "tempered glass at the same time. Next time, when a person buys a phone, he or she may see a \n",
      "recommendation to buy tempered glass as well. \n",
      "123. What is a Generative Adversarial Network? \n",
      "\n",
      "\n",
      "250. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      "Suppose there is a wine shop purchasing wine from dealers, which they resell later. But some dealers sell \n",
      "fake wine. In this case, the shop owner should be able to distinguish between fake and authentic wine. \n",
      "The forger will try different techniques to sell fake wine and make sure specific techniques go past the shop \n",
      "owner’s check. The shop owner would probably get some feedback from wine experts that some of the wine \n",
      "is not original. The owner would have to improve how he determines whether a wine is fake or authentic. \n",
      "The forger’s goal is to create wines that are indistinguishable from the authentic ones while the shop owner \n",
      "intends to tell if the wine is real or not accurately \n",
      "Let us understand this example with the help of an image. \n",
      "There is a noise vector coming into the \n",
      "forger who is generating fake wine. \n",
      "Here the forger acts as a Generator. \n",
      "The shop owner acts as a Discriminator. \n",
      "The Discriminator gets two inputs; one is the fake wine, while the other is the real authentic wine. The shop \n",
      "owner has to figure out whether it is real or fake. \n",
      "So, there are two primary components of Generative Adversarial Network (GAN) named: \n",
      "1. Generator \n",
      "2. Discriminator \n",
      "The generator is a CNN that keeps keys producing images and is closer in appearance to the real images \n",
      "while the discriminator tries to determine the difference between real and fake images The ultimate aim is to \n",
      "make the discriminator learn to identify real and fake images. \n",
      "Apart from the very technical questions, your interviewer could even hit you up with a few simple ones to \n",
      "check your overall confidence, in the likes of the following. \n",
      "124. You are given a dataset on cancer detection. You have built a classification model \n",
      "and achieved an accuracy of 96 percent. Why shouldn't you be happy with your model \n",
      "performance? What can you do about it? \n",
      "Cancer detection results in imbalanced data. In an imbalanced dataset, accuracy should not be based as a \n",
      "measure of performance. It is important to focus on the remaining four percent, which represents the \n",
      "patients who were wrongly diagnosed. Early diagnosis is crucial when it comes to cancer detection, and \n",
      "can greatly improve a patient's prognosis. \n",
      "\n",
      "\n",
      "251. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      "Hence, to evaluate model performance, we should use Sensitivity (True Positive Rate), Specificity (True \n",
      "Negative Rate), F measure to determine the class wise performance of the classifier. \n",
      "125. Which of the following machine learning algorithms can be used for inputting \n",
      "missing values of both categorical and continuous variables? \n",
      "• \n",
      "K-means clustering \n",
      "• \n",
      "Linear regression  \n",
      "• \n",
      "K-NN (k-nearest neighbor) \n",
      "• \n",
      "Decision trees  \n",
      "The K nearest neighbor algorithm can be used because it can compute the nearest neighbor and if it \n",
      "doesn't have a value, it just computes the nearest neighbor based on all the other features.  \n",
      "When you're dealing with K-means clustering or linear regression, you need to do that in your pre-\n",
      "processing, otherwise, they'll crash. Decision trees also have the same problem, although there is some \n",
      "variance. \n",
      "126. Below are the eight actual values of the target variable in the train file. What is the \n",
      "entropy of the target variable? \n",
      "[0, 0, 0, 1, 1, 1, 1, 1]  \n",
      "Choose the correct answer. \n",
      "1. -(5/8 log(5/8) + 3/8 log(3/8)) \n",
      "2. 5/8 log(5/8) + 3/8 log(3/8) \n",
      "3. 3/8 log(5/8) + 5/8 log(3/8) \n",
      "4. 5/8 log(3/8) – 3/8 log(5/8) \n",
      "The target variable, in this case, is 1.  \n",
      "The formula for calculating the entropy is: \n",
      "Putting p=5 and n=8, we get  \n",
      "Entropy = A = -(5/8 log(5/8) + 3/8 log(3/8)) \n",
      "127. We want to predict the probability of death from heart disease based on three risk \n",
      "factors: age, gender, and blood cholesterol level. What is the most appropriate \n",
      "algorithm for this case? \n",
      "Choose the correct option: \n",
      "\n",
      "\n",
      "252. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      "1. Logistic Regression  \n",
      "2. Linear Regression \n",
      "3. K-means clustering  \n",
      "4. Apriori algorithm \n",
      "The most appropriate algorithm for this case is A, logistic regression.  \n",
      "128. After studying the behavior of a population, you have identified four specific \n",
      "individual types that are valuable to your study. You would like to find all users who are \n",
      "most similar to each individual type. Which algorithm is most appropriate for this \n",
      "study? \n",
      "Choose the correct option: \n",
      "1. K-means clustering \n",
      "2. Linear regression \n",
      "3. Association rules \n",
      "4. Decision trees \n",
      "As we are looking for grouping people together specifically by four different similarities, it indicates the \n",
      "value of k. Therefore, K-means clustering (answer A) is the most appropriate algorithm for this study. \n",
      "129. You have run the association rules algorithm on your dataset, and the two rules \n",
      "{banana, apple} => {grape} and {apple, orange} => {grape} have been found to be \n",
      "relevant. What else must be true? \n",
      "Choose the right answer: \n",
      "1. {banana, apple, grape, orange} must be a frequent itemset \n",
      "2. {banana, apple} => {orange} must be a relevant rule \n",
      "3. {grape} => {banana, apple} must be a relevant rule \n",
      "4. {grape, apple} must be a frequent itemset \n",
      "The answer is A:  {grape, apple} must be a frequent itemset \n",
      "130. Your organization has a website where visitors randomly receive one of two \n",
      "coupons. It is also possible that visitors to the website will not receive a coupon. You \n",
      "have been asked to determine if offering a coupon to website visitors has any impact on \n",
      "their purchase decisions. Which analysis method should you use? \n",
      "1. One-way ANOVA  \n",
      "2. K-means clustering \n",
      "\n",
      "\n",
      "253. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      "3. Association rules  \n",
      "4. Student's t-test  \n",
      "The answer is A: One-way ANOVA  \n",
      "Additional Data Science Interview Questions on Basic Concepts \n",
      "131. What are the feature vectors? \n",
      "A feature vector is an n-dimensional vector of numerical features that represent an object. In machine \n",
      "learning, feature vectors are used to represent numeric or symbolic characteristics (called features) of an \n",
      "object in a mathematical way that's easy to analyze.  \n",
      "132. What are the steps in making a decision tree? \n",
      "1. Take the entire data set as input. \n",
      "2. Look for a split that maximizes the separation of the classes. A split is any test that divides the data into \n",
      "two sets. \n",
      "3. Apply the split to the input data (divide step). \n",
      "4. Re-apply steps one and two to the divided data. \n",
      "5. Stop when you meet any stopping criteria. \n",
      "6. This step is called pruning. Clean up the tree if you went too far doing splits. \n",
      "133. What is root cause analysis? \n",
      "Root cause analysis was initially developed to analyze industrial accidents but is now widely used in other \n",
      "areas. It is a problem-solving technique used for isolating the root causes of faults or problems. A factor is \n",
      "called a root cause if its deduction from the problem-fault-sequence averts the final undesirable event from \n",
      "recurring. \n",
      "134. What is logistic regression? \n",
      "Logistic regression is also known as the logit model. It is a technique used to forecast the binary outcome \n",
      "from a linear combination of predictor variables. \n",
      "135. What are recommender systems? \n",
      "Recommender systems are a subclass of information filtering systems that are meant to predict the \n",
      "preferences or ratings that a user would give to a product. \n",
      "136. Explain cross-validation. \n",
      "\n",
      "\n",
      "254. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      "Cross-validation is a model validation technique for evaluating how the outcomes of a statistical analysis \n",
      "will generalize to an independent data set. It is mainly used in backgrounds where the objective is to \n",
      "forecast and one wants to estimate how accurately a model will accomplish in practice.  \n",
      "The goal of cross-validation is to term a data set to test the model in the training phase (i.e. validation data \n",
      "set) to limit problems like overfitting and gain insight into how the model will generalize to an independent \n",
      "data set. \n",
      "137. What is collaborative filtering? \n",
      "Most recommender systems use this filtering process to find patterns and information by collaborating \n",
      "perspectives, numerous data sources, and several agents. \n",
      "138. Do gradient descent methods always converge to similar points? \n",
      "They do not, because in some cases, they reach a local minima or a local optima point. You would not \n",
      "reach the global optima point. This is governed by the data and the starting conditions. \n",
      "139. What is the goal of A/B Testing? \n",
      "This is statistical hypothesis testing for randomized experiments with two variables, A and B. The objective \n",
      "of A/B testing is to detect any changes to a web page to maximize or increase the outcome of a strategy. \n",
      "140. What are the drawbacks of the linear model? \n",
      "• \n",
      "The assumption of linearity of the errors \n",
      "• \n",
      "It can't be used for count outcomes or binary outcomes \n",
      "• \n",
      "There are overfitting problems that it can't solve \n",
      "141. What is the law of large numbers? \n",
      "It is a theorem that describes the result of performing the same experiment very frequently. This theorem \n",
      "forms the basis of frequency-style thinking. It states that the sample mean, sample variance and sample \n",
      "standard deviation converge to what they are trying to estimate. \n",
      "142.  What are the confounding variables? \n",
      "These are extraneous variables in a statistical model that correlates directly or inversely with both the \n",
      "dependent and the independent variable. The estimate fails to account for the confounding factor. \n",
      "143. What is star schema? \n",
      "It is a traditional database schema with a central table. Satellite tables map IDs to physical names or \n",
      "descriptions and can be connected to the central fact table using the ID fields; these tables are known as \n",
      "\n",
      "\n",
      "255. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      "lookup tables and are principally useful in real-time applications, as they save a lot of memory. Sometimes, \n",
      "star schemas involve several layers of summarization to recover information faster. \n",
      "144. How regularly must an algorithm be updated? \n",
      "You will want to update an algorithm when: \n",
      "• \n",
      "You want the model to evolve as data streams through infrastructure \n",
      "• \n",
      "The underlying data source is changing \n",
      "• \n",
      "There is a case of non-stationarity \n",
      "145.  What are eigenvalue and eigenvector? \n",
      "Eigenvalues are the directions along which a particular linear transformation acts by flipping, compressing, \n",
      "or stretching. \n",
      "Eigenvectors are for understanding linear transformations. In data analysis, we usually calculate the \n",
      "eigenvectors for a correlation or covariance matrix.  \n",
      "146. Why is resampling done? \n",
      "Resampling is done in any of these cases: \n",
      "• \n",
      "Estimating the accuracy of sample statistics by using subsets of accessible data, or drawing randomly \n",
      "with replacement from a set of data points \n",
      "• \n",
      "Substituting labels on data points when performing significance tests \n",
      "• \n",
      "Validating models by using random subsets (bootstrapping, cross-validation) \n",
      "147. What is selection bias? \n",
      "Selection bias, in general, is a problematic situation in which error is introduced due to a non-random \n",
      "population sample. \n",
      "148. What are the types of biases that can occur during sampling? \n",
      "1. Selection bias \n",
      "2. Undercoverage bias \n",
      "3. Survivorship bias \n",
      "149. What is survivorship bias? \n",
      "\n",
      "\n",
      "256. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      "Survivorship bias is the logical error of focusing on aspects that support surviving a process and casually \n",
      "overlooking those that did not because of their lack of prominence. This can lead to wrong conclusions in \n",
      "numerous ways. \n",
      "150. How do you work towards a random forest? \n",
      "The underlying principle of this technique is that several weak learners combine to provide a strong \n",
      "learner. The steps involved are: \n",
      "1. Build several decision trees on bootstrapped training samples of data \n",
      "2. On each tree, each time a split is considered, a random sample of mm predictors is chosen as split \n",
      "candidates out of all pp predictors \n",
      "3. Rule of thumb: At each split m=p√m=p \n",
      "4. Predictions: At the majority rule \n",
      " \n",
      "151. What are the important skills to have in Python with regard to data analysis? \n",
      "The following are some of the important skills to possess which will come handy when performing data \n",
      "analysis using Python. \n",
      "• \n",
      "Good understanding of the built-in data types especially lists, dictionaries, tuples, and sets. \n",
      "• \n",
      "Mastery of N-dimensional NumPy Arrays. \n",
      "• \n",
      "Mastery of Pandas dataframes. \n",
      "• \n",
      "Ability to perform element-wise vector and matrix operations on NumPy arrays. \n",
      "• \n",
      "Knowing that you should use the Anaconda distribution and the conda package manager. \n",
      "• \n",
      "Familiarity with Scikit-learn. **Scikit-Learn Cheat Sheet** \n",
      "• \n",
      "Ability to write efficient list comprehensions instead of traditional for loops. \n",
      "• \n",
      "Ability to write small, clean functions (important for any developer), preferably pure functions that don’t \n",
      "alter objects. \n",
      "• \n",
      "Knowing how to profile the performance of a Python script and how to optimize bottlenecks. \n",
      " \n",
      " \n",
      "Credit: kdnuggets, Simplilearn, Edureka, Guru99, Hackernoon, Datacamp, Nitin Panwar, Michael \n",
      "Rundell \n",
      "\n",
      "\n",
      "257. Top Deep Learning Interview Questions You Must Know\n",
      "1.3K Views\n",
      "Kurt\n",
      "Last updated on May 22,2019\n",
      "Deep Learning is one of the Hottest topics of 2018-19 and for a good reason. There have been so many advancements in the Industry wherein the time has come when machines or Computer\n",
      "Programs are actually replacing Humans. Arti+cial Intelligence is going to create 2.3 million Jobs by 2020 and to crack those job interview I have come up with a set of Deep Learning Interview\n",
      "Questions. I have divided this article into two sections:\n",
      "Basic Deep Learning Interview Questions\n",
      "Advance Deep Learning Interview Questions\n",
      "Basics Deep Learning Interview Questions\n",
      "Q1. Differentiate between AI, Machine Learning and Deep Learning.\n",
      "Artificial Intelligence is a technique which enables machines to mimic human behavior.\n",
      "Machine Learning is a subset of AI technique which uses statistical methods to enable machines to improve with experience.\n",
      "Deep learning is a subset of ML which make the computation of multi-layer neural network feasible. It uses Neural networks to simulate human-like decision making.\n",
      "Q2. Do you think Deep Learning is Better than Machine Learning? If so, why?\n",
      "Though traditional ML algorithms solve a lot of our cases, they are not useful while working with high dimensional data, that is where we have a large number of inputs and outputs. For example, in\n",
      "the case of handwriting recognition, we have a large amount of input where we will have a different type of inputs associated with different type of handwriting.\n",
      "The second major challenge is to tell the computer what are the features it should look for that will play an important role in predicting the outcome as well as to achieve better accuracy while\n",
      "doing so.\n",
      "Q3. What is Perceptron? And How does it Work?\n",
      "If we focus on the structure of a biological neuron, it has dendrites which are used to receive inputs. These inputs are summed in the cell body and using the Axon it is passed on to the next\n",
      "biological neuron as shown below.\n",
      "Dendrite: Receives signals from other neurons\n",
      "Cell Body: Sums all the inputs\n",
      "Axon: It is used to transmit signals to the other cells\n",
      "Similarly, a perceptron receives multiple inputs, applies various transformations and functions and provides an output. A Perceptron is a linear model used for binary classi+cation. It models a\n",
      "neuron which has a set of inputs, each of which is given a specific weight. The neuron computes some function on these weighted inputs and gives the output.\n",
      "\n",
      "Subscribe\n",
      "\n",
      "\n",
      "\n",
      "258. Q4. What is the role of weights and bias?\n",
      "For a perceptron, there can be one more input called bias. While the weights determine the slope of the classifier line, bias allows us to shift the line towards left or right. Normally bias is treated\n",
      "as another weighted input with the input value x\n",
      "Q5. What are the activation functions?\n",
      "Activation function translates the inputs into outputs. Activation function decides whether a neuron should be activated or not by calculating the weighted sum and further adding bias with it. The\n",
      "purpose of the activation function is to introduce non-linearity into the output of a neuron.\n",
      "There can be many Activation functions like:\n",
      "Linear or Identity\n",
      "Unit or Binary Step\n",
      "Sigmoid or Logistic\n",
      "Tanh\n",
      "ReLU\n",
      "Softmax\n",
      "Q6. Explain Learning of a Perceptron.\n",
      "1. Initializing the weights and threshold.\n",
      "2. Provide the input and calculate the output.\n",
      "3. Update the weights.\n",
      "4. Repeat Steps 2 and 3\n",
      "Wj (t+1) – Updated Weight\n",
      "Wj (t) – Old Weight\n",
      "d – Desired Output\n",
      "y – Actual Output\n",
      "x – Input\n",
      "Q7. What is the significance of a Cost/Loss function?\n",
      "A cost function is a measure of the accuracy of the neural network with respect to a given training sample and expected output. It provides the performance of a neural network as a whole. In\n",
      "deep learning, the goal is to minimize the cost function. For that, we use the concept of gradient descent.\n",
      "Q8. What is gradient descent?\n",
      "Gradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient.\n",
      "Stochastic Gradient Descent: Uses only a single training example to calculate the gradient and update parameters.\n",
      "Batch Gradient Descent: Calculate the gradients for the whole dataset and perform just one update at each iteration.\n",
      "Mini-batch Gradient Descent: Mini-batch gradient is a variation of stochastic gradient descent where instead of single training example, mini-batch of samples is used. It’s one of the most\n",
      "popular optimization algorithms.\n",
      "Q9. What are the benefits of mini-batch gradient descent?\n",
      "This is more efficient compared to stochastic gradient descent.\n",
      "The generalization by finding the flat minima.\n",
      "Mini-batches allows help to approximate the gradient of the entire training set which helps us to avoid local minima.\n",
      "0.\n",
      "\n",
      "\n",
      "259. Q10.What are the steps for using a gradient descent algorithm?\n",
      "Initialize random weight and bias.\n",
      "Pass an input through the network and get values from the output layer.\n",
      "Calculate the error between the actual value and the predicted value.\n",
      "Go to each neuron which contributes to the error and then change its respective values to reduce the error.\n",
      "Reiterate until you find the best weights of the network.\n",
      "Q11. Create a Gradient Descent in python.\n",
      "Q12. What are the shortcomings of a single layer perceptron?\n",
      "Well, there are two major problems:\n",
      "Single-Layer Perceptrons cannot classify non-linearly separable data points.\n",
      "Complex problems, that involve a lot of parameters cannot be solved by Single-Layer Perceptrons\n",
      "Q13. What is a Multi-Layer-Perceptron\n",
      "A multilayer perceptron (MLP) is a deep, arti+cial neural network. It is composed of more than one perceptron. They are composed of an input layer to receive the signal, an output layer that makes\n",
      "a decision or prediction about the input, and in between those two, an arbitrary number of hidden layers that are the true computational engine of the MLP.\n",
      "Q14. What are the different parts of a multi-layer perceptron?\n",
      "Input Nodes: The Input nodes provide information from the outside world to the network and are together referred to as the “Input Layer”.  No computation is performed in any of the Input\n",
      "nodes – they just pass on the information to the hidden nodes.\n",
      "Hidden Nodes: The Hidden nodes perform computations and transfer information from the input nodes to the output nodes. A collection of hidden nodes forms a “Hidden Layer”. While a network\n",
      "will only have a single input layer and a single output layer, it can have zero or multiple Hidden Layers.\n",
      "Output Nodes: The Output nodes are collectively referred to as the “Output Layer” and are responsible for computations and transferring information from the network to the outside world.\n",
      "Q15. What Is Data Normalization And Why Do We Need It?\n",
      "Data normalization is very important preprocessing step, used to rescale values to +t in a speci+c range to assure better convergence during backpropagation. In general, it boils down to\n",
      "subtracting the mean of each data point and dividing by its standard deviation.\n",
      "These were some basic Deep Learning Interview Questions. Now, let’s move on to some advanced ones.\n",
      "Advance Interview Questions\n",
      "Q16. Which is Better Deep Networks or Shallow ones? and Why?\n",
      "Both the Networks, be it shallow or Deep are capable of approximating any function. But what matters is how precise that network is in terms of getting the results. A shallow network works with\n",
      "only a few features, as it can’t extract more. But a deep network goes deep by computing efficiently and working on more features/parameters.\n",
      "Q17. Why is Weight Initialization important in Neural Networks?\n",
      "Weight initialization is one of the very important steps. A bad weight initialization can prevent a network from learning but good weight initialization helps in giving a quicker convergence and a\n",
      "better overall error.\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "params = [weights_hidden, weights_output, bias_hidden, bias_output]\n",
      " \n",
      "def sgd(cost, params, lr=0.05):\n",
      " \n",
      "grads = T.grad(cost=cost, wrt=params)\n",
      "updates = []\n",
      " \n",
      "for p, g in zip(params, grads):\n",
      "updates.append([p, p - g * lr])\n",
      " \n",
      "return updates\n",
      " \n",
      "updates = sgd(cost, params)\n",
      "\n",
      "\n",
      "260. Biases can be generally initialized to zero. The rule for setting the weights is to be close to zero without being too small.\n",
      "Q18. What’s the difference between a feed-forward and a backpropagation neural network?\n",
      "A Feed-Forward Neural Network is a type of Neural Network architecture where the connections are “fed forward”, i.e. do not form cycles.  The term “Feed-Forward” is also used when you input\n",
      "something at the input layer and it travels from input to hidden and from hidden to the output layer.\n",
      "Backpropagation is a training algorithm consisting of 2 steps:\n",
      "Feed-Forward the values.\n",
      "Calculate the error and propagate it back to the earlier layers.\n",
      "So to be precise, forward-propagation is part of the backpropagation algorithm but comes before back-propagating.\n",
      "Q19. What are the Hperparameteres? Name a few used in any Neural Network.\n",
      "Hyperparameters are the variables which determine the network structure(Eg: Number of Hidden Units) and the variables which determine how the network is trained(Eg: Learning Rate).\n",
      "Hyperparameters are set before training.\n",
      "Number of Hidden Layers\n",
      "Network Weight Initialization\n",
      "Activation Function\n",
      "Learning Rate\n",
      "Momentum\n",
      "Number of Epochs\n",
      "Batch Size\n",
      "Q20. Explain the different Hyperparameters related to Network and Training.\n",
      "Network Hyperparameters\n",
      "The number of Hidden Layers: Many hidden units within a layer with regularization techniques can increase accuracy. Smaller number of units may cause underfitting.\n",
      "Network Weight Initialization: Ideally, it may be better to use different weight initialization schemes according to the activation function used on each layer. Mostly uniform distribution is used.\n",
      "Activation function: Activation functions are used to introduce nonlinearity to models, which allows deep learning models to learn nonlinear prediction boundaries.\n",
      "Training Hyperparameters\n",
      "Learning Rate: The learning rate de+nes how quickly a network updates its parameters. Low learning rate slows down the learning process but converges smoothly. Larger learning rate speeds up\n",
      "the learning but may not converge.\n",
      "Momentum: Momentum helps to know the direction of the next step with the knowledge of the previous steps. It helps to prevent oscillations. A typical choice of momentum is between 0.5 to 0.9.\n",
      "The number of epochs: Number of epochs is the number of times the whole training data is shown to the network while training. Increase the number of epochs until the validation accuracy\n",
      "starts decreasing even when training accuracy is increasing(overfitting).\n",
      "Batch size: Mini batch size is the number of sub-samples given to the network after which parameter update happens. A good default for batch size might be 32. Also try 32, 64, 128, 256, and so\n",
      "on.\n",
      "Q21. What is Dropout?\n",
      "Dropout is a regularization technique to avoid over+tting thus increasing the generalizing power. Generally, we should use a small dropout value of 20%-50% of neurons with 20% providing a good\n",
      "starting point. A probability too low has minimal effect and a value too high results in under-learning by the network.\n",
      "Use a larger network. You are likely to get better performance when dropout is used on a larger network, giving the model more of an opportunity to learn independent representations.\n",
      "Q22. In training a neural network, you notice that the loss does not decrease in the few starting epochs. What could be the reason?\n",
      "The reasons for this could be:\n",
      "The learning is rate is low\n",
      "Regularization parameter is high\n",
      "Stuck at local minima\n",
      "\n",
      "\n",
      "261. Q23. Name a few deep learning frameworks\n",
      "TensorFlow\n",
      "Caffe\n",
      "The Microsoft Cognitive Toolkit/CNTK\n",
      "Torch/PyTorch\n",
      "MXNet\n",
      "Chainer\n",
      "Keras\n",
      "Q24. What are Tensors?\n",
      "Tensors are nothing but a de facto for representing the data in deep learning. They are just multidimensional arrays, that allows you to represent data having higher dimensions. In general, Deep\n",
      "Learning you deal with high dimensional data sets where dimensions refer to different features present in the data set.\n",
      "Q25. List a few advantages of TensorFlow?\n",
      "It has platform flexibility\n",
      "It is easily trainable on CPU as well as GPU for distributed computing.\n",
      "TensorFlow has auto differentiation capabilities\n",
      "It has advanced support for threads, asynchronous computation, and queue es.\n",
      "It is a customizable and open source.\n",
      "Q26. What is Computational Graph?\n",
      "A computational graph is a series of TensorFlow operations arranged as nodes in the graph. Each node takes zero or more tensors as input and produces a tensor as output.\n",
      "Basically, one can think of a Computational Graph as an alternative way of conceptualizing mathematical calculations that takes place in a TensorFlow program. The operations assigned to different\n",
      "nodes of a Computational Graph can be performed in parallel, thus, providing better performance in terms of computations.\n",
      "Q27. What is a CNN?\n",
      "Convolutional neural network (CNN, or ConvNet) is a class of deep neural networks, most commonly applied to analyzing visual imagery. Unlike neural networks, where the input is a vector, here\n",
      "the input is a multi-channeled image. CNNs use a variation of multilayer perceptrons designed to require minimal preprocessing.\n",
      "Q28. Explain the different Layers of CNN.\n",
      "There are four layered concepts we should understand in Convolutional Neural Networks:\n",
      "Convolution: The convolution layer comprises of a set of independent +lters. All these +lters are initialized randomly and become our parameters which will be learned by the network\n",
      "subsequently.\n",
      "ReLu: This layer is used with the convolutional layer.\n",
      "\n",
      "\n",
      "262. Pooling: Its function is to progressively reduce the spatial size of the representation to reduce the number of parameters and computation in the network. Pooling layer operates on each feature\n",
      "map independently.\n",
      "Full Connectedness: Neurons in a fully connected layer have full connections to all activations in the previous layer, as seen in regular Neural Networks. Their activations can hence be computed\n",
      "with a matrix multiplication followed by a bias offset.\n",
      "Q29. What is an RNN?\n",
      "Recurrent Networks are a type of arti+cial neural network designed to recognize patterns in sequences of data, such as text, genomes, handwriting, the spoken word, numerical times series data.\n",
      "Recurrent Neural Networks use backpropagation algorithm for training  Because of their internal memory, RNN’s are able to remember important things about the input they received, which\n",
      "enables them to be very precise in predicting what’s coming next.\n",
      "Q30. What are some issues faced while training an RNN?\n",
      "Recurrent Neural Networks use backpropagation algorithm for training, but it is applied for every timestamp. It is commonly known as Back-propagation Through Time (BTT).\n",
      "There are some issues with Back-propagation such as:\n",
      "Vanishing Gradient\n",
      "Exploding Gradient\n",
      "Q31. What is Vanishing Gradient? And how is this harmful?\n",
      "When we do Back-propagation, the gradients tend to get smaller and smaller as we keep on moving backward in the Network. This means that the neurons in the Earlier layers learn very slowly as\n",
      "compared to the neurons in the later layers in the Hierarchy.\n",
      "Earlier layers in the Network are important because they are responsible to learn and detecting the simple patterns and are actually the building blocks of our Network.\n",
      "Obviously, if they give improper and inaccurate results, then how can we expect the next layers and the complete Network to perform nicely and produce accurate results. The Training process\n",
      "takes too long and the Prediction Accuracy of the Model will decrease.\n",
      "Q32. What is Exploding Gradient Descent?\n",
      "Exploding gradients are a problem when large error gradients accumulate and result in very large updates to neural network model weights during training.\n",
      "Gradient Descent process works best when these updates are small and controlled. When the magnitudes of the gradients accumulate, an unstable network is likely to occur, which can cause poor\n",
      "prediction of results or even a model that reports nothing useful what so ever.\n",
      "Q33. Explain the importance of LSTM.\n",
      "Long short-term memory(LSTM) is an arti+cial recurrent neural network architecture used in the +eld of deep learning. Unlike standard feedforward neural networks, LSTM has feedback\n",
      "connections that make it a “general purpose computer”. It can not only process single data points, but also entire sequences of data.\n",
      "They are a special kind of Recurrent Neural Networks which are capable of learning long-term dependencies.\n",
      "Q34. What are capsules in Capsule Neural Network?\n",
      "Capsules are a vector specifying the features of the object and its likelihood. These features can be any of the instantiation parameters like position, size, orientation, deformation, velocity, hue,\n",
      "texture and much more.\n",
      "\n",
      "\n",
      "263. A capsule can also specify its attributes like angle and size so that it can represent the same generic information. Now, just like a neural network has layers of neurons, a capsule network can have\n",
      "layers of capsules.\n",
      "Now, let’s continue this Deep Learning Interview Questions and move to the section of autoencoders and RBMs.\n",
      "Q35. Explain Autoencoders and it’s uses.\n",
      "An autoencoder neural network is an Unsupervised Machine learning algorithm that applies backpropagation, setting the target values to be equal to the inputs. Autoencoders are used to reduce\n",
      "the size of our inputs into a smaller representation. If anyone needs the original data, they can reconstruct it from the compressed data.\n",
      "Q36. In terms of Dimensionality Reduction, How does Autoencoder differ from PCAs?\n",
      "An autoencoder can learn non-linear transformations with a non-linear activation function and multiple layers.\n",
      "It doesn’t have to learn dense layers. It can use convolutional layers to learn which is better for video, image and series data.\n",
      "It is more efficient to learn several layers with an autoencoder rather than learn one huge transformation with PCA.\n",
      "An autoencoder provides a representation of each layer as the output.\n",
      "It can make use of pre-trained layers from another model to apply transfer learning to enhance the encoder/decoder.\n",
      "Q37. Give some real-life examples where autoencoders can be applied.\n",
      "Image Coloring: Autoencoders are used for converting any black and white picture into a colored image. Depending on what is in the picture, it is possible to tell what the color should be.\n",
      "Feature variation: It extracts only the required features of an image and generates the output by removing any noise or unnecessary interruption.\n",
      "Dimensionality Reduction: The reconstructed image is the same as our input but with reduced dimensions. It helps in providing a similar image with a reduced pixel value.\n",
      "Denoising Image: The input seen by the autoencoder is not the raw input but a stochastically corrupted version. A denoising autoencoder is thus trained to reconstruct the original input from the\n",
      "noisy version.\n",
      "Q38. what are the different layers of Autoencoders?\n",
      "An Autoencoder consist of three layers:\n",
      "Encoder\n",
      "Code\n",
      "Decoder\n",
      "Q39. Explain the architecture of an Autoencoder.\n",
      "Encoder: This part of the network compresses the input into a latent space representation. The encoder layer encodes the input image as a compressed representation in a reduced dimension.\n",
      "The compressed image is the distorted version of the original image.\n",
      "\n",
      "\n",
      "264. Code: This part of the network represents the compressed input which is fed to the decoder.\n",
      "Decoder: This layer decodes the encoded image back to the original dimension. The decoded image is a lossy reconstruction of the original image and it is reconstructed from the latent space\n",
      "representation.\n",
      "Q40. What is a Bottleneck in autoencoder and why is it used?\n",
      "The layer between the encoder and decoder, ie. the code is also known as Bottleneck. This is a well-designed approach to decide which aspects of observed data are relevant information and what\n",
      "aspects can be discarded.\n",
      "It does this by balancing two criteria:\n",
      "Compactness of representation, measured as the compressibility.\n",
      "It retains some behaviourally relevant variables from the input.\n",
      "Q41. Is there any variation of Autoencoders?\n",
      "Convolution Autoencoders\n",
      "Sparse Autoencoders\n",
      "Deep Autoencoders\n",
      "Contractive Autoencoders\n",
      "Q42. What are Deep Autoencoders?\n",
      "The extension of the simple Autoencoder is the Deep Autoencoder. The +rst layer of the Deep Autoencoder is used for +rst-order features in the raw input. The second layer is used for second-\n",
      "order features corresponding to patterns in the appearance of first-order features. Deeper layers of the Deep Autoencoder tend to learn even higher-order features.\n",
      "A deep autoencoder is composed of two, symmetrical deep-belief networks:\n",
      "First four or five shallow layers representing the encoding half of the net.\n",
      "The second set of four or five layers that make up the decoding half.\n",
      "Q43. What is a Restricted Boltzmann Machine?\n",
      "Restricted Boltzmann Machine is an undirected graphical model that plays a major role in Deep Learning Framework in recent times.\n",
      "It is an algorithm which is useful for dimensionality reduction, classification, regression, collaborative filtering, feature learning, and topic modeling.\n",
      "Q44. How Does RBM differ from Autoencoders?\n",
      "Autoencoder is a simple 3-layer neural network where output units are directly connected back to input units. Typically, the number of hidden units is much less than the number of visible ones.\n",
      "The task of training is to minimize an error or reconstruction, i.e. find the most efficient compact representation for input data.\n",
      "RBM shares a similar idea, but it uses stochastic units with particular distribution instead of deterministic distribution. The task of training is to +nd out how these two sets of variables are actually\n",
      "\n",
      "\n",
      "265. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      " \n",
      "Interview Question Series #2 \n",
      "Python Programming \n",
      " \n",
      " \n",
      "Numpy \n",
      "1. Why is python numpy better than lists? \n",
      "Python numpy arrays should be considered instead of a list because they are fast, \n",
      "consume less memory and convenient with lots of functionality. \n",
      "2. Describe the map function in Python?  \n",
      "map function executes the function given as the first argument on all the elements of the \n",
      "iterable given as the second argument. \n",
      "3. Generate array of ‘100’ random numbers sampled from a standard normal \n",
      "distribution using Numpy \n",
      "np.random.rand(100) will create 100 random numbers generated from standard normal \n",
      "distribution with mean 0 and standard deviation 1. \n",
      "4. How to count the occurrence of each value in a numpy array? \n",
      " \n",
      "Use numpy.bincount() \n",
      ">>> arr = numpy.array([0, 5, 5, 0, 2, 4, 3, 0, 0, 5, 4, 1, 9, 9]) \n",
      ">>> numpy.bincount(arr) \n",
      "The argument to bincount() must consist of booleans or positive integers. Negative \n",
      "integers are invalid. \n",
      "5. Does Numpy Support Nan? \n",
      "nan, short for “not a number”, is a special floating point value defined by the IEEE-754 \n",
      "specification. Python numpy supports nan but the definition of nan is more system \n",
      "dependent and some systems don't have an all round support for it like older cray and vax \n",
      "computers. \n",
      "6. What does ravel() function in numpy do? \n",
      " \n",
      "It combines multiple numpy arrays into a single array \n",
      "7. What is the meaning of axis=0 and axis=1? \n",
      "Axis = 0 is meant for reading rows, Axis = 1 is meant for reading columns \n",
      " \n",
      " \n",
      "\n",
      "\n",
      "266. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      " \n",
      " \n",
      "8. What is numpy and describe its use cases? \n",
      "Numpy is a package library for Python, adding support for large, multi-dimensional arrays \n",
      "and matrices, along with a large collection of high level mathematical functions. In simple \n",
      "words, Numpy is an optimized version of Python lists like Financial functions, Linear \n",
      "Algebra, Statistics, Polynomials, Sorting and Searching etc. \n",
      "9. How to remove from one array those items that exist in another? \n",
      ">>> a = np.array([5, 4, 3, 2, 1]) \n",
      ">>> b = np.array([4, 8, 9, 10, 1]) \n",
      "# From 'a' remove all of 'b' \n",
      ">>> np.setdiff1d(a,b) \n",
      "# Output: \n",
      ">>> array([5, 3, 2]) \n",
      " \n",
      "10. How to sort a numpy array by a specific column in a 2D array?  \n",
      "#Choose column 2 as an example \n",
      ">>> import numpy as np \n",
      ">>> arr = np.array([[1, 2, 3], [4, 5, 6], [0,0,1]]) \n",
      ">>> arr[arr[:,1].argsort()] \n",
      "# Output \n",
      ">>> array([[0, 0, 1], [1, 2, 3], [4, 5, 6]]) \n",
      "11. How to reverse a numpy array in the most efficient way? \n",
      " \n",
      ">>> import numpy as np \n",
      " \n",
      ">>> arr = np.array([9, 10, 1, 2, 0]) \n",
      ">>> reverse_arr = arr[::-1] \n",
      "12. How to calculate percentiles when using numpy? \n",
      ">>> import numpy as np \n",
      ">>> arr = np.array([11, 22, 33, 44 ,55 ,66, 77]) \n",
      ">>> perc = np.percentile(arr, 40)  #Returns the 40th percentile \n",
      ">>> print(perc) \n",
      " \n",
      "13. What Is The Difference Between Numpy And Scipy? \n",
      "NumPy would contain nothing but the array data type and the most basic operations: \n",
      "indexing, sorting, reshaping, basic element wise functions, et cetera. All numerical code \n",
      "would reside in SciPy. SciPy contains more fully-featured versions of the linear algebra \n",
      "modules, as well as many other numerical algorithms. \n",
      " \n",
      " \n",
      "\n",
      "\n",
      "267. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      " \n",
      " \n",
      "14. What Is The Preferred Way To Check For An Empty (zero Element) Array? \n",
      "For a numpy array, use the size attribute. The size attribute is helpful for determining the \n",
      "length of numpy array: \n",
      ">>> arr = numpy.zeros((1,0)) \n",
      ">>> arr.size \n",
      " \n",
      "15. What Is The Difference Between Matrices And Arrays? \n",
      "Matrices can only be two-dimensional, whereas arrays can have any number of \n",
      "dimensions \n",
      " \n",
      "16. How can you find the indices of an array where a condition is true? \n",
      "Given an array a, the condition arr > 3 returns a boolean array and since False is \n",
      "interpreted as 0 in Python and NumPy. \n",
      ">>> import numpy as np \n",
      ">>> arr = np.array([[9,8,7],[6,5,4],[3,2,1]]) \n",
      ">>> arr > 3 \n",
      ">>> array([[True, True, True], \n",
      "       [ True,  True,  True], \n",
      "       [False,  False,  False]], dtype=bool) \n",
      " \n",
      "17. How to find the maximum and minimum value of a given flattened array? \n",
      ">>> import numpy as np \n",
      ">>> a = np.arange(4).reshape((2,2)) \n",
      ">>> max_val = np.amax(a) \n",
      ">>> min_val = np.amin(a) \n",
      " \n",
      "18. Write a NumPy program to calculate the difference between the maximum and the \n",
      "minimum values of a given array along the second axis. \n",
      " \n",
      ">>> import numpy as np \n",
      ">>> arr = np.arange(16).reshape((4, 7)) \n",
      ">>> res = np.ptp(arr, 1) \n",
      " \n",
      "19. Find median of a numpy flattened array  \n",
      " \n",
      ">>> import numpy as np \n",
      ">>> arr = np.arange(16).reshape((4, 5)) \n",
      ">>> res =  np.median(arr) \n",
      " \n",
      " \n",
      "\n",
      "\n",
      "268. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      " \n",
      "20. Write a NumPy program to compute the mean, standard deviation, and variance of \n",
      "a given array along the second axis \n",
      " \n",
      "import numpy as np \n",
      " \n",
      ">>> import numpy as np \n",
      ">>> x = np.arange(16) \n",
      ">>> mean = np.mean(x) \n",
      ">>> std = np.std(x) \n",
      ">>> var= np.var(x) \n",
      " \n",
      "21. Calculate covariance matrix between two numpy arrays \n",
      ">>> import numpy as np \n",
      ">>> x = np.array([2, 1, 0]) \n",
      ">>> y = np.array([2, 3, 3]) \n",
      ">>> cov_arr = np.cov(x, y) \n",
      " \n",
      "22. Compute Compute pearson product-moment correlation coefficients of two \n",
      "given numpy arrays \n",
      " \n",
      ">>> import numpy as np \n",
      ">>> x = np.array([0, 1, 3]) \n",
      ">>> y = np.array([2, 4, 5]) \n",
      ">>> cross_corr = np.corrcoef(x, y) \n",
      " \n",
      "23. Develop a numpy program to compute the histogram of nums against the bins \n",
      " \n",
      ">>> import numpy as np \n",
      ">>> nums = np.array([0.5, 0.7, 1.0, 1.2, 1.3, 2.1]) \n",
      ">>> bins = np.array([0, 1, 2, 3]) \n",
      ">>> np.histogram(nums, bins) \n",
      " \n",
      "24. Get the powers of an array values element-wise \n",
      ">>> import numpy as np \n",
      ">>> x = np.arange(7) \n",
      ">>> np.power(x, 3) \n",
      " \n",
      "25. Write a NumPy program to get true division of the element-wise array inputs \n",
      " \n",
      ">>> import numpy as np \n",
      ">>> x = np.arange(10) \n",
      ">>> np.true_divide(x, 3) \n",
      " \n",
      " \n",
      "\n",
      "\n",
      "269. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      " \n",
      "Pandas \n",
      " \n",
      " \n",
      "26. What is a series in pandas? \n",
      "A Series is defined as a one-dimensional array that is capable of storing various data \n",
      "types. The row labels of the series are called the index. By using a 'series' method, we \n",
      "can easily convert the list, tuple, and dictionary into series. A Series cannot contain \n",
      "multiple columns. \n",
      " \n",
      "27. What features make Pandas such a reliable option to store tabular data? \n",
      "Memory Efficient, Data Alignment, Reshaping, Merge and join and Time Series. \n",
      " \n",
      "28. What is reindexing in pandas? \n",
      "Reindexing is used to conform DataFrame to a new index with optional filling logic. It \n",
      "places NA/NaN in that location where the values are not present in the previous index. It \n",
      "returns a new object unless the new index is produced as equivalent to the current one, \n",
      "and the value of copy becomes False. It is used to change the index of the rows and \n",
      "columns of the DataFrame. \n",
      "29. How will you create a series from dict in Pandas? \n",
      "A Series is defined as a one-dimensional array that is capable of storing various data \n",
      "types. \n",
      ">>> import pandas as pd     \n",
      ">>> info = {'x' : 0., 'y' : 1., 'z' : 2.}     \n",
      ">>> a = pd.Series(info) \n",
      " \n",
      "30. How can we create a copy of the series in Pandas? \n",
      "Use pandas.Series.copy method \n",
      ">>> import pandas as pd \n",
      ">>> pd.Series.copy(deep=True) \n",
      "31. What is groupby in Pandas? \n",
      "GroupBy is used to split the data into groups. It groups the data based on some criteria. \n",
      "Grouping also provides a mapping of labels to the group names. It has a lot of variations \n",
      "that can be defined with the parameters and makes the task of splitting the data quick and \n",
      "easy. \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "32. What is vectorization in Pandas? \n",
      "\n",
      "\n",
      "270. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      " \n",
      "Vectorization is the process of running operations on the entire array. This is done to \n",
      "reduce the amount of iteration performed by the functions. Pandas have a number of \n",
      "vectorized functions like aggregations, and string functions that are optimized to operate \n",
      "specifically on series and DataFrames. So it is preferred to use the vectorized pandas \n",
      "functions to execute the operations quickly. \n",
      "33. Mention the different types of Data Structures in Pandas \n",
      "Pandas provide two data structures, which are supported by the pandas library, Series, \n",
      "and DataFrames. Both of these data structures are built on top of the NumPy. \n",
      " \n",
      "34. What Is Time Series In pandas \n",
      "A time series is an ordered sequence of data which basically represents how some \n",
      "quantity changes over time. pandas contains extensive capabilities and features for \n",
      "working with time series data for all domains. \n",
      " \n",
      "35. How to convert pandas dataframe to numpy array? \n",
      "The function to_numpy() is used to convert the DataFrame to a NumPy array. \n",
      "DataFrame.to_numpy(self, dtype=None, copy=False) \n",
      "The dtype parameter defines the data type to pass to the array and the copy ensures the \n",
      "returned value is not a view on another array. \n",
      " \n",
      "36. Write a Pandas program to get the first 5 rows of a given DataFrame \n",
      ">>> import pandas as pd \n",
      ">>> exam_data  = {'name': ['Anastasia', 'Dima', 'Katherine', 'James', 'Emily', 'Michael', \n",
      "'Matthew', 'Laura', 'Kevin', 'Jonas'],} \n",
      "labels = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j'] \n",
      ">>> df = pd.DataFrame(exam_data , index=labels) \n",
      ">>> df.iloc[:5] \n",
      " \n",
      "37. Develop a Pandas program to create and display a one-dimensional array-\n",
      "like object containing an array of data. \n",
      ">>> import pandas as pd \n",
      ">>> pd.Series([2, 4, 6, 8, 10]) \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "38. Write a Python program to convert a Panda module Series to Python list and \n",
      "it's type. \n",
      ">>> import pandas as pd \n",
      ">>> ds = pd.Series([2, 4, 6, 8, 10]) \n",
      "\n",
      "\n",
      "271. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      " \n",
      ">>> type(ds) \n",
      ">>> ds.tolist() \n",
      ">>> type(ds.tolist()) \n",
      " \n",
      "39. Develop a Pandas program to add, subtract, multiple and divide two Pandas \n",
      "Series. \n",
      ">>> import pandas as pd \n",
      ">>> ds1 = pd.Series([2, 4, 6, 8, 10]) \n",
      ">>> ds2 = pd.Series([1, 3, 5, 7, 9]) \n",
      ">>> sum = ds1 + ds2 \n",
      ">>> sub = ds1 - ds2 \n",
      ">>> mul = ds1 * ds2 \n",
      ">>> div = ds1 / ds2 \n",
      " \n",
      "40. Develop a Pandas program to compare the elements of the two Pandas \n",
      "Series. \n",
      ">>> import pandas as pd \n",
      ">>> ds1 = pd.Series([2, 4, 6, 8, 10]) \n",
      ">>> ds2 = pd.Series([1, 3, 5, 7, 10]) \n",
      ">>> ds1 == ds2 \n",
      ">>> ds1 > ds2 \n",
      ">>> ds1 < ds2 \n",
      " \n",
      "41. Develop a Pandas program to change the data type of given a column or a \n",
      "Series. \n",
      ">>> import pandas as pd \n",
      ">>> s1 = pd.Series(['100', '200', 'python', '300.12', '400']) \n",
      ">>> s2 = pd.to_numeric(s1, errors='coerce') \n",
      ">>> s2 \n",
      " \n",
      "42. Write a Pandas program to convert Series of lists to one Series \n",
      ">>> import pandas as pd \n",
      ">>> s = pd.Series([ ['Red', 'Black'], ['Red', 'Green', 'White'] , ['Yellow']]) \n",
      " >>> s = s.apply(pd.Series).stack().reset_index(drop=True) \n",
      " \n",
      " \n",
      " \n",
      "43. Write a Pandas program to create a subset of a given series based on value \n",
      "and condition \n",
      ">>> import pandas as pd \n",
      ">>> s = pd.Series([0, 1,2,3,4,5,6,7,8,9,10]) \n",
      ">>> n = 6 \n",
      "\n",
      "\n",
      "272. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      " \n",
      ">>> new_s = s[s < n] \n",
      ">>> new_s \n",
      " \n",
      "44. Develop a Pandas code to alter the order of index in a given series \n",
      ">>> import pandas as pd \n",
      ">>> s = pd.Series(data = [1,2,3,4,5], index = ['A', 'B', 'C','D','E']) \n",
      ">>> s.reindex(index = ['B','A','C','D','E']) \n",
      " \n",
      "45. Write a Pandas code to get the items of a given series not present in another \n",
      "given series. \n",
      " \n",
      ">>> import pandas as pd \n",
      ">>> sr1 = pd.Series([1, 2, 3, 4, 5]) \n",
      ">>> sr2 = pd.Series([2, 4, 6, 8, 10]) \n",
      ">>> result = sr1[~sr1.isin(sr2)] \n",
      ">>> result \n",
      " \n",
      "46. What is the difference between the two data series df[‘Name’] and      df.loc[:, \n",
      "‘Name’]? \n",
      ">>> First one is a view of the original dataframe and second one is a copy of the original \n",
      "dataframe. \n",
      " \n",
      "47. Write a Pandas program to display the most frequent value in a given series \n",
      "and replace everything else as “replaced” in the series. \n",
      " \n",
      ">>> import pandas as pd \n",
      ">>> import numpy as np \n",
      ">>> np.random.RandomState(100) \n",
      ">>> num_series = pd.Series(np.random.randint(1, 5, [15])) \n",
      ">>> result = num_series[~num_series.isin(num_series.value_counts().index[:1])] = \n",
      "'replaced' \n",
      " \n",
      " \n",
      "48. Write a Pandas program to find the positions of numbers that are multiples \n",
      "of 5 of a given series. \n",
      ">>> import pandas as pd \n",
      ">>> import numpy as np \n",
      ">>> num_series = pd.Series(np.random.randint(1, 10, 9)) \n",
      ">>> result = np.argwhere(num_series % 5==0) \n",
      " \n",
      "49. How will you add a column to a pandas DataFrame? \n",
      "\n",
      "\n",
      "273. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      " \n",
      "# importing the pandas library     \n",
      ">>> import pandas as pd       \n",
      ">>> info = {'one' : pd.Series([1, 2, 3, 4, 5], index=['a', 'b', 'c', 'd', 'e']),     \n",
      "             'two' : pd.Series([1, 2, 3, 4, 5, 6], index=['a', 'b', 'c', 'd', 'e', 'f'])}     \n",
      ">>> info = pd.DataFrame(info) \n",
      "# Add a new column to an existing DataFrame object \n",
      ">>> info['three']=pd.Series([20,40,60],index=['a','b','c']) \n",
      " \n",
      " \n",
      "50. How to iterate over a Pandas DataFrame? \n",
      "You can iterate over the rows of the DataFrame by using for loop in combination with an \n",
      "iterrows() call on the DataFrame. \n",
      " \n",
      " \n",
      "Python Language \n",
      " \n",
      "51. What type of language is python? Programming or scripting? \n",
      "Python is capable of scripting, but in general sense, it is considered as a general-purpose \n",
      "programming language. \n",
      "52. Is python case sensitive? \n",
      " \n",
      "Yes, python is a case sensitive language. \n",
      " \n",
      "53. What is a lambda function in python? \n",
      "An anonymous function is known as a lambda function. This function can have any \n",
      "number of parameters but can have just one statement. \n",
      " \n",
      "54. What is the difference between xrange and xrange in python? \n",
      "xrange and range are the exact same in terms of functionality.The only difference is that \n",
      "range returns a Python list object and x range returns an xrange object.  \n",
      " \n",
      "55. What are docstrings in python? \n",
      "Docstrings are not actually comments, but they are documentation strings. These \n",
      "docstrings are within triple quotes. They are not assigned to any variable and therefore, \n",
      "at times, serve the purpose of comments as well. \n",
      " \n",
      "56. Whenever Python exits, why isn’t all the memory deallocated? \n",
      "Whenever Python exits, especially those Python modules which are having circular \n",
      "references to other objects or the objects that are referenced from the global namespaces \n",
      "\n",
      "\n",
      "274. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      " \n",
      "are not always de-allocated or freed. It is impossible to de-allocate those portions of \n",
      "memory that are reserved by the C library. On exit, because of having its own efficient \n",
      "clean up mechanism, Python would try to de-allocate/destroy every other object. \n",
      " \n",
      "57. What does this mean: *args, **kwargs? And why would we use it? \n",
      "We use *args when we aren’t sure how many arguments are going to be passed to a \n",
      "function, or if we want to pass a stored list or tuple of arguments to a function. **kwargs is \n",
      "used when we don’t know how many keyword arguments will be passed to a function, or \n",
      "it can be used to pass the values of a dictionary as keyword arguments.  \n",
      " \n",
      " \n",
      "58. What is the difference between deep and shallow copy? \n",
      "Shallow copy is used when a new instance type gets created and it keeps the values that \n",
      "are copied in the new instance. Shallow copy is used to copy the reference pointers just \n",
      "like it copies the values. \n",
      "Deep copy is used to store the values that are already copied. Deep copy doesn’t copy \n",
      "the reference pointers to the objects. It makes the reference to an object and the new \n",
      "object that is pointed by some other object gets stored. \n",
      "59. Define encapsulation in Python? \n",
      "Encapsulation means binding the code and the data together. A Python class in an \n",
      "example of encapsulation. \n",
      "60. Does python make use of access specifiers? \n",
      "Python does not deprive access to an instance variable or function. Python lays down the \n",
      "concept of prefixing the name of the variable, function or method with a single or double \n",
      "underscore to imitate the behavior of protected and private access specifiers.  \n",
      " \n",
      " \n",
      " \n",
      "61. What are the generators in Python? \n",
      "Generators are a way of implementing iterators. A generator function is a normal function \n",
      "except that it contains yield expression in the function definition making it a generator \n",
      "function. \n",
      "62. How will you remove the duplicate elements from the given list? \n",
      "The set is another type available in Python. It doesn’t allow copies and provides some \n",
      "good functions to perform set operations like union, difference etc. \n",
      ">>> list(set(a)) \n",
      "63. Does Python allow arguments Pass by Value or Pass by Reference? \n",
      "\n",
      "\n",
      "275. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      " \n",
      "Neither the arguments are Pass by Value nor does Python supports Pass by reference. \n",
      "Instead, they are Pass by assignment. The parameter which you pass is originally a \n",
      "reference to the object not the reference to a fixed memory location. But the reference is \n",
      "passed by value. Additionally, some data types like strings and tuples are immutable \n",
      "whereas others are mutable. \n",
      "64. What is slicing in Python? \n",
      "Slicing in Python is a mechanism to select a range of items from Sequence types like \n",
      "strings, list, tuple, etc. \n",
      "65. Why is the “pass” keyword used in Python? \n",
      "The “pass” keyword is a no-operation statement in Python. It signals that no action is \n",
      "required. It works as a placeholder in compound statements which are intentionally left \n",
      "blank. \n",
      "66. What is PEP8 and why is it important? \n",
      "PEP stands for Python Enhancement Proposal. A PEP is an official design document \n",
      "providing information to the Python Community, or describing a new feature for Python or \n",
      "its processes. PEP 8 is especially important since it documents the style guidelines for \n",
      "Python Code. Apparently contributing in the Python open-source community requires you \n",
      "to follow these style guidelines sincerely and strictly. \n",
      "67. What are decorators in Python? \n",
      "Decorators in Python are essentially functions that add functionality to an existing function \n",
      "in Python without changing the structure of the function itself. They are represented by the \n",
      "@decorator_name in Python and are called in bottom-up fashion \n",
      " \n",
      "68. What is the key difference between lists and tuples in python? \n",
      "The key difference between the two is that while lists are mutable, tuples on the other \n",
      "hand are immutable objects. \n",
      " \n",
      "69. What is self in Python? \n",
      "Self is a keyword in Python used to define an instance or an object of a class. In Python, \n",
      "it is explicitly used as the first parameter, unlike in Java where it is optional. It helps in \n",
      "distinguishing between the methods and attributes of a class from its local variables. \n",
      "70. What is PYTHONPATH in Python? \n",
      "PYTHONPATH is an environment variable which you can set to add additional directories \n",
      "where Python will look for modules and packages. This is especially useful in maintaining \n",
      "Python libraries that you do not wish to install in the global default location.  \n",
      " \n",
      "71. What is the difference between .py and .pyc files? \n",
      "\n",
      "\n",
      "276. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      " \n",
      ".py files contain the source code of a program. Whereas, .pyc file contains the bytecode \n",
      "of your program. We get bytecode after compilation of .py file (source code). .pyc files are \n",
      "not created for all the files that you run. It is only created for the files that you import.  \n",
      " \n",
      " \n",
      "72. Explain how you can access a module written in Python from C? \n",
      " \n",
      "You can access a module written in Python from C by following method, \n",
      "Module = =PyImport_ImportModule(\"<modulename>\"); \n",
      " \n",
      "73. What is namespace in Python? \n",
      "In Python, every name introduced has a place where it lives and can be hooked for. This \n",
      "is known as namespace. It is like a box where a variable name is mapped to the object \n",
      "placed. Whenever the variable is searched out, this box will be searched, to get the \n",
      "corresponding object. \n",
      " \n",
      "74. What is pickling and unpickling? \n",
      "Pickle module accepts any Python object and converts it into a string representation and \n",
      "dumps it into a file by using the dump function, this process is called pickling. While the \n",
      "process of retrieving original Python objects from the stored string representation is called \n",
      "unpickling. \n",
      " \n",
      "75. How is Python interpreted? \n",
      "Python language is an interpreted language. The Python program runs directly from the \n",
      "source code. It converts the source code that is written by the programmer into an \n",
      "intermediate language, which is again translated into machine language that has to be \n",
      "executed. \n",
      " \n",
      " \n",
      "Jupyter Notebook \n",
      " \n",
      "76. What is the main use of a Jupyter notebook? \n",
      "Jupyter Notebook is an open-source web application that allows us to create and share \n",
      "codes and documents. It provides an environment, where you can document your code, \n",
      "run it, look at the outcome, visualize data and see the results without leaving the \n",
      "environment. \n",
      " \n",
      "77. How do I increase the cell width of the Jupyter/ipython notebook in my \n",
      "browser? \n",
      " \n",
      ">>> from IPython.core.display import display, HTML \n",
      ">>> display(HTML(\"<style>.container { width:100% !important; }</style>\")) \n",
      "\n",
      "\n",
      "277. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      " \n",
      " \n",
      "78. How do I convert an IPython Notebook into a Python file via command line? \n",
      " \n",
      ">>> jupyter nbconvert --to script [YOUR_NOTEBOOK].ipynb \n",
      " \n",
      "79. How to measure execution time in a jupyter notebook? \n",
      " \n",
      ">>> %%time is inbuilt magic command \n",
      " \n",
      "80. How to run a jupyter notebook from the command line? \n",
      " \n",
      ">>> jupyter nbconvert --to python nb.ipynb \n",
      " \n",
      " \n",
      "81. How to make inline plots larger in jupyter notebooks? \n",
      " \n",
      "Use figure size. \n",
      ">>> fig=plt.figure(figsize=(18, 16), dpi= 80, facecolor='w', edgecolor='k') \n",
      " \n",
      "82. How to display multiple images in a jupyter notebook? \n",
      ">>>for ima in images: \n",
      "    >>>plt.figure() \n",
      "    >>>plt.imshow(ima) \n",
      " \n",
      "83. Why is the Jupyter notebook interactive code and data exploration friendly? \n",
      "The ipywidgets package provides many common user interface controls for exploring code \n",
      "and data interactively. \n",
      " \n",
      " \n",
      "84. What is the default formatting option in jupyter notebook? \n",
      " \n",
      "Default formatting option is markdown \n",
      " \n",
      "85. What are kernel wrappers in jupyter? \n",
      "Jupyter brings a lightweight interface for kernel languages that can be wrapped in Python. \n",
      "Wrapper kernels can implement optional methods, notably for code completion and code \n",
      "inspection. \n",
      " \n",
      "86. What are the advantages of custom magic commands? \n",
      "Create IPython extensions with custom magic commands to make interactive computing \n",
      "even easier. Many third-party extensions and magic commands exist, for example, the \n",
      "%%cython magic that allows one to write Cython code directly in a notebook. \n",
      " \n",
      "87. Is the jupyter architecture language dependent? \n",
      " \n",
      "No. It is language independent. \n",
      " \n",
      "\n",
      "\n",
      "278. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      " \n",
      "88. Which tools allow jupyter notebooks to easily convert to pdf and html? \n",
      "Nbconvert converts it to pdf and html while Nbviewer renders the notebooks on the web \n",
      "platforms. \n",
      " \n",
      "89. What is a major disadvantage of a Jupyter notebook? \n",
      " \n",
      "It is very hard to run long asynchronous tasks. Less Secure. \n",
      " \n",
      "90. In which domain is the jupyter notebook widely used? \n",
      " \n",
      "It is mainly used for data analysis and machine learning related tasks. \n",
      " \n",
      "91. What are alternatives to jupyter notebook? \n",
      " \n",
      "PyCharm interact, VS Code  Python Interactive etc. \n",
      " \n",
      "92. Where can you make configuration changes to the jupyter notebook? \n",
      " \n",
      "In the config file located at ~/.ipython/profile_default/ipython_config.py \n",
      " \n",
      "93. Which magic command is used to run python code from jupyter notebook? \n",
      " \n",
      "%run can execute python code from .py files \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "94. How to pass variables across the notebooks? \n",
      "The %store command lets you pass variables between two different notebooks. \n",
      ">>> data = 'this is the string I want to pass to different notebook' \n",
      ">>> %store data \n",
      "# Stored 'data' (str) \n",
      "# In new notebook \n",
      ">>> %store -r data \n",
      ">>> print(data) \n",
      " \n",
      "95. Export the contents of a cell/Show the contents of an external script \n",
      "Using the %%writefile magic saves the contents of that cell to an external file. %pycat \n",
      "does the opposite and shows you (in a popup) the syntax highlighted contents of an \n",
      "external file. \n",
      " \n",
      "96. What inbuilt tool we use for debugging python code in a jupyter notebook? \n",
      "Jupyter has its own interface for The Python Debugger (pdb). This makes it possible to go \n",
      "inside the function and investigate what happens there. \n",
      " \n",
      "\n",
      "\n",
      "279. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      " \n",
      "97. How to make high resolution plots in a jupyter notebook? \n",
      " \n",
      ">>> %config InlineBackend.figure_format ='retina' \n",
      " \n",
      "98. How can one use latex in a jupyter notebook? \n",
      "When you write LaTeX in a Markdown cell, it will be rendered as a formula using MathJax. \n",
      " \n",
      "99. What is a jupyter lab? \n",
      "It is a next generation user interface for conventional jupyter notebooks. Users can drag \n",
      "and drop cells, arrange code workspace and live previews. It’s still in the early stage of \n",
      "development. \n",
      " \n",
      "100. \n",
      "What is the biggest limitation for a Jupyter notebook? \n",
      "Code versioning, management and debugging is not scalable in current jupyter notebook. \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\n",
      "\n",
      "280. Follow Steve Nouri for more AI and Data science posts: https://lnkd.in/gZu463X \n",
      " \n",
      "References \n",
      " \n",
      "[1] https://www.edureka.co \n",
      "[2] https://www.kausalvikash.in \n",
      "[3] https://www.wisdomjobs.com \n",
      "[4] https://blog.edugrad.com \n",
      "[5]https://stackoverflow.com \n",
      "[6]http://www.ezdev.org \n",
      "[7]https://www.techbeamers.com \n",
      "[8]https://www.w3resource.com \n",
      "[9]https://www.javatpoint.com \n",
      "[10]https://analyticsindiamag.com \n",
      "[11]https://www.onlineinterviewquestions.com \n",
      "[12]https://www.geeksforgeeks.org \n",
      "[13]https://www.springpeople.com \n",
      "[14]https://atraininghub.com \n",
      "[15]https://www.interviewcake.com \n",
      "[16]https://www.techbeamers.com \n",
      "[17]https://www.tutorialspoint.com \n",
      "[18]https://programmingwithmosh.com \n",
      "[19]https://www.interviewbit.com \n",
      "[20]https://www.guru99.com \n",
      "[21]https://hub.packtpub.com \n",
      "[22]https://analyticsindiamag.com \n",
      "[23]https://www.dataquest.io \n",
      "[24]https://www.infoworld.com \n",
      "\n",
      "\n",
      "281.                                                                                     https://career.guru99.com/\n",
      "Top 50 Machine Learning Interview\n",
      "Questions & Answers\n",
      "1)      What is Machine learning?\n",
      "Machine learning is a branch of computer science which deals with system programming in order to\n",
      "automatically learn and improve with experience.  For example: Robots are programed so that they\n",
      "can perform the task based on data they gather from sensors. It automatically learns programs from\n",
      "data.\n",
      "2)      Mention the diﬀerence between Data Mining and Machine learning?\n",
      "Machine learning relates with the study, design and development of the algorithms that give\n",
      "computers the capability to learn without being explicitly programmed.  While, data mining can be\n",
      "deﬁned as the process in which the unstructured data tries to extract knowledge or unknown\n",
      "interesting patterns.  During this process machine, learning algorithms are used.\n",
      "3)      What is ‘Overﬁtting’ in Machine learning?\n",
      "In machine learning, when a statistical model describes random error or noise instead of underlying\n",
      "relationship ‘overﬁtting’ occurs.  When a model is excessively complex, overﬁtting is normally\n",
      "observed, because of having too many parameters with respect to the number of training data types.\n",
      "The model exhibits poor performance which has been overﬁt.\n",
      "4)      Why overﬁtting happens?\n",
      "The possibility of overﬁtting exists as the criteria used for training the model is not the same as the\n",
      "criteria used to judge the eﬃcacy of a model.\n",
      "5)      How can you avoid overﬁtting ?\n",
      "By using a lot of data overﬁtting can be avoided, overﬁtting happens relatively as you have a small\n",
      "dataset, and you try to learn from it. But if you have a small database and you are forced to come\n",
      "with a model based on that. In such situation, you can use a technique known as cross validation. In\n",
      "this method the dataset splits into two section, testing and training datasets, the testing dataset will\n",
      "only test the model while, in training dataset, the datapoints will come up with the model.\n",
      "In this technique,  a model is usually given a dataset of a known data on which training (training data\n",
      "set) is run and a dataset of unknown data against which the model is tested. The idea of cross\n",
      "validation is to deﬁne a dataset to “test” the model in the training phase.\n",
      "6)      What is inductive machine learning?\n",
      "The inductive machine learning involves the process of learning by examples, where a system, from a\n",
      "set of observed instances tries to induce a general rule.\n",
      "\n",
      "\n",
      "282. 7)      What are the ﬁve popular algorithms of Machine Learning?\n",
      "a)      Decision Trees\n",
      "b)      Neural Networks (back propagation)\n",
      "c)       Probabilistic networks\n",
      "d)      Nearest Neighbor\n",
      "e)      Support vector machines\n",
      "8)      What are the diﬀerent Algorithm techniques in Machine Learning?\n",
      "The diﬀerent types of techniques in Machine Learning are\n",
      "a)      Supervised Learning\n",
      "b)      Unsupervised Learning\n",
      "c)       Semi-supervised Learning\n",
      "d)      Reinforcement Learning\n",
      "e)      Transduction\n",
      "f)       Learning to Learn\n",
      "9)      What are the three stages to build the hypotheses or model in machine learning?\n",
      "a)      Model building\n",
      "b)      Model testing\n",
      "c)       Applying the model\n",
      "10)   What is the standard approach to supervised learning?\n",
      "The standard approach to supervised learning is to split the set of example into the training set and\n",
      "the test.\n",
      "11)   What is ‘Training set’ and ‘Test set’?\n",
      "In various areas of information science like machine learning, a set of data is used to discover the\n",
      "potentially predictive relationship known as ‘Training Set’. Training set is an examples given to the\n",
      "learner, while Test set is used to test the accuracy of the hypotheses generated by the learner, and it\n",
      "is the set of example held back from the learner. Training set are distinct from Test set.\n",
      "12)   List down various approaches for machine learning?\n",
      "The diﬀerent approaches in Machine Learning are\n",
      "a)      Concept Vs Classiﬁcation Learning\n",
      "b)      Symbolic Vs Statistical Learning\n",
      "\n",
      "\n",
      "283. c)       Inductive Vs Analytical Learning\n",
      "13)   What is not Machine Learning?\n",
      "a)      Artiﬁcial Intelligence\n",
      "b)      Rule based inference\n",
      "14)   Explain what is the function of ‘Unsupervised Learning’?\n",
      "a)      Find clusters of the data\n",
      "b)      Find low-dimensional representations of the data\n",
      "c)       Find interesting directions in data\n",
      "d)      Interesting coordinates and correlations\n",
      "e)      Find novel observations/ database cleaning\n",
      "15)   Explain what is the function of ‘Supervised Learning’?\n",
      "a)      Classiﬁcations\n",
      "b)      Speech recognition\n",
      "c)       Regression\n",
      "d)      Predict time series\n",
      "e)      Annotate strings\n",
      "16)   What is algorithm independent machine learning?\n",
      "Machine learning in where mathematical foundations is independent of any particular classiﬁer or\n",
      "learning algorithm is referred as algorithm independent machine learning?\n",
      "17)   What is the diﬀerence between artiﬁcial learning and machine learning?\n",
      "Designing and developing algorithms according to the behaviours based on empirical data are known\n",
      "as Machine Learning.  While artiﬁcial intelligence in addition to machine learning, it also covers other\n",
      "aspects like knowledge representation, natural language processing, planning, robotics etc.\n",
      "18)   What is classiﬁer in machine learning?\n",
      "A classiﬁer in a Machine Learning is a system that inputs a vector of discrete or continuous feature\n",
      "values and outputs a single discrete value, the class.\n",
      "19)   What are the advantages of Naive Bayes?\n",
      "In Naïve Bayes classiﬁer will converge quicker than discriminative models like logistic regression, so\n",
      "you need less training data.  The main advantage is that it can’t learn interactions between features.\n",
      "20)   In what areas Pattern Recognition is used?\n",
      "Pattern Recognition can be used in\n",
      "\n",
      "\n",
      "284. a)      Computer Vision\n",
      "b)      Speech Recognition\n",
      "c)       Data Mining\n",
      "d)      Statistics\n",
      "e)      Informal Retrieval\n",
      "f)       Bio-Informatics\n",
      "21)   What is Genetic Programming?\n",
      "Genetic programming is one of the two techniques used in machine learning. The model is based on\n",
      "the testing and selecting the best choice among a set of results.\n",
      "22)   What is Inductive Logic Programming in Machine Learning?\n",
      "Inductive Logic Programming (ILP) is a subﬁeld of machine learning which uses logical programming\n",
      "representing background knowledge and examples.\n",
      "23)   What is Model Selection in Machine Learning?\n",
      "The process of selecting models among diﬀerent mathematical models, which are used to describe\n",
      "the same data set is known as Model Selection. Model selection is applied to the ﬁelds of statistics,\n",
      "machine learning and data mining.\n",
      "24)   What are the two methods used for the calibration in Supervised Learning?\n",
      "The two methods used for predicting good probabilities in Supervised Learning are\n",
      "a)      Platt Calibration\n",
      "b)      Isotonic Regression\n",
      "These methods are designed for binary classiﬁcation, and it is not trivial.\n",
      "25)   Which method is frequently used to prevent overﬁtting?\n",
      "When there is suﬃcient data ‘Isotonic Regression’ is used to prevent an overﬁtting issue.\n",
      "26)   What is the diﬀerence between heuristic for rule learning and heuristics for decision\n",
      "trees?\n",
      "The diﬀerence is that the heuristics for decision trees evaluate the average quality of a number of\n",
      "disjointed sets while rule learners only evaluate the quality of the set of instances that is covered with\n",
      "the candidate rule.\n",
      "27)   What is Perceptron in Machine Learning?\n",
      "In Machine Learning, Perceptron is an algorithm for supervised classiﬁcation of the input into one of\n",
      "several possible non-binary outputs.\n",
      "28)   Explain the two components of Bayesian logic program?\n",
      "\n",
      "\n",
      "285. Bayesian logic program consists of two components.  The ﬁrst component is a logical one ; it consists\n",
      "of a set of Bayesian Clauses, which captures the qualitative structure of the domain.  The second\n",
      "component is a quantitative one, it encodes the quantitative information about the domain.\n",
      "29)   What are Bayesian Networks (BN) ?\n",
      "Bayesian Network is used to represent the graphical model for probability relationship among a set of\n",
      "variables .\n",
      "30)   Why instance based learning algorithm sometimes referred as Lazy learning\n",
      "algorithm?\n",
      "Instance based learning algorithm is also referred as Lazy learning algorithm as they delay the\n",
      "induction or generalization process until classiﬁcation is performed.\n",
      "31)   What are the two classiﬁcation methods that SVM ( Support Vector Machine) can\n",
      "handle?\n",
      "a)      Combining binary classiﬁers\n",
      "b)      Modifying binary to incorporate multiclass learning\n",
      "32)   What is ensemble learning?\n",
      "To solve a particular computational program, multiple models such as classiﬁers or experts are\n",
      "strategically generated and combined. This process is known as ensemble learning.\n",
      "33)   Why ensemble learning is used?\n",
      "Ensemble learning is used to improve the classiﬁcation, prediction, function approximation etc of a\n",
      "model.\n",
      "34)   When to use ensemble learning?\n",
      "Ensemble learning is used when you build component classiﬁers that are more accurate and\n",
      "independent from each other.\n",
      "35)   What are the two paradigms of ensemble methods?\n",
      "The two paradigms of ensemble methods are\n",
      "a)      Sequential ensemble methods\n",
      "b)      Parallel ensemble methods\n",
      "36)   What is the general principle of an ensemble method and what is bagging and\n",
      "boosting in ensemble method?\n",
      "The general principle of an ensemble method is to combine the predictions of several models built\n",
      "with a given learning algorithm in order to improve robustness over a single model.  Bagging is a\n",
      "method in ensemble for improving unstable estimation or classiﬁcation schemes.  While boosting\n",
      "method are used sequentially to reduce the bias of the combined model.  Boosting and Bagging both\n",
      "can reduce errors by reducing the variance term.\n",
      "37)   What is bias-variance decomposition of classiﬁcation error in ensemble method?\n",
      "\n",
      "\n",
      "286. The expected error of a learning algorithm can be decomposed into bias and variance. A bias term\n",
      "measures how closely the average classiﬁer produced by the learning algorithm matches the target\n",
      "function.  The variance term measures how much the learning algorithm’s prediction ﬂuctuates for\n",
      "diﬀerent training sets.\n",
      "38)   What is an Incremental Learning algorithm in ensemble?\n",
      "Incremental learning method is the ability of an algorithm to learn from new data that may be\n",
      "available after classiﬁer has already been generated from already available dataset.\n",
      "39)   What is PCA, KPCA and ICA used for?\n",
      "PCA (Principal Components Analysis), KPCA ( Kernel based Principal Component Analysis) and ICA (\n",
      "Independent Component Analysis) are important feature extraction techniques used for\n",
      "dimensionality reduction.\n",
      "40)   What is dimension reduction in Machine Learning?\n",
      "In Machine Learning and statistics, dimension reduction is the process of reducing the number of\n",
      "random variables under considerations and can be divided into feature selection and feature\n",
      "extraction\n",
      "41)   What are support vector machines?\n",
      "Support vector machines are supervised learning algorithms used for classiﬁcation and regression\n",
      "analysis.\n",
      "42)   What are the components of relational evaluation techniques?\n",
      "The important components of relational evaluation techniques are\n",
      "a)      Data Acquisition\n",
      "b)      Ground Truth Acquisition\n",
      "c)       Cross Validation Technique\n",
      "d)      Query Type\n",
      "e)      Scoring Metric\n",
      "f)       Signiﬁcance Test\n",
      "43)   What are the diﬀerent methods for Sequential Supervised Learning?\n",
      "The diﬀerent methods to solve Sequential Supervised Learning problems are\n",
      "a)      Sliding-window methods\n",
      "b)      Recurrent sliding windows\n",
      "c)       Hidden Markow models\n",
      "d)      Maximum entropy Markow models\n",
      "e)      Conditional random ﬁelds\n",
      "\n",
      "\n",
      "287. f)       Graph transformer networks\n",
      "44)   What are the areas in robotics and information processing where sequential\n",
      "prediction problem arises?\n",
      "The areas in robotics and information processing  where sequential prediction problem arises are\n",
      "a)      Imitation Learning\n",
      "b)      Structured prediction\n",
      "c)       Model based reinforcement learning\n",
      "45)   What is batch statistical learning?\n",
      "Statistical learning techniques allow learning a function or predictor from a set of observed data that\n",
      "can make predictions about unseen or future data. These techniques provide guarantees on the\n",
      "performance of the learned predictor on the future unseen data based on a statistical assumption on\n",
      "the data generating process.\n",
      "46)   What is PAC Learning?\n",
      "PAC (Probably Approximately Correct) learning is a learning framework that has been introduced to\n",
      "analyze learning algorithms and their statistical eﬃciency.\n",
      "47)    What are the diﬀerent categories you can categorized the sequence learning\n",
      "process?\n",
      "a)      Sequence prediction\n",
      "b)      Sequence generation\n",
      "c)       Sequence recognition\n",
      "d)      Sequential decision\n",
      "48)   What is sequence learning?\n",
      "Sequence learning is a method of teaching and learning in a logical manner.\n",
      "49)   What are two techniques of Machine Learning ?\n",
      "The two techniques of Machine Learning are\n",
      "a)      Genetic Programming\n",
      "b)      Inductive Learning\n",
      "50)   Give a popular application of machine learning that you see on day to day basis?\n",
      "The recommendation engine implemented by major ecommerce websites uses Machine Learning\n",
      " Guru99  Provides  FREE ONLINE TUTORIAL   on Various courses like\n",
      "\n",
      "\n",
      "288. Java\n",
      "MIS\n",
      "MongoDB\n",
      "BigData\n",
      "Cassandra\n",
      "Web Services\n",
      "SQLite\n",
      "JSP\n",
      "Informatica\n",
      "Accounting\n",
      "SAP Training\n",
      "Python\n",
      "Excel\n",
      "ASP Net\n",
      "HBase\n",
      "Project\n",
      "Management\n",
      "Test Management\n",
      "Business Analyst\n",
      "Ethical Hacking\n",
      "PMP\n",
      "Live Project\n",
      "SoapUI\n",
      "Photoshop\n",
      "Manual Testing\n",
      "Mobile Testing\n",
      "Data Warehouse\n",
      "R Tutorial\n",
      "Tableau\n",
      "DevOps\n",
      "AWS\n",
      "Jenkins\n",
      "Agile Testing\n",
      "RPA\n",
      "JUnit\n",
      "Software\n",
      "Engineering\n",
      "Selenium\n",
      "CCNA\n",
      "AngularJS\n",
      "NodeJS\n",
      "PLSQL\n",
      "                                     \n",
      "\n",
      "\n",
      "289. 1 \n",
      " \n",
      " \n",
      "DIGITAL NOTES \n",
      "ON \n",
      "Machine Learning \n",
      "(R20D5803) \n",
      "M.Tech., II YEAR – I SEM \n",
      "(2021-2022) \n",
      " \n",
      " \n",
      " \n",
      "DEPARTMENT OF COMPUTER SCIENCE AND \n",
      "ENGINEERING \n",
      " \n",
      "MALLA REDDY COLLEGE OF ENGINEERING & TECHNOLOGY \n",
      "(Autonomous Institution – UGC, Govt. of India) \n",
      "(Affiliated to JNTUH, Hyderabad, Approved by AICTE - Accredited by NBA & NAAC – ‘A’ Grade - ISO 9001:2015 Certified) \n",
      "Maisammaguda, Dhulapally (Post Via. Hakimpet), Secunderabad – 500100, Telangana State, INDIA. \n",
      "\n",
      "\n",
      "290. 2 \n",
      " \n",
      "MALLA REDDY COLLEGE OF ENGINEERING & TECHNOLOGY \n",
      "DEPARTMENT OF COMPUTER SCIENCE AND ENGINEERING \n",
      "SYLLABUS  \n",
      "II Year M. Tech. CSE – I Sem \n",
      "L/T/P/ \n",
      "C \n",
      "3 / - / - \n",
      " 3 \n",
      "(R20D5803) Machine Learning \n",
      "Objectives: \n",
      "1. This course explains machine learning techniques such as decision tree learning, \n",
      "Bayesian learning etc. \n",
      "2. To understand computational learning theory. \n",
      "3. To study the pattern comparison techniques. \n",
      " \n",
      "UNIT - I \n",
      "Introduction Well-posed learning problems, designing a learning system Perspectives and issues in \n",
      "machine learning Concept learning and the general to specific ordering Introduction,A concept learning \n",
      "task, concept learning as search, Find-S: Finding a Maximally Specific Hypothesis, Version Spaces and \n",
      "the Candidate Elimination algorithm, Remarks on Version Spaces and Candidate Elimination, Inductive \n",
      "Bias. Decision Tree Learning-Introduction, Decision Tree Representation, Appropriate Problems for \n",
      "Decision Tree Learning, The Basic Decision Tree Learning Algorithm Hypothesis Space Search in \n",
      "Decision Tree Learning, Inductive Bias in Decision Tree Learning, Issues in Decision Tree Learning. \n",
      " \n",
      "UNIT - II \n",
      "Artificial Neural Networks -Introduction, Neural Network Representation, Appropriate Problems for \n",
      "Neural Network Learning, Perceptions, Multilayer Networks and the Back propagation Algorithm. \n",
      "Discussion on the Back Propagation Algorithm, An illustrative Example: Face Recognition \n",
      " \n",
      "UNIT - III \n",
      "Bayesian learning-Introduction, Byes Theorem, Bayes Theorem and Concept Learning Maximum \n",
      "Likelihood and Least Squared Error Hypotheses, Maximum Likelihood Hypotheses for Predicting \n",
      "Probabilities, Minimum Description Length Principle, Bayes Optimal Classifier, Gibs Algorithm, Naïve \n",
      "Bayes Classifier, An Example: Learning to Classify Text, Bayesian Belief Networks, EM Algorithm. \n",
      "Instance-Based Learning-Introduction, k-Nearest Neighbor Learning, Locally Weighted Regression, \n",
      "Radial Basis Functions, Case-Based Reasoning, Remarks on Lazy and Eager Learning. \n",
      " \n",
      "UNIT -IV \n",
      "Pattern Comparison Techniques-Temporal patterns, Dynamic Time Warping Methods,Clustering, \n",
      "Introduction to clustering, K-means clustering, K-Mode Clustering. Codebook Generation, Vector \n",
      "Quantization. \n",
      " \n",
      "UNIT - V \n",
      "Genetic Algorithms: Different search methods for induction - Explanation-based Learning: using prior \n",
      "knowledge to reduce sample complexity. Dimensionality reduction: feature selection, principal \n",
      "component analysis, linear discriminate analysis, factor analysis, independent component analysis, \n",
      "multidimensional scaling, and manifold learning. \n",
      "\n",
      "\n",
      "291. 3 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Textbooks: \n",
      "1. Machine Learning – Tom M. Mitchell, -MGH \n",
      "2. Fundamentals of Speech Recognition By Lawrence Rabiner and Biing – Hwang \n",
      "Juang .Ethem Alpaydin, ”Introduction to Machine Learning”, MIT Press, \n",
      "Prentice Hall of India, 3 rd Edition2014. \n",
      "3.  Mehryar Mohri, Afshin Rostamizadeh, Ameet Talwalkar ” Foundations of Machine \n",
      "Learning”,MIT Press,2012 \n",
      "References: \n",
      "1. Machine Learning : An Algorithmic Perspective, Stephen Marsland, Taylor & Francis . \n",
      "\n",
      "\n",
      "292. 4 \n",
      " \n",
      " \n",
      " \n",
      "INDEX \n",
      " \n",
      "S. No \n",
      "Unit \n",
      "Topic \n",
      "Page no \n",
      "1 \n",
      "I \n",
      "Introduction Well-posed learning problems \n",
      "1 \n",
      "2 \n",
      "I \n",
      "A concept learning task, concept learning as search \n",
      "6 \n",
      "3 \n",
      "I \n",
      "Find-S: Finding a Maximally Specific Hypothesis \n",
      "15 \n",
      "4 \n",
      "I \n",
      "Version Spaces and the Candidate Elimination \n",
      "algorithm \n",
      "17 \n",
      "5 \n",
      "I \n",
      "Remarks on Version Spaces and Candidate \n",
      "Elimination, Inductive Bias \n",
      "21 \n",
      "6 \n",
      "I \n",
      "Decision Tree Learning-Introduction, Decision Tree \n",
      "Representation \n",
      "22 \n",
      "7 \n",
      "I \n",
      "Appropriate Problems for Decision Tree Learning \n",
      "23 \n",
      "8 \n",
      "I \n",
      "Decision Tree Learning Algorithm, Issues in Decision \n",
      "Tree Learning. \n",
      "25 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "S. No \n",
      "Unit \n",
      "Topic \n",
      "Page no \n",
      "1 \n",
      "II \n",
      "Artificial Neural Networks -Introduction, \n",
      "Neural Network Representation \n",
      "26 \n",
      "2 \n",
      "II \n",
      "Appropriate Problems for Neural Network \n",
      "Learning \n",
      "28 \n",
      "3 \n",
      "II \n",
      "Perceptions, Multilayer Networks & the Back \n",
      "propagation Algorithm. \n",
      "29 \n",
      "4 \n",
      "II \n",
      "Discussion on the Back Propagation Algorithm \n",
      "34 \n",
      "MALLA REDDY COLLEGE OF ENGINEERING & TECHNOLOGY \n",
      "DEPARTMENT OF COMPUTER SCIENCE AND ENGINEERING \n",
      "\n",
      "\n",
      "293. 5 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "S. No \n",
      "Unit \n",
      "Topic \n",
      "Page no \n",
      "1 \n",
      "III \n",
      "Bayesian learning-Introduction ,Bayes \n",
      "Theorem & Concept Learning maximum \n",
      "36 \n",
      "2 \n",
      "III \n",
      "Maximum \n",
      "Likelihood Hypotheses for Predicting \n",
      "Probabilities(MAP) \n",
      "42 \n",
      "3 \n",
      "III \n",
      "Gibs Algorithm, Naïve Bayes Classifier \n",
      "46 \n",
      "4 \n",
      "III \n",
      "Minimum Description Length Principle , Bayes \n",
      "Optimal Classifier \n",
      "47 \n",
      "5 \n",
      "III \n",
      "An Example: Learning to Classify Text, Bayesian \n",
      "Belief Networks \n",
      "50 \n",
      "6 \n",
      "III \n",
      "EM Algorithm. Instance-Based Learning-Introduction \n",
      "51 \n",
      "7 \n",
      "III \n",
      "k-Nearest Neighbor Learning, Locally Weighted \n",
      "Regression \n",
      "55 \n",
      "8 \n",
      "III \n",
      "Radial Basis Functions, Case-Based Reasoning \n",
      "56 \n",
      "9 \n",
      "III \n",
      "Remarks on Lazy and Eager Learning. \n",
      "57 \n",
      "MALLA REDDY COLLEGE OF ENGINEERING & TECHNOLOGY \n",
      "DEPARTMENT OF COMPUTER SCIENCE AND ENGINEERING \n",
      "\n",
      "\n",
      "294. 6 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "S. No \n",
      "Unit \n",
      "Topic \n",
      "Page no \n",
      "1 \n",
      "IV \n",
      "Pattern Comparison Techniques-Temporal patterns, \n",
      "58 \n",
      "2 \n",
      "IV \n",
      "Dynamic Time Warping Methods \n",
      "61 \n",
      "3 \n",
      "IV \n",
      "Clustering \n",
      "67 \n",
      "5 \n",
      "IV \n",
      "K-means clustering \n",
      "69 \n",
      "6 \n",
      "IV \n",
      "K-Mode Clustering. Codebook Generation \n",
      "70 \n",
      "7 \n",
      "IV \n",
      "Vector Quantization. \n",
      "76 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "S. No \n",
      "Unit \n",
      "Topic \n",
      "Page no \n",
      "1 \n",
      "V \n",
      "Genetic Algorithms: Different search methods \n",
      "for induction \n",
      "78 \n",
      "2 \n",
      "V \n",
      "Explanation-based Learning: using prior knowledge to \n",
      "reduce sample complexity. \n",
      "79 \n",
      "3 \n",
      "V \n",
      "Dimensionality reduction \n",
      "82 \n",
      "4 \n",
      "V \n",
      "Principal component analysis \n",
      "84 \n",
      "5 \n",
      "V \n",
      "Linear discriminate analysis, factor analysis, \n",
      "85 \n",
      "6 \n",
      "V \n",
      "Independent component analysis: multidimensional \n",
      "scaling, and manifold learning. \n",
      "86 \n",
      "MALLA REDDY COLLEGE OF ENGINEERING & TECHNOLOGY \n",
      "DEPARTMENT OF COMPUTER SCIENCE AND ENGINEERING \n",
      "\n",
      "\n",
      "295. Department of CSE \n",
      "MRCET \n",
      "1 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "UNIT-I \n",
      " \n",
      " \n",
      "Machine Learning \n",
      "is the field of study that gives computers the capability to learn without \n",
      "being explicitly programmed. ML is one of the most exciting technologies \n",
      "that one would have ever come across. As it is evident from the name, it \n",
      "gives the computer that makes it more similar to humans: The ability to \n",
      "learn. Machine learning is actively being used today, perhaps in many more \n",
      "places than one would expect. \n",
      " \n",
      "Machine Learning is broadly categorized under the following headings: \n",
      " \n",
      " \n",
      "Machine learning evolved from left to right as shown in the above diagram. \n",
      "• Initially, researchers started out with Supervised Learning. This is the \n",
      "case of housing price prediction discussed earlier \n",
      ". • This was followed by unsupervised learning, where the machine is made \n",
      "to learn on its own without any supervision. \n",
      "• Scientists discovered further that it may be a good idea to reward the \n",
      "machine when it does the job the expected way and there came the \n",
      "Reinforcement Learning. \n",
      "• Very soon, the data that is available these days has become so humongous \n",
      "that the conventional techniques developed so far failed to analyse the big \n",
      "data and provide us the predictions. \n",
      "\n",
      "\n",
      "296. Department of CSE \n",
      "MRCET \n",
      "2 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "• Thus, came the deep learning where the human brain is simulated in the \n",
      "Artificial Neural Networks (ANN) created in our binary computers. \n",
      "• The machine now learns on its own using the high computing power and \n",
      "huge memory resources that are available today. \n",
      "• It is now observed that Deep Learning has solved many of the previously \n",
      "unsolvable problems. \n",
      "• The technique is now further advanced by giving incentives to Deep \n",
      "Learning networks as awards and there finally comes Deep Reinforcement \n",
      "Learning. \n",
      "Let us now study each of these categories in more details \n",
      "Supervised Learning: \n",
      "Supervised learning is analogous to training a child to walk. You will hold \n",
      "the child’s hand, show him how to take his foot forward, walk yourself for a \n",
      "demonstration and so on, until the child learns to walk on his own. \n",
      "Regression: \n",
      "Similarly, in the case of supervised learning, you give concrete known \n",
      "examples to the computer. You say that for given feature value x1 the output \n",
      "is y1, for x2 it is y2, for x3 it is y3, and so on. Based on this data, you let the \n",
      "computer figure out an empirical relationship between x and y. Once the \n",
      "machine is trained in this way with a sufficient number of data points, now \n",
      "you would ask the machine to predict Y for a given X. Assuming that you \n",
      "know the real value of Y for this given X, you will be able to deduce whether \n",
      "the machine’s prediction is correct. Thus, you will test whether the machine \n",
      "has learned by using the known test data. Once you are satisfied that the \n",
      "machine is able to do the predictions with a desired level of accuracy (say 80 \n",
      "to 90%) you can stop further training the machine. Now, you can safely use \n",
      "the machine to do the predictions on unknown data points, or ask the \n",
      "machine to predict Y for a given X for which you do not know the real value \n",
      "of Y. This training comes under the regression that we talked about earlier. \n",
      "\n",
      "\n",
      "297. Department of CSE \n",
      "MRCET \n",
      "3 \n",
      " \n",
      " \n",
      " \n",
      "Classification: \n",
      "You may also use machine learning techniques for classification problems. In \n",
      "classification problems, you classify objects of similar nature into a single \n",
      "group. For example, in a set of 100 students say, you may like to group them \n",
      "into three groups based on their heights - short, medium and long. Measuring \n",
      "the height of each student, you will place them in a proper group. Now, when \n",
      "a new student comes in, you will put him in an appropriate group by \n",
      "measuring his height. By following the principles in regression training, you \n",
      "will train the machine to classify a student based on his feature – the height. \n",
      "When the machine learns how the groups are formed, it will be able to \n",
      "classify any unknown new student correctly. Once again, you would use the \n",
      "test data to verify that the machine has learned your technique of \n",
      "classification before putting the developed model in production. Supervised \n",
      "Learning is where the AI really began its journey. This technique was \n",
      "applied successfully in several cases. You have used this model while doing \n",
      "the hand-written recognition on your machine. Several algorithms have been \n",
      "developed for supervised learning. You will learn about them in the \n",
      "following chapters. \n",
      "Unsupervised Learning: \n",
      "In unsupervised learning, we do not specify a target variable to the machine, \n",
      "rather we ask machine “What can you tell me about X?”. More specifically, \n",
      "we may ask questions such as given a huge data set X, “What are the five \n",
      "best groups we can make out of X?” or “What features occur together most \n",
      "frequently in X?”. To arrive at the answers to such questions, you can \n",
      "understand that the number of data points that the machine would require to \n",
      "deduce a strategy would be very large. In case of supervised learning, the \n",
      "machine can be trained with even about few thousands of data points. \n",
      "However, in case of unsupervised learning, the number of data points that is \n",
      "reasonably accepted for learning starts in a few millions. These days, the data \n",
      "is generally abundantly available. The data ideally requires curating. \n",
      "However, the amount of data that is continuously flowing in a social area \n",
      "network, in most cases data curation is an impossible task. The following \n",
      "figure shows the boundary between the yellow and red dots as determined by \n",
      "unsupervised machine learning. You  can see it clearly  that the machine \n",
      "\n",
      "\n",
      "298. Department of CSE \n",
      "MRCET \n",
      "4 \n",
      " \n",
      " \n",
      " \n",
      "would be able to determine the class of each of the black dots with a fairly \n",
      "good accuracy. \n",
      " \n",
      " \n",
      "Reinforcement Learning: \n",
      "Consider training a pet dog, we train our pet to bring a ball to us. We throw \n",
      "the ball at a certain distance and ask the dog to fetch it back to us. Every time \n",
      "the dog does this right, we reward the dog. Slowly, the dog learns that doing \n",
      "the job rightly gives him a reward and then the dog starts doing the job right \n",
      "way every time in future. Exactly, this concept is applied in “Reinforcement” \n",
      "type of learning. The technique was initially developed for machines to play \n",
      "games. The machine is given an algorithm to analyse all possible moves at \n",
      "each stage of the game. The machine may select one of the moves at random. \n",
      "If the move is right, the machine is rewarded, otherwise it may be penalized. \n",
      "Slowly, the machine will start differentiating between right and wrong moves \n",
      "and after several iterations would learn to solve the game puzzle with a better \n",
      "accuracy. The accuracy of winning the game would improve as the machine \n",
      "plays more and more games. \n",
      "The entire process may be depicted in the following diagram: \n",
      "\n",
      "\n",
      "299. Department of CSE \n",
      "MRCET \n",
      "5 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Deep Learning: \n",
      "The deep learning is a model based on Artificial Neural Networks (ANN), \n",
      "more specifically Convolutional Neural Networks (CNN)s. There are several \n",
      "architectures used in deep learning such as deep neural networks, deep belief \n",
      "networks, recurrent neural networks, and convolutional neural networks. \n",
      "These networks have been successfully applied in solving the problems of \n",
      "computer \n",
      "vision, speech recognition, natural \n",
      "language processing, \n",
      "bioinformatics, drug design, medical image analysis, and games. There are \n",
      "several other fields in which deep learning is proactively applied. The deep \n",
      "learning requires huge processing power and humongous data, which is \n",
      "generally easily available these days. We will talk about deep learning more \n",
      "in detail in the coming chapters. \n",
      "Deep Reinforcement Learning: \n",
      "The Deep Reinforcement Learning (DRL) combines the techniques of both \n",
      "deep and reinforcement learning. The reinforcement learning algorithms like \n",
      "Q learning are now combined with deep learning to create a powerful DRL \n",
      "model. The technique has been with a great success in the fields of robotics, \n",
      "video games, finance and healthcare. Many previously unsolvable problems \n",
      "are now solved by creating DRL models. There is lots of research going on \n",
      "in this area and this is very actively pursued by the industries. So far, you \n",
      "\n",
      "\n",
      "300. Department of CSE \n",
      "MRCET \n",
      "6 \n",
      " \n",
      " \n",
      " \n",
      "have got a brief introduction to various machine learning models, now let us \n",
      "explore slightly deeper into various algorithms that are available under these \n",
      "models. \n",
      "Well posed learning problems: \n",
      " \n",
      "A computer program is said to learn from experience E in context to some \n",
      "task T and some performance measure P, if its performance on T, as was \n",
      "measured by P, upgrades with experience E. \n",
      "Any problem can be segregated as well-posed learning problem if it has three \n",
      "traits – \n",
      "• \n",
      "Task \n",
      "• \n",
      "Performance Measure \n",
      "• \n",
      "Experience \n",
      "Certain example that efficiently defines the well-posed learning problems \n",
      "are: \n",
      " \n",
      "1. To better filter emails as spam or not \n",
      "• \n",
      "Task – Classifying emails as spam or not \n",
      "• \n",
      "Performance Measure – The fraction of emails accurately classified as spam \n",
      "or not spam \n",
      "• \n",
      "Experience – Observing you label emails as spam or not spam \n",
      "2. A checkers learning problem \n",
      "• \n",
      "Task – Playing checkers game \n",
      "• \n",
      "Performance Measure – percent of games won against opposer \n",
      "• \n",
      "Experience – playing implementation games against itself \n",
      "3. Handwriting Recognition Problem \n",
      "• \n",
      "Task – Acknowledging handwritten words within portrayal \n",
      "• \n",
      "Performance Measure – percent of words accurately classified \n",
      "• \n",
      "Experience – a directory of handwritten words with given classifications \n",
      "4. A Robot Driving Problem \n",
      "• \n",
      "Task – driving on public four-lane highways using sight scanners \n",
      "• \n",
      "Performance Measure – average distance progressed before a fallacy \n",
      "• \n",
      "Experience – order of images and steering instructions noted down while \n",
      "observing a human driver \n",
      "5. Fruit Prediction Problem \n",
      "\n",
      "\n",
      "301. Department of CSE \n",
      "MRCET \n",
      "7 \n",
      " \n",
      " \n",
      " \n",
      "• \n",
      "Task – forecasting different fruits for recognition \n",
      "• \n",
      "Performance Measure – able to predict maximum variety of fruits \n",
      "• \n",
      "Experience – training machine with the largest datasets of fruits images \n",
      " \n",
      "6. Face Recognition Problem \n",
      "• \n",
      "Task – predicting different types of faces \n",
      "• \n",
      "Performance Measure – able to predict maximum types of faces \n",
      "• \n",
      "Experience – training machine with maximum amount of datasets of \n",
      "different face images \n",
      "7. Automatic Translation of documents \n",
      "• \n",
      "Task – translating one type of language used in a document to other language \n",
      "• \n",
      "Performance Measure – able to convert one language to other efficiently \n",
      "• \n",
      "Experience – training machine with a large dataset of different types of \n",
      "languages \n",
      " \n",
      "Design of a learning system: \n",
      " \n",
      "Just now we looked into the learning process and also understood the goal \n",
      "of the learning. When we want to design a learning system that follows the \n",
      "learning process, we need to consider a few design choices. The design \n",
      "choices will be to decide the following key components: \n",
      "1. \n",
      "Type of training experience \n",
      "2. \n",
      "Choosing the Target Function \n",
      "3. \n",
      "Choosing a representation for the Target Function \n",
      "4. \n",
      "Choosing an approximation algorithm for the Target Function \n",
      "5. \n",
      "The final Design \n",
      "We will look into the game - checkers learning problem and apply the above \n",
      "design choices. For a checkers learning problem, the three elements will be, \n",
      "• Task T: To play checkers \n",
      "• Performance measure P: Total present of the game won in the tournament. \n",
      "• Training experience E: A set of games played against itself. \n",
      " \n",
      "Type of training experience: \n",
      "During the design of the checker's learning system, the type of training \n",
      "experience available for a learning system will have a significant effect on \n",
      "the success or failure of the learning. \n",
      "\n",
      "\n",
      "302. Department of CSE \n",
      "MRCET \n",
      "8 \n",
      " \n",
      " \n",
      " \n",
      "Direct or Indirect training experience: \n",
      "In the case of direct training experience, an individual board states and \n",
      "correct move for each board state are given. In case of indirect training \n",
      "experience, the move sequences for a game and the final result (win, lose or \n",
      "draw) are given for a number of games. How to assign credit or blame to \n",
      "individual moves is the credit assignment problem. \n",
      " \n",
      "1. Teacher or Not: \n",
      " Supervised: \n",
      "The training experience will be labelled, which means, all the board states \n",
      "will be labelled with the correct move. So the learning takes place in the \n",
      "presence of a supervisor or a teacher. \n",
      " Un-Supervised: \n",
      "The training experience will be unlabelled, which means, all the board \n",
      "states will not have the moves. So the learner generates random games and \n",
      "plays against itself with no supervision or teacher involvement. \n",
      " \n",
      " Semi-supervised: \n",
      "Learner generates game states and asks the teacher for help in finding \n",
      "the correct move if the board state is confusing. \n",
      "2. Is the training experience good: \n",
      " \n",
      " Do the training examples represent the distribution of examples over \n",
      "which the final system performance will be measured? Performance is best \n",
      "when training examples and test examples are from the same/a similar \n",
      "distribution. \n",
      " The checker player learns by playing against oneself. Its experience is \n",
      "indirect. It may not encounter moves that are common in human expert play. \n",
      "Once the proper training experience is available, the next design step will be \n",
      "choosing the Target Function. \n",
      "Choosing the Target Function: \n",
      "When you are playing the checkers game, at any moment of time, you make \n",
      "a decision on choosing the best move from different possibilities. You think \n",
      "and apply the learning that you have gained from the experience. Here the \n",
      "learning is, for a specific board, you move a checker such that your board \n",
      "\n",
      "\n",
      "303. Department of CSE \n",
      "MRCET \n",
      "9 \n",
      " \n",
      " \n",
      " \n",
      "state tends towards the winning situation. Now the same learning has to be \n",
      "defined in terms of the target function. \n",
      "Here there are 2 considerations — direct and indirect experience. \n",
      "• During the direct experience the checkers learning system, it needs only \n",
      "to learn how to choose the best move among some large search space. We \n",
      "need to find a target function that will help us choose the best move among \n",
      "alternatives. \n",
      "Let us call this function Choose Move and use the notation Choose Move: B \n",
      "→M to indicate that this function accepts as input any board from the set of \n",
      "legal board states B and produces as output some move from the set of legal \n",
      "moves M. \n",
      "• When there is an indirect experience it becomes difficult to learn such \n",
      "function. How about assigning a real score to the board state. \n",
      " \n",
      " \n",
      "So the function be V: B →R indicating that this accepts as input any board \n",
      "from the set of legal board states B and produces an output a real score. This \n",
      "function assigns the higher scores to better board states \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "If the system can successfully learn such a target function V, then it can \n",
      "easily use it to select the best move from any board position. \n",
      "Let us therefore define the target value V(b) for an arbitrary board state b in \n",
      "B, as follows: \n",
      "\n",
      "\n",
      "304. Department of CSE \n",
      "MRCET \n",
      "10 \n",
      " \n",
      " \n",
      " \n",
      "1. if b is a final board state that is won, then V(b) = 100 \n",
      "2. if b is a final board state that is lost, then V(b) = -100 \n",
      "3. if b is a final board state that is drawn, then V(b) = 0 \n",
      "4. if b is a not a final state in the game, then V (b) = V (b’), where b’ is the best \n",
      "final board state that can be achieved starting from b and playing optimally \n",
      "until the end of the game. \n",
      "The (4) is a recursive definition and to determine the value of V(b) for a \n",
      "particular board state, it performs the search ahead for the optimal line of \n",
      "play, all the way to the end of the game. So this definition is not efficiently \n",
      "computable by our checkers playing program, we say that it is a non- \n",
      "operational definition. \n",
      " \n",
      " \n",
      "Choosing a representation for the Target Function: \n",
      "Now that we have specified the ideal target function V, we must choose a \n",
      "representation that the learning program will use to describe the function ^V \n",
      "that it will learn. As with earlier design choices, we again have many options. \n",
      "We could, for example, allow the program to represent using a large table \n",
      "with a distinct entry specifying the value for each distinct board state. Or we \n",
      "could allow it to represent using a collection of rules that match against \n",
      "features of the board state, or a quadratic polynomial function of predefined \n",
      "board features, or an artificial neural network. In general, this choice of \n",
      "representation involves a crucial trade off. On one hand, we wish to pick a \n",
      "very expressive representation to allow representing as close an \n",
      "approximation as possible to the ideal target function V. \n",
      "On the other hand, the more expressive the representation, the more training \n",
      "data the program will require in order to choose among the alternative \n",
      "hypotheses it can represent. To keep the discussion brief, let us choose a \n",
      "simple representation: for any given board state, the function ^V will be \n",
      "calculated as a linear combination of the following board features: \n",
      "• x1(b) — number of black pieces on board b \n",
      "• x2(b) — number of red pieces on b \n",
      "• x3(b) — number of black kings on b \n",
      "\n",
      "\n",
      "305. Department of CSE \n",
      "MRCET \n",
      "11 \n",
      " \n",
      " \n",
      " \n",
      "• x4(b) — number of red kings on b \n",
      "• x5(b) — number of red pieces threatened by black • x6(b) — number of \n",
      "black pieces threatened by red \n",
      "^V = w0 + w1 · x1(b) + w2 · x2(b) + w3 · x3(b) + w4 · x4(b) +w5 · x5(b) + w6 · x6(b) \n",
      " \n",
      " \n",
      "Where w0 through w6 are numerical coefficients or weights to be obtained \n",
      "by a learning algorithm. Weights w1 to w6 will determine the relative \n",
      "importance of different board features. \n",
      "Specification of the Machine Learning Problem at this time: Till now we \n",
      "worked on choosing the type of training experience, choosing the target \n",
      "function and its representation. The checkers learning task can be \n",
      "summarized as below. \n",
      "• Task T: Play Checkers \n",
      "• Performance Measure: % of games won in world tournament \n",
      "• Training Experience E: opportunity to play against itself \n",
      "• Target Function: V: Board → R \n",
      "• Target Function Representation: ^V = w0 + w1 · x1(b) + w2 · x2(b) + w3 · \n",
      "x3(b) + w4 · x4(b) +w5 · x5(b) + w6 · x6(b) \n",
      "The first three items above correspond to the specification of the learning \n",
      "task, where as the final two items constitute design choices for the \n",
      "implementation of the learning program. \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Choosing an approximation algorithm for the Target Function: \n",
      "Generating training data — To train our learning program, we need a set of \n",
      "training data, each describing a specific board state b and the training value \n",
      "V_train (b) for b. Each training example is an ordered pair <b,v_train(b)>. \n",
      "\n",
      "\n",
      "306. Department of CSE \n",
      "MRCET \n",
      "12 \n",
      " \n",
      " \n",
      " \n",
      "Temporal difference (TD) learning is a concept central to reinforcement \n",
      "learning, in which learning happens through the iterative correction of your \n",
      "estimated returns towards a more accurate target return. \n",
      " V_train(b) ← ^V(Successor(b)) \n",
      " \n",
      "Final Design for Checkers Learning system: \n",
      "The final design of our checkers learning system can be naturally described \n",
      "by four distinct program modules that represent the central components in \n",
      "many learning systems. \n",
      "1. The performance System: Takes a new board as input and outputs a trace of \n",
      "the game it played against itself. \n",
      "2. The Critic: Takes the trace of a game as an input and outputs a set of training \n",
      "examples of the target function. \n",
      "3. The Generalizer: Takes training examples as input and outputs a hypothesis \n",
      "that estimates the target function. Good generalization to new cases is \n",
      "crucial. \n",
      "4. The Experiment Generator: Takes the current hypothesis (currently learned \n",
      "function) as input and outputs a new problem (an initial board state) for the \n",
      "performance system to explore. \n",
      " \n",
      " \n",
      " \n",
      "Issues in Machine Learning: \n",
      "\n",
      "\n",
      "307. Department of CSE \n",
      "MRCET \n",
      "13 \n",
      " \n",
      " \n",
      " \n",
      "Our checkers example raises a number of generic questions about machine \n",
      "learning. The field of machine learning, and much of this book, is concerned \n",
      "with answering questions such as the following: \n",
      "• What algorithms exist for learning general target functions from specific \n",
      "training examples? In what settings will particular algorithms converge to the \n",
      "desired function, given sufficient training data? Which algorithms perform \n",
      "best for which types of problems and representations? \n",
      "• How much training data is sufficient? What general bounds can be found to \n",
      "relate the confidence in learned hypotheses to the amount of training \n",
      "experience and the character of the learner's hypothesis space? \n",
      "• When and how can prior knowledge held by the learner guide the process of \n",
      "generalizing from examples? Can prior knowledge be helpful even when it is \n",
      "only approximately correct? \n",
      "• What is the best strategy for choosing a useful next training experience, and \n",
      "how does the choice of this strategy alter the complexity of the learning \n",
      "problem? \n",
      "• What is the best way to reduce the learning task to one or more function \n",
      "approximation problems? Put another way, what specific functions should \n",
      "the system attempt to learn? Can this process itself be automated? \n",
      "• How can the learner automatically alter its representation to improve its \n",
      "ability to represent and learn the target function? \n",
      "CONCEPT LEARNING: \n",
      " \n",
      "• Inducing general functions from specific training examples is a main issue of \n",
      "machine learning. \n",
      "• Concept Learning: Acquiring the definition of a general category from \n",
      "given sample positive and negative training examples of the category. \n",
      "• Concept Learning can see as a problem of searching through a predefined \n",
      "space of potential hypotheses for the hypothesis that best fits the training \n",
      "examples. \n",
      "• The hypothesis space has a general-to-specific ordering of hypotheses, and \n",
      "the search can be efficiently organized by taking advantage of a naturally \n",
      "occurring structure over the hypothesis space. \n",
      "A Formal Definition for Concept Learning: \n",
      "\n",
      "\n",
      "308. Department of CSE \n",
      "MRCET \n",
      "14 \n",
      " \n",
      " \n",
      " \n",
      "Inferring a Boolean-valued function from training examples of its input and \n",
      "output. \n",
      "• An example for concept-learning is the learning of bird-concept from the \n",
      "given examples of birds (positive examples) and non-birds (negative \n",
      "examples). \n",
      "• We are trying to learn the definition of a concept from given examples. \n",
      "A Concept Learning Task: Enjoy Sport Training Examples \n",
      " \n",
      " \n",
      "A set of example days, and each is described by six attributes. The task is to \n",
      "learn to predict the value of Enjoy Sport for arbitrary day, based on the \n",
      "values of its attribute values. \n",
      " \n",
      " \n",
      "Concept Learning as Search: \n",
      "• \n",
      "Concept learning can be viewed as the task of searching through a large \n",
      "space of hypotheses implicitly defined by the hypothesis representation. \n",
      "• \n",
      "The goal of this search is to find the hypothesis that best fits the training \n",
      "examples. \n",
      "• \n",
      "By selecting a hypothesis representation, the designer of the learning \n",
      "algorithm implicitly defines the space of all hypotheses that the program can \n",
      "ever represent and therefore can ever learn. \n",
      "\n",
      "\n",
      "309. Department of CSE \n",
      "MRCET \n",
      "15 \n",
      " \n",
      " \n",
      " \n",
      "FIND-S: \n",
      "• \n",
      "FIND-S Algorithm starts from the most specific hypothesis and generalize it \n",
      "by considering only positive examples. \n",
      "• \n",
      "FIND-S algorithm ignores negative example \n",
      ": As long as the hypothesis space contains a hypothesis that describes the \n",
      "true target concept, and the training data contains no errors, ignoring \n",
      "negative examples does not cause to any problem. \n",
      "• \n",
      "FIND-S algorithm finds the most specific hypothesis within H that is \n",
      "consistent with the positive training examples. – The final hypothesis will \n",
      "also be consistent with negative examples if the correct target concept is in \n",
      "H, and the training examples are correct. \n",
      "FIND-S Algorithm: \n",
      "1. \n",
      "Initialize h to the most specific hypothesis in H \n",
      "2. \n",
      "For each positive training instance x \n",
      "For each attribute \n",
      "constraint a, in h \n",
      "If the constraint a, is satisfied by x \n",
      "Then do nothing \n",
      "3. Else replace a, in h by the next more general constraint that is satisfied by \n",
      "x 4. Output hypothesis h \n",
      "FIND-S Algorithm – Example: \n",
      "Important-Representation: \n",
      " \n",
      "1. ? indicates that any value is acceptable for the attribute. \n",
      "2. specify a single required value (e.g., Cold) for the attribute. \n",
      "3. Φ indicates that no value is acceptable. \n",
      "4. The most general hypothesis is represented by: {?, ?, ?, ?, ?, ?} \n",
      "5. The most specific hypothesis is represented by: {ϕ, ϕ, ϕ, ϕ, ϕ, ϕ} \n",
      " \n",
      "Steps Involved in Find-S: \n",
      "1. Start with the most specific hypothesis. h = {ϕ, ϕ, ϕ, ϕ, ϕ, ϕ} \n",
      "\n",
      "\n",
      "310. Department of CSE \n",
      "MRCET \n",
      "16 \n",
      " \n",
      " \n",
      " \n",
      "2. Take the next example and if it is negative, then no changes occur to the \n",
      "hypothesis. \n",
      "3. If the example is positive and we find that our initial hypothesis is too \n",
      "specific then we update our current hypothesis to a general condition. \n",
      "4. Keep repeating the above steps till all the training examples are complete. \n",
      "5. After we have completed all the training examples we will have the final \n",
      "hypothesis when can use to classify the new examples. Example: Consider \n",
      "the following data set having the data about which particular seeds are \n",
      "poisonous. \n",
      " \n",
      " \n",
      " \n",
      "First, we consider the hypothesis to be a more specific hypothesis. Hence, \n",
      "our hypothesis would be: h = {ϕ, ϕ, ϕ, ϕ, ϕ, ϕ} \n",
      " \n",
      " \n",
      "Consider example 1: \n",
      "The data in example 1 is {GREEN, HARD, NO, WRINKLED}. We see that \n",
      "our initial hypothesis is more specific and we have to generalize it for this \n",
      "example. \n",
      "Hence, the hypothesis becomes: \n",
      "h = {GREEN, HARD, NO, WRINKLED} \n",
      "Consider example 2: \n",
      "\n",
      "\n",
      "311. Department of CSE \n",
      "MRCET \n",
      "17 \n",
      " \n",
      " \n",
      " \n",
      "Here we see that this example has a negative outcome. Hence we neglect \n",
      "this example and our hypothesis remains the same. h = {GREEN, \n",
      "HARD, NO, WRINKLED} \n",
      "Consider example 3: \n",
      "Here we see that this example has a negative outcome. hence we neglect \n",
      "this example and our hypothesis remains the same. h = {GREEN, \n",
      "HARD, NO, WRINKLED} \n",
      "Consider example 4: \n",
      "The data present in example 4 is {ORANGE, HARD, NO, WRINKLED}. \n",
      "We \n",
      "compare every single attribute with the initial data and if any mismatch is \n",
      "found we replace that particular attribute with a general case (“ ?”). After \n",
      "doing the process the hypothesis becomes: h = {?, HARD, \n",
      "NO, \n",
      "WRINKLED } \n",
      "Consider example 5: \n",
      "The data present in example 5 is {GREEN, SOFT, YES, SMOOTH}. We \n",
      "compare every single attribute with the initial data and if any mismatch is \n",
      "found we replace that particular attribute with a general case ( “?” ). After \n",
      "doing the process the hypothesis becomes: \n",
      "h = {?, ?, ?, ? } \n",
      "Since we have reached a point where all the attributes in our hypothesis \n",
      "have the general condition, example 6 and example 7 would result in the \n",
      "same hypothesizes with all general attributes. h = {?, ?, ?, ? } \n",
      "Hence, for the given data the final hypothesis would be: \n",
      "Final Hypothesis: h = { ?, ?, ?, ? }. \n",
      " \n",
      "Version Spaces \n",
      "Definition(Version space). A concept is complete if it covers all positive \n",
      "examples. \n",
      "A concept is consistent if it covers none of the negative examples. The \n",
      "version space is the set of all complete and consistent concepts. This set is \n",
      "convex and is fully defined by its least and most general elements. \n",
      "Candidate-Elimination Learning Algorithm \n",
      "\n",
      "\n",
      "312. Department of CSE \n",
      "MRCET \n",
      "18 \n",
      " \n",
      " \n",
      " \n",
      "The CANDIDATE-ELIMINTION algorithm computes the version space \n",
      "containing all hypotheses from H that are consistent with an observed \n",
      "sequence of training examples. \n",
      "Initialize G to the set of maximally general hypotheses in H Initialize S to \n",
      "the set of maximally specific hypotheses in H For each training example d, \n",
      "do \n",
      "• \n",
      "If d is a positive example \n",
      "• \n",
      "Remove from G any hypothesis inconsistent with d \n",
      "• \n",
      "For each hypothesis s in S that is not consistent with d \n",
      "• \n",
      "Remove s from S • Add to S all minimal generalizations h of s such that h is \n",
      "consistent with d, and some member of G is more general than h \n",
      "• \n",
      "Remove from S any hypothesis that is more general than another hypothesis \n",
      "in S \n",
      "• \n",
      "If d is a negative example \n",
      "• \n",
      "Remove from S any hypothesis inconsistent with d \n",
      "• \n",
      "For each hypothesis g in G that is not consistent with d \n",
      "• \n",
      "Remove g from G 18\\ \n",
      "• \n",
      "Add to G all minimal specializations h of g such that \n",
      "• \n",
      "h is consistent with d, and some member of S is more specific than h \n",
      "• \n",
      "Remove from G any hypothesis that is less general than another hypothesis \n",
      "in G. \n",
      "CANDIDATE- ELIMINTION algorithm using version spaces An \n",
      "Illustrative Example: \n",
      " \n",
      "\n",
      "\n",
      "313. Department of CSE \n",
      "MRCET \n",
      "19 \n",
      " \n",
      " \n",
      " \n",
      "CANDIDATE-ELIMINTION algorithm begins by initializing the version \n",
      "space to the set of all hypotheses in H; \n",
      "boundary set to contain the most general hypothesis in H, G0 ?, ?, ?, ?, ?, \n",
      "When the first training example is presented, the \n",
      "CANDIDATEELIMINTION algorithm checks the S boundary and finds that \n",
      "it is overly specific and it fails to cover the positive example. \n",
      "• \n",
      "The boundary is therefore revised by moving it to the least more general \n",
      "hypothesis that covers this new example. \n",
      "• \n",
      "No update of the G boundary is needed in response to this training example \n",
      "because Go correctly covers this example. \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "• \n",
      "When the second training example is observed, it has a similar effect of \n",
      "generalizing S further to S2, leaving G again unchanged i.e., G2 = G1 =G0 \n",
      "\n",
      "\n",
      "314. Department of CSE \n",
      "MRCET \n",
      "20 \n",
      " \n",
      " \n",
      " \n",
      "• \n",
      "Consider the third training example. This negative example reveals that the \n",
      "boundary of the version space is overly general, that is, the hypothesis in G \n",
      "incorrectly predicts that this new example is a positive example. \n",
      "• \n",
      "The hypothesis in the G boundary must therefore be specialized until it \n",
      "correctly classifies this new negative example. \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Given that there are six attributes that could be specified to specialize G2, \n",
      "why are there only three new hypotheses in G3? \n",
      " \n",
      "For example, the hypothesis h = (?, ?, Normal, ?, ?, ?) is a minimal \n",
      "specialization of G2 that correctly labels the new example as a negative \n",
      "example, but it is not included in G3. The reason this hypothesis is excluded \n",
      "is that it is inconsistent with the previously encountered positive examples. \n",
      "Consider the fourth training example. \n",
      "\n",
      "\n",
      "315. Department of CSE \n",
      "MRCET \n",
      "21 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "• \n",
      "This positive example further generalizes the S boundary of the version \n",
      "space. It also results in removing one member of the G boundary, because \n",
      "this member fails to cover the new positive example After processing these \n",
      "four examples, the boundary sets S4 and G4 delimit the version space of all \n",
      "hypotheses consistent with the set of incrementally observed training \n",
      "examples. \n",
      "• \n",
      "After processing these four examples, the boundary sets S4 and G4 delimit \n",
      "the version space of all hypotheses consistent with the set of incrementally \n",
      "observed training examples. \n",
      " \n",
      " \n",
      " \n",
      "Inductive bias: \n",
      "\n",
      "\n",
      "316. Department of CSE \n",
      "MRCET \n",
      " \n",
      " \n",
      "Decision Tre:e Decision Trees are a type of Supervised Machine Learning (that \n",
      "is you explain what the input is and what the corresponding output is in the \n",
      "training data) where th e data is continuously split according to a certain \n",
      "parameter. The tree can be explained by two entities, namely decision nodes and \n",
      "leaves. The leaves are the decisions or the final outcomes. And the decision nodes \n",
      "are where the data is split. \n",
      " \n",
      "Decision Tree Representation: \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "An example of a decision tree can be explained using above binary tree. Let’s say \n",
      "you want to predict whether a person is fit given their information like age, eating \n",
      "habit, and physical activity, etc. The decision nodes here are questions like \n",
      "‘What’s the age?’, ‘Does he exercise?’, and ‘Does he eat a lot of pizzas’? And the \n",
      "leaves, which are outcomes like either ‘fit’, or ‘unfit’. In this case this was a \n",
      "binary classification problem (a yes no type problem). There are two main types \n",
      "of Decision Trees: \n",
      " \n",
      "1. Classification trees (Yes/No types): \n",
      " \n",
      "What we have seen above is an example of classification tree, where the \n",
      "outcome was a variable like ‘fit’ or ‘unfit’. Here the decision variable is \n",
      "Categorical. \n",
      "Inductive bias refers to the restriction2s2 that are imposed by the assumptions \n",
      "\n",
      "\n",
      "317. Department of CSE \n",
      "MRCET \n",
      "23 \n",
      " \n",
      " \n",
      " \n",
      "Here the decision or the outcome variable is Continuous, e.g. a number like \n",
      "123. Working Now that we know what a Decision Tree is, we’ll see how it \n",
      "works internally. There are many algorithms out there which construct \n",
      "Decision Trees, but one of the best is called as ID3 Algorithm. ID3 Stands \n",
      "for Iterative Dichotomiser3. \n",
      " \n",
      "Before discussing the ID3 algorithm, we’ll go through few definitions. \n",
      "Entropy, also called as Shannon Entropy is denoted by H(S) for a finite set S, \n",
      "is the measure of the amount of uncertainty or randomness in data. \n",
      " \n",
      "Appropriate Problems for Decision Tree Learning: \n",
      "• \n",
      "Instances are represented by attribute-value pair \n",
      "• \n",
      "The target function has discrete output values \n",
      "• \n",
      "Disjunctive descriptions may be required \n",
      "• \n",
      "The training data may contain errors \n",
      "• \n",
      "The training data may contain missing attribute values. \n",
      "• \n",
      "Suitable for classifications. \n",
      " \n",
      "Hypothesis Space Search: \n",
      " \n",
      "The set of possible decision tree, Simple to complex, hill climbing search. \n",
      "Capability: \n",
      "• \n",
      "Hypothesis space of all decision trees is a complete space of finite discrete \n",
      "valued functions. \n",
      " \n",
      "• \n",
      "ID3 maintains only a single current hypothesis. \n",
      " \n",
      "• \n",
      "Cannot determine how many alternative decision trees are consistent with \n",
      "the available training data. \n",
      "\n",
      "\n",
      "318. Department of CSE \n",
      "MRCET \n",
      "24 \n",
      " \n",
      " \n",
      " \n",
      "• \n",
      "ID3 uses all training example at each step to make statistically based \n",
      "decisions regarding how to refine its current hypothesis. \n",
      " \n",
      "• \n",
      "The resulting search is much less sensitive to errors in individual training \n",
      "examples. \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Inductive Bias in Decision Tree Learning: Note H is the power set of \n",
      "instances X \n",
      " \n",
      "• \n",
      "Inductive Bias in ID3 – Approximate inductive bias of ID3 \n",
      " \n",
      " Shorter trees are preferred over larger tress \n",
      " \n",
      " BFS-ID3 \n",
      " \n",
      "Difference between (ID3 & C-E) && Restriction bias and Preference \n",
      "bias \n",
      "ID3 \n",
      "Candidate-Elimination \n",
      "Searches a complete hypothesis space \n",
      "incompletely \n",
      "Searches an incomplete hypothesis \n",
      "space completely \n",
      "Inductive bias is solely a consequence \n",
      "of the ordering of hypotheses by its \n",
      "search strategy \n",
      "Inductive bias is solely a \n",
      "consequence of the expressive \n",
      "power of its hypothesis \n",
      "representation \n",
      "sss \n",
      "Restriction bias \n",
      "Preference bias \n",
      "Candidate-Elimination \n",
      "ID3 \n",
      "Categorical restriction on the set of \n",
      "hypotheses considered \n",
      "Preference for certain hypotheses \n",
      "over others \n",
      "\n",
      "\n",
      "319. Department of CSE \n",
      "MRCET \n",
      "25 \n",
      " \n",
      " \n",
      " \n",
      "Possibility of excluding the unknown \n",
      "target function \n",
      "Work within a complete hypothesis \n",
      "space \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Issues in Decision Tree Learning: \n",
      " \n",
      "• Determine how deeply to grow the decision tree \n",
      "• Handling continuous attributes \n",
      "• Choosing an appropriate attribute selection measure \n",
      "• Handling training data with missing attribute values \n",
      "• Handling attributes with differing costs \n",
      "• Improving computational efficiency \n",
      "\n",
      "\n",
      "320. Department of CSE \n",
      "MRCET \n",
      "26 \n",
      " \n",
      " \n",
      " \n",
      "UNIT-II \n",
      "Artificial Neural Networks \n",
      "Introduction: \n",
      "Artificial Neural Networks (ANN) are algorithms based on brain function \n",
      "and are used to model complicated patterns and forecast issues. The Artificial \n",
      "Neural Network (ANN) is a deep learning method that arose from the \n",
      "concept of the human brain Biological Neural Networks. The development of \n",
      "ANN was the result of an attempt to replicate the workings of the human \n",
      "brain. The workings of ANN are extremely similar to those of biological \n",
      "neural networks, although they are not identical. ANN algorithm accepts \n",
      "only numeric and structured data. \n",
      "The ANN applications: \n",
      "Classification, the aim is to predict the class of an input vector \n",
      "• Pattern matching, the aim is to produce a pattern best associated with a given \n",
      "input vector. \n",
      "• Pattern completion, the aim is to complete the missing parts of a given input \n",
      "vector. \n",
      "• Optimization, the aim is to find the optimal values of parameters in an \n",
      "optimization problem. \n",
      "• Control, an appropriate action is suggested based on given an input vectors \n",
      "• Function approximation/times series modelling, the aim is to learn the \n",
      "functional relationships between input and desired output vectors. \n",
      "• Data mining, with the aim of discovering hidden patterns from data \n",
      "(knowledge discovery). ANN architectures \n",
      "• Neural Networks are known to be universal function approximators \n",
      "• Various architectures are available to approximate any nonlinear function \n",
      "• Different architectures allow for generation of functions of different \n",
      "complexity and power \n",
      " Feed forward networks \n",
      " Feedback networks \n",
      " Lateral networks \n",
      "\n",
      "\n",
      "321. Department of CSE \n",
      "MRCET \n",
      "27 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Advantages of Artificial Neural Networks \n",
      "Attribute-value pairs are used to represent problems in ANN. \n",
      "1. The output of ANNs can be discrete-valued, real-valued, or a vector of \n",
      "multiple real or discrete-valued characteristics, while the target function can \n",
      "be discrete-valued, real-valued, or a vector of numerous real or discrete- \n",
      "valued attributes. \n",
      "2. Noise in the training data is not a problem for ANN learning techniques. \n",
      "There may be mistakes in the training samples, but they will not affect the \n",
      "final result. \n",
      "3. It’s utilized when a quick assessment of the taught target function is \n",
      "necessary. \n",
      "4. The number of weights in the network. \n",
      "5. the number of training instances evaluated, and the settings of different \n",
      "learning algorithm parameters can all contribute to extended training periods \n",
      "for ANNs. \n",
      "Disadvantages of Artificial Neural Networks \n",
      "1. Hardware Dependence: \n",
      "• \n",
      "The construction of Artificial Neural Networks necessitates the use \n",
      "of \n",
      "parallel processors. \n",
      "• \n",
      "As a result, the equipment’s realization is contingent. \n",
      "2. Understanding the network’s operation: \n",
      "• \n",
      "This is the most serious issue with ANN. \n",
      "• \n",
      "When ANN provides a probing answer, it does not explain why or how it \n",
      "was chosen. \n",
      "• \n",
      "As a result, the network’s confidence is eroded. \n",
      "3. Assured network structure: \n",
      "\n",
      "\n",
      "322. Department of CSE \n",
      "MRCET \n",
      "28 \n",
      " \n",
      " \n",
      " \n",
      "• Any precise rule does not determine the structure of artificial neural \n",
      "networks. \n",
      "• Experience and trial and error are used to develop a suitable network \n",
      "structure. \n",
      "4. Difficulty in presenting the issue to the network: \n",
      "• \n",
      "ANNs are capable of working with numerical data. \n",
      "• \n",
      "Before being introduced to ANN, problems must be converted into \n",
      "numerical values. \n",
      "• \n",
      "The display method that is chosen will have a direct impact on the network’s \n",
      "performance. \n",
      "• \n",
      "The user’s skill is a factor here. \n",
      "5. The network’s lifetime is unknown: • When the network’s error on the \n",
      "sample is decreased to a specific amount, the training is complete. \n",
      "• \n",
      "The value does not produce the best outcomes. \n",
      "Appropriate Problems for Neural Network Learning: \n",
      "1. Instances are represented by many attribute-value pairs (e.g., the pixels of a \n",
      "picture. ALVINN [Mitchell, p. 84]). \n",
      "2. The target function output may be discrete-valued, real-valued, or a vector of \n",
      "several real- or discrete-valued attributes. \n",
      "3. The training examples may contain errors. \n",
      "4. Long training times are acceptable. \n",
      "5. Fast evaluation of the learned target function may be required. \n",
      "6. The ability for humans to understand the learned target function is not \n",
      "important. \n",
      "History of Neural Networks: \n",
      "1. 1943: McCulloch and Pitts proposed a model of a neuron Perceptron (read \n",
      "[Mitchell, section 4.4]) \n",
      "2. 1960s: Widrow and Hoff explored Perceptron networks (which they called \n",
      "“Adelines”) and the delta rule. \n",
      "3. 1962: Rosenblatt proved the convergence of the perceptron training rule. \n",
      "\n",
      "\n",
      "323. Department of CSE \n",
      "MRCET \n",
      "29 \n",
      " \n",
      " \n",
      " \n",
      "4. 1969: Minsky and Papert showed that the Perceptron cannot deal with \n",
      "nonlinearly-separable data sets---even those that represent simple function \n",
      "such as X-OR. \n",
      "5. 1970-1985: Very little research on Neural Nets \n",
      "6. 1986: Invention of Backpropagation Rumelhart and McClelland, but also \n",
      "Parker and earlier on: Werbos which can learn from nonlinearly-separable \n",
      "data sets. \n",
      "7. Since 1985: A lot of research in Neural Nets! \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Multilayer Neural Network: \n",
      "• A multiplayer perceptron is a feed forward neural network with one or more \n",
      "hidden layers \n",
      "• The network consists of an input layer of source neurons, at least one hidden \n",
      "layer of computational neurons, and an output layer of computational \n",
      "neurons. \n",
      "• The input signals are propagated in a forward direction on a layer-by-layer \n",
      "basis. \n",
      "• Neurons in the hidden layer cannot be observed through input/output \n",
      "behaviour of the network. \n",
      "• There is no obvious way to know what the desired output of the hidden layer \n",
      "should be. \n",
      "\n",
      "\n",
      "324. Department of CSE \n",
      "MRCET \n",
      "30 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\n",
      "\n",
      "325. Department of CSE \n",
      "MRCET \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "31 \n",
      "\n",
      "\n",
      "326. Department of CSE \n",
      "MRCET \n",
      "32 \n",
      " \n",
      " \n",
      " \n",
      "Back propagation: Overview \n",
      "• Back propagation works by applying the gradient descent rule to a feed \n",
      "forward network. \n",
      "• The algorithm is composed of two parts that get repeated over and over until \n",
      "a pre-set maximal number of epochs, EP max. \n",
      "• Part I, the feed forward pass: the activation values of the hidden and then \n",
      "output units are computed. \n",
      "• Part II, the back propagation pass: the weights of the network are updated- \n",
      "starting with the hidden to output weights and followed by the input to \n",
      "hidden weights--with respect to the sum of squares error and through a series \n",
      "of weight update rules called the Delta Rule. \n",
      "Definition: \n",
      "The Back propagation algorithm in neural network computes the gradient of \n",
      "the loss function for a single weight by the chain rule. It efficiently computes \n",
      "one layer at a time, unlike a native direct computation. It computes the \n",
      "gradient, but it does not define how the gradient is used. It generalizes the \n",
      "computation in the delta rule. \n",
      "Consider the following Back propagation neural network example diagram to \n",
      "understand: \n",
      "\n",
      "\n",
      "327. Department of CSE \n",
      "MRCET \n",
      "33 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "• Inputs X, arrive through the preconnected path \n",
      "• Input is modelled using real weights W. The weights are usually randomly \n",
      "selected. \n",
      "• Calculate the output for every neuron from the input layer, to the hidden \n",
      "layers, to the output layer. \n",
      "• Calculate the error in the outputs \n",
      "ErrorB= Actual Output – Desired Output \n",
      "• Travel back from the output layer to the hidden layer to adjust the weights \n",
      "such that the error is decreased. \n",
      "• Keep repeating the process until the desired output is achieved \n",
      " \n",
      " \n",
      "Why We Need Back propagation? \n",
      "• Most prominent advantages of Back propagation are: \n",
      "• Back propagation is fast, simple and easy to program \n",
      "• It has no parameters to tune apart from the numbers of input \n",
      "• It is a flexible method as it does not require prior knowledge about the \n",
      "network \n",
      "• It is a standard method that generally works well \n",
      "• It does not need any special mention of the features of the function to be \n",
      "learned. \n",
      "1. Inputs X, arrive through the preconnected path \n",
      "\n",
      "\n",
      "328. Department of CSE \n",
      "MRCET \n",
      "34 \n",
      " \n",
      " \n",
      " \n",
      "Types of Back propagation Networks \n",
      "Two Types of Back propagation Networks are: \n",
      "• Static Back-propagation \n",
      "• Recurrent Back propagation Static back-propagation: \n",
      "It is one kind of back propagation network which produces a mapping of a \n",
      "static input for static output. It is useful to solve static classification issues \n",
      "like optical character recognition. \n",
      "Recurrent Back propagation: \n",
      "Recurrent Back propagation in data mining is fed forward until a fixed value \n",
      "is achieved. After that, the error is computed and propagated backward. \n",
      " \n",
      "Disadvantages of using Back propagation \n",
      "• The actual performance of back propagation on a specific problem is \n",
      "dependent on the input data. \n",
      "• Back propagation algorithm in data mining can be quite sensitive to noisy \n",
      "data \n",
      "• You need to use the matrix-based approach for back propagation instead of \n",
      "mini-batch. \n",
      " \n",
      " \n",
      "Back propagation: The Algorithm \n",
      "• Initialize the weights to small random values; create a random pool of all the \n",
      "training patterns; set EP, the number of epochs of training to 0. \n",
      "• 2. Pick a training pattern from the remaining pool of patterns and propagate \n",
      "it forward through the network. \n",
      "• 3. Compute the deltas, k for the output layer. \n",
      "• 4. Compute the deltas, \n",
      "backward. \n",
      "for the hidden  layer by propagating  the error \n",
      "• Update all the connections such that \n",
      "• W Newji \n",
      "= wjiold \n",
      "+ wji and w Newkj \n",
      "= wkjOld \n",
      "+ wkj \n",
      "j \n",
      "\n",
      "\n",
      "329. Department of CSE \n",
      "MRCET \n",
      "35 \n",
      " \n",
      " \n",
      " \n",
      "• If any pattern remains in the pool, then go back to Step 2. If all the training \n",
      "patterns in the pool have been used, then set EP = EP+1, and if EP EPMax, \n",
      "then create a random pool of patterns and go to Step 2. If EP = EPMax, then \n",
      "stop. \n",
      " \n",
      "Back propagation: The Momentum: \n",
      "• To this point, Back propagation has the disadvantage of being too slow if \n",
      "is \n",
      "small and it can oscillate too widely if is large. \n",
      "• To solve this problem, we can add a momentum to give each connection \n",
      "some inertia, forcing it to change in the direction of the downhill “force”. \n",
      "• New Delta Rule: \n",
      "wpq(t+1) = -    E/ wpq +    wpq(t) \n",
      "• Where p and q are any input and hidden, or, hidden and output units; t is a \n",
      "time step or epoch; and \n",
      "is the momentum parameter which regulates the \n",
      "amount of inertia of the weights. \n",
      "\n",
      "\n",
      "330. Department Of CSE \n",
      "MRCET \n",
      "36 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "UNIT - III \n",
      " \n",
      "Introduction to Bayesian Learning \n",
      " \n",
      "Imagine a situation where your friend gives you a new coin and asks you the \n",
      "fairness of the coin (or the probability of observing heads) without even \n",
      "flipping the coin once. In fact, you are also aware that your friend has not \n",
      "made the coin biased. In general, you have seen that coins are fair, thus you \n",
      "expect the probability of observing heads is 0.50.5. In the absence of any \n",
      "such observations, you assert the fairness of the coin only using your past \n",
      "experiences or observations with coins. \n",
      "Suppose that you are allowed to flip the coin 1010 times in order to \n",
      "determine the fairness of the coin. Your observations from the experiment \n",
      "will fall under one of the following cases: \n",
      " \n",
      "• \n",
      "Case 1: observing 55 heads and 55 tails. \n",
      " \n",
      "• \n",
      "Case 2: observing hh heads and 10−h10−h tails, where h≠10−hh≠10−h. \n",
      " \n",
      "If case 1 is observed, you are now more certain that the coin is a fair coin, \n",
      "and you will decide that the probability of observing heads is 0.50.5 with \n",
      "more confidence. If case 2 is observed you can either: \n",
      " \n",
      "1. Neglect your prior beliefs since now you have new data, decide the \n",
      "probability of observing heads is h/10h/10 by solely depending on recent \n",
      "observations. \n",
      "2. Adjust your belief accordingly to the value of hh that you have just observed, \n",
      "and decide the probability of observing heads using your recent observations. \n",
      " \n",
      "The first method suggests that we use the frequentist method, where we \n",
      "omit our beliefs when making decisions. However, the second method \n",
      "seems to be more convenient because 1010 coins are insufficient to \n",
      "determine the fairness of a coin. Therefore, we can make better decisions \n",
      "by combining our recent observations and beliefs that we have gained \n",
      "through our past experiences. It is this thinking model which uses our most \n",
      "recent observations together with our beliefs or inclination for critical \n",
      "thinking that is known as Bayesian thinking. \n",
      "\n",
      "\n",
      "331. Department Of CSE \n",
      "MRCET \n",
      "37 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Moreover, assume that your friend allows you to conduct another 1010 coin \n",
      "flips. Then we can use these new observations to further update our beliefs. \n",
      "As we gain more data, we can incrementally update our beliefs increasing \n",
      "the certainty of our conclusions. This is known as incremental learning, \n",
      "where you update your knowledge incrementally with new evidence. \n",
      "Bayesian learning comes into play on such occasions, where we are unable \n",
      "to use frequentist statistics due to the drawbacks that we have discussed \n",
      "above. We can use Bayesian learning to address all these drawbacks and \n",
      "even with additional capabilities (such as incremental updates of the \n",
      "posterior) when testing a hypothesis to estimate unknown parameters of a \n",
      "machine learning models. Bayesian learning uses Bayes’ theorem to \n",
      "determine the conditional probability of a hypotheses given some evidence \n",
      "or observations. \n",
      "The Famous Coin Flip Experiment \n",
      " \n",
      "When we flip a coin, there are two possible outcomes - heads or tails. Of \n",
      "course, there is a third rare possibility where the coin balances on its edge \n",
      "without falling onto either side, which we assume is not a possible outcome \n",
      "of the coin flip for our discussion. We conduct a series of coin flips and \n",
      "record our observations i.e. the number of the heads (or tails) observed for a \n",
      "certain number of coin flips. In this experiment, we are trying to determine \n",
      "the fairness of the coin, using the number of heads (or tails) that we observe. \n",
      " \n",
      "Frequentist Statistics \n",
      " \n",
      "Let us think about how we can determine the fairness of the coin using our \n",
      "observations in the above mentioned experiment. Once we have conducted a \n",
      "sufficient number of coin flip trials, we can determine the frequency or the \n",
      "probability of observing the heads (or tails). If we observed heads and tails \n",
      "with equal frequencies or the probability of observing heads (or tails) is \n",
      "0.50.5, then it can be established that the coin is a fair coin. Failing that, it is \n",
      "a biased coin. Let's denote pp as the probability of observing the heads. \n",
      "Consequently, as the quantity that pp deviates from 0.50.5 indicates how \n",
      "biased the coin is, pp can be considered as the degree-of-fairness of the coin. \n",
      "\n",
      "\n",
      "332. Department Of CSE \n",
      "MRCET \n",
      "38 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Testing whether a hypothesis is true or false by calculating the probability \n",
      "of an event in a prolonged experiment is known as frequentist statistics. As \n",
      "such, determining the fairness of a coin by using the probability of \n",
      "observing the heads is an example of frequentist statistics (a.k.a. frequentist \n",
      "approach). \n",
      "Let us now further investigate the coin flip example using the frequentist \n",
      "approach. Since we have not intentionally altered the coin, it is reasonable to \n",
      "assume that we are using an unbiased coin for the experiment. When we flip \n",
      "the coin 1010 times, we observe the heads 66 times. Therefore, the pp is \n",
      "0.60.6 (note that pp is the number of heads observed over the number of total \n",
      "coin flips). Hence, according to frequencies statistics, the coin is a biased \n",
      "coin — which opposes our assumption of a fair coin. Perhaps one of your \n",
      "friends who is more skeptical than you extends this experiment to 100100 \n",
      "trails using the same coin. Then she observes heads 5555 times, which \n",
      "results in a different pp with 0.550.55. Even though the new value for pp \n",
      "does not change our previous conclusion (i.e. that the coin is biased), this \n",
      "observation raises several questions: \n",
      " \n",
      "• \n",
      "How confident are we of pp being 0.60.6? \n",
      " \n",
      "• \n",
      "How confident are of pp being 0.550.55? \n",
      " \n",
      "• \n",
      "Which of these values is the accurate estimation of pp? \n",
      "\n",
      "\n",
      "333. 39 \n",
      " \n",
      "Department of CSE \n",
      "MRCET \n",
      "Will pp continue to change when we further increase the number of coin flip \n",
      "trails? \n",
      " \n",
      "We cannot find out the exact answers to the first three questions using \n",
      "frequentist statistics.  We may assume that true value of pp is closer to \n",
      "0.550.55 than 0.60.6 because the former is computed using observations from \n",
      "a considerable number of trials compared to what we used to compute the \n",
      "latter. Yet there is no way of confirming that hypothesis. However, if we \n",
      "further increase the number of trials, we may get a different probability from \n",
      "both of the above values for observing the heads and eventually, we may \n",
      "even discover that the coin is a fair coin. \n",
      "Number \n",
      "of \n",
      "coin \n",
      "Number of heads \n",
      "Probability of observing heads \n",
      "flips \n",
      "10 \n",
      "6 \n",
      "0.6 \n",
      "50 \n",
      "29 \n",
      "0.58 \n",
      "100 \n",
      "55 \n",
      "0.55 \n",
      "200 \n",
      "94 \n",
      "0.47 \n",
      "500 \n",
      "245 \n",
      "0.49 \n",
      "Table 1 - Coin flip experiment results when increasing the number of \n",
      "trials \n",
      " \n",
      " \n",
      "Table 1 presents some of the possible outcomes of a hypothetical coin flip \n",
      "experiment when we are increasing the number of trials. The fairness (pp) of \n",
      "the coin changes when increasing the number of coin-flips in this experiment. \n",
      "Our confidence of estimated pp may also increase when increasing the \n",
      "number of coin-flips, yet the frequentist statistic does not facilitate any \n",
      "indication of the confidence of the estimated pp value. We can attempt to \n",
      "understand the importance of such a confident measure by studying the \n",
      "following cases: \n",
      " \n",
      "• An experiment with an infinite number of trials guarantees pp with absolute \n",
      "accuracy (100% confidence). Yet, it is not practical to conduct an experiment \n",
      "with an infinite number of trials and we should stop the experiment after a \n",
      "sufficiently large number of trials. However, deciding the value of this \n",
      "sufficient number of trials is a challenge when using frequentist statistics. \n",
      "If we can determine the confidence of the estimated pp value or the inferred \n",
      "conclusion, in a situation where the number of trials is limited, this will allow \n",
      "\n",
      "\n",
      "334. Department Of CSE \n",
      "MRCET \n",
      "40 \n",
      " \n",
      " \n",
      " \n",
      "us to decide whether to accept the conclusion or to extend the experiment \n",
      "with more trials until it achieves sufficient confidence. \n",
      " \n",
      "Moreover, we may have valuable insights or prior beliefs (for example, coins \n",
      "are usually fair and the coin used is not made biased intentionally, therefore \n",
      "p≈0.5p≈0.5) that describes the value of pp. Embedding that information can \n",
      "significantly improve the accuracy of the final conclusion. Such beliefs play a \n",
      "significant role in shaping the outcome of a hypothesis test especially when \n",
      "we have limited data. However, with frequentist statistics, it is not possible to \n",
      "incorporate such beliefs or past experience to increase the accuracy of the \n",
      "hypothesis test. \n",
      "Some Terms to Understand \n",
      " \n",
      "Before delving into Bayesian learning, it is essential to understand the \n",
      "definition of some terminologies used. I will not provide lengthy explanations \n",
      "of the mathematical definition since there is a lot of widely available content \n",
      "that you can use to understand these concepts. \n",
      " \n",
      "• \n",
      "Random variable (Stochastic variable) - In statistics, the random variable is a \n",
      "variable whose possible values are a result of a random event. Therefore, \n",
      "each possible value of a random variable has some probability attached to it \n",
      "to represent the likelihood of those values. \n",
      "• \n",
      "Probability distribution - The function that defines the probability of different \n",
      "outcomes/values of a random variable. The continuous probability \n",
      "distributions are described using probability density functions whereas \n",
      "discrete probability distributions can be represented using probability mass \n",
      "functions. \n",
      "Conditional probability - This is a measure of probability P(A|B)P(A|B) of an \n",
      "event A given that another event B has occurred. \n",
      "• \n",
      "Joint probability distribution \n",
      " \n",
      " \n",
      "Bayes’ Theorem \n",
      " \n",
      "Bayes’ theorem describes how the conditional probability of an event or a \n",
      "hypothesis can be computed using evidence and prior knowledge. It is similar \n",
      "to concluding that our code has no bugs given the evidence that it has passed \n",
      "\n",
      "\n",
      "335. Department Of CSE \n",
      "MRCET \n",
      "41 \n",
      " \n",
      " \n",
      " \n",
      "all the test cases, including our prior belief that we have rarely observed any \n",
      "bugs in our code. However, this intuition goes beyond that simple hypothesis \n",
      "test where there are multiple events or hypotheses involved (let us not worry \n",
      "about this for the moment). \n",
      " \n",
      "The Bayes’ theorem is given by: \n",
      " \n",
      " \n",
      "P(θ|X)=P(X|θ)P(θ)P(X)P(θ|X)=P(X|θ)P(θ)P(X) \n",
      " \n",
      " \n",
      "I will now explain each term in Bayes’ theorem using the above example. \n",
      "Consider the hypothesis that there are no bugs in our code. θθ and XX denote \n",
      "that our code is bug free and passes all the test cases respectively. \n",
      " \n",
      "• \n",
      "P(θ)P(θ) - Prior Probability is the probability of the hypothesis θθ being true \n",
      "before applying the Bayes’ theorem. Prior represents the beliefs that we have \n",
      "gained through past experience, which refers to either common sense or an \n",
      "outcome of Bayes’ theorem for some past observations. For the example \n",
      "given, prior probability denotes the probability of observing no bugs in our \n",
      "code. However, since this is the first time we are applying Bayes’ theorem, \n",
      "we have to decide the priors using other means \n",
      "(Otherwise we could use the previous posterior as the new prior). Let us \n",
      "assume that it is very unlikely to find bugs in our code because rarely have \n",
      "we observed bugs in our code in the past. With our past experience of \n",
      "observing fewer bugs in our code, we can assign our prior P(θ)P(θ) with a \n",
      "higher probability. However, for now, let us assume that P(θ)=pP(θ) \n",
      "This term depends on the test coverage of the test cases. Even though we do \n",
      "not know the value of this term without proper measurements, in order to \n",
      "continue this discussion let us assume that P(X|¬θ)=0.5P(X|¬θ)=0.5. \n",
      "Accordingly, \n",
      "P(X)=1×p+0.5×(1−p)=0.5(1+p)P(X)=1×p+0.5×(1−p)=0.5(1+p) \n",
      " \n",
      "• \n",
      "P(θ|X)P(θ|X) - Posteriori probability denotes the conditional probability of \n",
      "the hypothesis θθ after observing the evidence XX. This is the probability of \n",
      "observing no bugs in our code given that it passes all the test cases. Since we \n",
      "\n",
      "\n",
      "336. Department Of CSE \n",
      "MRCET \n",
      "42 \n",
      " \n",
      " \n",
      " \n",
      "now know the values for the other three terms in the Bayes’ theorem, we can \n",
      "calculate the posterior probability using the following formula: \n",
      " \n",
      "P(θ|X)=1×p0.5(1+p)P(θ|X)=1×p0.5(1+p) \n",
      "We can also calculate the probability of observing a bug, given that our code \n",
      "passes all the test cases P(¬θ|X)P(¬θ|X) . \n",
      "P(¬θ|X)=P(X|¬θ).P(¬θ)P(X)=0.5×(1−p)0.5×(1+p)=(1−p)(1+p)P(¬θ|X)=P(X|¬ \n",
      "θ).P(¬θ) \n",
      "P(X)=0.5×(1−p)0.5×(1+p)=(1−p)(1+p) \n",
      "We now know both conditional probabilities of observing a bug in the code \n",
      "and not observing the bug in the code. Yet how are we going to confirm the \n",
      "valid hypothesis using these posterior probabilities? \n",
      " \n",
      "Maximum a Posteriori (MAP) \n",
      " \n",
      "We can use MAP to determine the valid hypothesis from a set of hypotheses. \n",
      "According to MAP, the hypothesis that has the maximum posterior \n",
      "probability is considered as the valid hypothesis. Therefore, we can express \n",
      "the hypothesis θMAPθMAP that is concluded using MAP as follows: \n",
      "θMAP=argmaxθP(θi|X)=argmaxθ(P(X|θi)P(θi)P(X))θMAP=argmaxθP(θi|X) \n",
      "=argmaxθ(P(X|θ i)P(θi)P(X)) \n",
      "The argmaxθargmaxθ operator estimates the event or hypothesis θiθi that \n",
      "maximizes the posterior probability P(θi|X)P(θi|X). Let us apply MAP to the \n",
      "above example in order to determine the true hypothesis: \n",
      "θMAP=argmaxθ{θ:P(θ|X)=p0.5(1+p),¬θ:P(¬θ|X)=(1−p)(1+p)}θMAP=argma \n",
      "xθ{θ:P(θ|X)=p0.5(1+p),¬θ:P(¬θ|X)=(1−p)(1+p)} \n",
      "\n",
      "\n",
      "337. Department Of CSE \n",
      "MRCET \n",
      "43 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Figure 1 - P(θ|X)P(θ|X) and P(¬θ|X)P(¬θ|X) when changing the \n",
      "P(θ)=pP(θ)=p Figure 1 illustrates how the posterior probabilities of possible \n",
      "hypotheses change with the value of prior probability. Unlike frequentist \n",
      "statistics where our belief or past experience had no influence on the \n",
      "concluded hypothesis, Bayesian learning is capable of incorporating our \n",
      "belief to improve the accuracy of predictions. Assuming that we have fairly \n",
      "good programmers and therefore the probability of observing a bug is \n",
      "P(θ)=0.4P(θ)=0.4 , then we find the θMAPθMAP: \n",
      "MAP=argmaxθ{θ:P(|X)=0.40.5(1+0.4),¬θ:P(¬θ|X)=0.5(1−0.4)0.5(1+0.4)}=ar \n",
      "gmaxθ{θ:P(θ|X)=0.57,¬θ:P(¬θ|X)=0.43}=θ⟹No \n",
      "bugs \n",
      "present \n",
      "in \n",
      "our \n",
      "codeMAP=argmaxθ{θ:P(|X)=0.40.5(1+0.4),¬θ:P(¬θ|X)=0.5(1−0.4)0.5(1+0.4 \n",
      ")}=argmaxθ{θ:P(θ|X)=0.57,¬θ:P(¬θ|X)=0.43}=θ⟹No bugs present in our \n",
      "code \n",
      "\n",
      "\n",
      "338. Department Of CSE \n",
      "MRCET \n",
      "44 \n",
      " \n",
      " \n",
      " \n",
      "However, P(X)P(X) is independent of θθ, and thus P(X)P(X) is same for all \n",
      "the events or hypotheses. Therefore, we can simplify the θMAPθMAP \n",
      "estimation, without the denominator of each posterior computation as shown \n",
      "below: θMAP=argmaxθ(P(X|θi)P(θi))θMAP=argmaxθ(P(X|θi)P(θi)) \n",
      "Notice that MAP estimation algorithms do not compute posterior probability \n",
      "of each hypothesis to decide which is the most probable hypothesis. \n",
      "Assuming that our hypothesis space is continuous (i.e. fairness of the coin \n",
      "encoded as probability of observing heads, coefficient of a regression model, \n",
      "etc.), where endless possible hypotheses are present even in the smallest \n",
      "range that the human mind can think of, or for even a discrete hypothesis \n",
      "space with a large number of possible outcomes for an event, we do not need \n",
      "to find the posterior of each hypothesis in order to decide which is the most \n",
      "probable hypothesis. Therefore, the practical implementation of MAP \n",
      "estimation algorithms use approximation techniques, which are capable of \n",
      "finding the most probable hypothesis without computing posteriors or only \n",
      "by computing some of them. \n",
      " \n",
      "Using the Bayesian theorem, we can now incorporate our belief as the prior \n",
      "probability, which was not possible when we used frequentist statistics. \n",
      "However, we still have the problem of deciding a sufficiently large number of \n",
      "trials or attaching a confidence to the concluded hypothesis. This is because \n",
      "the above example was solely designed to introduce the Bayesian theorem \n",
      "and each of its terms. Let us now gain a better understanding of \n",
      "Bayesian learning to learn about the full potential of Bayes’ theorem. \n",
      " \n",
      "Binomial Likelihood \n",
      "The likelihood for the coin flip experiment is given by the probability of \n",
      "observing heads out of all the coin flips given the fairness of the coin. As we \n",
      "have defined the fairness of the coins (θθ) using the probability of observing \n",
      "heads for each coin flip, we can define the probability of observing heads or \n",
      "\n",
      "\n",
      "339. Department Of CSE \n",
      "MRCET \n",
      "45 \n",
      " \n",
      " \n",
      " \n",
      "tails given the fairness of the coin P(y|θ)P(y|θ) where y=1y=1 for observing \n",
      "heads and y=0y=0 for observing tails. Accordingly: \n",
      "P(y=1|θ)=θP(y=0|θ)=(1−θ)P(y=1|θ)=θP(y=0|θ)=(1−θ) \n",
      "Now that we have defined two conditional probabilities for each outcome \n",
      " \n",
      "above, let us now try to find the P(Y=y|θ)P(Y=y|θ) joint probability of \n",
      "observing heads or tails: \n",
      "P(Y=y|θ)={θ, if y=11−θ, otherwise P(Y=y|θ)={θ, if y=11−θ, otherwise \n",
      "Note that yy can only take either 00 or 11, and θθ will lie within the range of \n",
      "[0,1][0,1]. We can rewrite the above expression in a single expression as \n",
      "follows: \n",
      "P(Y=y|θ)=θy×(1−θ)1−yP(Y=y|θ)=θy×(1−θ)1−y \n",
      "The above equation represents the likelihood of a single test coin flip \n",
      "experiment. \n",
      "Interestingly, the likelihood function of the single coin flip experiment is \n",
      "similar to the Bernoulli probability distribution. The Bernoulli distribution is \n",
      "the probability distribution of a single trial experiment with only two \n",
      "opposite   outcomes.   As   the   Bernoulli   probability   distribution   is   the \n",
      "simplification of Binomial probability distribution for a single trail, we can \n",
      " \n",
      "represent the likelihood of a coin flip experiment that we observe kk number \n",
      "of heads out of NN number of trials as a Binomial probability distribution as \n",
      "shown below: \n",
      "P(k,N|θ)=(Nk)θk(1−θ)N−k \n",
      "\n",
      "\n",
      "340. Department Of CSE \n",
      "MRCET \n",
      "46 \n",
      " \n",
      " \n",
      " \n",
      "Maximum likelihood estimation method (MLE) \n",
      " \n",
      "The likelihood function indicates how likely the observed sample is as a \n",
      "function of possible parameter values. Therefore, maximizing the likelihood \n",
      "function determines the parameters that are most likely to produce the \n",
      "observed data. From a statistical point of view, MLE is usually recommended \n",
      "for large samples because it is versatile, applicable to most models and \n",
      "different types of data, and produces the most precise estimates. \n",
      " \n",
      "Least squares estimation method (LSE) \n",
      " \n",
      "Least squares estimates are calculated by fitting a regression line to the points \n",
      "from a data set that has the minimal sum of the deviations squared (least \n",
      "square error). In reliability analysis, the line and the data are plotted on a \n",
      "probability plot. \n",
      "Bayes Optimal Classifier \n",
      " \n",
      "The Bayes optimal classifier is a probabilistic model that makes the most \n",
      "probable prediction for a new example, given the training dataset. \n",
      " \n",
      "This model is also referred to as the Bayes optimal learner, the Bayes \n",
      "classifier, Bayes optimal decision boundary, or the Bayes optimal \n",
      "discriminant function. \n",
      " \n",
      "Gibbs Sampling Algorithm \n",
      "We start off by selecting an initial value for the random variables X & Y. \n",
      "Then, we sample from the conditional probability distribution of X given Y = \n",
      "Y⁰ denoted p(X|Y⁰). In the next step, we sample a new value of Y conditional \n",
      "on X¹, which we just computed. We repeat the procedure for an additional n - \n",
      "1 iterations, alternating between drawing a new sample from the conditional \n",
      "probability distribution of X and the conditional probability distribution of Y, \n",
      "given the current value of the other random variable. \n",
      "\n",
      "\n",
      "341. Department Of CSE \n",
      "MRCET \n",
      "47 \n",
      " \n",
      " \n",
      " \n",
      "Let’s take a look at an example. Suppose we had the following posterior and \n",
      "conditional probability distributions. \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Naive Bayes Classifier Algorithm \n",
      "• \n",
      "Naïve Bayes algorithm is a supervised learning algorithm, which is based on \n",
      "Bayes \n",
      "theorem and used for solving classification problems. \n",
      "\n",
      "\n",
      "342. Department Of CSE \n",
      "MRCET \n",
      "48 \n",
      " \n",
      " \n",
      " \n",
      "• \n",
      "It is mainly used in text classification that includes a high-dimensional \n",
      "training dataset. \n",
      "• \n",
      "Naïve Bayes Classifier is one of the simple and most effective Classification \n",
      "algorithms which helps in building the fast machine learning models that can \n",
      "make quick predictions. \n",
      "• \n",
      "It is a probabilistic classifier, which means it predicts on the basis of the \n",
      "probability of an object. \n",
      "• \n",
      "Some popular examples of Naïve Bayes Algorithm are spam filtration, \n",
      "Sentimental analysis, and classifying articles. \n",
      "EXAMPLE \n",
      "Suppose we have a dataset of weather conditions and corresponding target \n",
      "variable \"Play\". So using this dataset we need to decide that whether we \n",
      "should play or not on a particular day according to the weather conditions. So \n",
      "to solve this problem, we need to follow the below steps: \n",
      " \n",
      "1. Convert the given dataset into frequency tables. \n",
      "2. Generate Likelihood table by finding the probabilities of given features. \n",
      "3. Now, use Bayes theorem to calculate the posterior probability. \n",
      "Problem: If the weather is sunny, then the Player should play or not? \n",
      "Solution: To solve this, first consider the below dataset: \n",
      "Outlook \n",
      "Play \n",
      "0 \n",
      "Rainy \n",
      "Yes \n",
      "1 \n",
      "Sunny \n",
      "Yes \n",
      "2 \n",
      "Overcast \n",
      "Yes \n",
      "3 \n",
      "Overcast \n",
      "Yes \n",
      "4 \n",
      "Sunny \n",
      "No \n",
      "5 \n",
      "Rainy \n",
      "Yes \n",
      "6 \n",
      "Sunny \n",
      "Yes \n",
      "\n",
      "\n",
      "343. Department Of CSE \n",
      "MRCET \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Frequency table for the Weather Conditions: \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Likelihood table weather condition: \n",
      "Weather \n",
      "No \n",
      "Yes \n",
      " \n",
      "Overcast \n",
      "0 \n",
      "5 \n",
      "5/14= 0.35 \n",
      "Rainy \n",
      "2 \n",
      "2 \n",
      "4/14=0.29 \n",
      " \n",
      "Sunny \n",
      "2 \n",
      "3 \n",
      "5/14=0.35 \n",
      "All \n",
      "4/14=0.29 \n",
      "10/14=0.71 \n",
      " \n",
      "Applying Bayes'theorem: \n",
      " \n",
      "P(Yes|Sunny)= P(Sunny|Yes)*P(Yes)/P(Sunny) \n",
      "49 \n",
      " \n",
      "7 \n",
      "Overcast \n",
      "Yes \n",
      "8 \n",
      "Rainy \n",
      "No \n",
      "9 \n",
      "Sunny \n",
      "No \n",
      "10 \n",
      "Sunny \n",
      "Yes \n",
      "11 \n",
      "Rainy \n",
      "No \n",
      "12 \n",
      "Overcast \n",
      "Yes \n",
      "13 \n",
      "Overcast \n",
      "Yes \n",
      "Weather \n",
      "Yes \n",
      "No \n",
      "Overcast \n",
      "5 \n",
      "0 \n",
      "Rainy \n",
      "2 \n",
      "2 \n",
      "Sunny \n",
      "3 \n",
      "2 \n",
      "Total \n",
      "10 \n",
      "5 \n",
      "\n",
      "\n",
      "344. Department Of CSE \n",
      "MRCET \n",
      "50 \n",
      " \n",
      " \n",
      " \n",
      "P(Sunny|Yes)= 3/10= 0.3 \n",
      " \n",
      "P(Sunny)= 0.35 \n",
      "P(Yes)=0.71 \n",
      "So P(Yes|Sunny) = 0.3*0.71/0.35= 0.60 \n",
      "P(No|Sunny)= P(Sunny|No)*P(No)/P(Sunny) \n",
      "P(Sunny|NO)= 2/4=0.5 \n",
      "P(No)= 0.29 \n",
      " \n",
      "P(Sunny)= 0.35 \n",
      " \n",
      "So P(No|Sunny)= 0.5*0.29/0.35 = 0.41 \n",
      " \n",
      "Bayesian Belief Network: \n",
      " \n",
      "It is a graphical representation of different probabilistic relationships among \n",
      "random variables in a  particular set. It is a classifier with no dependency on \n",
      "attributes i.e it is condition independent. Due to its feature of joint probability, the \n",
      "probability in Bayesian Belief Network is derived, based on a condition — \n",
      "P(attribute/parent) i.e probability of an attribute, true over parent attribute. \n",
      " \n",
      "Consider this example: \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "• \n",
      "In the above figure, we have an alarm ‘A’ – a node, say installed in a house \n",
      "of a person ‘gfg’, which rings upon two probabilities i.e burglary ‘B’ and fire \n",
      "\n",
      "\n",
      "345. Department Of CSE \n",
      "MRCET \n",
      "51 \n",
      " \n",
      " \n",
      " \n",
      "‘F’, which are – parent nodes of the alarm node. The alarm is the parent node \n",
      "of two probabilities P1 calls ‘P1’ & P2 calls ‘P2’ person nodes. \n",
      " \n",
      "• \n",
      "Upon the instance of burglary and fire, ‘P1’ and ‘P2’ call person ‘gfg’, \n",
      "respectively. But, there are few drawbacks in this case, as sometimes ‘P1’ \n",
      "may forget to call the person ‘gfg’, even after hearing the alarm, as he has a \n",
      "tendency to forget things, quick. Similarly, ‘P2’, sometimes fails to call the \n",
      "person ‘gfg’, as he is only able to hear the alarm, from a certain distance. \n",
      "Expectation-Maximization Algorithm \n",
      "In the real-world applications of machine learning, it is very common that \n",
      "there are many relevant features available for learning but only a small subset \n",
      "of them are observable. So, for the variables which are sometimes observable \n",
      "and sometimes not, then  we can use the instances when that variable is \n",
      "visible is observed for the purpose of learning and then predict its value in the \n",
      "instances when it is not observable. \n",
      "On the other hand, Expectation-Maximization algorithm can be used for the \n",
      "latent variables (variables that are not directly observable and are actually \n",
      "inferred from the values of the other observed variables) too in order to \n",
      "predict their values with the condition that the general form of probability \n",
      "distribution governing those latent variables is known to us. This algorithm is \n",
      "actually at the base of many unsupervised clustering algorithms in the field of \n",
      "machine learning. \n",
      "It was explained, proposed and given its name in a paper published in 1977 \n",
      "by Arthur Dempster, Nan Laird, and Donald Rubin. It is used to find the local \n",
      "maximum likelihood parameters of a statistical model in the cases where \n",
      "latent variables are involved and the data is missing or incomplete. \n",
      " \n",
      " \n",
      "Algorithm: \n",
      "1. Given a set of incomplete data, consider a set of starting parameters. \n",
      "2. Expectation step (E – step): Using the observed available data of the \n",
      "dataset, estimate (guess) the values of the missing data. \n",
      "3. Maximization step (M – step): Complete data generated after the \n",
      "expectation (E) step is used in order to update the parameters. \n",
      "4. Repeat step 2 and step 3 until convergence. \n",
      "\n",
      "\n",
      "346. Department Of CSE \n",
      "MRCET \n",
      "52 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "The essence of Expectation-Maximization algorithm is to use the available \n",
      "observed data of the dataset to estimate the missing data and then using that \n",
      "data to update the values of the parameters. Let us understand the EM \n",
      "algorithm in detail. \n",
      "• \n",
      "Initially, a set of initial values of the parameters are considered. A set of \n",
      "incomplete observed data is given to the system with the assumption that the \n",
      "observed data comes from a specific model. \n",
      "• \n",
      "The next step is known as “Expectation” – step or E-step. In this step, we use \n",
      "the observed data in order to estimate or guess the values of the missing or \n",
      "incomplete data. It is basically used to update the variables. \n",
      "• \n",
      "The next step is known as “Maximization”-step or M-step. In this step, we \n",
      "use the complete data generated in the preceding “Expectation” – step in \n",
      "order to update the values of the parameters. It is basically used to update the \n",
      "hypothesis. \n",
      "• \n",
      "Now, in the fourth step, it is checked whether the values are converging or \n",
      "not, if yes, then stop otherwise repeat step-2 and step-3 i.e. “Expectation” – \n",
      "step and \n",
      "“Maximization” – step until the convergence occurs. \n",
      " \n",
      "Flow chart for EM algorithm \n",
      "\n",
      "\n",
      "347. Department Of CSE \n",
      "MRCET \n",
      "53 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Usage of EM algorithm \n",
      "• \n",
      "It can be used to fill the missing data in a sample. \n",
      "• \n",
      "It can be used as the basis of unsupervised learning of clusters. \n",
      "• \n",
      "It can be used for the purpose of estimating the parameters of Hidden Markov \n",
      "Model (HMM). \n",
      "• \n",
      "It can be used for discovering the values of latent variables. \n",
      " \n",
      "Advantages of EM algorithm \n",
      "• \n",
      "It is always guaranteed that likelihood will increase with each iteration. \n",
      "• \n",
      "The E-step and M-step are often pretty easy for many problems in terms of \n",
      "implementation. \n",
      "• \n",
      "Solutions to the M-steps often exist in the closed form. \n",
      "\n",
      "\n",
      "348. Department Of CSE \n",
      "MRCET \n",
      "54 \n",
      " \n",
      " \n",
      " \n",
      "Instance-based learning \n",
      "The Machine Learning systems which are categorized as instance-based \n",
      "learning are the systems that learn the training examples by heart and then \n",
      "generalizes to new instances based on some similarity measure. It is called \n",
      "instance-based because it builds the hypotheses from the training instances. \n",
      "It is also known as memory-based learning or lazy-learning. The time \n",
      "complexity of this algorithm depends upon the size of training data. The \n",
      "worst-case time complexity of this algorithm is O (n), where n is the \n",
      "number of training instances. \n",
      "For example, If we were to create a spam filter with an instance-based \n",
      "learning algorithm, instead of just flagging emails that are already marked as \n",
      "spam emails, our spam filter would be programmed to also flag emails that \n",
      "are very similar to them. This requires a measure of resemblance between \n",
      "two emails. A similarity measure between two emails could be the same \n",
      "sender or the repetitive use of the same keywords or something else. \n",
      " \n",
      " \n",
      "Advantages: \n",
      "1. Instead of estimating for the entire instance set, local approximations can be \n",
      "made to the target function. \n",
      "2. This algorithm can adapt to new data easily, one which is collected as we go. \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Disadvantages: \n",
      "1. Classification costs are high \n",
      "2. Large amount of memory required to store the data, and each \n",
      "query involves starting the identification of a local model from scratch. \n",
      "Some of the instance-based learning algorithms are : \n",
      "1. K Nearest Neighbor (KNN) \n",
      "2. Self-Organizing Map (SOM) \n",
      "3. Learning Vector Quantization (LVQ) \n",
      "4. Locally Weighted Learning (LWL) \n",
      "\n",
      "\n",
      "349. Department Of CSE \n",
      "MRCET \n",
      "55 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "K-Nearest Neighbor(KNN) Algorithm \n",
      "• \n",
      "K-Nearest Neighbour is one of the simplest Machine Learning algorithms \n",
      "based on Supervised Learning technique. \n",
      "• \n",
      "K-NN algorithm assumes the similarity between the new case/data and \n",
      "available cases and put the new case into the category that is most similar to \n",
      "the available categories. \n",
      "• \n",
      "K-NN algorithm stores all the available data and classifies a new data point \n",
      "based on the similarity. This means when new data appears then it can be \n",
      "easily classified into a well suite category by using K- NN algorithm. \n",
      "• \n",
      "K-NN algorithm can be used for Regression as well as for Classification but \n",
      "mostly it is used for the Classification problems. \n",
      "• \n",
      "K-NN is a non-parametric algorithm, which means it does not make any \n",
      "assumption on underlying data. \n",
      "• \n",
      "It is also called a lazy learner algorithm because it does not learn from the \n",
      "training set immediately instead it stores the dataset and at the time of \n",
      "classification, it performs an action on the dataset. \n",
      "• \n",
      "KNN algorithm at the training phase just stores the dataset and when it gets \n",
      "new data, then it classifies that data into a category that is much similar to the \n",
      "new data. \n",
      "Working of KNN Algorithm \n",
      "K-nearest neighbours (KNN) algorithm uses ‘feature similarity’ to predict \n",
      "the values of new data points which further means that the new data point \n",
      "will be assigned a value based on how closely it matches the points in the \n",
      "training set. We can understand its working with the help of following steps \n",
      "− \n",
      "Step 1 − For implementing any algorithm, we need dataset. So during the \n",
      "first step of KNN, we must load the training as well as test data. \n",
      "Step 2 − Next, we need to choose the value of K i.e. the nearest data points. \n",
      "K can be any integer. \n",
      "Step 3 − For each point in the test data do the following \n",
      "\n",
      "\n",
      "350. Department Of CSE \n",
      "MRCET \n",
      "56 \n",
      " \n",
      " \n",
      " \n",
      "• 3.1 − Calculate the distance between test data and each row of training data \n",
      "with the help of any of the method namely: Euclidean, Manhattan or \n",
      "Hamming distance. The most commonly used method to calculate distance is \n",
      "Euclidean. \n",
      "• 3.2 − Now, based on the distance value, sort them in ascending order. \n",
      "• 3.3 − Next, it will choose the top K rows from the sorted array. \n",
      "• 3.4 − Now, it will assign a class to the test point based on most frequent class \n",
      "of these rows. \n",
      "Step 4 – End \n",
      "EXAMPLE : \n",
      " \n",
      "Case Based Reasoning \n",
      " \n",
      "As we know Nearest Neighbour classifiers stores training tuples as points in \n",
      "Euclidean space. But Case-Based Reasoning classifiers (CBR) use a \n",
      "database of problem solutions to solve new problems. It stores the tuples or \n",
      "cases for problem-solving as complex symbolic descriptions. \n",
      "How CBR works? \n",
      "When a new case arrises to classify, a Case-based Reasoner(CBR) will first \n",
      "check if an identical training case exists. If one is found, then the \n",
      "accompanying solution to that case is returned. If no identical case is found, \n",
      "then the CBR will search for training cases having components that are \n",
      "similar to those of the new case. Conceptually, these training cases may be \n",
      "considered as neighbours of the new case. If cases are represented as graphs, \n",
      "this involves searching for subgraphs that are similar to subgraphs within the \n",
      "new case. The CBR tries to combine the solutions of the neighbouring \n",
      "training cases to propose a solution for the new case. If compatibilities arise \n",
      "with the individual solutions, then backtracking to search for other solutions \n",
      "\n",
      "\n",
      "351. Department Of CSE \n",
      "MRCET \n",
      "57 \n",
      " \n",
      " \n",
      " \n",
      "may be necessary. The CBR may employ background knowledge and \n",
      "problem-solving strategies to propose a feasible solution. \n",
      "Applications of CBR includes: \n",
      "1. Problem resolution for customer service help desks, where cases describe \n",
      "product-related diagnostic problems. \n",
      "2. It is also applied to areas such as engineering and law, where cases are either \n",
      "technical designs or legal rulings, respectively. \n",
      "3. Medical educations, where patient case histories and treatments are used to \n",
      "help diagnose and treat new patients. \n",
      " \n",
      "Challenges with CBR \n",
      "• \n",
      "Finding a good similarity metric (eg for matching subgraphs) and suitable \n",
      "methods for combining solutions. \n",
      "• \n",
      "Selecting salient features for indexing training cases and the development of \n",
      "efficient indexing techniques. \n",
      " \n",
      "CBR becomes more intelligent as the number of the trade-off between \n",
      "accuracy and efficiency evolves as the number of stored cases becomes very \n",
      "large. But after a certain point, the system’s efficiency will suffer as the time \n",
      "required to search for and process relevant cases increases. \n",
      " \n",
      " \n",
      "Some differences on eager and lazy learning \n",
      "• \n",
      "Eager learning methods construct general, explicit description of the target \n",
      "function based on the provided training examples. \n",
      "• \n",
      "Lazy learning methods simply store the data and generalizing beyond these \n",
      "data is postponed until an explicit request is made. \n",
      "• \n",
      "Lazy learning methods can construct a different approximation to the target \n",
      "function for each encountered query instance. \n",
      " \n",
      "Lazy learning is very suitable for complex and incomplete problem domains, \n",
      "where a complex target function can be represented by a collection of less \n",
      "complex local approximations. \n",
      "Eager learning methods use the same approximation to the target function, \n",
      "which must be learned based on training examples and before input queries \n",
      "are observed. \n",
      "\n",
      "\n",
      "352. Department Of CSE \n",
      "MRCET \n",
      "58 \n",
      " \n",
      " \n",
      " \n",
      "UNIT - IV \n",
      "PATTERN COMPARISON TECHNIQUES \n",
      " \n",
      " \n",
      "Pattern recognition is a process of finding regularities and similarities in data \n",
      "using machine learning data. Now, these similarities can be found based on \n",
      "statistical analysis, historical data, or the already gained knowledge by the \n",
      "machine itself. A pattern is a regularity in the world or in abstract notions. If we \n",
      "discuss sports, a description of a type would be a pattern. If a person keeps \n",
      "watching videos related to cricket, YouTube wouldn’t recommend them chess \n",
      "tutorials videos. \n",
      "Examples: Speech recognition, speaker identification, multimedia document \n",
      "recognition (MDR), automatic medical diagnosis. \n",
      " \n",
      "Before searching for a pattern there are some certain steps and the first one is to \n",
      "collect the data from the real world. The collected data needs to be filtered and \n",
      "preprocessed so that its system can extract the features from the data. Then \n",
      "based on the type of the data system will choose the appropriate algorithm \n",
      "among Classification, Regression, and Regression to recognize the pattern. \n",
      "• Classification. In classification, the algorithm assigns labels to data based on \n",
      "the predefined features. This is an example of supervised learning. \n",
      "• Clustering. An algorithm splits data into a number of clusters based on the \n",
      "similarity of features. This is an example of unsupervised learning. \n",
      "• Regression. Regression algorithms try to find a relationship between variables \n",
      "and predict unknown dependent variables based on known data. It is based on \n",
      "supervised learning. [2] \n",
      "• Features can be represented as continuous, discrete, or discrete binary \n",
      "variables. A feature is basically a function of one or more measurements, \n",
      "computed to quantify the significant characteristics of the object. The feature is \n",
      "one of the most important components in the Pattern Recognition system. \n",
      "Example: consider a football, shape, size and color, etc. are features of the \n",
      "football. \n",
      "\n",
      "\n",
      "353. Department Of CSE \n",
      "MRCET \n",
      "59 \n",
      " \n",
      " \n",
      "A feature vector is a set of features that are taken together. \n",
      "Example: In the above example of football, if all the features (shape, size, color \n",
      "etc.) taken together then the sequence is feature vector ([shape, size, color]). \n",
      "The feature vector is the sequence of features represented as an n-dimensional \n",
      "column vector. In the case of speech, MFCC (Mel-frequency Cepstral \n",
      "Coefficient) is the spectral features of the speech. The sequence of the first 13 \n",
      "features forms a feature vector. \n",
      " \n",
      " \n",
      "Temporal patterns \n",
      " \n",
      " \n",
      "Temporal patterns are one of the pattern comparison techniques that is defined \n",
      "as a segment of signals that recurs frequently in the whole temporal signal \n",
      "sequence. For example, the temporal signal sequences could be the movements \n",
      "of head, hand, and body, a piece of music, and so on. \n",
      "Temporal abstraction and data mining are two research fields that have tried to \n",
      "synthesis time oriented data and bring out an understanding on the hidden \n",
      "relationships that may exist between time oriented events. In clinical settings, \n",
      "having the ability to know the hidden relationships on patient data as they \n",
      "unfold could help save a life by aiding in detection of conditions that are not \n",
      "obvious to clinicians and healthcare workers. Understanding the hidden patterns \n",
      "is a huge challenge due to the exponential search space unique to time-series \n",
      "data. In this paper, we propose a temporal pattern recognition model based on \n",
      "dimension reduction and similarity measures thereby maintaining the temporal \n",
      "nature of the raw data \n",
      " \n",
      " \n",
      "INTRODUCTION \n",
      "Temporal pattern processing is important for various intelligent behaviours, \n",
      "including hearing, vision, speech, music and motor control. Because we live in \n",
      "an ever-changing environment, an intelligent system, whether it be a human or a \n",
      "robot, must encode patterns over time, recognize and generate temporal \n",
      "patterns. Time is embodied in a temporal pattern in two different ways: • \n",
      "Temporal order. It refers to the ordering among the components of a sequence. \n",
      "For example, the sequence N-E-T is different from T-E-N. Temporal order may \n",
      "\n",
      "\n",
      "354. Department Of CSE \n",
      "MRCET \n",
      "60 \n",
      " \n",
      " \n",
      "also refer to a syntactic structure, such as subject-verb-object, where each \n",
      "component may be any of a category of possible symbols \n",
      "• Time duration. Duration can play a critical role for temporal processing. In \n",
      "speech recognition, for example, we want rate invariance while distinguishing \n",
      "relative durations of the vowel /i:/ (as in beet) and /i/ (as in bit) \n",
      "TEMPORAL PATTERN RECOGNITION \n",
      "The shared goal of all STM models is to make input history available \n",
      "simultaneously when recognition takes place. With a STM model in place, \n",
      "recognition is not much different from the recognition of static patterns. \n",
      " \n",
      "Template Matching Using Hebbian Learning \n",
      "The architecture for this type of recognition is simply a two-layer network: the \n",
      "input layer that incorporates STM, and the sequence recognition layer where \n",
      "each unit encodes an individual sequence. The recognition scheme is essentially \n",
      "template matching, where templates are formed through following Hebbian \n",
      "learning \n",
      " \n",
      "Wij(t) = Wij(t–1) + C si (t)[xj (t) – Wij(t–1)] \n",
      " \n",
      "where Wij is the connection weight from unit xj in the input layer to sequence \n",
      "recognizer si in the recognition layer. Parameter C controls learning rate. \n",
      "Hebbian learning is applied after the presentation of the entire sequence is \n",
      "completed. The templates thus formed can be used to recognize specific input \n",
      "sequences. The recognition layer typically includes recurrent connections for \n",
      "selecting a winner by self-organization (e.g. winner-take-all) during training or \n",
      "recognition. \n",
      " \n",
      "Associative Memory Approach \n",
      "The dynamics of the Hopfield associative memory model can be characterized \n",
      "as evolving towards the memory state most similar to the current input pattern. \n",
      "\n",
      "\n",
      "355. Department Of CSE \n",
      "MRCET \n",
      "61 \n",
      " \n",
      " \n",
      "If one views each memory state as a category, the Hopfield net performs pattern \n",
      "recognition: the recalled category is the recognized pattern. This process of \n",
      "dynamic evolution can also be viewed as an optimization process, which \n",
      "minimizes a cost function until equilibrium is reached. \n",
      "With normalized exponential kernel STM, Tank and Hopfield (1987) described \n",
      "a recognition network based on associative memory dynamics. A layer of \n",
      "sequence recognizers receives inputs from the STM model. Each recognizer \n",
      "encodes a different template sequence by its unique weight vector acting upon \n",
      "the inputs in STM. In addition, recognizers form a competitive network. The \n",
      "recognition process uses the current input sequence (evidence) to bias a \n",
      "minimization process so that the most similar template wins the competition, \n",
      "thus activating its corresponding recognizer. Due to the exponential kernels, \n",
      "they demonstrated that recognition is fairly robust to time warping, distortions \n",
      "in duration. A similar architecture is later applied to speakerindependent spoken \n",
      "digit recognition. \n",
      " \n",
      "Multilayer Perceptrons \n",
      "A popular approach to temporal pattern learning is multilayer perceptrons \n",
      "(MLP). MLPs have been demonstrated to be effective for static pattern \n",
      "recognition. It is natural to combine MLP with an STM model to do temporal \n",
      "pattern recognition. For example, using delay line STM Waibel et al. (1989) \n",
      "reported an architecture called Time Delay Neural Networks (TDNN) for \n",
      "spoken phoneme recognition. Besides the input layer, TDNN uses 2 hidden \n",
      "layers and an output layer where each unit encodes one phoneme. The feed \n",
      "forward connections converge from the input layer to each successive layer so \n",
      "that each unit in a specific layer receives inputs within a limited time window \n",
      "from the previous layer. They demonstrated good recognition performance: for \n",
      "the three stop consonants /b/, /d/, and /g/, the accuracy of speaker dependent \n",
      "recognition reached 98.5%. \n",
      "DYNAMIC TIME WARPING \n",
      "Sounds like time traveling or some kind of future technic, however, it is not. \n",
      "Dynamic Time Warping is used to compare the similarity or calculate the \n",
      "\n",
      "\n",
      "356. Department Of CSE \n",
      "MRCET \n",
      "62 \n",
      " \n",
      " \n",
      "distance between two arrays or time series with different length. Suppose we \n",
      "want to calculate the distance of two equal-length arrays: \n",
      "a = [1, 2, \n",
      "3] b = [3, \n",
      "2, 2] \n",
      "How to do that? One obvious way is to match up a and b in 1-to-1 fashion and \n",
      "sum up the total distance of each component. This sounds easy, but what if a \n",
      "and b have different lengths? \n",
      "a = [1, 2, 3] b \n",
      "= [2, 2, 2, 3, \n",
      "4] \n",
      "How to match them up? Which should map to which? To solve the problem, \n",
      "there comes dynamic time warping. Just as its name indicates, to warp the series \n",
      "so that they can match up. \n",
      " \n",
      "Use Cases \n",
      "Before digging into the algorithm, you might have the question that is it useful? \n",
      "Do we really need to compare the distance between two unequal-length time \n",
      "series? \n",
      "Yes, in a lot of scenarios DTW is playing a key role. \n",
      "Sound Pattern Recognition \n",
      "One use case is to detect the sound pattern of the same kind. Suppose we want \n",
      "to recognise the voice of a person by analysing his sound track, and we are able \n",
      "to collect his sound track of saying Hello in one scenario. However, people \n",
      "speak in the same word in different ways, what if he speaks hello in a much \n",
      "slower pace like Heeeeeeelloooooo , we will need an algorithm to match up the \n",
      "sound track of different lengths and be able to identify they come from the same \n",
      "person. \n",
      "\n",
      "\n",
      "357. Department Of CSE \n",
      "MRCET \n",
      "63 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Stock Market \n",
      "In a stock market, people always hope to be able to predict the future, however \n",
      "using general machine learning algorithms can be exhaustive, as most prediction \n",
      "task requires test and training set to have the same dimension of features. \n",
      "However, if you ever speculate in the stock market, you will know that even the \n",
      "same pattern of a stock can have very different length reflection on klines and \n",
      "indicators. \n",
      "\n",
      "\n",
      "358. Department Of CSE \n",
      "MRCET \n",
      "64 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "In time series analysis, dynamic time warping (DTW) is one of the algorithms \n",
      "for measuring similarity between two temporal sequences, which may vary in \n",
      "speed. DTW has been applied to temporal sequences of video, audio, and \n",
      "graphics data — indeed, any data that can be turned into a linear sequence can \n",
      "be analysed with DTW. \n",
      "The idea to compare arrays with different length is to build one-to-many and \n",
      "many-to-one matches so that the total distance can be minimised between the \n",
      "two. \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Suppose we have two different arrays red and blue with different length: \n",
      "\n",
      "\n",
      "359. Department Of CSE \n",
      "MRCET \n",
      "65 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Clearly these two series follow the same pattern, but the blue curve is longer \n",
      "than the red. If we apply the one-to-one match, shown in the top, the mapping is \n",
      "not perfectly synced up and the tail of the blue curve is being left out. \n",
      " \n",
      "DTW overcomes the issue by developing a one-to-many match so that the \n",
      "troughs and peaks with the same pattern are perfectly matched, and there is no \n",
      "left out for both curves(shown in the bottom top). \n",
      "\n",
      "\n",
      "360. Department Of CSE \n",
      "MRCET \n",
      "66 \n",
      " \n",
      " \n",
      "l > \n",
      "k \n",
      "Rules \n",
      "In general, DTW is a method that calculates an optimal match between two \n",
      "given sequences (e.g. time series) with certain restriction and rules(comes from \n",
      "wiki): \n",
      " \n",
      "• \n",
      "Every index from the first sequence must be matched with one or more indices \n",
      "from the other sequence and vice versa \n",
      " \n",
      "• \n",
      "The first index from the first sequence must be matched with the first index from \n",
      "the other sequence (but it does not have to be its only match) \n",
      "• \n",
      "The last index from the first sequence must be matched with the last index from \n",
      "the other sequence (but it does not have to be its only match) \n",
      "• \n",
      "The mapping of the indices from the first sequence to indices from the other \n",
      "sequence must be monotonically increasing, and vice versa, i.e. if \n",
      "from the first sequence, then \n",
      "are indices \n",
      "there must not be two indices \n",
      "in the other sequence, such \n",
      " \n",
      " \n",
      "that index i is matched with index l and index j is matched with index k , and \n",
      "vice versa. \n",
      "The optimal match is denoted by the match that satisfies all the restrictions and \n",
      "the rules and that has the minimal cost, where the cost is computed as the sum of \n",
      "absolute differences, for each matched pair of indices, between their values. \n",
      "j > i \n",
      "\n",
      "\n",
      "361. Department of CSE \n",
      "MRCET \n",
      "67 \n",
      " \n",
      " \n",
      "Introduction to Clustering: \n",
      "It is basically a type of unsupervised learning method. An unsupervised learning method \n",
      "is a method in which we draw references from datasets consisting of input data without \n",
      "labelled responses. Generally, it is used as a process to find meaningful structure, \n",
      "explanatory underlying processes, generative features, and groupings inherent in a set of \n",
      "examples. \n",
      "Clustering is the task of dividing the population or data points into a number of groups \n",
      "such that data points in the same groups are more similar to other data points in the same \n",
      "group and dissimilar to the data points in other groups. It is basically a collection of \n",
      "objects on the basis of similarity and dissimilarity between them. \n",
      "For ex– The data points in the graph below clustered together can be classified into one \n",
      "single group. We can distinguish the clusters, and we can identify that there are 3 clusters \n",
      "in the below picture. \n",
      " \n",
      " \n",
      "It is not necessary for clusters to be spherical. Such as: \n",
      "\n",
      "\n",
      "362. Department of CSE \n",
      "MRCET \n",
      "68 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "DBSCAN: Density-based Spatial Clustering of Applications with Noise \n",
      "These data points are clustered by using the basic concept that the data point lies within \n",
      "the given constraint from the cluster center. Various distance methods and techniques are \n",
      "used for the calculation of the outliers. \n",
      "Why Clustering? \n",
      "Clustering is very much important as it determines the intrinsic grouping among the \n",
      "unlabelled data present. There are no criteria for good clustering. It depends on the user, \n",
      "what is the criteria they may use which satisfy their need. For instance, we could be \n",
      "interested in finding representatives for homogeneous groups (data reduction), in finding \n",
      "“natural clusters” and describe their unknown properties (“natural” data types), in finding \n",
      "useful and suitable groupings (“useful” data classes) or in finding unusual data objects \n",
      "(outlier detection). This algorithm must make some assumptions that constitute the \n",
      "similarity of points and each assumption make different and equally valid clusters. \n",
      "Clustering Methods : \n",
      "• \n",
      "Density-Based Methods: These methods consider the clusters as the dense region having \n",
      "some similarities and differences from the lower dense region of the space. These \n",
      "methods have good accuracy and the ability to merge two clusters. Example DBSCAN \n",
      "(Density-Based Spatial Clustering of Applications with Noise), OPTICS (Ordering Points \n",
      "to Identify Clustering Structure), etc. \n",
      "• \n",
      "Hierarchical Based Methods: The clusters formed in this method form a treetype \n",
      "structure based on the hierarchy. New clusters are formed using the previously formed \n",
      "one. It is divided into two category \n",
      "• \n",
      "Agglomerative (bottom-up approach) \n",
      "• \n",
      "Divisive (top-down approach) \n",
      "\n",
      "\n",
      "363. Department of CSE \n",
      "MRCET \n",
      "69 \n",
      " \n",
      " \n",
      "examples CURE (Clustering Using Representatives), BIRCH (Balanced \n",
      "Iterative \n",
      "Reducing Clustering and using Hierarchies), etc. \n",
      "• \n",
      "Partitioning Methods: These methods partition the objects into k clusters and each \n",
      "partition forms one cluster. This method is used to optimize an objective criterion \n",
      "similarity function such as when the distance is a major parameter example K-means, \n",
      "CLARANS (Clustering Large Applications based upon Randomized Search), etc. \n",
      "• \n",
      "Grid-based Methods: In this method, the data space is formulated into a finite number of \n",
      "cells that form a grid-like structure. All the clustering operations done on these grids are \n",
      "fast and independent of the number of data objects example STING (Statistical \n",
      "Information Grid), wave cluster, CLIQUE (CLustering In Quest), etc. \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "K means Clustering: \n",
      "It is the simplest unsupervised learning algorithm that solves clustering problem.K-means \n",
      "algorithm partitions n observations into k clusters where each observation belongs to the \n",
      "cluster with the nearest mean serving as a prototype of the cluster. \n",
      " \n",
      "Applications of Clustering in different fields \n",
      "• \n",
      "Marketing: It can be used to characterize & discover customer segments for marketing \n",
      "purposes. \n",
      "• \n",
      "Biology: It can be used for classification among different species of plants and animals. \n",
      "• \n",
      "Libraries: It is used in clustering different books on the basis of topics and information. \n",
      "• \n",
      "Insurance: It is used to acknowledge the customers, their policies and identifying the \n",
      "frauds. \n",
      "• \n",
      "City Planning: It is used to make groups of houses and to study their values based on \n",
      "their geographical locations and other factors present. \n",
      "\n",
      "\n",
      "364. Department of CSE \n",
      "MRCET \n",
      "70 \n",
      " \n",
      " \n",
      "• \n",
      "Earthquake studies: By learning the earthquake-affected areas we can determine the \n",
      "dangerous zones. \n",
      "The algorithm will categorize the items into k groups of similarity. To calculate \n",
      "that similarity, we will use the euclidean distance as measurement. The algorithm \n",
      "works as follows: \n",
      " \n",
      "1. First, we initialize k points, called means, randomly. \n",
      "2. We categorize each item to its closest mean and we update the mean’s coordinates, which \n",
      "are the averages of the items categorized in that mean so far. \n",
      "3. We repeat the process for a given number of iterations and at the end, we have our \n",
      "clusters. \n",
      "The “points” mentioned above are called means because they hold the mean values of the \n",
      "items categorized in them. To initialize these means, we have a lot of options. An intuitive \n",
      "method is to initialize the means at random items in the data set. Another method is to \n",
      "initialize the means at random values between the boundaries of the data set (if for a \n",
      "feature x the items have values in [0,3], we will initialize the means with values for x at \n",
      "[0,3]). \n",
      " \n",
      " \n",
      " \n",
      "The above algorithm in pseudocode: \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "K-MODE CLUSTERING \n",
      "KModes clustering is one of the unsupervised Machine Learning algorithms that is used to \n",
      "cluster categorical variables. \n",
      "How does the KModes algorithm work? \n",
      " \n",
      "1. Pick K observations at random and use them as leaders/clusters \n",
      "2. Calculate the dissimilarities and assign each observation to its closest cluster \n",
      "3. Define new modes for the clusters \n",
      "\n",
      "\n",
      "365. Department of CSE \n",
      "MRCET \n",
      "71 \n",
      " \n",
      " \n",
      "4. Repeat 2–3 steps until there are is no re-assignment required \n",
      "Example: Imagine we have a dataset that has the information about hair color, eye color, and \n",
      "skin color of persons. We aim to group them based on the available information(maybe we \n",
      "want to suggest some styling ideas) \n",
      "Hair color, eye color, and skin color are all categorical variables. Below is how our dataset \n",
      "looks like. \n",
      " \n",
      " \n",
      "Alright, we have the sample data now. Let us proceed by defining the number of \n",
      "clusters(K)=3 \n",
      "Step 1: Pick K observations at random and use them as leaders/clusters \n",
      "I am choosing P1, P7, P8 as leaders/clusters \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Step 2: Calculate the dissimilarities(no. of mismatches) and assign each observation to its \n",
      "closest cluster \n",
      "Iteratively compare the cluster data points to each of the observations. Similar data points \n",
      "give 0, dissimilar data points give 1. \n",
      "\n",
      "\n",
      "366. Department of CSE \n",
      "MRCET \n",
      "72 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Comparing leader/Cluster P1 to the observation P1 gives 0 dissimilarities \n",
      " \n",
      "Comparing leader/cluster P1 to the observation P2 gives 3(1+1+1) \n",
      "dissimilarities. Likewise, calculate all the dissimilarities and put them in a matrix as shown \n",
      "below and assign the observations to their closest cluster (cluster that has the least \n",
      "dissimilarity) \n",
      "\n",
      "\n",
      "367. Department of CSE \n",
      "MRCET \n",
      "73 \n",
      " \n",
      " \n",
      "After step 2, the observations P1, P2, P5 are assigned to cluster 1; P3, P7 are assigned to \n",
      "Cluster 2; and P4, P6, P8 are assigned to cluster 3. \n",
      "Step 3: Define new modes for the clusters \n",
      "Mode is simply the most observed value. Mark the observations according to the cluster \n",
      "they belong to. Observations of Cluster 1 are marked in Yellow, Cluster 2 are marked in \n",
      "Brick red, and Cluster 3 are marked in Purple. \n",
      " \n",
      "Considering one cluster at a time, for each feature, look for the Mode and update the new \n",
      "leaders. \n",
      " \n",
      "Explanation: Cluster 1 observations(P1, P2, P5) has brunette as the most observed hair \n",
      "color, amber as the most observed eye color, and fair as the most observed skin color. \n",
      "Below are our new leaders after the update. \n",
      " \n",
      " \n",
      "Repeat steps 2–4 : After obtaining the new leaders, again calculate the dissimilarities \n",
      "between the observations and the newly obtained leaders. \n",
      "\n",
      "\n",
      "368. Department of CSE \n",
      "MRCET \n",
      "74 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Comparing Cluster 1 to the observation P1 gives 1 dissimilarity. \n",
      " \n",
      " \n",
      " \n",
      "Comparing Cluster 1 to the observation P2 gives 2 dissimilarities. \n",
      " \n",
      "Likewise, calculate all the dissimilarities and put them in a matrix. Assign each \n",
      "observation to its closest cluster. \n",
      "\n",
      "\n",
      "369. Department of CSE \n",
      "MRCET \n",
      "75 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "The observations P1, P2, P5 are assigned to Cluster 1; P3, P7 are assigned to Cluster 2; \n",
      "and P4, P6, P8 are assigned to Cluster 3. \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "We stop here as we see there is no change in the assignment of observations. \n",
      "Implementation of KModes in Python: \n",
      "Begin with Importing necessary libraries \n",
      "\n",
      "\n",
      "370. Department of CSE \n",
      "MRCET \n",
      "76 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Vector Quantization \n",
      "Learning Vector Quantization ( or LVQ ) is a type of Artificial Neural Network which \n",
      "also inspired by biological models of neural systems. It is based on prototype supervised \n",
      "learning classification algorithm and trained its network through a competitive learning \n",
      "algorithm similar to Self Organizing Map. It can also deal with the multiclass \n",
      "classification problem. LVQ has two layers, one is the Input layer and the other one is the \n",
      "Output layer. The architecture of the Learning Vector Quantization with the number of \n",
      "classes in an input data and n number of input features for any sample is given below: \n",
      "\n",
      "\n",
      "371. Department of CSE \n",
      "MRCET \n",
      "77 \n",
      " \n",
      " \n",
      "i\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Let say an input data of size ( m, n ) where m is number of training example and n is the \n",
      "number of features in each example and a label vector of size ( m, 1 ). First, it initializes \n",
      "the weights of size ( n, c ) from the first c number of training samples with different labels \n",
      "and should be discarded from all training samples. Here, c is the number of classes. Then \n",
      "iterate over the remaining input data, for each training example, it updates the winning \n",
      "vector ( weight vector with the shortest distance ( e.g Euclidean distance ) from training \n",
      "example ). Weight updation rule is given by : \n",
      "wij = wij(old) - alpha(t) * (x k - wij(old)) \n",
      " \n",
      "where alpha is a learning rate at time t, j denotes the winning vector, i denotes the ith \n",
      "feature of training example and k denotes the kth training example from the input data. \n",
      "After training the LVQ network, trained weights are used for classifying new examples. \n",
      "A new example labeled with the class of winning vector. \n",
      " \n",
      "Algorithm \n",
      " \n",
      "Steps involved are : \n",
      "• \n",
      "Weight initialization \n",
      "• \n",
      "For 1 to N number of epochs \n",
      "• \n",
      "Select a training example \n",
      "• \n",
      "Compute the winning vector \n",
      "• \n",
      "Update the winning vector \n",
      "• \n",
      "Repeat steps 3, 4, 5 for all training example. \n",
      "• \n",
      "Classify test sample \n",
      "\n",
      "\n",
      "372. Department of CSE \n",
      "MRCET \n",
      " \n",
      " \n",
      " \n",
      "Genetic Algorithms \n",
      "UNIT- V \n",
      " \n",
      " \n",
      "Genetic Algorithms(GAs) are adaptive heuristic search algorithms that \n",
      "belong to the larger part of evolutionary algorithms. Genetic algorithms \n",
      "are based on the ideas of natural selection and genetics. These are \n",
      "intelligent exploitation of random search provided with historical data to \n",
      "direct the search into the region of better performance in solution space. \n",
      "They are commonly used to generate high-quality solutions for \n",
      "optimization problems and search problems. \n",
      "Genetic algorithms simulate the process of natural selection which means \n",
      "those species who can adapt to changes in their environment are able to \n",
      "survive and reproduce and go to next generation. In simple words, they \n",
      "simulate “survival of the fittest” among individual of consecutive \n",
      "generation for solving a problem. Each generation consist of a population \n",
      "of individuals and each individual represents a point in search space and \n",
      "possible solution. Each individual is represented as a string of \n",
      "character/integer/float/bits. This string is analogous to the Chromosome. \n",
      "Different search methods for induction \n",
      "In the field of machine learning, an induction algorithm represents an \n",
      "example of using mathematical principles for the development of \n",
      "sophisticated computing systems. Machine learning systems go beyond a \n",
      "simple “rote input/output” function, and evolve the results that they supply \n",
      "with continued use. Induction algorithms can help with the real-time \n",
      "handling of sophisticated data sets, or more long-term efforts. \n",
      " \n",
      "The induction algorithm is something that applies to systems that show \n",
      "complex results depending on what they are set up for. One of the most \n",
      "fundamental ways that engineers use an induction algorithm is to enhance \n",
      "knowledge acquisition in a given system. In other words, with the \n",
      "algorithm in place, the set of “knowledge data” that end users get is \n",
      "somehow improved, whether that’s regarding the quantity of data, the \n",
      "filtering of noise and undesirable results, or the refinement of some data \n",
      "points. \n",
      "Machine Learning \n",
      "R20D5803 \n",
      "\n",
      "\n",
      "373. Department of CSE \n",
      "MRCET \n",
      "79 \n",
      " \n",
      " \n",
      "Although the technical descriptions of induction algorithms are largely the \n",
      "territory of mathematical and scientific journals, one of the basic ideas \n",
      "about using the induction algorithm is that it can organize “classification \n",
      "rules” according to the induction principle and separate corollary results \n",
      "from different kinds of \n",
      " \n",
      "system noise or exceptions. Filtering out noise from a domain is a \n",
      "prominent use of the induction algorithm in general. There is the idea that \n",
      "in real-world data filtering, induction algorithms can compose different \n",
      "sets of rules for both the legitimate results and the system noise, in order to \n",
      "distinguish one from the other. \n",
      " \n",
      "By setting up induction algorithms according to certain training examples, \n",
      "stakeholders are looking for the ability of these systems to identify and \n",
      "assess consistent rules and data that represents exceptions to these rules. In \n",
      "a sense, the use of an induction algorithm uses the induction principle to \n",
      "“prove” certain results that can aid knowledge, because they provide more \n",
      "marked delineations in a data set (or multiple data sets) – distinctions that \n",
      "can drive all sorts of end user capabilities. \n",
      " \n",
      "Like other kinds of machine learning software, induction algorithms are \n",
      "often thought of as a form of “decision support.” \n",
      " \n",
      "“We consider the principal task of a real-world induction system to be \n",
      "assisting the expert in expressing his or her expertise,” write the authors of \n",
      "a Turing Institute paper on induction in machine learning back in the \n",
      "1980s. “Consequently, we require that the induced rules are highly \n",
      "predictive and are easily comprehensible to the expert.” \n",
      " \n",
      "With this in mind, induction algorithms can be part of many kinds of \n",
      "software products that seek to refine data and produce evolving results for \n",
      "human users. In general, machine learning and the use of visual \n",
      "dashboards is generating new tools through which users can more rapidly \n",
      "develop in-depth knowledge about any given system, whether it's related \n",
      "to marine research, medical diagnosis, e-commerce, or any other kind of \n",
      "data-rich system. \n",
      " \n",
      "Explanation-Based Learning (EBL) \n",
      "\n",
      "\n",
      "374. Department of CSE \n",
      "MRCET \n",
      "80 \n",
      " \n",
      " \n",
      "In simple terms, it is the ability to gain basic problem-solving techniques \n",
      "by observing and analysing solutions to specific problems. In terms of \n",
      "Machine Learning, it is an algorithm that aims to understand why an \n",
      "example is a part of a particular concept to make generalizations or form \n",
      "concepts from training examples. For example, EBL uses a domain \n",
      "theory and creates a program that learns to play chess. EBL involves 2 \n",
      "steps: \n",
      "1. Explanation — The domain theory is used to eliminate all the unimportant \n",
      "training example while retaining the important ones that best describe the goal \n",
      "concept. \n",
      "2. Generalization — The explanation of the goal concept is made as general \n",
      "and widely applicable as possible. This ensures that all cases are covered, \n",
      "not just certain specific ones. \n",
      "EBL Architecture: \n",
      "• \n",
      "EBL model during training \n",
      "• \n",
      "During training, the model generalizes the training example in such a way that \n",
      "all scenarios lead to the Goal Concept, not just in specific cases. (As shown in \n",
      "Fig 1) \n",
      "\n",
      "\n",
      "375. Department of CSE \n",
      "MRCET \n",
      "81 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "• \n",
      "EBL model after training \n",
      "• \n",
      "Post training, EBL model tends to directly reach the hypothesis space involving \n",
      "the goal concept.  (As shown in Fig 2) \n",
      "\n",
      "\n",
      "376. Department of CSE \n",
      "MRCET \n",
      "82 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Dimensionality Reduction \n",
      "An intuitive example of dimensionality reduction can be discussed through \n",
      "a simple e-mail classification problem, where we need to classify whether \n",
      "the e-mail is spam or not. This can involve a large number of features, \n",
      "such as whether or not the e-mail has a generic title, the content of the e- \n",
      "mail, whether the e-mail uses a template, etc. However, some of these \n",
      "features may overlap. In another condition, a classification problem that \n",
      "relies on both humidity and rainfall can be collapsed into just one \n",
      "underlying feature, since both of the aforementioned are correlated to a \n",
      "high degree. Hence, we can reduce the number of features in such \n",
      "problems. A 3D classification problem can be hard to visualize, whereas a \n",
      "2-D one can be mapped to a simple 2 dimensional space, and a 1-D \n",
      "problem to a simple line. The below figure illustrates this concept, where a \n",
      "3-D feature space is split into two 1-D feature spaces, and later, if found to \n",
      "be correlated, the number of features can be reduced even further. \n",
      "\n",
      "\n",
      "377. Department of CSE \n",
      "MRCET \n",
      "83 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Components of Dimensionality \n",
      "Reduction There are two components of dimensionality \n",
      "reduction: \n",
      "• \n",
      "Feature selection: In this, we try to find a subset of the original set of variables, \n",
      "or features, to get a smaller subset which can be used to model the problem. It \n",
      "usually involves three ways: \n",
      "1. Filter \n",
      "2. Wrapper \n",
      "3. Embedded \n",
      "• \n",
      "Feature extraction: This reduces the data in a high dimensional space to a \n",
      "lower dimension space, i.e. a space with lesser no. of dimensions. \n",
      "Methods of Dimensionality Reduction The various \n",
      "methods used for dimensionality reduction include: \n",
      "• \n",
      "Principal Component Analysis (PCA) \n",
      "• \n",
      "Linear Discriminant Analysis (LDA) \n",
      "• \n",
      "Generalized Discriminant Analysis (GDA) \n",
      "Dimensionality reduction may be both linear or non-linear, depending \n",
      "upon the method used. The prime linear method, called Principal \n",
      "Component Analysis, or PCA, is discussed below. \n",
      "\n",
      "\n",
      "378. Department of CSE \n",
      "MRCET \n",
      "84 \n",
      " \n",
      " \n",
      " \n",
      "Principal Component Analysis \n",
      "This method was introduced by Karl Pearson. It works on a condition that \n",
      "while the data in a higher dimensional space is mapped to data in a lower \n",
      "dimension space, the variance of the data in the lower dimensional space \n",
      "should be maximum. \n",
      " \n",
      "It involves the following steps: \n",
      "• \n",
      "Construct the covariance matrix of the data. \n",
      "• \n",
      "Compute the eigenvectors of this matrix. \n",
      "• \n",
      "Eigenvectors corresponding to the largest eigenvalues are used to reconstruct a \n",
      "large fraction of variance of the original data. \n",
      "Hence, we are left with a lesser number of eigenvectors, and there might \n",
      "have been some data loss in the process. But, the most important variances \n",
      "should be retained by the remaining eigenvectors. \n",
      "Advantages of Dimensionality Reduction \n",
      "• \n",
      "It helps in data compression, and hence reduced storage space. \n",
      "• \n",
      "It reduces computation time. \n",
      "• \n",
      "It also helps remove redundant features, if any. Disadvantages of \n",
      "Dimensionality Reduction • \n",
      "It may lead to some amount of data loss. \n",
      "• \n",
      "PCA tends to find linear correlations between variables, which is sometimes \n",
      "undesirable. \n",
      "• \n",
      "PCA fails in cases where mean and covariance are not enough to define \n",
      "datasets. \n",
      "\n",
      "\n",
      "379. Department of CSE \n",
      "MRCET \n",
      "85 \n",
      " \n",
      " \n",
      "• \n",
      "We may not know how many principal components to keep- in practice, some \n",
      "thumb rules are applied. \n",
      " \n",
      " \n",
      " \n",
      "Factor analysis. \n",
      "Factor analysis is a statistical method used to describe variability among \n",
      "observed, correlated variables in terms of a potentially lower number of \n",
      "observed variables called factors. For example, it is possible that variations in \n",
      "six observed variables mainly reflect the variations in two unobserved \n",
      "(underlying) variables. Factor analysis searches for such joint variations in \n",
      "response to unobserved latent variables. The observed variables are modelled \n",
      "as linear combinations of the potential factors plus \"error\" terms, hence factor \n",
      "analysis can be thought of as a special case of errors-invariables models. \n",
      " \n",
      "Here,There is a party going into a room full of people. There is ‘n’ number of \n",
      "speakers in that room and they are speaking simultaneously at the party. In the \n",
      "same room, there are also ‘n’ number of microphones placed at different \n",
      "\n",
      "\n",
      "380. Department of CSE \n",
      "MRCET \n",
      "86 \n",
      " \n",
      " \n",
      "distances from the speakers which are recording ‘n’ speakers’ voice signals. \n",
      "Hence, the number of speakers is equal to the number must of microphones in \n",
      "the room. \n",
      "Now, using these microphones’ recordings, we want to separate all the ‘n’ \n",
      "speakers’ voice signals in the room given each microphone recorded the voice \n",
      "signals coming from each speaker of different intensity due to the difference in \n",
      "distances between them. Decomposing the mixed signal of each microphone’s \n",
      "recording into independent source’s speech signal can be done by using the \n",
      "machine learning technique, independent component analysis. \n",
      "[ X1, X2, ….., Xn ] => [ Y1, Y2, ….., Yn ] \n",
      "where, X1, X2, …, Xn are the original signals present in the mixed signal and \n",
      "Y1, Y2, …, Yn are the new features and are independent components which are \n",
      "independent of each other. \n",
      " \n",
      " \n",
      " \n",
      "Restrictions on ICA \n",
      " \n",
      "1. The independent components generated by the ICA are assumed to be \n",
      "statistically independent of each other. \n",
      "2. The independent components generated by the ICA must have non-gaussian \n",
      "distribution. \n",
      "3. The number of independent components generated by the ICA is equal to the \n",
      "number of observed mixtures. \n",
      " \n",
      "Multidimensional scaling \n",
      "Multidimensional scaling is a visual representation of distances or \n",
      "dissimilarities between sets of objects. \n",
      "“Objects” can be colors, faces, map coordinates, political persuasion, or any \n",
      "kind of real or conceptual stimuli \n",
      "(Kruskal and Wish, 1978). Objects that are more similar (or have shorter \n",
      "distances) are closer together on the graph than objects that are less similar (or \n",
      "have longer distances). As well as interpreting dissimilarities as distances on a \n",
      "\n",
      "\n",
      "381. Department of CSE \n",
      "MRCET \n",
      "87 \n",
      " \n",
      " \n",
      "graph, MDS can also serve as a dimension reduction technique for high- \n",
      "dimensional data (Buja et. al, 2007). \n",
      "The term scaling comes from psychometrics, where abstract concepts \n",
      "(“objects”) are assigned numbers according to a rule (Trochim, 2006). For \n",
      "example, you may want to quantify a person’s attitude to global warming. You \n",
      "could assign a “1” to “doesn’t believe in global warming”, a 10 to “firmly \n",
      "believes in global warming” and a scale of 2 to 9 for attitudes in between. You \n",
      "can also think of “scaling” as the fact that you’re essentially scaling down the \n",
      "data (i.e. \n",
      "making it simpler by creating lower-dimensional data). Data that is scaled down \n",
      "in dimension keeps similar properties. For example, two data points that are \n",
      "close together in high-dimensional space will also be close together in low- \n",
      "dimensional space (Martinez, 2005). The “multidimensional” part is due to the \n",
      "fact that you aren’t limited to two dimensional graphs or data. Three- \n",
      "dimensional, four-dimensional and higher plots are possible. \n",
      "MDS is now used over a wide variety of disciplines. It’s use isn’t limited to a \n",
      "specific matrix or set of data; In fact, just about any matrix can be analyzed with \n",
      "the technique as long as the matrix contains some type of relational data \n",
      "(Young, 2013). Examples of relational data include correlations, distances, \n",
      "multiple rating scales or similarities. \n",
      "Manifold learning \n",
      " \n",
      "What is a manifold? \n",
      " \n",
      "A two-dimensional manifold is any 2-D shape that can be made to fit in a higher \n",
      "dimensional space by twisting or bending it, loosely speaking. \n",
      "\n",
      "\n",
      "382. Department of CSE \n",
      "MRCET \n",
      "88 \n",
      " \n",
      " \n",
      "What is the Manifold Hypothesis? \n",
      " \n",
      "“The Manifold Hypothesis states that real-world high-dimensional data lie on \n",
      "low dimensional manifolds embedded within the high-dimensional space.” \n",
      "In simpler terms, it means that higher-dimensional data most of the time lies on \n",
      "a much closer lower-dimensional manifold. The process of modelling the \n",
      "manifold on which training instances lie is called Manifold Learning. \n",
      "Locally Linear Embedding (LLE) \n",
      " \n",
      "Locally Linear Embedding (LLE) is a Manifold Learning technique that is used \n",
      "for non-linear dimensionality reduction. It is an unsupervised learning algorithm \n",
      "that produces low-dimensional embeddings of high-dimensional inputs, relating \n",
      "each training instance to its closest neighbor. \n",
      "How does LLE work? \n",
      " \n",
      "For each training instance x(i), the algorithm first finds its k nearest neighbors \n",
      "and then tries to express x(i) as a linear function of them. In general, if there are \n",
      "m training instances in total, then it tries to find the set of weights w which \n",
      "minimizes the squared distance between x(i) and its linear representation. \n",
      " \n",
      "So, the cost function is given by \n",
      " \n",
      " \n",
      "where wi,j =0, if j is not included in the k closest neighbors of i. \n",
      " \n",
      "Also, it normalizes the weights for each training instance x(i), \n",
      "\n",
      "\n",
      "383. Department of CSE \n",
      "MRCET \n",
      "89 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Finally, each high-dimensional training instance x(i) is mapped to a low- \n",
      "dimensional (say, d dimensions) vector y(i) while preserving the neighborhood \n",
      "relationships. This is done by choosing d-dimensional coordinates which \n",
      "minimize the cost function, \n",
      " \n",
      "Here the weights wi,j are kept fixed while we try to find the optimum coordinates \n",
      "y(i) \n",
      "\n",
      "\n",
      "384. INTRODUCTION\n",
      "TO\n",
      "MACHINE LEARNING\n",
      "AN EARLY DRAFT OF A PROPOSED\n",
      "TEXTBOOK\n",
      "Nils J. Nilsson\n",
      "Robotics Laboratory\n",
      "Department of Computer Science\n",
      "Stanford University\n",
      "Stanford, CA 94305\n",
      "e-mail: nilsson@cs.stanford.edu\n",
      "November 3, 1998\n",
      "Copyright c⃝2005 Nils J. Nilsson\n",
      "This material may not be copied, reproduced, or distributed without the\n",
      "written permission of the copyright holder.\n",
      "\n",
      "\n",
      "385. ii\n",
      "\n",
      "\n",
      "386. Contents\n",
      "1\n",
      "Preliminaries\n",
      "1\n",
      "1.1\n",
      "Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "1\n",
      "1.1.1\n",
      "What is Machine Learning? . . . . . . . . . . . . . . . . .\n",
      "1\n",
      "1.1.2\n",
      "Wellsprings of Machine Learning . . . . . . . . . . . . . .\n",
      "3\n",
      "1.1.3\n",
      "Varieties of Machine Learning . . . . . . . . . . . . . . . .\n",
      "4\n",
      "1.2\n",
      "Learning Input-Output Functions . . . . . . . . . . . . . . . . . .\n",
      "5\n",
      "1.2.1\n",
      "Types of Learning\n",
      ". . . . . . . . . . . . . . . . . . . . . .\n",
      "5\n",
      "1.2.2\n",
      "Input Vectors . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "7\n",
      "1.2.3\n",
      "Outputs . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "8\n",
      "1.2.4\n",
      "Training Regimes . . . . . . . . . . . . . . . . . . . . . . .\n",
      "8\n",
      "1.2.5\n",
      "Noise\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "9\n",
      "1.2.6\n",
      "Performance Evaluation . . . . . . . . . . . . . . . . . . .\n",
      "9\n",
      "1.3\n",
      "Learning Requires Bias . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "9\n",
      "1.4\n",
      "Sample Applications . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "11\n",
      "1.5\n",
      "Sources\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "13\n",
      "1.6\n",
      "Bibliographical and Historical Remarks\n",
      ". . . . . . . . . . . . . .\n",
      "13\n",
      "2\n",
      "Boolean Functions\n",
      "15\n",
      "2.1\n",
      "Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "15\n",
      "2.1.1\n",
      "Boolean Algebra . . . . . . . . . . . . . . . . . . . . . . .\n",
      "15\n",
      "2.1.2\n",
      "Diagrammatic Representations . . . . . . . . . . . . . . .\n",
      "16\n",
      "2.2\n",
      "Classes of Boolean Functions\n",
      ". . . . . . . . . . . . . . . . . . . .\n",
      "17\n",
      "2.2.1\n",
      "Terms and Clauses . . . . . . . . . . . . . . . . . . . . . .\n",
      "17\n",
      "2.2.2\n",
      "DNF Functions . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "18\n",
      "2.2.3\n",
      "CNF Functions . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "21\n",
      "2.2.4\n",
      "Decision Lists . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "22\n",
      "2.2.5\n",
      "Symmetric and Voting Functions . . . . . . . . . . . . . .\n",
      "23\n",
      "2.2.6\n",
      "Linearly Separable Functions . . . . . . . . . . . . . . . .\n",
      "23\n",
      "2.3\n",
      "Summary\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "24\n",
      "2.4\n",
      "Bibliographical and Historical Remarks\n",
      ". . . . . . . . . . . . . .\n",
      "25\n",
      "iii\n",
      "\n",
      "\n",
      "387. 3\n",
      "Using Version Spaces for Learning\n",
      "27\n",
      "3.1\n",
      "Version Spaces and Mistake Bounds\n",
      ". . . . . . . . . . . . . . . .\n",
      "27\n",
      "3.2\n",
      "Version Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "29\n",
      "3.3\n",
      "Learning as Search of a Version Space\n",
      ". . . . . . . . . . . . . . .\n",
      "32\n",
      "3.4\n",
      "The Candidate Elimination Method\n",
      ". . . . . . . . . . . . . . . .\n",
      "32\n",
      "3.5\n",
      "Bibliographical and Historical Remarks\n",
      ". . . . . . . . . . . . . .\n",
      "34\n",
      "4\n",
      "Neural Networks\n",
      "35\n",
      "4.1\n",
      "Threshold Logic Units . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "35\n",
      "4.1.1\n",
      "Deﬁnitions and Geometry . . . . . . . . . . . . . . . . . .\n",
      "35\n",
      "4.1.2\n",
      "Special Cases of Linearly Separable Functions . . . . . . .\n",
      "37\n",
      "4.1.3\n",
      "Error-Correction Training of a TLU\n",
      ". . . . . . . . . . . .\n",
      "38\n",
      "4.1.4\n",
      "Weight Space . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "40\n",
      "4.1.5\n",
      "The Widrow-HoﬀProcedure . . . . . . . . . . . . . . . . .\n",
      "42\n",
      "4.1.6\n",
      "Training a TLU on Non-Linearly-Separable Training Sets\n",
      "44\n",
      "4.2\n",
      "Linear Machines\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "44\n",
      "4.3\n",
      "Networks of TLUs\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "46\n",
      "4.3.1\n",
      "Motivation and Examples . . . . . . . . . . . . . . . . . .\n",
      "46\n",
      "4.3.2\n",
      "Madalines . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "49\n",
      "4.3.3\n",
      "Piecewise Linear Machines . . . . . . . . . . . . . . . . . .\n",
      "50\n",
      "4.3.4\n",
      "Cascade Networks\n",
      ". . . . . . . . . . . . . . . . . . . . . .\n",
      "51\n",
      "4.4\n",
      "Training Feedforward Networks by Backpropagation . . . . . . .\n",
      "52\n",
      "4.4.1\n",
      "Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "52\n",
      "4.4.2\n",
      "The Backpropagation Method . . . . . . . . . . . . . . . .\n",
      "53\n",
      "4.4.3\n",
      "Computing Weight Changes in the Final Layer . . . . . .\n",
      "56\n",
      "4.4.4\n",
      "Computing Changes to the Weights in Intermediate Layers 58\n",
      "4.4.5\n",
      "Variations on Backprop\n",
      ". . . . . . . . . . . . . . . . . . .\n",
      "59\n",
      "4.4.6\n",
      "An Application: Steering a Van . . . . . . . . . . . . . . .\n",
      "60\n",
      "4.5\n",
      "Synergies Between Neural Network and Knowledge-Based Methods 61\n",
      "4.6\n",
      "Bibliographical and Historical Remarks\n",
      ". . . . . . . . . . . . . .\n",
      "61\n",
      "5\n",
      "Statistical Learning\n",
      "63\n",
      "5.1\n",
      "Using Statistical Decision Theory . . . . . . . . . . . . . . . . . .\n",
      "63\n",
      "5.1.1\n",
      "Background and General Method . . . . . . . . . . . . . .\n",
      "63\n",
      "5.1.2\n",
      "Gaussian (or Normal) Distributions\n",
      ". . . . . . . . . . . .\n",
      "65\n",
      "5.1.3\n",
      "Conditionally Independent Binary Components . . . . . .\n",
      "68\n",
      "5.2\n",
      "Learning Belief Networks\n",
      ". . . . . . . . . . . . . . . . . . . . . .\n",
      "70\n",
      "5.3\n",
      "Nearest-Neighbor Methods . . . . . . . . . . . . . . . . . . . . . .\n",
      "70\n",
      "5.4\n",
      "Bibliographical and Historical Remarks\n",
      ". . . . . . . . . . . . . .\n",
      "72\n",
      "iv\n",
      "\n",
      "\n",
      "388. 6\n",
      "Decision Trees\n",
      "73\n",
      "6.1\n",
      "Deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "73\n",
      "6.2\n",
      "Supervised Learning of Univariate Decision Trees . . . . . . . . .\n",
      "74\n",
      "6.2.1\n",
      "Selecting the Type of Test . . . . . . . . . . . . . . . . . .\n",
      "75\n",
      "6.2.2\n",
      "Using Uncertainty Reduction to Select Tests\n",
      ". . . . . . .\n",
      "75\n",
      "6.2.3\n",
      "Non-Binary Attributes . . . . . . . . . . . . . . . . . . . .\n",
      "79\n",
      "6.3\n",
      "Networks Equivalent to Decision Trees . . . . . . . . . . . . . . .\n",
      "79\n",
      "6.4\n",
      "Overﬁtting and Evaluation\n",
      ". . . . . . . . . . . . . . . . . . . . .\n",
      "80\n",
      "6.4.1\n",
      "Overﬁtting\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "80\n",
      "6.4.2\n",
      "Validation Methods\n",
      ". . . . . . . . . . . . . . . . . . . . .\n",
      "81\n",
      "6.4.3\n",
      "Avoiding Overﬁtting in Decision Trees . . . . . . . . . . .\n",
      "82\n",
      "6.4.4\n",
      "Minimum-Description Length Methods . . . . . . . . . . .\n",
      "83\n",
      "6.4.5\n",
      "Noise in Data . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "84\n",
      "6.5\n",
      "The Problem of Replicated Subtrees . . . . . . . . . . . . . . . .\n",
      "84\n",
      "6.6\n",
      "The Problem of Missing Attributes . . . . . . . . . . . . . . . . .\n",
      "86\n",
      "6.7\n",
      "Comparisons\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "86\n",
      "6.8\n",
      "Bibliographical and Historical Remarks\n",
      ". . . . . . . . . . . . . .\n",
      "87\n",
      "7\n",
      "Inductive Logic Programming\n",
      "89\n",
      "7.1\n",
      "Notation and Deﬁnitions . . . . . . . . . . . . . . . . . . . . . . .\n",
      "90\n",
      "7.2\n",
      "A Generic ILP Algorithm . . . . . . . . . . . . . . . . . . . . . .\n",
      "91\n",
      "7.3\n",
      "An Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "94\n",
      "7.4\n",
      "Inducing Recursive Programs . . . . . . . . . . . . . . . . . . . .\n",
      "98\n",
      "7.5\n",
      "Choosing Literals to Add\n",
      ". . . . . . . . . . . . . . . . . . . . . . 100\n",
      "7.6\n",
      "Relationships Between ILP and Decision Tree Induction . . . . . 101\n",
      "7.7\n",
      "Bibliographical and Historical Remarks\n",
      ". . . . . . . . . . . . . . 104\n",
      "8\n",
      "Computational Learning Theory\n",
      "107\n",
      "8.1\n",
      "Notation and Assumptions for PAC Learning Theory . . . . . . . 107\n",
      "8.2\n",
      "PAC Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109\n",
      "8.2.1\n",
      "The Fundamental Theorem . . . . . . . . . . . . . . . . . 109\n",
      "8.2.2\n",
      "Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\n",
      "8.2.3\n",
      "Some Properly PAC-Learnable Classes . . . . . . . . . . . 112\n",
      "8.3\n",
      "The Vapnik-Chervonenkis Dimension . . . . . . . . . . . . . . . . 113\n",
      "8.3.1\n",
      "Linear Dichotomies . . . . . . . . . . . . . . . . . . . . . . 113\n",
      "8.3.2\n",
      "Capacity\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . 115\n",
      "8.3.3\n",
      "A More General Capacity Result . . . . . . . . . . . . . . 116\n",
      "8.3.4\n",
      "Some Facts and Speculations About the VC Dimension\n",
      ". 117\n",
      "8.4\n",
      "VC Dimension and PAC Learning\n",
      ". . . . . . . . . . . . . . . . . 118\n",
      "8.5\n",
      "Bibliographical and Historical Remarks\n",
      ". . . . . . . . . . . . . . 118\n",
      "v\n",
      "\n",
      "\n",
      "389. 9\n",
      "Unsupervised Learning\n",
      "119\n",
      "9.1\n",
      "What is Unsupervised Learning? . . . . . . . . . . . . . . . . . . 119\n",
      "9.2\n",
      "Clustering Methods . . . . . . . . . . . . . . . . . . . . . . . . . . 120\n",
      "9.2.1\n",
      "A Method Based on Euclidean Distance . . . . . . . . . . 120\n",
      "9.2.2\n",
      "A Method Based on Probabilities . . . . . . . . . . . . . . 124\n",
      "9.3\n",
      "Hierarchical Clustering Methods\n",
      ". . . . . . . . . . . . . . . . . . 125\n",
      "9.3.1\n",
      "A Method Based on Euclidean Distance . . . . . . . . . . 125\n",
      "9.3.2\n",
      "A Method Based on Probabilities . . . . . . . . . . . . . . 126\n",
      "9.4\n",
      "Bibliographical and Historical Remarks\n",
      ". . . . . . . . . . . . . . 130\n",
      "10 Temporal-Diﬀerence Learning\n",
      "131\n",
      "10.1 Temporal Patterns and Prediction Problems . . . . . . . . . . . . 131\n",
      "10.2 Supervised and Temporal-Diﬀerence Methods . . . . . . . . . . . 131\n",
      "10.3 Incremental Computation of the (∆W)i . . . . . . . . . . . . . . 134\n",
      "10.4 An Experiment with TD Methods\n",
      ". . . . . . . . . . . . . . . . . 135\n",
      "10.5 Theoretical Results . . . . . . . . . . . . . . . . . . . . . . . . . . 138\n",
      "10.6 Intra-Sequence Weight Updating . . . . . . . . . . . . . . . . . . 138\n",
      "10.7 An Example Application: TD-gammon . . . . . . . . . . . . . . . 140\n",
      "10.8 Bibliographical and Historical Remarks\n",
      ". . . . . . . . . . . . . . 141\n",
      "11 Delayed-Reinforcement Learning\n",
      "143\n",
      "11.1 The General Problem\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . 143\n",
      "11.2 An Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144\n",
      "11.3 Temporal Discounting and Optimal Policies . . . . . . . . . . . . 145\n",
      "11.4 Q-Learning\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147\n",
      "11.5 Discussion, Limitations, and Extensions of Q-Learning . . . . . . 150\n",
      "11.5.1 An Illustrative Example . . . . . . . . . . . . . . . . . . . 150\n",
      "11.5.2 Using Random Actions\n",
      ". . . . . . . . . . . . . . . . . . . 152\n",
      "11.5.3 Generalizing Over Inputs\n",
      ". . . . . . . . . . . . . . . . . . 153\n",
      "11.5.4 Partially Observable States . . . . . . . . . . . . . . . . . 154\n",
      "11.5.5 Scaling Problems . . . . . . . . . . . . . . . . . . . . . . . 154\n",
      "11.6 Bibliographical and Historical Remarks\n",
      ". . . . . . . . . . . . . . 155\n",
      "vi\n",
      "\n",
      "\n",
      "390. 12 Explanation-Based Learning\n",
      "157\n",
      "12.1 Deductive Learning . . . . . . . . . . . . . . . . . . . . . . . . . . 157\n",
      "12.2 Domain Theories . . . . . . . . . . . . . . . . . . . . . . . . . . . 158\n",
      "12.3 An Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159\n",
      "12.4 Evaluable Predicates . . . . . . . . . . . . . . . . . . . . . . . . . 162\n",
      "12.5 More General Proofs . . . . . . . . . . . . . . . . . . . . . . . . . 164\n",
      "12.6 Utility of EBL\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . 164\n",
      "12.7 Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164\n",
      "12.7.1 Macro-Operators in Planning . . . . . . . . . . . . . . . . 164\n",
      "12.7.2 Learning Search Control Knowledge\n",
      ". . . . . . . . . . . . 167\n",
      "12.8 Bibliographical and Historical Remarks\n",
      ". . . . . . . . . . . . . . 168\n",
      "vii\n",
      "\n",
      "\n",
      "391. viii\n",
      "\n",
      "\n",
      "392. Preface\n",
      "These notes are in the process of becoming a textbook. The process is quite\n",
      "unﬁnished, and the author solicits corrections, criticisms, and suggestions from\n",
      "students and other readers. Although I have tried to eliminate errors, some un-\n",
      "doubtedly remain—caveat lector. Many typographical infelicities will no doubt\n",
      "persist until the ﬁnal version. More material has yet to be added. Please let\n",
      "Some of my plans for additions and\n",
      "other reminders are mentioned in\n",
      "marginal notes.\n",
      "me have your suggestions about topics that are too important to be left out.\n",
      "I hope that future versions will cover Hopﬁeld nets, Elman nets and other re-\n",
      "current nets, radial basis functions, grammar and automata learning, genetic\n",
      "algorithms, and Bayes networks . . .. I am also collecting exercises and project\n",
      "suggestions which will appear in future versions.\n",
      "My intention is to pursue a middle ground between a theoretical textbook\n",
      "and one that focusses on applications. The book concentrates on the important\n",
      "ideas in machine learning. I do not give proofs of many of the theorems that I\n",
      "state, but I do give plausibility arguments and citations to formal proofs. And, I\n",
      "do not treat many matters that would be of practical importance in applications;\n",
      "the book is not a handbook of machine learning practice. Instead, my goal is\n",
      "to give the reader suﬃcient preparation to make the extensive literature on\n",
      "machine learning accessible.\n",
      "Students in my Stanford courses on machine learning have already made\n",
      "several useful suggestions, as have my colleague, Pat Langley, and my teaching\n",
      "assistants, Ron Kohavi, Karl Pﬂeger, Robert Allen, and Lise Getoor.\n",
      "ix\n",
      "\n",
      "\n",
      "393. Chapter 1\n",
      "Preliminaries\n",
      "1.1\n",
      "Introduction\n",
      "1.1.1\n",
      "What is Machine Learning?\n",
      "Learning, like intelligence, covers such a broad range of processes that it is dif-\n",
      "ﬁcult to deﬁne precisely. A dictionary deﬁnition includes phrases such as “to\n",
      "gain knowledge, or understanding of, or skill in, by study, instruction, or expe-\n",
      "rience,” and “modiﬁcation of a behavioral tendency by experience.” Zoologists\n",
      "and psychologists study learning in animals and humans. In this book we fo-\n",
      "cus on learning in machines. There are several parallels between animal and\n",
      "machine learning. Certainly, many techniques in machine learning derive from\n",
      "the eﬀorts of psychologists to make more precise their theories of animal and\n",
      "human learning through computational models. It seems likely also that the\n",
      "concepts and techniques being explored by researchers in machine learning may\n",
      "illuminate certain aspects of biological learning.\n",
      "As regards machines, we might say, very broadly, that a machine learns\n",
      "whenever it changes its structure, program, or data (based on its inputs or in\n",
      "response to external information) in such a manner that its expected future\n",
      "performance improves. Some of these changes, such as the addition of a record\n",
      "to a data base, fall comfortably within the province of other disciplines and are\n",
      "not necessarily better understood for being called learning. But, for example,\n",
      "when the performance of a speech-recognition machine improves after hearing\n",
      "several samples of a person’s speech, we feel quite justiﬁed in that case to say\n",
      "that the machine has learned.\n",
      "Machine learning usually refers to the changes in systems that perform tasks\n",
      "associated with artiﬁcial intelligence (AI). Such tasks involve recognition, diag-\n",
      "nosis, planning, robot control, prediction, etc. The “changes” might be either\n",
      "enhancements to already performing systems or ab initio synthesis of new sys-\n",
      "tems. To be slightly more speciﬁc, we show the architecture of a typical AI\n",
      "1\n",
      "\n",
      "\n",
      "394. 2\n",
      "CHAPTER 1. PRELIMINARIES\n",
      "“agent” in Fig. 1.1. This agent perceives and models its environment and com-\n",
      "putes appropriate actions, perhaps by anticipating their eﬀects. Changes made\n",
      "to any of the components shown in the ﬁgure might count as learning. Diﬀerent\n",
      "learning mechanisms might be employed depending on which subsystem is being\n",
      "changed. We will study several diﬀerent learning methods in this book.\n",
      "Sensory signals\n",
      "Perception\n",
      "Actions\n",
      "Action\n",
      "Computation\n",
      "Model\n",
      "Planning and\n",
      "Reasoning\n",
      "Goals\n",
      "Figure 1.1: An AI System\n",
      "One might ask “Why should machines have to learn? Why not design ma-\n",
      "chines to perform as desired in the ﬁrst place?” There are several reasons why\n",
      "machine learning is important. Of course, we have already mentioned that the\n",
      "achievement of learning in machines might help us understand how animals and\n",
      "humans learn. But there are important engineering reasons as well. Some of\n",
      "these are:\n",
      "• Some tasks cannot be deﬁned well except by example; that is, we might be\n",
      "able to specify input/output pairs but not a concise relationship between\n",
      "inputs and desired outputs. We would like machines to be able to adjust\n",
      "their internal structure to produce correct outputs for a large number of\n",
      "sample inputs and thus suitably constrain their input/output function to\n",
      "approximate the relationship implicit in the examples.\n",
      "• It is possible that hidden among large piles of data are important rela-\n",
      "tionships and correlations. Machine learning methods can often be used\n",
      "to extract these relationships (data mining).\n",
      "\n",
      "\n",
      "395. 1.1. INTRODUCTION\n",
      "3\n",
      "• Human designers often produce machines that do not work as well as\n",
      "desired in the environments in which they are used. In fact, certain char-\n",
      "acteristics of the working environment might not be completely known\n",
      "at design time.\n",
      "Machine learning methods can be used for on-the-job\n",
      "improvement of existing machine designs.\n",
      "• The amount of knowledge available about certain tasks might be too large\n",
      "for explicit encoding by humans.\n",
      "Machines that learn this knowledge\n",
      "gradually might be able to capture more of it than humans would want to\n",
      "write down.\n",
      "• Environments change over time. Machines that can adapt to a changing\n",
      "environment would reduce the need for constant redesign.\n",
      "• New knowledge about tasks is constantly being discovered by humans.\n",
      "Vocabulary changes.\n",
      "There is a constant stream of new events in the\n",
      "world. Continuing redesign of AI systems to conform to new knowledge is\n",
      "impractical, but machine learning methods might be able to track much\n",
      "of it.\n",
      "1.1.2\n",
      "Wellsprings of Machine Learning\n",
      "Work in machine learning is now converging from several sources. These dif-\n",
      "ferent traditions each bring diﬀerent methods and diﬀerent vocabulary which\n",
      "are now being assimilated into a more uniﬁed discipline. Here is a brief listing\n",
      "of some of the separate disciplines that have contributed to machine learning;\n",
      "more details will follow in the the appropriate chapters:\n",
      "• Statistics: A long-standing problem in statistics is how best to use sam-\n",
      "ples drawn from unknown probability distributions to help decide from\n",
      "which distribution some new sample is drawn. A related problem is how\n",
      "to estimate the value of an unknown function at a new point given the\n",
      "values of this function at a set of sample points.\n",
      "Statistical methods\n",
      "for dealing with these problems can be considered instances of machine\n",
      "learning because the decision and estimation rules depend on a corpus of\n",
      "samples drawn from the problem environment. We will explore some of\n",
      "the statistical methods later in the book. Details about the statistical the-\n",
      "ory underlying these methods can be found in statistical textbooks such\n",
      "as [Anderson, 1958].\n",
      "• Brain\n",
      "Models:\n",
      "Non-linear\n",
      "elements\n",
      "with\n",
      "weighted\n",
      "inputs\n",
      "have\n",
      "been\n",
      "suggested\n",
      "as\n",
      "simple\n",
      "models\n",
      "of\n",
      "biological\n",
      "neu-\n",
      "rons.\n",
      "Networks\n",
      "of\n",
      "these\n",
      "elements\n",
      "have\n",
      "been\n",
      "studied\n",
      "by\n",
      "sev-\n",
      "eral\n",
      "researchers\n",
      "including\n",
      "[McCulloch & Pitts, 1943,\n",
      "Hebb, 1949,\n",
      "Rosenblatt, 1958] and,\n",
      "more recently by [Gluck & Rumelhart, 1989,\n",
      "Sejnowski, Koch, & Churchland, 1988].\n",
      "Brain modelers are interested\n",
      "in how closely these networks approximate the learning phenomena of\n",
      "\n",
      "\n",
      "396. 4\n",
      "CHAPTER 1. PRELIMINARIES\n",
      "living brains.\n",
      "We shall see that several important machine learning\n",
      "techniques are based on networks of nonlinear elements—often called\n",
      "neural networks.\n",
      "Work inspired by this school is sometimes called\n",
      "connectionism, brain-style computation, or sub-symbolic processing.\n",
      "• Adaptive Control Theory: Control theorists study the problem of con-\n",
      "trolling a process having unknown parameters which must be estimated\n",
      "during operation. Often, the parameters change during operation, and the\n",
      "control process must track these changes. Some aspects of controlling a\n",
      "robot based on sensory inputs represent instances of this sort of problem.\n",
      "For an introduction see [Bollinger & Duﬃe, 1988].\n",
      "• Psychological Models: Psychologists have studied the performance of\n",
      "humans in various learning tasks. An early example is the EPAM net-\n",
      "work for storing and retrieving one member of a pair of words when\n",
      "given another [Feigenbaum, 1961].\n",
      "Related work led to a number of\n",
      "early decision tree [Hunt, Marin, & Stone, 1966] and semantic network\n",
      "[Anderson & Bower, 1973] methods.\n",
      "More recent work of this sort has\n",
      "been inﬂuenced by activities in artiﬁcial intelligence which we will be pre-\n",
      "senting.\n",
      "Some of the work in reinforcement learning can be traced to eﬀorts to\n",
      "model how reward stimuli inﬂuence the learning of goal-seeking behavior in\n",
      "animals [Sutton & Barto, 1987]. Reinforcement learning is an important\n",
      "theme in machine learning research.\n",
      "• Artiﬁcial Intelligence: From the beginning, AI research has been con-\n",
      "cerned with machine learning. Samuel developed a prominent early pro-\n",
      "gram that learned parameters of a function for evaluating board posi-\n",
      "tions in the game of checkers [Samuel, 1959].\n",
      "AI researchers have also\n",
      "explored the role of analogies in learning [Carbonell, 1983] and how fu-\n",
      "ture actions and decisions can be based on previous exemplary cases\n",
      "[Kolodner, 1993].\n",
      "Recent work has been directed at discovering rules\n",
      "for expert systems using decision-tree methods [Quinlan, 1990] and in-\n",
      "ductive logic programming [Muggleton, 1991, Lavraˇc & Dˇzeroski, 1994].\n",
      "Another theme has been saving and generalizing the results of prob-\n",
      "lem solving using explanation-based learning [DeJong & Mooney, 1986,\n",
      "Laird, et al., 1986, Minton, 1988, Etzioni, 1993].\n",
      "• Evolutionary Models:\n",
      "In nature, not only do individual animals learn to perform better, but\n",
      "species evolve to be better ﬁt in their individual niches. Since the distinc-\n",
      "tion between evolving and learning can be blurred in computer systems,\n",
      "techniques that model certain aspects of biological evolution have been\n",
      "proposed as learning methods to improve the performance of computer\n",
      "programs. Genetic algorithms [Holland, 1975] and genetic programming\n",
      "[Koza, 1992, Koza, 1994] are the most prominent computational tech-\n",
      "niques for evolution.\n",
      "\n",
      "\n",
      "397. 1.2. LEARNING INPUT-OUTPUT FUNCTIONS\n",
      "5\n",
      "1.1.3\n",
      "Varieties of Machine Learning\n",
      "Orthogonal to the question of the historical source of any learning technique is\n",
      "the more important question of what is to be learned. In this book, we take it\n",
      "that the thing to be learned is a computational structure of some sort. We will\n",
      "consider a variety of diﬀerent computational structures:\n",
      "• Functions\n",
      "• Logic programs and rule sets\n",
      "• Finite-state machines\n",
      "• Grammars\n",
      "• Problem solving systems\n",
      "We will present methods both for the synthesis of these structures from examples\n",
      "and for changing existing structures.\n",
      "In the latter case, the change to the\n",
      "existing structure might be simply to make it more computationally eﬃcient\n",
      "rather than to increase the coverage of the situations it can handle. Much of\n",
      "the terminology that we shall be using throughout the book is best introduced\n",
      "by discussing the problem of learning functions, and we turn to that matter\n",
      "ﬁrst.\n",
      "1.2\n",
      "Learning Input-Output Functions\n",
      "We use Fig. 1.2 to help deﬁne some of the terminology used in describing the\n",
      "problem of learning a function. Imagine that there is a function, f, and the task\n",
      "of the learner is to guess what it is. Our hypothesis about the function to be\n",
      "learned is denoted by h. Both f and h are functions of a vector-valued input\n",
      "X = (x1, x2, . . . , xi, . . . , xn) which has n components. We think of h as being\n",
      "implemented by a device that has X as input and h(X) as output. Both f and\n",
      "h themselves may be vector-valued. We assume a priori that the hypothesized\n",
      "function, h, is selected from a class of functions H. Sometimes we know that\n",
      "f also belongs to this class or to a subset of this class. We select h based on a\n",
      "training set, Ξ, of m input vector examples. Many important details depend on\n",
      "the nature of the assumptions made about all of these entities.\n",
      "1.2.1\n",
      "Types of Learning\n",
      "There are two major settings in which we wish to learn a function. In one,\n",
      "called supervised learning, we know (sometimes only approximately) the values\n",
      "of f for the m samples in the training set, Ξ. We assume that if we can ﬁnd\n",
      "a hypothesis, h, that closely agrees with f for the members of Ξ, then this\n",
      "hypothesis will be a good guess for f—especially if Ξ is large.\n",
      "\n",
      "\n",
      "398. 6\n",
      "CHAPTER 1. PRELIMINARIES\n",
      "h(X)\n",
      "h\n",
      "U = {X1, X2, . . . Xi, . . ., Xm}\n",
      "Training Set:\n",
      "X =\n",
      "x1\n",
      ".\n",
      ".\n",
      ".\n",
      "xi\n",
      ".\n",
      ".\n",
      ".\n",
      "xn\n",
      "h D H\n",
      "Figure 1.2: An Input-Output Function\n",
      "Curve-ﬁtting is a simple example of supervised learning of a function. Sup-\n",
      "pose we are given the values of a two-dimensional function, f, at the four sample\n",
      "points shown by the solid circles in Fig. 1.3. We want to ﬁt these four points\n",
      "with a function, h, drawn from the set, H, of second-degree functions. We show\n",
      "there a two-dimensional parabolic surface above the x1, x2 plane that ﬁts the\n",
      "points. This parabolic function, h, is our hypothesis about the function, f, that\n",
      "produced the four samples. In this case, h = f at the four samples, but we need\n",
      "not have required exact matches.\n",
      "In the other setting, termed unsupervised learning, we simply have a train-\n",
      "ing set of vectors without function values for them. The problem in this case,\n",
      "typically, is to partition the training set into subsets, Ξ1, . . . , ΞR, in some ap-\n",
      "propriate way. (We can still regard the problem as one of learning a function;\n",
      "the value of the function is the name of the subset to which an input vector be-\n",
      "longs.) Unsupervised learning methods have application in taxonomic problems\n",
      "in which it is desired to invent ways to classify data into meaningful categories.\n",
      "We shall also describe methods that are intermediate between supervised\n",
      "and unsupervised learning.\n",
      "We might either be trying to ﬁnd a new function, h, or to modify an existing\n",
      "one. An interesting special case is that of changing an existing function into an\n",
      "equivalent one that is computationally more eﬃcient. This type of learning is\n",
      "sometimes called speed-up learning. A very simple example of speed-up learning\n",
      "involves deduction processes. From the formulas A ⊃B and B ⊃C, we can\n",
      "deduce C if we are given A. From this deductive process, we can create the\n",
      "formula A ⊃C—a new formula but one that does not sanction any more con-\n",
      "\n",
      "\n",
      "399. 1.2. LEARNING INPUT-OUTPUT FUNCTIONS\n",
      "7\n",
      "-10\n",
      "-5\n",
      "0\n",
      "5\n",
      "10-10\n",
      "-5\n",
      "0\n",
      "5\n",
      "10\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "-10\n",
      "-5\n",
      "0\n",
      "5\n",
      "10-10\n",
      "-5\n",
      "0\n",
      "5\n",
      "10\n",
      "0\n",
      "00\n",
      "00\n",
      "0\n",
      "x1\n",
      "x2\n",
      "h\n",
      "sample f-value\n",
      "Figure 1.3: A Surface that Fits Four Points\n",
      "clusions than those that could be derived from the formulas that we previously\n",
      "had. But with this new formula we can derive C more quickly, given A, than\n",
      "we could have done before. We can contrast speed-up learning with methods\n",
      "that create genuinely new functions—ones that might give diﬀerent results after\n",
      "learning than they did before. We say that the latter methods involve inductive\n",
      "learning. As opposed to deduction, there are no correct inductions—only useful\n",
      "ones.\n",
      "1.2.2\n",
      "Input Vectors\n",
      "Because machine learning methods derive from so many diﬀerent traditions, its\n",
      "terminology is rife with synonyms, and we will be using most of them in this\n",
      "book. For example, the input vector is called by a variety of names. Some\n",
      "of these are: input vector, pattern vector, feature vector, sample, example, and\n",
      "instance. The components, xi, of the input vector are variously called features,\n",
      "attributes, input variables, and components.\n",
      "The values of the components can be of three main types.\n",
      "They might\n",
      "be real-valued numbers, discrete-valued numbers, or categorical values. As an\n",
      "example illustrating categorical values, information about a student might be\n",
      "represented by the values of the attributes class, major, sex, adviser. A par-\n",
      "ticular student would then be represented by a vector such as: (sophomore,\n",
      "history, male, higgins). Additionally, categorical values may be ordered (as in\n",
      "{small, medium, large}) or unordered (as in the example just given). Of course,\n",
      "mixtures of all these types of values are possible.\n",
      "In all cases, it is possible to represent the input in unordered form by listing\n",
      "the names of the attributes together with their values. The vector form assumes\n",
      "that the attributes are ordered and given implicitly by a form. As an example\n",
      "of an attribute-value representation, we might have: (major: history, sex: male,\n",
      "\n",
      "\n",
      "400. 8\n",
      "CHAPTER 1. PRELIMINARIES\n",
      "class: sophomore, adviser: higgins, age: 19). We will be using the vector form\n",
      "exclusively.\n",
      "An important specialization uses Boolean values, which can be regarded as\n",
      "a special case of either discrete numbers (1,0) or of categorical variables (True,\n",
      "False).\n",
      "1.2.3\n",
      "Outputs\n",
      "The output may be a real number, in which case the process embodying the\n",
      "function, h, is called a function estimator, and the output is called an output\n",
      "value or estimate.\n",
      "Alternatively, the output may be a categorical value, in which case the pro-\n",
      "cess embodying h is variously called a classiﬁer, a recognizer, or a categorizer,\n",
      "and the output itself is called a label, a class, a category, or a decision. Classi-\n",
      "ﬁers have application in a number of recognition problems, for example in the\n",
      "recognition of hand-printed characters. The input in that case is some suitable\n",
      "representation of the printed character, and the classiﬁer maps this input into\n",
      "one of, say, 64 categories.\n",
      "Vector-valued outputs are also possible with components being real numbers\n",
      "or categorical values.\n",
      "An important special case is that of Boolean output values. In that case,\n",
      "a training pattern having value 1 is called a positive instance, and a training\n",
      "sample having value 0 is called a negative instance. When the input is also\n",
      "Boolean, the classiﬁer implements a Boolean function. We study the Boolean\n",
      "case in some detail because it allows us to make important general points in\n",
      "a simpliﬁed setting. Learning a Boolean function is sometimes called concept\n",
      "learning, and the function is called a concept.\n",
      "1.2.4\n",
      "Training Regimes\n",
      "There are several ways in which the training set, Ξ, can be used to produce a\n",
      "hypothesized function. In the batch method, the entire training set is available\n",
      "and used all at once to compute the function, h. A variation of this method\n",
      "uses the entire training set to modify a current hypothesis iteratively until an\n",
      "acceptable hypothesis is obtained. By contrast, in the incremental method, we\n",
      "select one member at a time from the training set and use this instance alone\n",
      "to modify a current hypothesis. Then another member of the training set is\n",
      "selected, and so on. The selection method can be random (with replacement)\n",
      "or it can cycle through the training set iteratively. If the entire training set\n",
      "becomes available one member at a time, then we might also use an incremental\n",
      "method—selecting and using training set members as they arrive. (Alterna-\n",
      "tively, at any stage all training set members so far available could be used in a\n",
      "“batch” process.) Using the training set members as they become available is\n",
      "called an online method. Online methods might be used, for example, when the\n",
      "\n",
      "\n",
      "401. 1.3. LEARNING REQUIRES BIAS\n",
      "9\n",
      "next training instance is some function of the current hypothesis and the previ-\n",
      "ous instance—as it would be when a classiﬁer is used to decide on a robot’s next\n",
      "action given its current set of sensory inputs. The next set of sensory inputs\n",
      "will depend on which action was selected.\n",
      "1.2.5\n",
      "Noise\n",
      "Sometimes the vectors in the training set are corrupted by noise. There are two\n",
      "kinds of noise. Class noise randomly alters the value of the function; attribute\n",
      "noise randomly alters the values of the components of the input vector. In either\n",
      "case, it would be inappropriate to insist that the hypothesized function agree\n",
      "precisely with the values of the samples in the training set.\n",
      "1.2.6\n",
      "Performance Evaluation\n",
      "Even though there is no correct answer in inductive learning, it is important\n",
      "to have methods to evaluate the result of learning. We will discuss this matter\n",
      "in more detail later, but, brieﬂy, in supervised learning the induced function is\n",
      "usually evaluated on a separate set of inputs and function values for them called\n",
      "the testing set . A hypothesized function is said to generalize when it guesses\n",
      "well on the testing set. Both mean-squared-error and the total number of errors\n",
      "are common measures.\n",
      "1.3\n",
      "Learning Requires Bias\n",
      "Long before now the reader has undoubtedly asked why is learning a function\n",
      "possible at all? Certainly, for example, there are an uncountable number of\n",
      "diﬀerent functions having values that agree with the four samples shown in Fig.\n",
      "1.3. Why would a learning procedure happen to select the quadratic one shown\n",
      "in that ﬁgure? In order to make that selection we had at least to limit a priori\n",
      "the set of hypotheses to quadratic functions and then to insist that the one we\n",
      "chose passed through all four sample points. This kind of a priori information\n",
      "is called bias, and useful learning without bias is impossible.\n",
      "We can gain more insight into the role of bias by considering the special case\n",
      "of learning a Boolean function of n dimensions. There are 2n diﬀerent Boolean\n",
      "inputs possible. Suppose we had no bias; that is H is the set of all 22n Boolean\n",
      "functions, and we have no preference among those that ﬁt the samples in the\n",
      "training set. In this case, after being presented with one member of the training\n",
      "set and its value we can rule out precisely one-half of the members of H—those\n",
      "Boolean functions that would misclassify this labeled sample. The remaining\n",
      "functions constitute what is called a “version space;” we’ll explore that concept\n",
      "in more detail later. As we present more members of the training set, the graph\n",
      "of the number of hypotheses not yet ruled out as a function of the number of\n",
      "diﬀerent patterns presented is as shown in Fig. 1.4. At any stage of the process,\n",
      "\n",
      "\n",
      "402. 10\n",
      "CHAPTER 1. PRELIMINARIES\n",
      "half of the remaining Boolean functions have value 1 and half have value 0 for\n",
      "any training pattern not yet seen. No generalization is possible in this case\n",
      "because the training patterns give no clue about the value of a pattern not yet\n",
      "seen. Only memorization is possible here, which is a trivial sort of learning.\n",
      "log2|Hv|\n",
      "2n\n",
      "2n\n",
      "j = no. of labeled\n",
      "patterns already seen\n",
      "0\n",
      "0\n",
      "2n < j\n",
      "(generalization is not possible)\n",
      "|Hv| = no. of functions not ruled out\n",
      "Figure 1.4: Hypotheses Remaining as a Function of Labeled Patterns Presented\n",
      "But suppose we limited H to some subset, Hc, of all Boolean functions.\n",
      "Depending on the subset and on the order of presentation of training patterns,\n",
      "a curve of hypotheses not yet ruled out might look something like the one\n",
      "shown in Fig. 1.5. In this case it is even possible that after seeing fewer than\n",
      "all 2n labeled samples, there might be only one hypothesis that agrees with\n",
      "the training set. Certainly, even if there is more than one hypothesis remaining,\n",
      "most of them may have the same value for most of the patterns not yet seen! The\n",
      "theory of Probably Approximately Correct (PAC) learning makes this intuitive\n",
      "idea precise. We’ll examine that theory later.\n",
      "Let’s look at a speciﬁc example of how bias aids learning. A Boolean function\n",
      "can be represented by a hypercube each of whose vertices represents a diﬀerent\n",
      "input pattern. We show a 3-dimensional version in Fig. 1.6. There, we show a\n",
      "training set of six sample patterns and have marked those having a value of 1 by\n",
      "a small square and those having a value of 0 by a small circle. If the hypothesis\n",
      "set consists of just the linearly separable functions—those for which the positive\n",
      "and negative instances can be separated by a linear surface, then there is only\n",
      "one function remaining in this hypothsis set that is consistent with the training\n",
      "set. So, in this case, even though the training set does not contain all possible\n",
      "patterns, we can already pin down what the function must be—given the bias.\n",
      "\n",
      "\n",
      "403. 1.4. SAMPLE APPLICATIONS\n",
      "11\n",
      "log2|Hv|\n",
      "2n\n",
      "2n\n",
      "j = no. of labeled\n",
      "patterns already seen\n",
      "0\n",
      "0\n",
      "|Hv| = no. of functions not ruled out\n",
      "depends on order\n",
      "of presentation\n",
      "log2|Hc|\n",
      "Figure 1.5: Hypotheses Remaining From a Restricted Subset\n",
      "Machine learning researchers have identiﬁed two main varieties of bias, ab-\n",
      "solute and preference. In absolute bias (also called restricted hypothesis-space\n",
      "bias), one restricts H to a deﬁnite subset of functions. In our example of Fig. 1.6,\n",
      "the restriction was to linearly separable Boolean functions. In preference bias,\n",
      "one selects that hypothesis that is minimal according to some ordering scheme\n",
      "over all hypotheses. For example, if we had some way of measuring the complex-\n",
      "ity of a hypothesis, we might select the one that was simplest among those that\n",
      "performed satisfactorily on the training set. The principle of Occam’s razor,\n",
      "used in science to prefer simple explanations to more complex ones, is a type\n",
      "of preference bias. (William of Occam, 1285-?1349, was an English philosopher\n",
      "who said: “non sunt multiplicanda entia praeter necessitatem,” which means\n",
      "“entities should not be multiplied unnecessarily.”)\n",
      "1.4\n",
      "Sample Applications\n",
      "Our main emphasis in this book is on the concepts of machine learning—not\n",
      "on its applications. Nevertheless, if these concepts were irrelevant to real-world\n",
      "problems they would probably not be of much interest. As motivation, we give\n",
      "a short summary of some areas in which machine learning techniques have been\n",
      "successfully applied. [Langley, 1992] cites some of the following applications and\n",
      "others:\n",
      "a. Rule discovery using a variant of ID3 for a printing industry problem\n",
      "\n",
      "\n",
      "404. 12\n",
      "CHAPTER 1. PRELIMINARIES\n",
      "x1\n",
      "x2\n",
      "x3\n",
      "Figure 1.6: A Training Set That Completely Determines a Linearly Separable\n",
      "Function\n",
      "[Evans & Fisher, 1992].\n",
      "b. Electric power load forecasting using a k-nearest-neighbor rule system\n",
      "[Jabbour, K., et al., 1987].\n",
      "c. Automatic\n",
      "“help\n",
      "desk”\n",
      "assistant\n",
      "using\n",
      "a\n",
      "nearest-neighbor\n",
      "system\n",
      "[Acorn & Walden, 1992].\n",
      "d. Planning and scheduling for a steel mill using ExpertEase, a marketed\n",
      "(ID3-like) system [Michie, 1992].\n",
      "e. Classiﬁcation of stars and galaxies [Fayyad, et al., 1993].\n",
      "Many application-oriented papers are presented at the annual conferences\n",
      "on Neural Information Processing Systems. Among these are papers on: speech\n",
      "recognition, dolphin echo recognition, image processing, bio-engineering, diag-\n",
      "nosis, commodity trading, face recognition, music composition, optical character\n",
      "recognition, and various control applications [Various Editors, 1989-1994].\n",
      "As additional examples, [Hammerstrom, 1993] mentions:\n",
      "a. Sharp’s Japanese kanji character recognition system processes 200 char-\n",
      "acters per second with 99+% accuracy. It recognizes 3000+ characters.\n",
      "b. NeuroForecasting Centre’s (London Business School and University Col-\n",
      "lege London) trading strategy selection network earned an average annual\n",
      "proﬁt of 18% against a conventional system’s 12.3%.\n",
      "\n",
      "\n",
      "405. 1.5. SOURCES\n",
      "13\n",
      "c. Fujitsu’s (plus a partner’s) neural network for monitoring a continuous\n",
      "steel casting operation has been in successful operation since early 1990.\n",
      "In summary, it is rather easy nowadays to ﬁnd applications of machine learn-\n",
      "ing techniques. This fact should come as no surprise inasmuch as many machine\n",
      "learning techniques can be viewed as extensions of well known statistical meth-\n",
      "ods which have been successfully applied for many years.\n",
      "1.5\n",
      "Sources\n",
      "Besides\n",
      "the\n",
      "rich\n",
      "literature\n",
      "in\n",
      "machine\n",
      "learning\n",
      "(a\n",
      "small\n",
      "part\n",
      "of\n",
      "which\n",
      "is\n",
      "referenced\n",
      "in\n",
      "the\n",
      "Bibliography),\n",
      "there\n",
      "are\n",
      "several\n",
      "text-\n",
      "books\n",
      "that\n",
      "are\n",
      "worth\n",
      "mentioning\n",
      "[Hertz, Krogh, & Palmer, 1991,\n",
      "Weiss & Kulikowski, 1991,\n",
      "Natarjan, 1991,\n",
      "Fu, 1994,\n",
      "Langley, 1996].\n",
      "[Shavlik & Dietterich, 1990,\n",
      "Buchanan & Wilkins, 1993]\n",
      "are\n",
      "edited\n",
      "vol-\n",
      "umes containing some of the most important papers.\n",
      "A survey paper by\n",
      "[Dietterich, 1990] gives a good overview of many important topics. There are\n",
      "also well established conferences and publications where papers are given and\n",
      "appear including:\n",
      "• The Annual Conferences on Advances in Neural Information Processing\n",
      "Systems\n",
      "• The Annual Workshops on Computational Learning Theory\n",
      "• The Annual International Workshops on Machine Learning\n",
      "• The Annual International Conferences on Genetic Algorithms\n",
      "(The Proceedings of the above-listed four conferences are published by\n",
      "Morgan Kaufmann.)\n",
      "• The journal Machine Learning (published by Kluwer Academic Publish-\n",
      "ers).\n",
      "There is also much information, as well as programs and datasets, available over\n",
      "the Internet through the World Wide Web.\n",
      "1.6\n",
      "Bibliographical and Historical Remarks\n",
      "To be added. Every chapter will\n",
      "contain a brief survey of the history\n",
      "of the material covered in that\n",
      "chapter.\n",
      "\n",
      "\n",
      "406. 14\n",
      "CHAPTER 1. PRELIMINARIES\n",
      "\n",
      "\n",
      "407. Chapter 2\n",
      "Boolean Functions\n",
      "2.1\n",
      "Representation\n",
      "2.1.1\n",
      "Boolean Algebra\n",
      "Many important ideas about learning of functions are most easily presented\n",
      "using the special case of Boolean functions. There are several important sub-\n",
      "classes of Boolean functions that are used as hypothesis classes for function\n",
      "learning. Therefore, we digress in this chapter to present a review of Boolean\n",
      "functions and their properties. (For a more thorough treatment see, for example,\n",
      "[Unger, 1989].)\n",
      "A Boolean function, f(x1, x2, . . . , xn) maps an n-tuple of (0,1) values to\n",
      "{0, 1}. Boolean algebra is a convenient notation for representing Boolean func-\n",
      "tions. Boolean algebra uses the connectives ·, +, and\n",
      ". For example, the and\n",
      "function of two variables is written x1 · x2. By convention, the connective, “·”\n",
      "is usually suppressed, and the and function is written x1x2. x1x2 has value 1 if\n",
      "and only if both x1 and x2 have value 1; if either x1 or x2 has value 0, x1x2 has\n",
      "value 0. The (inclusive) or function of two variables is written x1 + x2. x1 + x2\n",
      "has value 1 if and only if either or both of x1 or x2 has value 1; if both x1 and\n",
      "x2 have value 0, x1 + x2 has value 0. The complement or negation of a variable,\n",
      "x, is written x. x has value 1 if and only if x has value 0; if x has value 1, x has\n",
      "value 0.\n",
      "These deﬁnitions are compactly given by the following rules for Boolean\n",
      "algebra:\n",
      "1 + 1 = 1, 1 + 0 = 1, 0 + 0 = 0,\n",
      "1 · 1 = 1, 1 · 0 = 0, 0 · 0 = 0, and\n",
      "1 = 0, 0 = 1.\n",
      "Sometimes the arguments and values of Boolean functions are expressed in\n",
      "terms of the constants T (True) and F (False) instead of 1 and 0, respectively.\n",
      "15\n",
      "\n",
      "\n",
      "408. 16\n",
      "CHAPTER 2. BOOLEAN FUNCTIONS\n",
      "The connectives · and + are each commutative and associative. Thus, for\n",
      "example, x1(x2x3) = (x1x2)x3, and both can be written simply as x1x2x3.\n",
      "Similarly for +.\n",
      "A Boolean formula consisting of a single variable, such as x1 is called an\n",
      "atom. One consisting of either a single variable or its complement, such as x1,\n",
      "is called a literal.\n",
      "The operators · and + do not commute between themselves. Instead, we\n",
      "have DeMorgan’s laws (which can be veriﬁed by using the above deﬁnitions):\n",
      "x1x2 = x1 + x2, and\n",
      "x1 + x2 = x1 x2.\n",
      "2.1.2\n",
      "Diagrammatic Representations\n",
      "We saw in the last chapter that a Boolean function could be represented by\n",
      "labeling the vertices of a cube. For a function of n variables, we would need\n",
      "an n-dimensional hypercube. In Fig. 2.1 we show some 2- and 3-dimensional\n",
      "examples. Vertices having value 1 are labeled with a small square, and vertices\n",
      "having value 0 are labeled with a small circle.\n",
      "x1\n",
      "x2\n",
      "x1\n",
      "x2\n",
      "x1\n",
      "x2\n",
      "and\n",
      "or\n",
      "xor (exclusive or)\n",
      "x1x2\n",
      "x1 + x2\n",
      "x1x2  +  x1x2\n",
      "even parity function\n",
      "x1\n",
      "x2\n",
      "x3\n",
      "x1x2x3  +  x1x2x3\n",
      "+ x1x2x3 + x1x2x3\n",
      "Figure 2.1: Representing Boolean Functions on Cubes\n",
      "Using the hypercube representations, it is easy to see how many Boolean\n",
      "functions of n dimensions there are. A 3-dimensional cube has 23 = 8 vertices,\n",
      "and each may be labeled in two diﬀerent ways; thus there are 2(23) = 256\n",
      "\n",
      "\n",
      "409. 2.2. CLASSES OF BOOLEAN FUNCTIONS\n",
      "17\n",
      "diﬀerent Boolean functions of 3 variables. In general, there are 22n Boolean\n",
      "functions of n variables.\n",
      "We will be using 2- and 3-dimensional cubes later to provide some intuition\n",
      "about the properties of certain Boolean functions. Of course, we cannot visualize\n",
      "hypercubes (for n > 3), and there are many surprising properties of higher\n",
      "dimensional spaces, so we must be careful in using intuitions gained in low\n",
      "dimensions. One diagrammatic technique for dimensions slightly higher than\n",
      "3 is the Karnaugh map. A Karnaugh map is an array of values of a Boolean\n",
      "function in which the horizontal rows are indexed by the values of some of\n",
      "the variables and the vertical columns are indexed by the rest. The rows and\n",
      "columns are arranged in such a way that entries that are adjacent in the map\n",
      "correspond to vertices that are adjacent in the hypercube representation. We\n",
      "show an example of the 4-dimensional even parity function in Fig. 2.2. (An\n",
      "even parity function is a Boolean function that has value 1 if there are an even\n",
      "number of its arguments that have value 1; otherwise it has value 0.) Note\n",
      "that all adjacent cells in the table correspond to inputs diﬀering in only one\n",
      "component.\n",
      "Also describe general logic\n",
      "diagrams, [Wnek, et al., 1990].\n",
      "00 01\n",
      "10\n",
      "11\n",
      "00\n",
      "01\n",
      "10\n",
      "11\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "x1,x2\n",
      "x3,x4\n",
      "Figure 2.2: A Karnaugh Map\n",
      "2.2\n",
      "Classes of Boolean Functions\n",
      "2.2.1\n",
      "Terms and Clauses\n",
      "To use absolute bias in machine learning, we limit the class of hypotheses. In\n",
      "learning Boolean functions, we frequently use some of the common sub-classes of\n",
      "those functions. Therefore, it will be important to know about these subclasses.\n",
      "One basic subclass is called terms. A term is any function written in the\n",
      "form l1l2 · · · lk, where the li are literals. Such a form is called a conjunction of\n",
      "literals. Some example terms are x1x7 and x1x2x4. The size of a term is the\n",
      "number of literals it contains. The examples are of sizes 2 and 3, respectively.\n",
      "(Strictly speaking, the class of conjunctions of literals is called the monomials,\n",
      "\n",
      "\n",
      "410. 18\n",
      "CHAPTER 2. BOOLEAN FUNCTIONS\n",
      "and a conjunction of literals itself is called a term. This distinction is a ﬁne one\n",
      "which we elect to blur here.)\n",
      "It is easy to show that there are exactly 3n possible terms of n variables.\n",
      "The number of terms of size k or less is bounded from above by Pk\n",
      "i=0 C(2n, i) =\n",
      "O(nk), where C(i, j) =\n",
      "i!\n",
      "(i−j)!j! is the binomial coeﬃcient.\n",
      "Probably I’ll put in a simple\n",
      "term-learning algorithm here—so\n",
      "we can get started on learning!\n",
      "Also for DNF functions and\n",
      "decision lists—as they are deﬁned\n",
      "in the next few pages.\n",
      "A clause is any function written in the form l1 +l2 +· · ·+lk, where the li are\n",
      "literals. Such a form is called a disjunction of literals. Some example clauses\n",
      "are x3 + x5 + x6 and x1 + x4. The size of a clause is the number of literals it\n",
      "contains. There are 3n possible clauses and fewer than Pk\n",
      "i=0 C(2n, i) clauses of\n",
      "size k or less. If f is a term, then (by De Morgan’s laws) f is a clause, and vice\n",
      "versa. Thus, terms and clauses are duals of each other.\n",
      "In psychological experiments, conjunctions of literals seem easier for humans\n",
      "to learn than disjunctions of literals.\n",
      "2.2.2\n",
      "DNF Functions\n",
      "A Boolean function is said to be in disjunctive normal form (DNF) if it can be\n",
      "written as a disjunction of terms. Some examples in DNF are: f = x1x2+x2x3x4\n",
      "and f = x1x3 + x2 x3 + x1x2x3. A DNF expression is called a k-term DNF\n",
      "expression if it is a disjunction of k terms; it is in the class k-DNF if the size of\n",
      "its largest term is k. The examples above are 2-term and 3-term expressions,\n",
      "respectively. Both expressions are in the class 3-DNF.\n",
      "Each term in a DNF expression for a function is called an implicant because\n",
      "it “implies” the function (if the term has value 1, so does the function). In\n",
      "general, a term, t, is an implicant of a function, f, if f has value 1 whenever\n",
      "t does. A term, t, is a prime implicant of f if the term, t′, formed by taking\n",
      "any literal out of an implicant t is no longer an implicant of f. (The implicant\n",
      "cannot be “divided” by any term and remain an implicant.)\n",
      "Thus, both x2x3 and x1 x3 are prime implicants of f = x2x3+x1 x3+x2x1x3,\n",
      "but x2x1x3 is not.\n",
      "The relationship between implicants and prime implicants can be geometri-\n",
      "cally illustrated using the cube representation for Boolean functions. Consider,\n",
      "for example, the function f = x2x3 + x1 x3 + x2x1x3. We illustrate it in Fig.\n",
      "2.3.\n",
      "Note that each of the three planes in the ﬁgure “cuts oﬀ” a group of\n",
      "vertices having value 1, but none cuts oﬀany vertices having value 0. These\n",
      "planes are pictorial devices used to isolate certain lower dimensional subfaces\n",
      "of the cube. Two of them isolate one-dimensional edges, and the third isolates\n",
      "a zero-dimensional vertex. Each group of vertices on a subface corresponds to\n",
      "one of the implicants of the function, f, and thus each implicant corresponds\n",
      "to a subface of some dimension. A k-dimensional subface corresponds to an\n",
      "(n −k)-size implicant term. The function is written as the disjunction of the\n",
      "implicants—corresponding to the union of all the vertices cut oﬀby all of the\n",
      "planes. Geometrically, an implicant is prime if and only if its corresponding\n",
      "subface is the largest dimensional subface that includes all of its vertices and\n",
      "\n",
      "\n",
      "411. 2.2. CLASSES OF BOOLEAN FUNCTIONS\n",
      "19\n",
      "no other vertices having value 0. Note that the term x2x1x3 is not a prime\n",
      "implicant of f. (In this case, we don’t even have to include this term in the\n",
      "function because the vertex cut oﬀby the plane corresponding to x2x1x3 is\n",
      "already cut oﬀby the plane corresponding to x2x3.) The other two implicants\n",
      "are prime because their corresponding subfaces cannot be expanded without\n",
      "including vertices having value 0.\n",
      "x2\n",
      "x1\n",
      "x3\n",
      "1, 0, 0\n",
      "1, 0, 1\n",
      "1, 1, 1\n",
      "0, 0, 1\n",
      "f = x2x3 + x1x3 + x2x1x3\n",
      "   = x2x3 + x1x3\n",
      "x2x3 and  x1x3 are prime implicants\n",
      "Figure 2.3: A Function and its Implicants\n",
      "Note that all Boolean functions can be represented in DNF—trivially by\n",
      "disjunctions of terms of size n where each term corresponds to one of the vertices\n",
      "whose value is 1. Whereas there are 22n functions of n dimensions in DNF (since\n",
      "any Boolean function can be written in DNF), there are just 2O(nk) functions\n",
      "in k-DNF.\n",
      "All Boolean functions can also be represented in DNF in which each term is\n",
      "a prime implicant, but that representation is not unique, as shown in Fig. 2.4.\n",
      "If we can express a function in DNF form, we can use the consensus method\n",
      "to ﬁnd an expression for the function in which each term is a prime implicant.\n",
      "The consensus method relies on two results:\n",
      "We may replace this section with\n",
      "one describing the\n",
      "Quine-McCluskey method instead.\n",
      "• Consensus:\n",
      "\n",
      "\n",
      "412. 20\n",
      "CHAPTER 2. BOOLEAN FUNCTIONS\n",
      "x2\n",
      "x1\n",
      "x3\n",
      "1, 0, 0\n",
      "1, 0, 1\n",
      "1, 1, 1\n",
      "0, 0, 1\n",
      "f = x2x3 + x1x3 + x1x2\n",
      "   = x1x2 + x1x3\n",
      "All of the terms are prime implicants, but there\n",
      "is not a unique representation\n",
      "Figure 2.4: Non-Uniqueness of Representation by Prime Implicants\n",
      "xi · f1 + xi · f2 = xi · f1 + xi · f2 + f1 · f2\n",
      "where f1 and f2 are terms such that no literal appearing in f1 appears\n",
      "complemented in f2.\n",
      "f1 · f2 is called the consensus of xi · f1 and xi ·\n",
      "f2. Readers familiar with the resolution rule of inference will note that\n",
      "consensus is the dual of resolution.\n",
      "Examples: x1 is the consensus of x1x2 and x1x2. The terms x1x2 and x1x2\n",
      "have no consensus since each term has more than one literal appearing\n",
      "complemented in the other.\n",
      "• Subsumption:\n",
      "xi · f1 + f1 = f1\n",
      "where f1 is a term. We say that f1 subsumes xi · f1.\n",
      "Example: x1 x4x5 subsumes x1 x4 x2x5\n",
      "\n",
      "\n",
      "413. 2.2. CLASSES OF BOOLEAN FUNCTIONS\n",
      "21\n",
      "The consensus method for ﬁnding a set of prime implicants for a function,\n",
      "f, iterates the following operations on the terms of a DNF expression for f until\n",
      "no more such operations can be applied:\n",
      "a. initialize the process with the set, T , of terms in the DNF expression of\n",
      "f,\n",
      "b. compute the consensus of a pair of terms in T and add the result to T ,\n",
      "c. eliminate any terms in T that are subsumed by other terms in T .\n",
      "When this process halts, the terms remaining in T are all prime implicants of\n",
      "f.\n",
      "Example: Let f = x1x2 + x1 x2x3 + x1 x2 x3 x4x5. We show a derivation of\n",
      "a set of prime implicants in the consensus tree of Fig. 2.5. The circled numbers\n",
      "adjoining the terms indicate the order in which the consensus and subsumption\n",
      "operations were performed. Shaded boxes surrounding a term indicate that it\n",
      "was subsumed. The ﬁnal form of the function in which all terms are prime\n",
      "implicants is: f = x1x2 +x1x3 +x1 x4x5. Its terms are all of the non-subsumed\n",
      "terms in the consensus tree.\n",
      " x1x2\n",
      "x1x2x3\n",
      "x1x2x3x4x5\n",
      " x1x3\n",
      "x1x2x4x5\n",
      "x1x4x5\n",
      "f =  x1x2 +\n",
      "+\n",
      " x1x3\n",
      "x1x4x5\n",
      "1\n",
      "2\n",
      "6\n",
      "4\n",
      "5\n",
      "3\n",
      "Figure 2.5: A Consensus Tree\n",
      "2.2.3\n",
      "CNF Functions\n",
      "Disjunctive normal form has a dual: conjunctive normal form (CNF). A Boolean\n",
      "function is said to be in CNF if it can be written as a conjunction of clauses.\n",
      "\n",
      "\n",
      "414. 22\n",
      "CHAPTER 2. BOOLEAN FUNCTIONS\n",
      "An example in CNF is: f = (x1 +x2)(x2 +x3 +x4). A CNF expression is called\n",
      "a k-clause CNF expression if it is a conjunction of k clauses; it is in the class\n",
      "k-CNF if the size of its largest clause is k. The example is a 2-clause expression\n",
      "in 3-CNF. If f is written in DNF, an application of De Morgan’s law renders f\n",
      "in CNF, and vice versa. Because CNF and DNF are duals, there are also 2O(nk)\n",
      "functions in k-CNF.\n",
      "2.2.4\n",
      "Decision Lists\n",
      "Rivest has proposed a class of Boolean functions called decision lists [Rivest, 1987].\n",
      "A decision list is written as an ordered list of pairs:\n",
      "(tq, vq)\n",
      "(tq−1, vq−1)\n",
      "· · ·\n",
      "(ti, vi)\n",
      "· · ·\n",
      "(t2, v2)\n",
      "(T, v1)\n",
      "where the vi are either 0 or 1, the ti are terms in (x1, . . . , xn), and T is a term\n",
      "whose value is 1 (regardless of the values of the xi). The value of a decision list\n",
      "is the value of vi for the ﬁrst ti in the list that has value 1. (At least one ti will\n",
      "have value 1, because the last one does; v1 can be regarded as a default value of\n",
      "the decision list.) The decision list is of size k, if the size of the largest term in\n",
      "it is k. The class of decision lists of size k or less is called k-DL.\n",
      "An example decision list is:\n",
      "f =\n",
      "(x1x2, 1)\n",
      "(x1 x2x3, 0)\n",
      "x2x3, 1)\n",
      "(1, 0)\n",
      "f has value 0 for x1 = 0, x2 = 0, and x3 = 1. It has value 1 for x1 = 1, x2 = 0,\n",
      "and x3 = 1. This function is in 3-DL.\n",
      "It has been shown that the class k-DL is a strict superset of the union of\n",
      "k-DNF and k-CNF. There are 2O[nkk log(n)] functions in k-DL [Rivest, 1987].\n",
      "Interesting generalizations of decision lists use other Boolean functions in\n",
      "place of the terms, ti. For example we might use linearly separable functions in\n",
      "place of the ti (see below and [Marchand & Golea, 1993]).\n",
      "\n",
      "\n",
      "415. 2.2. CLASSES OF BOOLEAN FUNCTIONS\n",
      "23\n",
      "2.2.5\n",
      "Symmetric and Voting Functions\n",
      "A Boolean function is called symmetric if it is invariant under permutations\n",
      "of the input variables. For example, any function that is dependent only on\n",
      "the number of input variables whose values are 1 is a symmetric function. The\n",
      "parity functions, which have value 1 depending on whether or not the number\n",
      "of input variables with value 1 is even or odd is a symmetric function. (The\n",
      "exclusive or function, illustrated in Fig. 2.1, is an odd-parity function of two\n",
      "dimensions. The or and and functions of two dimensions are also symmetric.)\n",
      "An important subclass of the symmetric functions is the class of voting func-\n",
      "tions (also called m-of-n functions). A k-voting function has value 1 if and only\n",
      "if k or more of its n inputs has value 1. If k = 1, a voting function is the same\n",
      "as an n-sized clause; if k = n, a voting function is the same as an n-sized term;\n",
      "if k = (n + 1)/2 for n odd or k = 1 + n/2 for n even, we have the majority\n",
      "function.\n",
      "2.2.6\n",
      "Linearly Separable Functions\n",
      "The linearly separable functions are those that can be expressed as follows:\n",
      "f = thresh(\n",
      "n\n",
      "X\n",
      "i=1\n",
      "wixi, θ)\n",
      "where wi, i = 1, . . . , n, are real-valued numbers called weights, θ is a real-valued\n",
      "number called the threshold, and thresh(σ, θ) is 1 if σ ≥θ and 0 otherwise.\n",
      "(Note that the concept of linearly separable functions can be extended to non-\n",
      "Boolean inputs.) The k-voting functions are all members of the class of linearly\n",
      "separable functions in which the weights all have unit value and the threshold\n",
      "depends on k. Thus, terms and clauses are special cases of linearly separable\n",
      "functions.\n",
      "A convenient way to write linearly separable functions uses vector notation:\n",
      "f = thresh(X · W, θ)\n",
      "where X = (x1, . . . , xn) is an n-dimensional vector of input variables, W =\n",
      "(w1, . . . , wn) is an n-dimensional vector of weight values, and X · W is the dot\n",
      "(or inner) product of the two vectors. Input vectors for which f has value 1 lie\n",
      "in a half-space on one side of (and on) a hyperplane whose orientation is normal\n",
      "to W and whose position (with respect to the origin) is determined by θ. We\n",
      "saw an example of such a separating plane in Fig. 1.6. With this idea in mind,\n",
      "it is easy to see that two of the functions in Fig. 2.1 are linearly separable, while\n",
      "two are not. Also note that the terms in Figs. 2.3 and 2.4 are linearly separable\n",
      "functions as evidenced by the separating planes shown.\n",
      "There is no closed-form expression for the number of linearly separable func-\n",
      "tions of n dimensions, but the following table gives the numbers for n up to 6.\n",
      "\n",
      "\n",
      "416. 24\n",
      "CHAPTER 2. BOOLEAN FUNCTIONS\n",
      "n\n",
      "Boolean\n",
      "Linearly Separable\n",
      "Functions\n",
      "Functions\n",
      "1\n",
      "4\n",
      "4\n",
      "2\n",
      "16\n",
      "14\n",
      "3\n",
      "256\n",
      "104\n",
      "4\n",
      "65,536\n",
      "1,882\n",
      "5\n",
      "≈4.3 × 109\n",
      "94,572\n",
      "6\n",
      "≈1.8 × 1019\n",
      "15,028,134\n",
      "[Muroga, 1971] has shown that (for n > 1) there are no more than 2n2 linearly\n",
      "separable functions of n dimensions. (See also [Winder, 1961, Winder, 1962].)\n",
      "2.3\n",
      "Summary\n",
      "The diagram in Fig. 2.6 shows some of the set inclusions of the classes of Boolean\n",
      "functions that we have considered. We will be confronting these classes again\n",
      "in later chapters.\n",
      "DNF\n",
      "(All)\n",
      "k-DL\n",
      "k-DNF\n",
      "k-size-\n",
      "terms\n",
      "terms\n",
      "lin sep\n",
      "Figure 2.6: Classes of Boolean Functions\n",
      "The sizes of the various classes are given in the following table (adapted from\n",
      "[Dietterich, 1990, page 262]):\n",
      "\n",
      "\n",
      "417. 2.4. BIBLIOGRAPHICAL AND HISTORICAL REMARKS\n",
      "25\n",
      "Class\n",
      "Size of Class\n",
      "terms\n",
      "3n\n",
      "clauses\n",
      "3n\n",
      "k-term DNF\n",
      "2O(kn)\n",
      "k-clause CNF\n",
      "2O(kn)\n",
      "k-DNF\n",
      "2O(nk)\n",
      "k-CNF\n",
      "2O(nk)\n",
      "k-DL\n",
      "2O[nkk log(n)]\n",
      "lin sep\n",
      "2O(n2)\n",
      "DNF\n",
      "22n\n",
      "2.4\n",
      "Bibliographical and Historical Remarks\n",
      "To be added.\n",
      "\n",
      "\n",
      "418. 26\n",
      "CHAPTER 2. BOOLEAN FUNCTIONS\n",
      "\n",
      "\n",
      "419. Chapter 3\n",
      "Using Version Spaces for\n",
      "Learning\n",
      "3.1\n",
      "Version Spaces and Mistake Bounds\n",
      "The ﬁrst learning methods we present are based on the concepts of version\n",
      "spaces and version graphs. These ideas are most clearly explained for the case\n",
      "of Boolean function learning. Given an initial hypothesis set H (a subset of\n",
      "all Boolean functions) and the values of f(X) for each X in a training set, Ξ,\n",
      "the version space is that subset of hypotheses, Hv, that is consistent with these\n",
      "values. A hypothesis, h, is consistent with the values of X in Ξ if and only if\n",
      "h(X) = f(X) for all X in Ξ. We say that the hypotheses in H that are not\n",
      "consistent with the values in the training set are ruled out by the training set.\n",
      "We could imagine (conceptually only!) that we have devices for implement-\n",
      "ing every function in H.\n",
      "An incremental training procedure could then be\n",
      "deﬁned which presented each pattern in Ξ to each of these functions and then\n",
      "eliminated those functions whose values for that pattern did not agree with its\n",
      "given value. At any stage of the process we would then have left some subset\n",
      "of functions that are consistent with the patterns presented so far; this subset\n",
      "is the version space for the patterns already presented. This idea is illustrated\n",
      "in Fig. 3.1.\n",
      "Consider the following procedure for classifying an arbitrary input pattern,\n",
      "X: the pattern is put in the same class (0 or 1) as are the majority of the\n",
      "outputs of the functions in the version space. During the learning procedure,\n",
      "if this majority is not equal to the value of the pattern presented, we say a\n",
      "mistake is made, and we revise the version space accordingly—eliminating all\n",
      "those (majority of the) functions voting incorrectly. Thus, whenever a mistake\n",
      "is made, we rule out at least half of the functions remaining in the version space.\n",
      "How many mistakes can such a procedure make? Obviously, we can make\n",
      "no more than log2(|H|) mistakes, where |H| is the number of hypotheses in the\n",
      "27\n",
      "\n",
      "\n",
      "420. 28\n",
      "CHAPTER 3. USING VERSION SPACES FOR LEARNING\n",
      "h1\n",
      "h2\n",
      "hi\n",
      "hK\n",
      "X\n",
      "A Subset, H,  of all\n",
      "Boolean Functions\n",
      "Rule out hypotheses not\n",
      "consistent with training patterns\n",
      "hj\n",
      "Hypotheses not ruled out\n",
      "constitute the version space\n",
      "K = |H|\n",
      "1 or 0\n",
      "Figure 3.1: Implementing the Version Space\n",
      "original hypothesis set, H. (Note, though, that the number of training patterns\n",
      "seen before this maximum number of mistakes is made might be much greater.)\n",
      "This theoretical (and very impractical!) result (due to [Littlestone, 1988]) is an\n",
      "example of a mistake bound—an important concept in machine learning theory.\n",
      "It shows that there must exist a learning procedure that makes no more mistakes\n",
      "than this upper bound. Later, we’ll derive other mistake bounds.\n",
      "As a special case, if our bias was to limit H to terms, we would make no\n",
      "more than log2(3n) = n log2(3) = 1.585n mistakes before exhausting the version\n",
      "space. This result means that if f were a term, we would make no more than\n",
      "1.585n mistakes before learning f, and otherwise we would make no more than\n",
      "that number of mistakes before being able to decide that f is not a term.\n",
      "Even if we do not have suﬃcient training patterns to reduce the version\n",
      "space to a single function, it may be that there are enough training patterns\n",
      "to reduce the version space to a set of functions such that most of them assign\n",
      "the same values to most of the patterns we will see henceforth. We could select\n",
      "one of the remaining functions at random and be reasonably assured that it\n",
      "will generalize satisfactorily. We next discuss a computationally more feasible\n",
      "method for representing the version space.\n",
      "\n",
      "\n",
      "421. 3.2. VERSION GRAPHS\n",
      "29\n",
      "3.2\n",
      "Version Graphs\n",
      "Boolean functions can be ordered by generality. A Boolean function, f1, is more\n",
      "general than a function, f2, (and f2 is more speciﬁc than f1), if f1 has value 1\n",
      "for all of the arguments for which f2 has value 1, and f1 ̸= f2. For example, x3\n",
      "is more general than x2x3 but is not more general than x3 + x2.\n",
      "We can form a graph with the hypotheses, {hi}, in the version space as\n",
      "nodes. A node in the graph, hi, has an arc directed to node, hj, if and only if\n",
      "hj is more general than hi. We call such a graph a version graph. In Fig. 3.2,\n",
      "we show an example of a version graph over a 3-dimensional input space for\n",
      "hypotheses restricted to terms (with none of them yet ruled out).\n",
      "0\n",
      "x1\n",
      "x2\n",
      "x3\n",
      "x2\n",
      "x3\n",
      "1\n",
      "x1x2 x3\n",
      "x1x2\n",
      "x1\n",
      "Version Graph for Terms\n",
      "x1\n",
      "x2\n",
      "x3\n",
      "(for simplicity, only some arcs in the graph are shown)\n",
      "(none yet ruled out)\n",
      "(k = 1)\n",
      "(k = 2)\n",
      "(k = 3)\n",
      "x1 x3\n",
      "Figure 3.2: A Version Graph for Terms\n",
      "That function, denoted here by “1,” which has value 1 for all inputs, corre-\n",
      "sponds to the node at the top of the graph. (It is more general than any other\n",
      "term.) Similarly, the function “0” is at the bottom of the graph. Just below\n",
      "“1” is a row of nodes corresponding to all terms having just one literal, and just\n",
      "below them is a row of nodes corresponding to terms having two literals, and\n",
      "\n",
      "\n",
      "422. 30\n",
      "CHAPTER 3. USING VERSION SPACES FOR LEARNING\n",
      "so on. There are 33 = 27 functions altogether (the function “0,” included in\n",
      "the graph, is technically not a term). To make our portrayal of the graph less\n",
      "cluttered only some of the arcs are shown; each node in the actual graph has an\n",
      "arc directed to all of the nodes above it that are more general.\n",
      "We use this same example to show how the version graph changes as we\n",
      "consider a set of labeled samples in a training set, Ξ. Suppose we ﬁrst consider\n",
      "the training pattern (1, 0, 1) with value 0. Some of the functions in the version\n",
      "graph of Fig. 3.2 are inconsistent with this training pattern. These ruled out\n",
      "nodes are no longer in the version graph and are shown shaded in Fig. 3.3. We\n",
      "also show there the three-dimensional cube representation in which the vertex\n",
      "(1, 0, 1) has value 0.\n",
      "0\n",
      "x1\n",
      "x2\n",
      "x3\n",
      "x2\n",
      "x3\n",
      "1\n",
      "x1x2 x3\n",
      "x1x2\n",
      "x1\n",
      "New Version Graph\n",
      "1, 0, 1 has\n",
      "value 0\n",
      "x1x3\n",
      "x1x2\n",
      "x2x3\n",
      "x1x2x3\n",
      "x1\n",
      "x2\n",
      "x3\n",
      "x1x3\n",
      "(only some arcs in the graph are shown)\n",
      "ruled out nodes\n",
      "Figure 3.3: The Version Graph Upon Seeing (1, 0, 1)\n",
      "In a version graph, there are always a set of hypotheses that are maximally\n",
      "general and a set of hypotheses that are maximally speciﬁc. These are called\n",
      "the general boundary set (gbs) and the speciﬁc boundary set (sbs), respectively.\n",
      "In Fig. 3.4, we have the version graph as it exists after learning that (1,0,1) has\n",
      "value 0 and (1, 0, 0) has value 1. The gbs and sbs are shown.\n",
      "\n",
      "\n",
      "423. 3.2. VERSION GRAPHS\n",
      "31\n",
      "0\n",
      "x1\n",
      "x2\n",
      "x3\n",
      "x2\n",
      "x3\n",
      "1\n",
      "x1x2 x3\n",
      "x1\n",
      "x2x3\n",
      "x1x3\n",
      "general boundary set\n",
      "(gbs)\n",
      "specific boundary set (sbs)\n",
      "x1x2\n",
      "more specific than gbs,\n",
      "more general than sbs\n",
      "1, 0, 1 has\n",
      "value 0\n",
      "x1\n",
      "x2\n",
      "x3\n",
      "1, 0, 0 has\n",
      "value 1\n",
      "Figure 3.4: The Version Graph Upon Seeing (1, 0, 1) and (1, 0, 0)\n",
      "Boundary sets are important because they provide an alternative to repre-\n",
      "senting the entire version space explicitly, which would be impractical. Given\n",
      "only the boundary sets, it is possible to determine whether or not any hypoth-\n",
      "esis (in the prescribed class of Boolean functions we are using) is a member or\n",
      "not of the version space. This determination is possible because of the fact that\n",
      "any member of the version space (that is not a member of one of the boundary\n",
      "sets) is more speciﬁc than some member of the general boundary set and is more\n",
      "general than some member of the speciﬁc boundary set.\n",
      "If we limit our Boolean functions that can be in the version space to terms,\n",
      "it is a simple matter to determine maximally general and maximally speciﬁc\n",
      "functions (assuming that there is some term that is in the version space). A\n",
      "maximally speciﬁc one corresponds to a subface of minimal dimension that\n",
      "contains all the members of the training set labelled by a 1 and no members\n",
      "labelled by a 0. A maximally general one corresponds to a subface of maximal\n",
      "dimension that contains all the members of the training set labelled by a 1 and\n",
      "no members labelled by a 0. Looking at Fig. 3.4, we see that the subface of\n",
      "minimal dimension that contains (1, 0, 0) but does not contain (1, 0, 1) is just\n",
      "the vertex (1, 0, 0) itself—corresponding to the function x1x2 x3. The subface\n",
      "\n",
      "\n",
      "424. 32\n",
      "CHAPTER 3. USING VERSION SPACES FOR LEARNING\n",
      "of maximal dimension that contains (1, 0, 0) but does not contain (1, 0, 1) is\n",
      "the bottom face of the cube—corresponding to the function x3. In Figs. 3.2\n",
      "through 3.4 the sbs is always singular. Version spaces for terms always have\n",
      "singular speciﬁc boundary sets.\n",
      "As seen in Fig.\n",
      "3.3, however, the gbs of a\n",
      "version space for terms need not be singular.\n",
      "3.3\n",
      "Learning as Search of a Version Space\n",
      "[To be written.\n",
      "Relate to term learning algorithm presented in Chapter\n",
      "Two. Also discuss best-ﬁrst search methods. See Pat Langley’s example us-\n",
      "ing “pseudo-cells” of how to generate and eliminate hypotheses.]\n",
      "Selecting a hypothesis from the version space can be thought of as a search\n",
      "problem. One can start with a very general function and specialize it through\n",
      "various specialization operators until one ﬁnds a function that is consistent (or\n",
      "adequately so) with a set of training patterns.\n",
      "Such procedures are usually\n",
      "called top-down methods. Or, one can start with a very special function and\n",
      "generalize it—resulting in bottom-up methods. We shall see instances of both\n",
      "styles of learning in this book.\n",
      "Compare this view of top-down\n",
      "versus bottom-up with the\n",
      "divide-and-conquer and the\n",
      "covering (or AQ) methods of\n",
      "decision-tree induction.\n",
      "3.4\n",
      "The Candidate Elimination Method\n",
      "The candidate elimination method, is an incremental method for computing the\n",
      "boundary sets. Quoting from [Hirsh, 1994, page 6]:\n",
      "“The candidate-elimination algorithm manipulates the boundary-set\n",
      "representation of a version space to create boundary sets that rep-\n",
      "resent a new version space consistent with all the previous instances\n",
      "plus the new one. For a positive exmple the algorithm generalizes\n",
      "the elements of the [sbs] as little as possible so that they cover the\n",
      "new instance yet remain consistent with past data, and removes\n",
      "those elements of the [gbs] that do not cover the new instance. For\n",
      "a negative instance the algorithm specializes elements of the [gbs]\n",
      "so that they no longer cover the new instance yet remain consis-\n",
      "tent with past data, and removes from the [sbs] those elements that\n",
      "mistakenly cover the new, negative instance.”\n",
      "The\n",
      "method\n",
      "uses\n",
      "the\n",
      "following\n",
      "deﬁnitions\n",
      "(adapted\n",
      "from\n",
      "[Genesereth & Nilsson, 1987]):\n",
      "• a hypothesis is called suﬃcient if and only if it has value 1 for all training\n",
      "samples labeled by a 1,\n",
      "• a hypothesis is called necessary if and only if it has value 0 for all training\n",
      "samples labeled by a 0.\n",
      "\n",
      "\n",
      "425. 3.4. THE CANDIDATE ELIMINATION METHOD\n",
      "33\n",
      "Here is how to think about these deﬁnitions: A hypothesis implements a suﬃ-\n",
      "cient condition that a training sample has value 1 if the hypothesis has value 1\n",
      "for all of the positive instances; a hypothesis implements a necessary condition\n",
      "that a training sample has value 1 if the hypothesis has value 0 for all of the\n",
      "negative instances. A hypothesis is consistent with the training set (and thus is\n",
      "in the version space) if and only if it is both suﬃcient and necessary.\n",
      "We start (before receiving any members of the training set) with the function\n",
      "“0” as the singleton element of the speciﬁc boundary set and with the function\n",
      "“1” as the singleton element of the general boundary set. Upon receiving a new\n",
      "labeled input vector, the boundary sets are changed as follows:\n",
      "a. If the new vector is labelled with a 1:\n",
      "The new general boundary set is obtained from the previous one by ex-\n",
      "cluding any elements in it that are not suﬃcient. (That is, we exclude any\n",
      "elements that have value 0 for the new vector.)\n",
      "The new speciﬁc boundary set is obtained from the previous one by re-\n",
      "placing each element, hi, in it by all of its least generalizations.\n",
      "The hypothesis hg is a least generalization\n",
      "of h if and only if: a) h is\n",
      "more speciﬁc than hg, b) hg is suﬃcient, c) no function (including h) that\n",
      "is more speciﬁc than hg is suﬃcient, and d) hg is more speciﬁc than some\n",
      "member of the new general boundary set. It might be that hg = h. Also,\n",
      "least generalizations of two diﬀerent functions in the speciﬁc boundary set\n",
      "may be identical.\n",
      "b. If the new vector is labelled with a 0:\n",
      "The new speciﬁc boundary set is obtained from the previous one by ex-\n",
      "cluding any elements in it that are not necessary. (That is, we exclude\n",
      "any elements that have value 1 for the new vector.)\n",
      "The new general boundary set is obtained from the previous one by re-\n",
      "placing each element, hi, in it by all of its least specializations.\n",
      "The hypothesis hs is a least specialization of h if and only if: a) h is more\n",
      "general than hs, b) hs is necessary, c) no function (including h) that is\n",
      "more general than hs is necessary, and d) hs is more general than some\n",
      "member of the new speciﬁc boundary set. Again, it might be that hs = h,\n",
      "and least specializations of two diﬀerent functions in the general boundary\n",
      "set may be identical.\n",
      "As an example, suppose we present the vectors in the following order:\n",
      "vector\n",
      "label\n",
      "(1, 0, 1)\n",
      "0\n",
      "(1, 0, 0)\n",
      "1\n",
      "(1, 1, 1)\n",
      "0\n",
      "(0, 0, 1)\n",
      "0\n",
      "\n",
      "\n",
      "426. 34\n",
      "CHAPTER 3. USING VERSION SPACES FOR LEARNING\n",
      "We start with general boundary set, “1”, and speciﬁc boundary set, “0.”\n",
      "After seeing the ﬁrst sample, (1, 0, 1), labeled with a 0, the speciﬁc boundary\n",
      "set stays at “0” (it is necessary), and we change the general boundary set to\n",
      "{x1, x2, x3}. Each of the functions, x1, x2, and x3, are least specializations of\n",
      "“1” (they are necessary, “1” is not, they are more general than “0”, and there\n",
      "are no functions that are more general than they and also necessary).\n",
      "Then, after seeing (1, 0, 0), labeled with a 1, the general boundary set\n",
      "changes to {x3} (because x1 and x2 are not suﬃcient), and the speciﬁc boundary\n",
      "set is changed to {x1x2 x3}. This single function is a least generalization of “0”\n",
      "(it is suﬃcient, “0” is more speciﬁc than it, no function (including “0”) that is\n",
      "more speciﬁc than it is suﬃcient, and it is more speciﬁc than some member of\n",
      "the general boundary set.\n",
      "When we see (1, 1, 1), labeled with a 0, we do not change the speciﬁc\n",
      "boundary set because its function is still necessary.\n",
      "We do not change the\n",
      "general boundary set either because x3 is still necessary.\n",
      "Finally, when we see (0, 0, 1), labeled with a 0, we do not change the speciﬁc\n",
      "boundary set because its function is still necessary. We do not change the general\n",
      "boundary set either because x3 is still necessary.\n",
      "Maybe I’ll put in an example of a\n",
      "version graph for non-Boolean\n",
      "functions.\n",
      "3.5\n",
      "Bibliographical and Historical Remarks\n",
      "The concept of version spaces and their role in learning was ﬁrst investigated\n",
      "by Tom Mitchell [Mitchell, 1982]. Although these ideas are not used in prac-\n",
      "tical machine learning procedures, they do provide insight into the nature of\n",
      "hypothesis selection. In order to accomodate noisy data, version spaces have\n",
      "been generalized by [Hirsh, 1994] to allow hypotheses that are not necessarily\n",
      "consistent with the training set.\n",
      "More to be added.\n",
      "\n",
      "\n",
      "427. Chapter 4\n",
      "Neural Networks\n",
      "In chapter two we deﬁned several important subsets of Boolean functions. Sup-\n",
      "pose we decide to use one of these subsets as a hypothesis set for supervised\n",
      "function learning. We next have the question of how best to implement the\n",
      "function as a device that gives the outputs prescribed by the function for arbi-\n",
      "trary inputs. In this chapter we describe how networks of non-linear elements\n",
      "can be used to implement various input-output functions and how they can be\n",
      "trained using supervised learning methods.\n",
      "Networks of non-linear elements, interconnected through adjustable weights,\n",
      "play a prominent role in machine learning. They are called neural networks be-\n",
      "cause the non-linear elements have as their inputs a weighted sum of the outputs\n",
      "of other elements—much like networks of biological neurons do. These networks\n",
      "commonly use the threshold element which we encountered in chapter two in\n",
      "our study of linearly separable Boolean functions. We begin our treatment of\n",
      "neural nets by studying this threshold element and how it can be used in the\n",
      "simplest of all networks, namely ones composed of a single threshold element.\n",
      "4.1\n",
      "Threshold Logic Units\n",
      "4.1.1\n",
      "Deﬁnitions and Geometry\n",
      "Linearly separable (threshold) functions are implemented in a straightforward\n",
      "way by summing the weighted inputs and comparing this sum to a threshold\n",
      "value as shown in Fig. 4.1. This structure we call a threshold logic unit (TLU).\n",
      "Its output is 1 or 0 depending on whether or not the weighted sum of its inputs is\n",
      "greater than or equal to a threshold value, θ. It has also been called an Adaline\n",
      "(for adaptive linear element) [Widrow, 1962, Widrow & Lehr, 1990], an LTU\n",
      "(linear threshold unit), a perceptron, and a neuron. (Although the word “per-\n",
      "ceptron” is often used nowadays to refer to a single TLU, Rosenblatt originally\n",
      "deﬁned it as a class of networks of threshold elements [Rosenblatt, 1958].)\n",
      "35\n",
      "\n",
      "\n",
      "428. 36\n",
      "CHAPTER 4. NEURAL NETWORKS\n",
      "!\n",
      "x1\n",
      "x2\n",
      "xn+1 = 1\n",
      "xi\n",
      "w1\n",
      "w2\n",
      "wn+1\n",
      "wi\n",
      "wn\n",
      "X\n",
      "threshold weight\n",
      "xn\n",
      "W\n",
      "threshold  \"  = 0\n",
      "f\n",
      "f = thresh( ! wi xi,  0)\n",
      "i = 1\n",
      "n+1\n",
      "Figure 4.1: A Threshold Logic Unit (TLU)\n",
      "The n-dimensional feature or input vector is denoted by X = (x1, . . . , xn).\n",
      "When we want to distinguish among diﬀerent feature vectors, we will attach\n",
      "subscripts, such as Xi. The components of X can be any real-valued numbers,\n",
      "but we often specialize to the binary numbers 0 and 1. The weights of a TLU\n",
      "are represented by an n-dimensional weight vector, W = (w1, . . . , wn).\n",
      "Its\n",
      "components are real-valued numbers (but we sometimes specialize to integers).\n",
      "The TLU has output 1 if Pn\n",
      "i=1 xiwi ≥θ; otherwise it has output 0.\n",
      "The\n",
      "weighted sum that is calculated by the TLU can be simply represented as a\n",
      "vector dot product, X•W. (If the pattern and weight vectors are thought of as\n",
      "“column” vectors, this dot product is then sometimes written as XtW, where\n",
      "the “row” vector Xt is the transpose of X.) Often, the threshold, θ, of the TLU\n",
      "is ﬁxed at 0; in that case, arbitrary thresholds are achieved by using (n + 1)-\n",
      "dimensional “augmented” vectors, Y, and V, whose ﬁrst n components are the\n",
      "same as those of X and W, respectively. The (n + 1)-st component, xn+1, of\n",
      "the augmented feature vector, Y, always has value 1; the (n + 1)-st component,\n",
      "wn+1, of the augmented weight vector, V, is set equal to the negative of the\n",
      "desired threshold value. (When we want to emphasize the use of augmented\n",
      "vectors, we’ll use the Y,V notation; however when the context of the discussion\n",
      "makes it clear about what sort of vectors we are talking about, we’ll lapse back\n",
      "into the more familiar X,W notation.) In the Y,V notation, the TLU has an\n",
      "output of 1 if Y•V ≥0. Otherwise, the output is 0.\n",
      "We can give an intuitively useful geometric description of a TLU. A TLU\n",
      "divides the input space by a hyperplane as sketched in Fig. 4.2. The hyperplane\n",
      "is the boundary between patterns for which X•W + wn+1 > 0 and patterns\n",
      "for which X•W + wn+1 < 0. Thus, the equation of the hyperplane itself is\n",
      "X•W+wn+1 = 0. The unit vector that is normal to the hyperplane is n =\n",
      "W\n",
      "|W|,\n",
      "where |W| =\n",
      "p\n",
      "(w2\n",
      "1 + . . . + w2n) is the length of the vector W. (The normal\n",
      "\n",
      "\n",
      "429. 4.1. THRESHOLD LOGIC UNITS\n",
      "37\n",
      "form of the hyperplane equation is X•n +\n",
      "W\n",
      "|W| = 0.) The distance from the\n",
      "hyperplane to the origin is wn+1\n",
      "|W| , and the distance from an arbitrary point, X,\n",
      "to the hyperplane is X•W+wn+1\n",
      "|W|\n",
      ". When the distance from the hyperplane to the\n",
      "origin is negative (that is, when wn+1 < 0), then the origin is on the negative\n",
      "side of the hyperplane (that is, the side for which X•W + wn+1 < 0).\n",
      "X.W + wn+1 > 0\n",
      "on this side\n",
      "W\n",
      "X\n",
      "W\n",
      "n =\n",
      "W\n",
      "|W|\n",
      "Origin\n",
      "Unit vector normal\n",
      "to hyperplane\n",
      "W + wn+1 = 0\n",
      "X\n",
      "n +           = 0\n",
      "X\n",
      "Equations of hyperplane:\n",
      "wn+1\n",
      "|W|\n",
      "wn+1\n",
      "W + wn+1\n",
      "X\n",
      "X.W + wn+1 < 0\n",
      "on this side\n",
      "Figure 4.2: TLU Geometry\n",
      "Adjusting the weight vector, W, changes the orientation of the hyperplane;\n",
      "adjusting wn+1 changes the position of the hyperplane (relative to the origin).\n",
      "Thus, training of a TLU can be achieved by adjusting the values of the weights.\n",
      "In this way the hyperplane can be moved so that the TLU implements diﬀerent\n",
      "(linearly separable) functions of the input.\n",
      "4.1.2\n",
      "Special Cases of Linearly Separable Functions\n",
      "Terms\n",
      "Any term of size k can be implemented by a TLU with a weight from each of\n",
      "those inputs corresponding to variables occurring in the term. A weight of +1 is\n",
      "used from an input corresponding to a positive literal, and a weight of −1 is used\n",
      "from an input corresponding to a negative literal. (Literals not mentioned in\n",
      "the term have weights of zero—that is, no connection at all—from their inputs.)\n",
      "The threshold, θ, is set equal to kp −1/2, where kp is the number of positive\n",
      "literals in the term. Such a TLU implements a hyperplane boundary that is\n",
      "\n",
      "\n",
      "430. 38\n",
      "CHAPTER 4. NEURAL NETWORKS\n",
      "parallel to a subface of dimension (n −k) of the unit hypercube. We show a\n",
      "three-dimensional example in Fig. 4.3. Thus, linearly separable functions are a\n",
      "superset of terms.\n",
      "(1,1,1)\n",
      "(1,1,0)\n",
      "x2\n",
      "x1\n",
      "x3\n",
      "f = x1x2\n",
      "x1 + x2 - 3/2 = 0\n",
      "Equation of plane is:\n",
      "Figure 4.3: Implementing a Term\n",
      "Clauses\n",
      "The negation of a clause is a term. For example, the negation of the clause\n",
      "f = x1 + x2 + x3 is the term f = x1 x2 x3. A hyperplane can be used to\n",
      "implement this term.\n",
      "If we “invert” the hyperplane, it will implement the\n",
      "clause instead. Inverting a hyperplane is done by multiplying all of the TLU\n",
      "weights—even wn+1—by −1. This process simply changes the orientation of the\n",
      "hyperplane—ﬂipping it around by 180 degrees and thus changing its “positive\n",
      "side.” Therefore, linearly separable functions are also a superset of clauses. We\n",
      "show an example in Fig. 4.4.\n",
      "4.1.3\n",
      "Error-Correction Training of a TLU\n",
      "There are several procedures that have been proposed for adjusting the weights\n",
      "of a TLU. We present next a family of incremental training procedures with\n",
      "parameter c. These methods make adjustments to the weight vector only when\n",
      "the TLU being trained makes an error on a training pattern; they are called\n",
      "error-correction procedures. We use augmented feature and weight vectors in\n",
      "describing them.\n",
      "a. We start with a ﬁnite training set, Ξ, of vectors, Yi , and their binary\n",
      "labels.\n",
      "\n",
      "\n",
      "431. 4.1. THRESHOLD LOGIC UNITS\n",
      "39\n",
      "f = x1 + x2 + x3\n",
      "x1\n",
      "x1 + x2 + x3 < 1/2 = 0\n",
      "f = x1x2x3\n",
      "Equation of plane is:\n",
      "x2\n",
      "x3\n",
      "Figure 4.4: Implementing a Clause\n",
      "b. Compose an inﬁnite training sequence, Σ, of vectors from Ξ and their\n",
      "labels such that each member of Ξ occurs inﬁnitely often in Σ. Set the\n",
      "initial weight values of an TLU to arbitrary values.\n",
      "c. Repeat forever:\n",
      "Present the next vector, Yi, in Σ to the TLU and note its response.\n",
      "(a) If the TLU responds correctly, make no change in the weight vector.\n",
      "(b) If Yi is supposed to produce an output of 0 and produces an output\n",
      "of 1 instead, modify the weight vector as follows:\n",
      "V ←−V −ciYi\n",
      "where ci is a positive real number called the learning rate parame-\n",
      "ter (whose value is diﬀererent in diﬀerent instances of this family of\n",
      "procedures and may depend on i).\n",
      "Note that after this adjustment the new dot product will be (V −\n",
      "ciYi)•Yi = V•Yi −ciYi•Yi, which is smaller than it was before the\n",
      "weight adjustment.\n",
      "(c) If Yi is supposed to produce an output of 1 and produces an output\n",
      "of 0 instead, modify the weight vector as follows:\n",
      "V ←−V + ciYi\n",
      "In this case, the new dot product will be (V + ciYi)•Yi = V•Yi +\n",
      "ciYi•Yi, which is larger than it was before the weight adjustment.\n",
      "Note that all three of these cases can be combined in the following rule:\n",
      "\n",
      "\n",
      "432. 40\n",
      "CHAPTER 4. NEURAL NETWORKS\n",
      "V ←−V + ci(di −fi)Yi\n",
      "where di is the desired response (1 or 0) for Yi , and fi is the actual\n",
      "response (1 or 0) for Yi.]\n",
      "Note also that because the weight vector V now includes the wn+1 thresh-\n",
      "old component, the threshold of the TLU is also changed by these adjust-\n",
      "ments.\n",
      "We identify two versions of this procedure:\n",
      "1) In the ﬁxed-increment procedure, the learning rate parameter, ci, is the\n",
      "same ﬁxed, positive constant for all i. Depending on the value of this constant,\n",
      "the weight adjustment may or may not correct the response to an erroneously\n",
      "classiﬁed feature vector.\n",
      "2) In the fractional-correction procedure, the parameter ci is set to λ Yi•V\n",
      "Yi•Yi ,\n",
      "where V is the weight vector before it is changed.\n",
      "Note that if λ = 0, no\n",
      "correction takes place at all. If λ = 1, the correction is just suﬃcient to make\n",
      "Yi•V = 0. If λ > 1, the error will be corrected.\n",
      "It can be proved that if there is some weight vector, V, that produces a\n",
      "correct output for all of the feature vectors in Ξ, then after a ﬁnite number\n",
      "of feature vector presentations, the ﬁxed-increment procedure will ﬁnd such a\n",
      "weight vector and thus make no more weight changes. The same result holds\n",
      "for the fractional-correction procedure if 1 < λ ≤2.\n",
      "For additional background, proofs, and examples of error-correction proce-\n",
      "dures, see [Nilsson, 1990].\n",
      "See [Maass & Tur´an, 1994] for a\n",
      "hyperplane-ﬁnding procedure that\n",
      "makes no more than O(n2 log n)\n",
      "mistakes.\n",
      "4.1.4\n",
      "Weight Space\n",
      "We can give an intuitive idea about how these procedures work by considering\n",
      "what happens to the augmented weight vector in “weight space” as corrections\n",
      "are made. We use augmented vectors in our discussion here so that the threshold\n",
      "function compares the dot product, Yi•V, against a threshold of 0. A particular\n",
      "weight vector, V, then corresponds to a point in (n + 1)-dimensional weight\n",
      "space.\n",
      "Now, for any pattern vector, Yi, consider the locus of all points in\n",
      "weight space corresponding to weight vectors yielding Yi•V = 0. This locus is\n",
      "a hyperplane passing through the origin of the (n + 1)-dimensional space. Each\n",
      "pattern vector will have such a hyperplane corresponding to it. Weight points\n",
      "in one of the half-spaces deﬁned by this hyperplane will cause the corresponding\n",
      "pattern to yield a dot product less than 0, and weight points in the other half-\n",
      "space will cause the corresponding pattern to yield a dot product greater than\n",
      "0.\n",
      "We show a schematic representation of such a weight space in Fig.\n",
      "4.5.\n",
      "There are four pattern hyperplanes, 1, 2, 3, 4 , corresponding to patterns Y1,\n",
      "\n",
      "\n",
      "433. 4.1. THRESHOLD LOGIC UNITS\n",
      "41\n",
      "Y2, Y3, Y4, respectively, and we indicate by an arrow the half-space for each\n",
      "in which weight vectors give dot products greater than 0. Suppose we wanted\n",
      "weight values that would give positive responses for patterns Y1, Y3, and Y4,\n",
      "and a negative response for pattern Y2. The weight point, V, indicated in the\n",
      "ﬁgure is one such set of weight values.\n",
      "2\n",
      "3\n",
      "4\n",
      "1\n",
      "V\n",
      "Figure 4.5: Weight Space\n",
      "The question of whether or not there exists a weight vector that gives desired\n",
      "responses for a given set of patterns can be given a geometric interpretation. To\n",
      "do so involves reversing the “polarity” of those hyperplanes corresponding to\n",
      "patterns for which a negative response is desired. If we do that for our example\n",
      "above, we get the weight space diagram shown in Fig. 4.6.\n",
      "2\n",
      "3\n",
      "4\n",
      "1\n",
      "V\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "2\n",
      "3\n",
      "4\n",
      "Figure 4.6: Solution Region in Weight Space\n",
      "\n",
      "\n",
      "434. 42\n",
      "CHAPTER 4. NEURAL NETWORKS\n",
      "If a weight vector exists that correctly classiﬁes a set of patterns, then the\n",
      "half-spaces deﬁned by the correct responses for these patterns will have a non-\n",
      "empty intersection, called the solution region. The solution region will be a\n",
      "“hyper-wedge” region whose vertex is at the origin of weight space and whose\n",
      "cross-section increases with increasing distance from the origin.\n",
      "This region\n",
      "is shown shaded in Fig. 4.6. (The boxed numbers show, for later purposes,\n",
      "the number of errors made by weight vectors in each of the regions.)\n",
      "The\n",
      "ﬁxed-increment error-correction procedure changes a weight vector by moving it\n",
      "normal to any pattern hyperplane for which that weight vector gives an incorrect\n",
      "response. Suppose in our example that we present the patterns in the sequence\n",
      "Y1, Y2, Y3, Y4, and start the process with a weight point V1, as shown in Fig.\n",
      "4.7. Starting at V1, we see that it gives an incorrect response for pattern Y1, so\n",
      "we move V1 to V2 in a direction normal to plane 1. (That is what adding Y1 to\n",
      "V1 does.) Y2 gives an incorrect response for pattern Y2, and so on. Ultimately,\n",
      "the responses are only incorrect for planes bounding the solution region. Some\n",
      "of the subsequent corrections may overshoot the solution region, but eventually\n",
      "we work our way out far enough in the solution region that corrections (for\n",
      "a ﬁxed increment size) take us within it. The proofs for convergence of the\n",
      "ﬁxed-increment rule make this intuitive argument precise.\n",
      "2\n",
      "3\n",
      "4\n",
      "1\n",
      "V\n",
      "V1\n",
      "V2\n",
      "V3\n",
      "V4\n",
      "V5\n",
      "V6\n",
      "Figure 4.7: Moving Into the Solution Region\n",
      "4.1.5\n",
      "The Widrow-HoﬀProcedure\n",
      "The Widrow-Hoﬀprocedure (also called the LMS or the delta procedure) at-\n",
      "tempts to ﬁnd weights that minimize a squared-error function between the pat-\n",
      "tern labels and the dot product computed by a TLU. For this purpose, the\n",
      "pattern labels are assumed to be either +1 or −1 (instead of 1 or 0).\n",
      "The\n",
      "\n",
      "\n",
      "435. 4.1. THRESHOLD LOGIC UNITS\n",
      "43\n",
      "squared error for a pattern, Xi, with label di (for desired output) is:\n",
      "εi = (di −\n",
      "n+1\n",
      "X\n",
      "j=1\n",
      "xijwj)2\n",
      "where xij is the j-th component of Xi. The total squared error (over all patterns\n",
      "in a training set, Ξ, containing m patterns) is then:\n",
      "ε =\n",
      "m\n",
      "X\n",
      "i=1\n",
      "(di −\n",
      "n+1\n",
      "X\n",
      "j=1\n",
      "xijwj)2\n",
      "We want to choose the weights wj to minimize this squared error. One way to\n",
      "ﬁnd such a set of weights is to start with an arbitrary weight vector and move it\n",
      "along the negative gradient of ε as a function of the weights. Since ε is quadratic\n",
      "in the wj, we know that it has a global minimum, and thus this steepest descent\n",
      "procedure is guaranteed to ﬁnd the minimum. Each component of the gradient\n",
      "is the partial derivative of ε with respect to one of the weights. One problem\n",
      "with taking the partial derivative of ε is that ε depends on all the input vectors\n",
      "in Ξ. Often, it is preferable to use an incremental procedure in which we try the\n",
      "TLU on just one element, Xi, of Ξ at a time, compute the gradient of the single-\n",
      "pattern squared error, εi, make the appropriate adjustment to the weights, and\n",
      "then try another member of Ξ. Of course, the results of the incremental version\n",
      "can only approximate those of the batch one, but the approximation is usually\n",
      "quite eﬀective. We will be describing the incremental version here.\n",
      "The j-th component of the gradient of the single-pattern error is:\n",
      "∂εi\n",
      "∂wj\n",
      "= −2(di −\n",
      "n+1\n",
      "X\n",
      "j=1\n",
      "xijwj)xij\n",
      "An adjustment in the direction of the negative gradient would then change each\n",
      "weight as follows:\n",
      "wj ←−wj + ci(di −fi)xij\n",
      "where fi = Pn+1\n",
      "j=1 xijwj, and ci governs the size of the adjustment. The entire\n",
      "weight vector (in augmented, or V, notation) is thus adjusted according to the\n",
      "following rule:\n",
      "V ←−V + ci(di −fi)Yi\n",
      "where, as before, Yi is the i-th augmented pattern vector.\n",
      "The Widrow-Hoﬀprocedure makes adjustments to the weight vector when-\n",
      "ever the dot product itself, Yi•V, does not equal the speciﬁed desired target\n",
      "\n",
      "\n",
      "436. 44\n",
      "CHAPTER 4. NEURAL NETWORKS\n",
      "value, di (which is either 1 or −1).\n",
      "The learning-rate factor, ci, might de-\n",
      "crease with time toward 0 to achieve asymptotic convergence. The Widrow-\n",
      "Hoﬀformula for changing the weight vector has the same form as the standard\n",
      "ﬁxed-increment error-correction formula. The only diﬀerence is that fi is the\n",
      "thresholded response of the TLU in the error-correction case while it is the dot\n",
      "product itself for the Widrow-Hoﬀprocedure.\n",
      "Finding weight values that give the desired dot products corresponds to solv-\n",
      "ing a set of linear equalities, and the Widrow-Hoﬀprocedure can be interpreted\n",
      "as a descent procedure that attempts to minimize the mean-squared-error be-\n",
      "tween the actual and desired values of the dot product. (For more on Widrow-\n",
      "Hoﬀand other related procedures, see [Duda & Hart, 1973, pp. 151ﬀ].)\n",
      "Examples of training curves for\n",
      "TLU’s; performance on training\n",
      "set; performance on test set;\n",
      "cumulative number of corrections.\n",
      "4.1.6\n",
      "Training a TLU on Non-Linearly-Separable Training\n",
      "Sets\n",
      "When the training set is not linearly separable (perhaps because of noise or\n",
      "perhaps inherently), it may still be desired to ﬁnd a “best” separating hy-\n",
      "perplane. Typically, the error-correction procedures will not do well on non-\n",
      "linearly-separable training sets because they will continue to attempt to correct\n",
      "inevitable errors, and the hyperplane will never settle into an acceptable place.\n",
      "Several methods have been proposed to deal with this case. First, we might\n",
      "use the Widrow-Hoﬀprocedure, which (although it will not converge to zero\n",
      "error on non-linearly separable problems) will give us a weight vector that min-\n",
      "imizes the mean-squared-error. A mean-squared-error criterion often gives un-\n",
      "satisfactory results, however, because it prefers many small errors to a few large\n",
      "ones. As an alternative, error correction with a continuous decrease toward zero\n",
      "of the value of the learning rate constant, c, will result in ever decreasing changes\n",
      "to the hyperplane. Duda [Duda, 1966] has suggested keeping track of the average\n",
      "value of the weight vector during error correction and using this average to give a\n",
      "separating hyperplane that performs reasonably well on non-linearly-separable\n",
      "problems. Gallant [Gallant, 1986] proposed what he called the “pocket algo-\n",
      "rithm.” As described in [Hertz, Krogh, & Palmer, 1991, p. 160]:\n",
      ". . . the pocket algorithm . . . consists simply in storing (or “putting\n",
      "in your pocket”) the set of weights which has had the longest un-\n",
      "modiﬁed run of successes so far. The algorithm is stopped after some\n",
      "chosen time t . . .\n",
      "After stopping, the weights in the pocket are used as a set that should give a\n",
      "small number of errors on the training set. Error-correction proceeds as usual\n",
      "with the ordinary set of weights.\n",
      "Also see methods proposed by\n",
      "[John, 1995] and by\n",
      "[Marchand & Golea, 1993]. The\n",
      "latter is claimed to outperform the\n",
      "pocket algorithm.\n",
      "4.2\n",
      "Linear Machines\n",
      "The natural generalization of a (two-category) TLU to an R-category classiﬁer\n",
      "is the structure, shown in Fig. 4.8, called a linear machine. Here, to use more\n",
      "\n",
      "\n",
      "437. 4.2. LINEAR MACHINES\n",
      "45\n",
      "familiar notation, the Ws and X are meant to be augmented vectors (with an\n",
      "(n+1)-st component). Such a structure is also sometimes called a “competitive”\n",
      "net or a “winner-take-all” net.\n",
      "The output of the linear machine is one of\n",
      "the numbers, {1, . . . , R}, corresponding to which dot product is largest. Note\n",
      "that when R = 2, the linear machine reduces to a TLU with weight vector\n",
      "W = (W1 −W2).\n",
      "X\n",
      "W1\n",
      "WR\n",
      ". . .\n",
      "Y\n",
      "Y\n",
      "ARGMAX\n",
      "W1.X\n",
      "WR.X\n",
      "Figure 4.8: A Linear Machine\n",
      "The diagram in Fig. 4.9 shows the character of the regions in a 2-dimensional\n",
      "space created by a linear machine for R = 5. In n dimensions, every pair of\n",
      "regions is either separated by a section of a hyperplane or is non-adjacent.\n",
      "R1\n",
      "R3\n",
      "R4\n",
      "R5\n",
      "X.W4 * X.Wi for i & 4\n",
      "R2\n",
      "In this region:\n",
      "Figure 4.9: Regions For a Linear Machine\n",
      "To train a linear machine, there is a straightforward generalization of the\n",
      "2-category error-correction rule. Assemble the patterns in the training set into\n",
      "a sequence as before.\n",
      "a. If the machine classiﬁes a pattern correctly, no change is made to any of\n",
      "\n",
      "\n",
      "438. 46\n",
      "CHAPTER 4. NEURAL NETWORKS\n",
      "the weight vectors.\n",
      "b. If the machine mistakenly classiﬁes a category u pattern, Xi, in category\n",
      "v (u ̸= v), then:\n",
      "Wu ←−Wu + ciXi\n",
      "and\n",
      "Wv ←−Wv −ciXi\n",
      "and all other weight vectors are not changed.\n",
      "This correction increases the value of the u-th dot product and decreases the\n",
      "value of the v-th dot product. Just as in the 2-category ﬁxed increment proce-\n",
      "dure, this procedure is guaranteed to terminate, for constant ci, if there exists\n",
      "weight vectors that make correct separations of the training set. Note that when\n",
      "R = 2, this procedure reduces to the ordinary TLU error-correction procedure.\n",
      "A proof that this procedure terminates is given in [Nilsson, 1990, pp. 88-90]\n",
      "and in [Duda & Hart, 1973, pp. 174-177].\n",
      "4.3\n",
      "Networks of TLUs\n",
      "4.3.1\n",
      "Motivation and Examples\n",
      "Layered Networks\n",
      "To classify correctly all of the patterns in non-linearly-separable training sets re-\n",
      "quires separating surfaces more complex than hyperplanes. One way to achieve\n",
      "more complex surfaces is with networks of TLUs. Consider, for example, the 2-\n",
      "dimensional, even parity function, f = x1x2 + x1 x2. No single line through the\n",
      "2-dimensional square can separate the vertices (1,1) and (0,0) from the vertices\n",
      "(1,0) and (0,1)—the function is not linearly separable and thus cannot be im-\n",
      "plemented by a single TLU. But, the network of three TLUs shown in Fig. 4.10\n",
      "does implement this function. In the ﬁgure, we show the weight values along\n",
      "input lines to each TLU and the threshold value inside the circle representing\n",
      "the TLU.\n",
      "The function implemented by a network of TLUs depends on its topology\n",
      "as well as on the weights of the individual TLUs. Feedforward networks have\n",
      "no cycles; in a feedforward network no TLU’s input depends (through zero\n",
      "or more intermediate TLUs) on that TLU’s output. (Networks that are not\n",
      "feedforward are called recurrent networks). If the TLUs of a feedforward network\n",
      "are arranged in layers, with the elements of layer j receiving inputs only from\n",
      "TLUs in layer j −1, then we say that the network is a layered, feedforward\n",
      "\n",
      "\n",
      "439. 4.3. NETWORKS OF TLUS\n",
      "47\n",
      "f\n",
      "x1\n",
      "x2\n",
      "1.5\n",
      "-0.5\n",
      "0.5\n",
      "1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "1\n",
      "Figure 4.10: A Network for the Even Parity Function\n",
      "network. The network shown in Fig. 4.10 is a layered, feedforward network\n",
      "having two layers (of weights). (Some people count the layers of TLUs and\n",
      "include the inputs as a layer also; they would call this network a three-layer\n",
      "network.) In general, a feedforward, layered network has the structure shown\n",
      "in Fig. 4.11. All of the TLUs except the “output” units are called hidden units\n",
      "(they are “hidden” from the output).\n",
      "X\n",
      "hidden units\n",
      "output units\n",
      "Figure 4.11: A Layered, Feedforward Network\n",
      "Implementing DNF Functions by Two-Layer Networks\n",
      "We have already deﬁned k-term DNF functions—they are DNF functions having\n",
      "k terms. A k-term DNF function can be implemented by a two-layer network\n",
      "with k units in the hidden layer—to implement the k terms—and one output\n",
      "unit to implement the disjunction of these terms. Since any Boolean function\n",
      "has a DNF form, any Boolean function can be implemented by some two-layer\n",
      "network of TLUs. As an example, consider the function f = x1x2 + x2x3 +\n",
      "x1x3. The form of the network that implements this function is shown in Fig.\n",
      "4.12. (We leave it to the reader to calculate appropriate values of weights and\n",
      "\n",
      "\n",
      "440. 48\n",
      "CHAPTER 4. NEURAL NETWORKS\n",
      "thresholds.) The 3-cube representation of the function is shown in Fig. 4.13.\n",
      "The network of Fig. 4.12 can be designed so that each hidden unit implements\n",
      "one of the planar boundaries shown in Fig. 4.13.\n",
      "x\n",
      "conjuncts\n",
      "disjunct\n",
      "A Feedforward, 2-layer Network\n",
      "TLUs\n",
      "disjunction\n",
      "of terms\n",
      "conjunctions\n",
      "of literals\n",
      "(terms)\n",
      "Figure 4.12: A Two-Layer Network\n",
      "x2\n",
      "x1\n",
      "x3\n",
      "f = x1x2 + x2x3 + x1x3\n",
      "Figure 4.13: Three Planes Implemented by the Hidden Units\n",
      "To train a two-layer network that implements a k-term DNF function, we\n",
      "ﬁrst note that the output unit implements a disjunction, so the weights in the\n",
      "ﬁnal layer are ﬁxed. The weights in the ﬁrst layer (except for the “threshold\n",
      "weights”) can all have values of 1, −1, or 0. Later, we will present a training\n",
      "procedure for this ﬁrst layer of weights.\n",
      "Discuss half-space intersections,\n",
      "half-space unions, NP-hardness of\n",
      "optimal versions,\n",
      "single-side-error-hypeplane\n",
      "methods, relation to “AQ”\n",
      "methods.\n",
      "\n",
      "\n",
      "441. 4.3. NETWORKS OF TLUS\n",
      "49\n",
      "Important Comment About Layered Networks\n",
      "Adding additional layers cannot compensate for an inadequate ﬁrst layer of\n",
      "TLUs. The ﬁrst layer of TLUs partitions the feature space so that no two dif-\n",
      "ferently labeled vectors are in the same region (that is, so that no two such\n",
      "vectors yield the same set of outputs of the ﬁrst-layer units). If the ﬁrst layer\n",
      "does not partition the feature space in this way, then regardless of what subse-\n",
      "quent layers do, the ﬁnal outputs will not be consistent with the labeled training\n",
      "set.\n",
      "Add diagrams showing the\n",
      "non-linear transformation\n",
      "performed by a layered network.\n",
      "4.3.2\n",
      "Madalines\n",
      "Two-Category Networks\n",
      "An interesting example of a layered, feedforward network is the two-layer one\n",
      "which has an odd number of hidden units, and a “vote-taking” TLU as the\n",
      "output unit. Such a network was called a “Madaline” (for many adalines by\n",
      "Widrow. Typically, the response of the vote taking unit is deﬁned to be the\n",
      "response of the majority of the hidden units, although other output logics are\n",
      "possible. Ridgway [Ridgway, 1962] proposed the following error-correction rule\n",
      "for adjusting the weights of the hidden units of a Madaline:\n",
      "• If the Madaline correctly classiﬁes a pattern, Xi, no corrections are made\n",
      "to any of the hidden units’ weight vectors,\n",
      "• If the Madaline incorrectly classiﬁes a pattern, Xi, then determine the\n",
      "minimum number of hidden units whose responses need to be changed\n",
      "(from 0 to 1 or from 1 to 0—depending on the type of error) in order that\n",
      "the Madaline would correctly classify Xi. Suppose that minimum number\n",
      "is ki. Of those hidden units voting incorrectly, change the weight vectors\n",
      "of those ki of them whose dot products are closest to 0 by using the error\n",
      "correction rule:\n",
      "W ←−W + ci(di −fi)Xi\n",
      "where di is the desired response of the hidden unit (0 or 1) and fi is the\n",
      "actual response (0 or 1). (We assume augmented vectors here even though\n",
      "we are using X, W notation.)\n",
      "That is, we perform error-correction on just enough hidden units to correct\n",
      "the vote to a majority voting correctly, and we change those that are easiest to\n",
      "change. There are example problems in which even though a set of weight values\n",
      "exists for a given Madaline structure such that it could classify all members of\n",
      "a training set correctly, this procedure will fail to ﬁnd them. Nevertheless, the\n",
      "procedure works eﬀectively in most experiments with it.\n",
      "We leave it to the reader to think about how this training procedure could\n",
      "be modiﬁed if the output TLU implemented an or function (or an and function).\n",
      "\n",
      "\n",
      "442. 50\n",
      "CHAPTER 4. NEURAL NETWORKS\n",
      "R-Category Madalines and Error-Correcting Output Codes\n",
      "If there are k hidden units (k > 1) in a two-layer network, their responses\n",
      "correspond to vertices of a k-dimensional hypercube. The ordinary two-category\n",
      "Madaline identiﬁes two special points in this space, namely the vertex consisting\n",
      "of k 1’s and the vertex consisting of k 0’s. The Madaline’s response is 1 if the\n",
      "point in “hidden-unit-space” is closer to the all 1’s vertex than it is to the all\n",
      "0’s vertex. We could design an R-category Madaline by identifying R vertices\n",
      "in hidden-unit space and then classifying a pattern according to which of these\n",
      "vertices the hidden-unit response is closest to. A machine using that idea was\n",
      "implemented in the early 1960s at SRI [Brain, et al., 1962]. It used the fact\n",
      "that the 2p so-called maximal-length shift-register sequences [Peterson, 1961, pp.\n",
      "147ﬀ] in a (2p −1)-dimensional Boolean space are mutually equidistant (for any\n",
      "integer p). For similar, more recent work see [Dietterich & Bakiri, 1991].\n",
      "4.3.3\n",
      "Piecewise Linear Machines\n",
      "A two-category training set is linearly separable if there exists a threshold func-\n",
      "tion that correctly classiﬁes all members of the training set. Similarly, we can\n",
      "say that an R-category training set is linearly separable if there exists a linear\n",
      "machine that correctly classiﬁes all members of the training set. When an R-\n",
      "category problem is not linearly separable, we need a more powerful classiﬁer.\n",
      "A candidate is a structure called a piecewise linear (PWL) machine illustrated\n",
      "in Fig. 4.14.\n",
      "X\n",
      "W1\n",
      "W1\n",
      ". . .\n",
      "Y\n",
      "Y\n",
      "MAX\n",
      ". . .\n",
      "Y\n",
      "Y\n",
      "MAX\n",
      ". . .\n",
      "WR\n",
      "WR\n",
      "ARG\n",
      "MAX\n",
      "1\n",
      "R\n",
      "1\n",
      "N1\n",
      "1\n",
      "NR\n",
      "Figure 4.14: A Piecewise Linear Machine\n",
      "\n",
      "\n",
      "443. 4.3. NETWORKS OF TLUS\n",
      "51\n",
      "The PWL machine groups its weighted summing units into R banks corre-\n",
      "sponding to the R categories. An input vector X is assigned to that category\n",
      "corresponding to the bank with the largest weighted sum. We can use an error-\n",
      "correction training algorithm similar to that used for a linear machine. If a\n",
      "pattern is classiﬁed incorrectly, we subtract (a constant times) the pattern vec-\n",
      "tor from the weight vector producing the largest dot product (it was incorrectly\n",
      "the largest) and add (a constant times) the pattern vector to that weight vector\n",
      "in the correct bank of weight vectors whose dot product is locally largest in\n",
      "that bank. (Again, we use augmented vectors here.) Unfortunately, there are\n",
      "example training sets that are separable by a given PWL machine structure\n",
      "but for which this error-correction training method fails to ﬁnd a solution. The\n",
      "method does appear to work well in some situations [Duda & Fossum, 1966], al-\n",
      "though [Nilsson, 1965, page 89] observed that “it is probably not a very eﬀective\n",
      "method for training PWL machines having more than three [weight vectors] in\n",
      "each bank.”\n",
      "4.3.4\n",
      "Cascade Networks\n",
      "Another interesting class of feedforward networks is that in which all of the TLUs\n",
      "are ordered and each TLU receives inputs from all of the pattern components\n",
      "and from all TLUs lower in the ordering. Such a network is called a cascade\n",
      "network. An example is shown in Fig. 4.15 in which the TLUs are labeled by\n",
      "the linearly separable functions (of their inputs) that they implement. Each\n",
      "TLU in the network implements a set of 2k parallel hyperplanes, where k is\n",
      "the number of TLUs from which it receives inputs. (Each of the k preceding\n",
      "TLUs can have an output of 1 or 0; that’s 2k diﬀerent combinations—resulting\n",
      "in 2k diﬀerent positions for the parallel hyperplanes.) We show a 3-dimensional\n",
      "sketch for a network of two TLUs in Fig. 4.16. The reader might consider how\n",
      "the n-dimensional parity function might be implemented by a cascade network\n",
      "having log2 n TLUs.\n",
      "x\n",
      "L1\n",
      "L2\n",
      "output\n",
      "L3\n",
      "Figure 4.15: A Cascade Network\n",
      "\n",
      "\n",
      "444. 52\n",
      "CHAPTER 4. NEURAL NETWORKS\n",
      "L1\n",
      "L2\n",
      "L2\n",
      "Figure 4.16: Planes Implemented by a Cascade Network with Two TLUs\n",
      "Cascade networks might be trained by ﬁrst training L1 to do as good a job\n",
      "as possible at separating all the training patterns (perhaps by using the pocket\n",
      "algorithm, for example), then training L2 (including the weight from L1 to L2)\n",
      "also to do as good a job as possible at separating all the training patterns,\n",
      "and so on until the resulting network classiﬁes the patterns in the training set\n",
      "satisfactorily.\n",
      "Also mention the\n",
      "“cascade-correlation” method of\n",
      "[Fahlman & Lebiere, 1990].\n",
      "4.4\n",
      "Training Feedforward Networks by Back-\n",
      "propagation\n",
      "4.4.1\n",
      "Notation\n",
      "The general problem of training a network of TLUs is diﬃcult. Consider, for\n",
      "example, the layered, feedforward network of Fig. 4.11. If such a network makes\n",
      "an error on a pattern, there are usually several diﬀerent ways in which the error\n",
      "can be corrected. It is diﬃcult to assign “blame” for the error to any particular\n",
      "TLU in the network. Intuitively, one looks for weight-adjusting procedures that\n",
      "move the network in the correct direction (relative to the error) by making\n",
      "minimal changes. In this spirit, the Widrow-Hoﬀmethod of gradient descent\n",
      "has been generalized to deal with multilayer networks.\n",
      "In explaining this generalization, we use Fig. 4.17 to introduce some nota-\n",
      "tion. This network has only one output unit, but, of course, it is possible to have\n",
      "several TLUs in the output layer—each implementing a diﬀerent function. Each\n",
      "of the layers of TLUs will have outputs that we take to be the components of\n",
      "vectors, just as the input features are components of an input vector. The j-th\n",
      "layer of TLUs (1 ≤j < k) will have as their outputs the vector X(j). The input\n",
      "feature vector is denoted by X(0), and the ﬁnal output (of the k-th layer TLU)\n",
      "is f. Each TLU in each layer has a weight vector (connecting it to its inputs)\n",
      "and a threshold; the i-th TLU in the j-th layer has a weight vector denoted by\n",
      "W(j)\n",
      "i . (We will assume that the “threshold weight” is the last component of\n",
      "the associated weight vector; we might have used V notation instead to include\n",
      "\n",
      "\n",
      "445. 4.4. TRAINING FEEDFORWARD NETWORKS BY BACKPROPAGATION53\n",
      "this threshold component, but we have chosen here to use the familiar X,W\n",
      "notation, assuming that these vectors are “augmented” as appropriate.) We\n",
      "denote the weighted sum input to the i-th threshold unit in the j-th layer by\n",
      "s(j)\n",
      "i . (That is, s(j)\n",
      "i\n",
      "= X(j−1)•W(j)\n",
      "i .) The number of TLUs in the j-th layer is\n",
      "given by mj. The vector W(j)\n",
      "i\n",
      "has components w(j)\n",
      "l,i for l = 1, . . . , m(j−1) + 1.\n",
      "X(0)\n",
      ". . .\n",
      ". . .\n",
      ". . .\n",
      ". . .\n",
      "Wi(1)\n",
      "W(k)\n",
      "X(1)\n",
      "m1 TLUs\n",
      ". . .\n",
      "Wi(j)\n",
      ". . .\n",
      "X(j)\n",
      ". . .\n",
      "Wi(k-1)\n",
      "X(k-1)\n",
      "mj TLUs\n",
      "m(k-1) TLUs\n",
      "wli(j)\n",
      "wl(k)\n",
      "First Layer\n",
      "j-th Layer\n",
      "(k-1)-th Layer\n",
      "k-th Layer\n",
      ". . .\n",
      "f\n",
      "si(1)\n",
      "si(j)\n",
      "si(k-1)\n",
      "s(k)\n",
      "Figure 4.17: A k-layer Network\n",
      "4.4.2\n",
      "The Backpropagation Method\n",
      "A gradient descent method, similar to that used in the Widrow Hoﬀmethod,\n",
      "has been proposed by various authors for training a multi-layer, feedforward\n",
      "network.\n",
      "As before, we deﬁne an error function on the ﬁnal output of the\n",
      "network and we adjust each weight in the network so as to minimize the error.\n",
      "If we have a desired response, di, for the i-th input vector, Xi, in the training\n",
      "set, Ξ, we can compute the squared error over the entire training set to be:\n",
      "ε =\n",
      "X\n",
      "Xi ϵ Ξ\n",
      "(di −fi)2\n",
      "where fi is the actual response of the network for input Xi. To do gradient\n",
      "descent on this squared error, we adjust each weight in the network by an\n",
      "amount proportional to the negative of the partial derivative of ε with respect\n",
      "to that weight. Again, we use a single-pattern error function so that we can\n",
      "use an incremental weight adjustment procedure. The squared error for a single\n",
      "input vector, X, evoking an output of f when the desired output is d is:\n",
      "\n",
      "\n",
      "446. 54\n",
      "CHAPTER 4. NEURAL NETWORKS\n",
      "ε = (d −f)2\n",
      "It is convenient to take the partial derivatives of ε with respect to the various\n",
      "weights in groups corresponding to the weight vectors.\n",
      "We deﬁne a partial\n",
      "derivative of a quantity φ, say, with respect to a weight vector, W(j)\n",
      "i , thus:\n",
      "∂φ\n",
      "∂W(j)\n",
      "i\n",
      "def\n",
      "=\n",
      "\"\n",
      "∂φ\n",
      "∂w(j)\n",
      "1i\n",
      ", . . . ,\n",
      "∂φ\n",
      "∂w(j)\n",
      "li\n",
      ", . . . ,\n",
      "∂φ\n",
      "∂w(j)\n",
      "mj−1+1,i\n",
      "#\n",
      "where w(j)\n",
      "li\n",
      "is the l-th component of W(j)\n",
      "i . This vector partial derivative of φ is\n",
      "called the gradient of φ with respect to W and is sometimes denoted by ∇Wφ.\n",
      "Since ε’s dependence on W(j)\n",
      "i\n",
      "is entirely through s(j)\n",
      "i , we can use the chain\n",
      "rule to write:\n",
      "∂ε\n",
      "∂W(j)\n",
      "i\n",
      "=\n",
      "∂ε\n",
      "∂s(j)\n",
      "i\n",
      "∂s(j)\n",
      "i\n",
      "∂W(j)\n",
      "i\n",
      "Because s(j)\n",
      "i\n",
      "= X(j−1)•W(j)\n",
      "i ,\n",
      "∂s(j)\n",
      "i\n",
      "∂W(j)\n",
      "i\n",
      "= X(j−1). Substituting yields:\n",
      "∂ε\n",
      "∂W(j)\n",
      "i\n",
      "=\n",
      "∂ε\n",
      "∂s(j)\n",
      "i\n",
      "X(j−1)\n",
      "Note that\n",
      "∂ε\n",
      "∂s(j)\n",
      "i\n",
      "= −2(d −f) ∂f\n",
      "∂s(j)\n",
      "i\n",
      ". Thus,\n",
      "∂ε\n",
      "∂W(j)\n",
      "i\n",
      "= −2(d −f) ∂f\n",
      "∂s(j)\n",
      "i\n",
      "X(j−1)\n",
      "The quantity (d−f) ∂f\n",
      "∂s(j)\n",
      "i\n",
      "plays an important role in our calculations; we shall\n",
      "denote it by δ(j)\n",
      "i\n",
      ". Each of the δ(j)\n",
      "i\n",
      "’s tells us how sensitive the squared error of\n",
      "the network output is to changes in the input to each threshold function. Since\n",
      "we will be changing weight vectors in directions along their negative gradient,\n",
      "our fundamental rule for weight changes throughout the network will be:\n",
      "W(j)\n",
      "i\n",
      "←W(j)\n",
      "i\n",
      "+ c(j)\n",
      "i δ(j)\n",
      "i\n",
      "X(j−1)\n",
      "where c(j)\n",
      "i\n",
      "is the learning rate constant for this weight vector. (Usually, the\n",
      "learning rate constants for all weight vectors in the network are the same.) We\n",
      "see that this rule is quite similar to that used in the error correction procedure\n",
      "\n",
      "\n",
      "447. 4.4. TRAINING FEEDFORWARD NETWORKS BY BACKPROPAGATION55\n",
      "for a single TLU. A weight vector is changed by the addition of a constant times\n",
      "its vector of (unweighted) inputs.\n",
      "Now, we must turn our attention to the calculation of the δ(j)\n",
      "i\n",
      "’s. Using the\n",
      "deﬁnition, we have:\n",
      "δ(j)\n",
      "i\n",
      "= (d −f) ∂f\n",
      "∂s(j)\n",
      "i\n",
      "We have a problem, however, in attempting to carry out the partial deriva-\n",
      "tives of f with respect to the s’s. The network output, f, is not continuously\n",
      "diﬀerentiable with respect to the s’s because of the presence of the threshold\n",
      "functions. Most small changes in these sums do not change f at all, and when\n",
      "f does change, it changes abruptly from 1 to 0 or vice versa.\n",
      "A way around this diﬃculty was proposed by Werbos [Werbos, 1974] and\n",
      "(perhaps independently) pursued by several other researchers, for example\n",
      "[Rumelhart, Hinton, & Williams, 1986].\n",
      "The trick involves replacing all the\n",
      "threshold functions by diﬀerentiable functions called sigmoids.1\n",
      "The output\n",
      "of a sigmoid function, superimposed on that of a threshold function, is shown\n",
      "in Fig. 4.18. Usually, the sigmoid function used is f(s) =\n",
      "1\n",
      "1+e−s , where s is\n",
      "the input and f is the output.\n",
      "sigmoid\n",
      "threshold function\n",
      "f (s)\n",
      "s\n",
      "f (s) = 1/[1 + e<s]\n",
      "Figure 4.18: A Sigmoid Function\n",
      "1[Russell & Norvig 1995, page 595] attributes the use of this idea to [Bryson & Ho 1969].\n",
      "\n",
      "\n",
      "448. 56\n",
      "CHAPTER 4. NEURAL NETWORKS\n",
      "We show the network containing sigmoid units in place of TLUs in Fig. 4.19.\n",
      "The output of the i-th sigmoid unit in the j-th layer is denoted by f (j)\n",
      "i\n",
      ". (That\n",
      "is, f (j)\n",
      "i\n",
      "=\n",
      "1\n",
      "1+e−s(j)\n",
      "i\n",
      ".)\n",
      "X(0)\n",
      ". . .\n",
      ". . .\n",
      ". . .\n",
      ". . .\n",
      "Wi(1)\n",
      "si(1)\n",
      "W(k)\n",
      "X(1)\n",
      "fi(1)\n",
      "m1 sigmoids\n",
      ". . .\n",
      "Wi(j) fi(j)\n",
      "si(j)\n",
      ". . .\n",
      "X(j)\n",
      ". . .\n",
      "Wi(k-1)\n",
      "fi(k-1)\n",
      "si(k-1)\n",
      "f(k)\n",
      "s(k)\n",
      "X(k-1)\n",
      "mj sigmoids\n",
      "m(k-1) sigmoids\n",
      "wli(j)\n",
      "wl(k)\n",
      "bi(j)\n",
      "bi(1)\n",
      "bi(k-1)\n",
      "b(k)\n",
      "First Layer\n",
      "j-th Layer\n",
      "(k-1)-th Layer\n",
      "k-th Layer\n",
      ". . .\n",
      "Figure 4.19: A Network with Sigmoid Units\n",
      "4.4.3\n",
      "Computing Weight Changes in the Final Layer\n",
      "We ﬁrst calculate δ(k) in order to compute the weight change for the ﬁnal sigmoid\n",
      "unit:\n",
      "\n",
      "\n",
      "449. 4.4. TRAINING FEEDFORWARD NETWORKS BY BACKPROPAGATION57\n",
      "δ(k) = (d −f (k))∂f (k)\n",
      "∂s(k)\n",
      "Given the sigmoid function that we are using, namely f(s) =\n",
      "1\n",
      "1+e−s , we have\n",
      "that ∂f\n",
      "∂s = f(1 −f). Substituting gives us:\n",
      "δ(k) = (d −f (k))f (k)(1 −f (k))\n",
      "Rewriting our general rule for weight vector changes, the weight vector in\n",
      "the ﬁnal layer is changed according to the rule:\n",
      "W(k) ←W(k) + c(k)δ(k)X(k−1)\n",
      "where δ(k) = (d −f (k))f (k)(1 −f (k))\n",
      "It is interesting to compare backpropagation to the error-correction rule and\n",
      "to the Widrow-Hoﬀrule. The backpropagation weight adjustment for the single\n",
      "element in the ﬁnal layer can be written as:\n",
      "W ←−W + c(d −f)f(1 −f)X\n",
      "Written in the same format, the error-correction rule is:\n",
      "W ←−W + c(d −f)X\n",
      "and the Widrow-Hoﬀrule is:\n",
      "W ←−W + c(d −f)X\n",
      "The only diﬀerence (except for the fact that f is not thresholded in Widrow-\n",
      "Hoﬀ) is the f(1 −f) term due to the presence of the sigmoid function. With\n",
      "the sigmoid function, f(1 −f) can vary in value from 0 to 1. When f is 0,\n",
      "f(1 −f) is also 0; when f is 1, f(1 −f) is 0; f(1 −f) obtains its maximum\n",
      "value of 1/4 when f is 1/2 (that is, when the input to the sigmoid is 0). The\n",
      "sigmoid function can be thought of as implementing a “fuzzy” hyperplane. For\n",
      "a pattern far away from this fuzzy hyperplane, f(1 −f) has value close to 0,\n",
      "and the backpropagation rule makes little or no change to the weight values\n",
      "regardless of the desired output. (Small changes in the weights will have little\n",
      "eﬀect on the output for inputs far from the hyperplane.) Weight changes are\n",
      "only made within the region of “fuzz” surrounding the hyperplane, and these\n",
      "changes are in the direction of correcting the error, just as in the error-correction\n",
      "and Widrow-Hoﬀrules.\n",
      "\n",
      "\n",
      "450. 58\n",
      "CHAPTER 4. NEURAL NETWORKS\n",
      "4.4.4\n",
      "Computing Changes to the Weights in Intermediate\n",
      "Layers\n",
      "Using our expression for the δ’s, we can similarly compute how to change each\n",
      "of the weight vectors in the network. Recall:\n",
      "δ(j)\n",
      "i\n",
      "= (d −f) ∂f\n",
      "∂s(j)\n",
      "i\n",
      "Again we use a chain rule. The ﬁnal output, f, depends on s(j)\n",
      "i\n",
      "through\n",
      "each of the summed inputs to the sigmoids in the (j + 1)-th layer. So:\n",
      "δ(j)\n",
      "i\n",
      "= (d −f) ∂f\n",
      "∂s(j)\n",
      "i\n",
      "= (d −f)\n",
      "\"\n",
      "∂f\n",
      "∂s(j+1)\n",
      "1\n",
      "∂s(j+1)\n",
      "1\n",
      "∂s(j)\n",
      "i\n",
      "+ · · · +\n",
      "∂f\n",
      "∂s(j+1)\n",
      "l\n",
      "∂s(j+1)\n",
      "l\n",
      "∂s(j)\n",
      "i\n",
      "+ · · · +\n",
      "∂f\n",
      "∂s(j+1)\n",
      "mj+1\n",
      "∂s(j+1)\n",
      "mj+1\n",
      "∂s(j)\n",
      "i\n",
      "#\n",
      "=\n",
      "mj+1\n",
      "X\n",
      "l=1\n",
      "(d −f)\n",
      "∂f\n",
      "∂s(j+1)\n",
      "l\n",
      "∂s(j+1)\n",
      "l\n",
      "∂s(j)\n",
      "i\n",
      "=\n",
      "mj+1\n",
      "X\n",
      "l=1\n",
      "δ(j+1)\n",
      "l\n",
      "∂s(j+1)\n",
      "l\n",
      "∂s(j)\n",
      "i\n",
      "It remains to compute the\n",
      "∂s(j+1)\n",
      "l\n",
      "∂s(j)\n",
      "i\n",
      "’s. To do that we ﬁrst write:\n",
      "s(j+1)\n",
      "l\n",
      "= X(j)•W(j+1)\n",
      "l\n",
      "=\n",
      "mj+1\n",
      "X\n",
      "ν=1\n",
      "f (j)\n",
      "ν w(j+1)\n",
      "νl\n",
      "And then, since the weights do not depend on the s’s:\n",
      "∂s(j+1)\n",
      "l\n",
      "∂s(j)\n",
      "i\n",
      "=\n",
      "∂\n",
      "hPmj+1\n",
      "ν=1\n",
      "f (j)\n",
      "ν w(j+1)\n",
      "νl\n",
      "i\n",
      "∂s(j)\n",
      "i\n",
      "=\n",
      "mj+1\n",
      "X\n",
      "ν=1\n",
      "w(j+1)\n",
      "νl\n",
      "∂f (j)\n",
      "ν\n",
      "∂s(j)\n",
      "i\n",
      "Now, we note that ∂f (j)\n",
      "ν\n",
      "∂s(j)\n",
      "i\n",
      "= 0 unless ν = i, in which case ∂f (j)\n",
      "ν\n",
      "∂s(j)\n",
      "ν\n",
      "= f (j)\n",
      "ν (1 −f (j)\n",
      "ν ).\n",
      "Therefore:\n",
      "∂s(j+1)\n",
      "l\n",
      "∂s(j)\n",
      "i\n",
      "= w(j+1)\n",
      "il\n",
      "f (j)\n",
      "i\n",
      "(1 −f (j)\n",
      "i\n",
      ")\n",
      "\n",
      "\n",
      "451. 4.4. TRAINING FEEDFORWARD NETWORKS BY BACKPROPAGATION59\n",
      "We use this result in our expression for δ(j)\n",
      "i\n",
      "to give:\n",
      "δ(j)\n",
      "i\n",
      "= f (j)\n",
      "i\n",
      "(1 −f (j)\n",
      "i\n",
      ")\n",
      "mj+1\n",
      "X\n",
      "l=1\n",
      "δ(j+1)\n",
      "l\n",
      "w(j+1)\n",
      "il\n",
      "The above equation is recursive in the δ’s. (It is interesting to note that\n",
      "this expression is independent of the error function; the error function explicitly\n",
      "aﬀects only the computation of δ(k).) Having computed the δ(j+1)\n",
      "i\n",
      "’s for layer\n",
      "j + 1, we can use this equation to compute the δ(j)\n",
      "i\n",
      "’s. The base case is δ(k),\n",
      "which we have already computed:\n",
      "δ(k) = (d −f (k))f (k)(1 −f (k))\n",
      "We use this expression for the δ’s in our generic weight changing rule, namely:\n",
      "W(j)\n",
      "i\n",
      "←W(j)\n",
      "i\n",
      "+ c(j)\n",
      "i δ(j)\n",
      "i\n",
      "X(j−1)\n",
      "Although this rule appears complex, it has an intuitively reasonable explanation.\n",
      "The quantity δ(k) = (d −f)f(1 −f) controls the overall amount and sign of all\n",
      "weight adjustments in the network. (Adjustments diminish as the ﬁnal output,\n",
      "f, approaches either 0 or 1, because they have vanishing eﬀect on f then.) As\n",
      "the recursion equation for the δ’s shows, the adjustments for the weights going\n",
      "in to a sigmoid unit in the j-th layer are proportional to the eﬀect that such\n",
      "adjustments have on that sigmoid unit’s output (its f (j)(1−f (j)) factor). They\n",
      "are also proportional to a kind of “average” eﬀect that any change in the output\n",
      "of that sigmoid unit will have on the ﬁnal output. This average eﬀect depends\n",
      "on the weights going out of the sigmoid unit in the j-th layer (small weights\n",
      "produce little downstream eﬀect) and the eﬀects that changes in the outputs of\n",
      "(j + 1)-th layer sigmoid units will have on the ﬁnal output (as measured by the\n",
      "δ(j+1)’s). These calculations can be simply implemented by “backpropagating”\n",
      "the δ’s through the weights in reverse direction (thus, the name backprop for\n",
      "this algorithm).\n",
      "4.4.5\n",
      "Variations on Backprop\n",
      "[To be written: problem of local minima, simulated annealing, momemtum\n",
      "(Plaut, et al., 1986, see [Hertz, Krogh, & Palmer, 1991]), quickprop, regulariza-\n",
      "tion methods]\n",
      "Simulated Annealing\n",
      "To apply simulated annealing, the value of the learning rate constant is gradually\n",
      "decreased with time. If we fall early into an error-function valley that is not\n",
      "very deep (a local minimum), it typically will neither be very broad, and soon\n",
      "\n",
      "\n",
      "452. 60\n",
      "CHAPTER 4. NEURAL NETWORKS\n",
      "a subsequent large correction will jostle us out of it. It is less likely that we will\n",
      "move out of deep valleys, and at the end of the process (with very small values\n",
      "of the learning rate constant), we descend to its deepest point. The process\n",
      "gets its name by analogy with annealing in metallurgy, in which a material’s\n",
      "temperature is gradually decreased allowing its crystalline structure to reach a\n",
      "minimal energy state.\n",
      "4.4.6\n",
      "An Application: Steering a Van\n",
      "A neural network system called ALVINN (Autonomous Land Vehicle in a Neural\n",
      "Network) has been trained to steer a Chevy van successfully on ordinary roads\n",
      "and highways at speeds of 55 mph [Pomerleau, 1991, Pomerleau, 1993]. The\n",
      "input to the network is derived from a low-resolution (30 x 32) television image.\n",
      "The TV camera is mounted on the van and looks at the road straight ahead.\n",
      "This image is sampled and produces a stream of 960-dimensional input vectors\n",
      "to the neural network. The network is shown in Fig. 4.20.\n",
      "960 inputs\n",
      "30 x 32 retina\n",
      ". . .\n",
      "5 hidden\n",
      "units connected\n",
      "to all 960 inputs\n",
      "30 output units\n",
      "connected to all\n",
      "hidden units\n",
      ". . .\n",
      "sharp left\n",
      "sharp right\n",
      "straight ahead\n",
      "centroid\n",
      "of outputs\n",
      "steers\n",
      "vehicle\n",
      "Figure 4.20: The ALVINN Network\n",
      "The network has ﬁve hidden units in its ﬁrst layer and 30 output units in the\n",
      "second layer; all are sigmoid units. The output units are arranged in a linear\n",
      "order and control the van’s steering angle. If a unit near the top of the array\n",
      "of output units has a higher output than most of the other units, the van is\n",
      "steered to the left; if a unit near the bottom of the array has a high output, the\n",
      "van is steered to the right. The “centroid” of the responses of all of the output\n",
      "\n",
      "\n",
      "453. 4.5. SYNERGIES BETWEEN NEURAL NETWORK AND KNOWLEDGE-BASED METHODS61\n",
      "units is computed, and the van’s steering angle is set at a corresponding value\n",
      "between hard left and hard right.\n",
      "The system is trained by a modiﬁed on-line training regime. A driver drives\n",
      "the van, and his actual steering angles are taken as the correct labels for the\n",
      "corresponding inputs.\n",
      "The network is trained incrementally by backprop to\n",
      "produce the driver-speciﬁed steering angles in response to each visual pattern\n",
      "as it occurs in real time while driving.\n",
      "This simple procedure has been augmented to avoid two potential problems.\n",
      "First, since the driver is usually driving well, the network would never get any\n",
      "experience with far-from-center vehicle positions and/or incorrect vehicle orien-\n",
      "tations. Also, on long, straight stretches of road, the network would be trained\n",
      "for a long time only to produce straight-ahead steering angles; this training\n",
      "would swamp out earlier training to follow a curved road. We wouldn’t want\n",
      "to try to avoid these problems by instructing the driver to drive erratically\n",
      "occasionally, because the system would learn to mimic this erratic behavior.\n",
      "Instead, each original image is shifted and rotated in software to create 14\n",
      "additional images in which the vehicle appears to be situated diﬀerently relative\n",
      "to the road. Using a model that tells the system what steering angle ought to\n",
      "be used for each of these shifted images, given the driver-speciﬁed steering angle\n",
      "for the original image, the system constructs an additional 14 labeled training\n",
      "patterns to add to those encountered during ordinary driver training.\n",
      "4.5\n",
      "Synergies\n",
      "Between\n",
      "Neural\n",
      "Network\n",
      "and\n",
      "Knowledge-Based Methods\n",
      "To be written; discuss\n",
      "rule-generating procedures (such as\n",
      "[Towell & Shavlik, 1992]) and how\n",
      "expert-provided rules can aid\n",
      "neural net training and vice-versa\n",
      "[Towell, Shavlik, & Noordweier, 1990].\n",
      "4.6\n",
      "Bibliographical and Historical Remarks\n",
      "To be added.\n",
      "\n",
      "\n",
      "454. 62\n",
      "CHAPTER 4. NEURAL NETWORKS\n",
      "\n",
      "\n",
      "455. Chapter 5\n",
      "Statistical Learning\n",
      "5.1\n",
      "Using Statistical Decision Theory\n",
      "5.1.1\n",
      "Background and General Method\n",
      "Suppose the pattern vector, X, is a random variable whose probability distri-\n",
      "bution for category 1 is diﬀerent than it is for category 2. (The treatment given\n",
      "here can easily be generalized to R-category problems.) Speciﬁcally, suppose we\n",
      "have the two probability distributions (perhaps probability density functions),\n",
      "p(X | 1) and p(X | 2). Given a pattern, X, we want to use statistical tech-\n",
      "niques to determine its category—that is, to determine from which distribution\n",
      "it was drawn. These techniques are based on the idea of minimizing the ex-\n",
      "pected value of a quantity similar to the error function we used in deriving the\n",
      "weight-changing rules for backprop.\n",
      "In developing a decision method, it is necessary to know the relative serious-\n",
      "ness of the two kinds of mistakes that might be made. (We might decide that a\n",
      "pattern really in category 1 is in category 2, and vice versa.) We describe this\n",
      "information by a loss function, λ(i | j), for i, j = 1, 2. λ(i | j) represents the loss\n",
      "incurred when we decide a pattern is in category i when really it is in category\n",
      "j. We assume here that λ(1 | 1) and λ(2 | 2) are both 0. For any given pattern,\n",
      "X, we want to decide its category in such a way that minimizes the expected\n",
      "value of this loss.\n",
      "Given a pattern, X, if we decide category i, the expected value of the loss\n",
      "will be:\n",
      "LX(i) = λ(i | 1)p(1 | X) + λ(i | 2)p(2 | X)\n",
      "where p(j | X) is the probability that given a pattern X, its category is j. Our\n",
      "decision rule will be to decide that X belongs to category 1 if LX(1) ≤LX(2),\n",
      "and to decide on category 2 otherwise.\n",
      "63\n",
      "\n",
      "\n",
      "456. 64\n",
      "CHAPTER 5. STATISTICAL LEARNING\n",
      "We can use Bayes’ Rule to get expressions for p(j | X) in terms of p(X | j),\n",
      "which we assume to be known (or estimatible):\n",
      "p(j | X) = p(X | j)p(j)\n",
      "p(X)\n",
      "where p(j) is the (a priori) probability of category j (one category may be much\n",
      "more probable than the other); and p(X) is the (a priori) probability of pattern\n",
      "X being the pattern we are asked to classify. Performing the substitutions given\n",
      "by Bayes’ Rule, our decision rule becomes:\n",
      "Decide category 1 iﬀ:\n",
      "λ(1 | 1)p(X | 1)p(1)\n",
      "p(X)\n",
      "+ λ(1 | 2)p(X | 2)p(2)\n",
      "p(X)\n",
      "≤λ(2 | 1)p(X | 1)p(1)\n",
      "p(X)\n",
      "+ λ(2 | 2)p(X | 2)p(2)\n",
      "p(X)\n",
      "Using the fact that λ(i | i) = 0, and noticing that p(X) is common to both\n",
      "expressions, we obtain,\n",
      "Decide category 1 iﬀ:\n",
      "λ(1 | 2)p(X | 2)p(2) ≤λ(2 | 1)p(X | 1)p(1)\n",
      "If λ(1 | 2) = λ(2 | 1) and if p(1) = p(2), then the decision becomes particu-\n",
      "larly simple:\n",
      "Decide category 1 iﬀ:\n",
      "p(X | 2) ≤p(X | 1)\n",
      "Since p(X | j) is called the likelihood of j with respect to X, this simple decision\n",
      "rule implements what is called a maximum-likelihood decision. More generally,\n",
      "if we deﬁne k(i | j) as λ(i | j)p(j), then our decision rule is simply,\n",
      "Decide category1 iﬀ:\n",
      "k(1 | 2)p(X | 2) ≤k(2 | 1)p(X | 1)\n",
      "In any case, we need to compare the (perhaps weighted) quantities p(X | i) for\n",
      "i = 1 and 2. The exact decision rule depends on the the probability distributions\n",
      "assumed. We will treat two interesting distributions.\n",
      "\n",
      "\n",
      "457. 5.1. USING STATISTICAL DECISION THEORY\n",
      "65\n",
      "5.1.2\n",
      "Gaussian (or Normal) Distributions\n",
      "The multivariate (n-dimensional) Gaussian distribution is given by the proba-\n",
      "bility density function:\n",
      "p(X) =\n",
      "1\n",
      "(2π)n/2|Σ|1/2 e\n",
      "−(X−M)tΣ\n",
      "−1\n",
      "(X−M)\n",
      "2\n",
      "where n is the dimension of the column vector X, the column vector M is called\n",
      "the mean vector, (X −M)t is the transpose of the vector (X −M), Σ is the\n",
      "covariance matrix of the distribution (an n × n symmetric, positive deﬁnite\n",
      "matrix), Σ−1 is the inverse of the covariance matrix, and |Σ| is the determinant\n",
      "of the covariance matrix.\n",
      "The mean vector, M, with components (m1, . . . , mn), is the expected value\n",
      "of X (using this distribution); that is, M = E[X].\n",
      "The components of the\n",
      "covariance matrix are given by:\n",
      "σ2\n",
      "ij = E[(xi −mi)(xj −mj)]\n",
      "In particular, σ2\n",
      "ii is called the variance of xi.\n",
      "Although the formula appears complex, an intuitive idea for Gaussian dis-\n",
      "tributions can be given when n = 2.\n",
      "We show a two-dimensional Gaussian\n",
      "distribution in Fig. 5.1. A three-dimensional plot of the distribution is shown\n",
      "at the top of the ﬁgure, and contours of equal probability are shown at the bot-\n",
      "tom. In this case, the covariance matrix, Σ, is such that the elliptical contours\n",
      "of equal probability are skewed. If the covariance matrix were diagonal, that is\n",
      "if all oﬀ-diagonal terms were 0, then the major axes of the elliptical contours\n",
      "would be aligned with the coordinate axes. In general the principal axes are\n",
      "given by the eigenvectors of Σ. In any case, the equi-probability contours are\n",
      "all centered on the mean vector, M, which in our ﬁgure happens to be at the\n",
      "origin. In general, the formula in the exponent in the Gaussian distribution\n",
      "is a positive deﬁnite quadratic form (that is, its value is always positive); thus\n",
      "equi-probability contours are hyper-ellipsoids in n-dimensional space.\n",
      "Suppose we now assume that the two classes of pattern vectors that we\n",
      "want to distinguish are each distributed according to a Gaussian distribution\n",
      "but with diﬀerent means and covariance matrices. That is, one class tends to\n",
      "have patterns clustered around one point in the n-dimensional space, and the\n",
      "other class tends to have patterns clustered around another point. We show a\n",
      "two-dimensional instance of this problem in Fig. 5.2. (In that ﬁgure, we have\n",
      "plotted the sum of the two distributions.) What decision rule should we use to\n",
      "separate patterns into the two appropriate categories?\n",
      "Substituting the Gaussian distributions into our maximum likelihood for-\n",
      "mula yields:\n",
      "\n",
      "\n",
      "458. 66\n",
      "CHAPTER 5. STATISTICAL LEARNING\n",
      "-5\n",
      "0\n",
      "5\n",
      "-5\n",
      "0\n",
      "5\n",
      "0\n",
      "0.25\n",
      "0.5\n",
      "0.75\n",
      "1\n",
      "-5\n",
      "0\n",
      "5\n",
      "-5\n",
      "0\n",
      "5\n",
      "0\n",
      "25\n",
      ".5\n",
      "75\n",
      "1\n",
      "-6\n",
      "-4\n",
      "-2\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "-6\n",
      "-4\n",
      "-2\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "x1\n",
      "x2\n",
      "p(x1,x2)\n",
      "2\n",
      "4\n",
      "6\n",
      "2\n",
      "4\n",
      "6\n",
      "x1\n",
      "x2\n",
      "Figure 5.1: The Two-Dimensional Gaussian Distribution\n",
      "Decide category 1 iﬀ:\n",
      "1\n",
      "(2π)n/2|Σ2|1/2 e−1/2(X−M2)tΣ\n",
      "−1\n",
      "2\n",
      "(X−M2)\n",
      "is less than or equal to\n",
      "1\n",
      "(2π)n/2|Σ1|1/2 e−1/2(X−M1)tΣ\n",
      "−1\n",
      "1\n",
      "(X−M1)\n",
      "where the category 1 patterns are distributed with mean and covariance M1\n",
      "and Σ1, respectively, and the category 2 patterns are distributed with mean\n",
      "and covariance M2 and Σ2.\n",
      "The result of the comparison isn’t changed if we compare logarithms instead.\n",
      "After some manipulation, our decision rule is then:\n",
      "\n",
      "\n",
      "459. 5.1. USING STATISTICAL DECISION THEORY\n",
      "67\n",
      "-5\n",
      "0\n",
      "5\n",
      "10\n",
      "-5\n",
      "0\n",
      "5\n",
      "10\n",
      "0\n",
      "0.25\n",
      "0.5\n",
      "0.75\n",
      "1\n",
      "-5\n",
      "0\n",
      "5\n",
      "10\n",
      "-5\n",
      "0\n",
      "5\n",
      "10\n",
      "0\n",
      "25\n",
      ".5\n",
      "75\n",
      "1\n",
      "x1\n",
      "x2\n",
      "p(x1,x2)\n",
      "-5\n",
      "-2.5\n",
      "0\n",
      "2.5\n",
      "5\n",
      "7.5\n",
      "10\n",
      "-5\n",
      "-2.5\n",
      "0\n",
      "2.5\n",
      "5\n",
      "7.5\n",
      "10\n",
      "Figure 5.2: The Sum of Two Gaussian Distributions\n",
      "Decide category 1 iﬀ:\n",
      "(X −M1)tΣ−1\n",
      "1 (X −M1) < (X −M2)tΣ−1\n",
      "2 (X −M2) + B\n",
      "where B, a constant bias term, incorporates the logarithms of the fractions\n",
      "preceding the exponential, etc.\n",
      "When the quadratic forms are multiplied out and represented in terms of\n",
      "the components xi, the decision rule involves a quadric surface (a hyperquadric)\n",
      "in n-dimensional space. The exact shape and position of this hyperquadric is\n",
      "determined by the means and the covariance matrices. The surface separates\n",
      "the space into two parts, one of which contains points that will be assigned to\n",
      "category 1 and the other contains points that will be assigned to category 2.\n",
      "It is interesting to look at a special case of this surface. If the covariance\n",
      "matrices for each category are identical and diagonal, with all σii equal to each\n",
      "other, then the contours of equal probability for each of the two distributions\n",
      "\n",
      "\n",
      "460. 68\n",
      "CHAPTER 5. STATISTICAL LEARNING\n",
      "are hyperspherical. The quadric forms then become (1/|Σ|)(X−Mi)t(X−Mi),\n",
      "and the decision rule is:\n",
      "Decide category 1 iﬀ:\n",
      "(X −M1)t(X −M1) < (X −M2)t(X −M2)\n",
      "Multiplying out yields:\n",
      "X•X −2X•M1 + M1•M1 < X•X −2X•M2 + M2•M2\n",
      "or ﬁnally,\n",
      "Decide category 1 iﬀ:\n",
      "X•M1 ≥X•M2 + Constant\n",
      "or\n",
      "X•(M1 −M2) ≥Constant\n",
      "where the constant depends on the lengths of the mean vectors.\n",
      "We see that the optimal decision surface in this special case is a hyperplane.\n",
      "In fact, the hyperplane is perpendicular to the line joining the two means. The\n",
      "weights in a TLU implementation are equal to the diﬀerence in the mean vectors.\n",
      "If the parameters (Mi, Σi) of the probability distributions of the categories\n",
      "are not known, there are various techniques for estimating them, and then using\n",
      "those estimates in the decision rule. For example, if there are suﬃcient training\n",
      "patterns, one can use sample means and sample covariance matrices. (Caution:\n",
      "the sample covariance matrix will be singular if the training patterns happen to\n",
      "lie on a subspace of the whole n-dimensional space—as they certainly will, for\n",
      "example, if the number of training patterns is less than n.)\n",
      "5.1.3\n",
      "Conditionally Independent Binary Components\n",
      "Suppose the vector X is a random variable having binary (0,1) components.\n",
      "We continue to denote the two probability distributions by p(X | 1) and p(X |\n",
      "2).\n",
      "Further suppose that the components of these vectors are conditionally\n",
      "independent given the category. By conditional independence in this case, we\n",
      "mean that the formulas for the distribution can be expanded as follows:\n",
      "\n",
      "\n",
      "461. 5.1. USING STATISTICAL DECISION THEORY\n",
      "69\n",
      "p(X | i) = p(x1 | i)p(x2 | i) · · · p(xn | i)\n",
      "for i = 1, 2\n",
      "Recall the minimum-average-loss decision rule,\n",
      "Decide category 1 iﬀ:\n",
      "λ(1 | 2)p(X | 2)p(2) ≤λ(2 | 1)p(X | 1)p(1)\n",
      "Assuming conditional independence of the components and that λ(1 | 2) = λ(2 |\n",
      "1), we obtain,\n",
      "Decide category 1 iﬀ:\n",
      "p(1)p(x1 | 1)p(x2 | 1) · · · p(xn | 1) ≥p(x1 | 2)p(x2 | 2) · · · p(xn | 2)p(2)\n",
      "or iﬀ:\n",
      "p(x1 | 1)p(x2 | 1) . . . p(xn | 1)\n",
      "p(x1 | 2)p(x2 | 2) . . . p(xn | 2) ≥p(2)\n",
      "p(1)\n",
      "or iﬀ:\n",
      "log p(x1 | 1)\n",
      "p(x1 | 2) + log p(x2 | 1)\n",
      "p(x2 | 2) + · · · + log p(xn | 1)\n",
      "p(xn | 2) + log p(1)\n",
      "p(2) ≥0\n",
      "Let us deﬁne values of the components of the distribution for speciﬁc values of\n",
      "their arguments, xi :\n",
      "p(xi = 1 | 1) = pi\n",
      "p(xi = 0 | 1) = 1 −pi\n",
      "p(xi = 1 | 2) = qi\n",
      "p(xi = 0 | 2) = 1 −qi\n",
      "Now, we note that since xi can only assume the values of 1 or 0:\n",
      "log p(xi | 1)\n",
      "p(xi | 2) = xi log pi\n",
      "qi\n",
      "+ (1 −xi) log (1 −pi)\n",
      "(1 −qi)\n",
      "\n",
      "\n",
      "462. 70\n",
      "CHAPTER 5. STATISTICAL LEARNING\n",
      "= xi log pi(1 −qi)\n",
      "qi(1 −pi) + log (1 −pi)\n",
      "(1 −qi)\n",
      "Substituting these expressions into our decision rule yields:\n",
      "Decide category 1 iﬀ:\n",
      "n\n",
      "X\n",
      "i=1\n",
      "xi log pi(1 −qi)\n",
      "qi(1 −pi) +\n",
      "n\n",
      "X\n",
      "i=1\n",
      "log (1 −pi)\n",
      "(1 −qi) + log p(1)\n",
      "p(2) ≥0\n",
      "We see that we can achieve this decision with a TLU with weight values as\n",
      "follows:\n",
      "wi = log pi(1 −qi)\n",
      "qi(1 −pi)\n",
      "for i = 1, . . . , n, and\n",
      "wn+1 = log\n",
      "p(1)\n",
      "1 −p(1) +\n",
      "n\n",
      "X\n",
      "i=1\n",
      "log (1 −pi)\n",
      "(1 −qi)\n",
      "If we do not know the pi, qi and p(1), we can use a sample of labeled training\n",
      "patterns to estimate these parameters.\n",
      "5.2\n",
      "Learning Belief Networks\n",
      "To be added.\n",
      "5.3\n",
      "Nearest-Neighbor Methods\n",
      "Another class of methods can be related to the statistical ones. These are called\n",
      "nearest-neighbor methods or, sometimes, memory-based methods. (A collection\n",
      "of papers on this subject is in [Dasarathy, 1991].) Given a training set Ξ of m\n",
      "labeled patterns, a nearest-neighbor procedure decides that some new pattern,\n",
      "X, belongs to the same category as do its closest neighbors in Ξ. More precisely,\n",
      "a k-nearest-neighbor method assigns a new pattern, X, to that category to which\n",
      "the plurality of its k closest neighbors belong. Using relatively large values of\n",
      "k decreases the chance that the decision will be unduly inﬂuenced by a noisy\n",
      "training pattern close to X. But large values of k also reduce the acuity of the\n",
      "method. The k-nearest-neighbor method can be thought of as estimating the\n",
      "values of the probabilities of the classes given X. Of course the denser are the\n",
      "points around X, and the larger the value of k, the better the estimate.\n",
      "\n",
      "\n",
      "463. 5.3. NEAREST-NEIGHBOR METHODS\n",
      "71\n",
      "The distance metric used in nearest-neighbor methods (for numerical at-\n",
      "tributes) can be simple Euclidean distance. That is, the distance between two\n",
      "patterns (x11, x12, . . . , x1n) and (x21, x22, . . . , x2n) is\n",
      "qPn\n",
      "j=1(x1j −x2j)2. This\n",
      "distance measure is often modiﬁed by scaling the features so that the spread of\n",
      "attribute values along each dimension is approximately the same. In that case,\n",
      "the distance between the two vectors would be\n",
      "qPn\n",
      "j=1 a2\n",
      "j(x1j −x2j)2, where\n",
      "aj is the scale factor for dimension j.\n",
      "An example of a nearest-neighbor decision problem is shown in Fig. 5.3. In\n",
      "the ﬁgure the class of a training pattern is indicated by the number next to it.\n",
      "k = 8\n",
      "X (a pattern to be classified)\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "training pattern\n",
      "class of training pattern\n",
      "four patterns of category 1\n",
      "two patterns of category 2\n",
      "two patterns of category 3\n",
      "plurality are in category 1, so\n",
      "decide X is in category 1\n",
      "Figure 5.3: An 8-Nearest-Neighbor Decision\n",
      "See [Baum, 1994] for theoretical\n",
      "analysis of error rate as a function\n",
      "of the number of training patterns\n",
      "for the case in which points are\n",
      "randomly distributed on the surface\n",
      "of a unit sphere and underlying\n",
      "function is linearly separable.\n",
      "Nearest-neighbor methods are memory intensive because a large number of\n",
      "training patterns must be stored to achieve good generalization. Since memory\n",
      "cost is now reasonably low, the method and its derivatives have seen several\n",
      "practical applications.\n",
      "(See, for example, [Moore, 1992, Moore, et al., 1994].\n",
      "Also, the distance calculations required to ﬁnd nearest neighbors can often be\n",
      "eﬃciently computed by kd-tree methods [Friedman, et al., 1977].\n",
      "A theorem by Cover and Hart [Cover & Hart, 1967] relates the performance\n",
      "of the 1-nearest-neighbor method to the performance of a minimum-probability-\n",
      "of-error classiﬁer. As mentioned earlier, the minimum-probability-of-error clas-\n",
      "siﬁer would assign a new pattern X to that category that maximized p(i)p(X | i),\n",
      "where p(i) is the a priori probability of category i, and p(X | i) is the probability\n",
      "(or probability density function) of X given that X belongs to category i, for\n",
      "categories i = 1, . . . , R. Suppose the probability of error in classifying patterns\n",
      "of such a minimum-probability-of-error classiﬁer is ε.\n",
      "The Cover-Hart theo-\n",
      "rem states that under very mild conditions (having to do with the smoothness\n",
      "\n",
      "\n",
      "464. 72\n",
      "CHAPTER 5. STATISTICAL LEARNING\n",
      "of probability density functions) the probability of error, εnn, of a 1-nearest-\n",
      "neighbor classiﬁer is bounded by:\n",
      "ε ≤εnn ≤ε\n",
      "\u0012\n",
      "2 −ε\n",
      "R\n",
      "R −1\n",
      "\u0013\n",
      "≤2ε\n",
      "where R is the number of categories.\n",
      "Also see [Aha, 1991].\n",
      "5.4\n",
      "Bibliographical and Historical Remarks\n",
      "To be added.\n",
      "\n",
      "\n",
      "465. Chapter 6\n",
      "Decision Trees\n",
      "6.1\n",
      "Deﬁnitions\n",
      "A decision tree (generally deﬁned) is a tree whose internal nodes are tests (on\n",
      "input patterns) and whose leaf nodes are categories (of patterns). We show an\n",
      "example in Fig. 6.1. A decision tree assigns a class number (or output) to an\n",
      "input pattern by ﬁltering the pattern down through the tests in the tree. Each\n",
      "test has mutually exclusive and exhaustive outcomes. For example, test T2 in\n",
      "the tree of Fig. 6.1 has three outcomes; the left-most one assigns the input\n",
      "pattern to class 3, the middle one sends the input pattern down to test T4, and\n",
      "the right-most one assigns the pattern to class 1. We follow the usual convention\n",
      "of depicting the leaf nodes by the class number.1 Note that in discussing decision\n",
      "trees we are not limited to implementing Boolean functions—they are useful for\n",
      "general, categorically valued functions.\n",
      "There are several dimensions along which decision trees might diﬀer:\n",
      "a. The tests might be multivariate (testing on several features of the input\n",
      "at once) or univariate (testing on only one of the features).\n",
      "b. The tests might have two outcomes or more than two. (If all of the tests\n",
      "have two outcomes, we have a binary decision tree.)\n",
      "c. The features or attributes might be categorical or numeric. (Binary-valued\n",
      "ones can be regarded as either.)\n",
      "1One of the researchers who has done a lot of work on learning decision trees is Ross\n",
      "Quinlan. Quinlan distinguishes between classes and categories. He calls the subsets of patterns\n",
      "that ﬁlter down to each tip categories and subsets of patterns having the same label classes.\n",
      "In Quinlan’s terminology, our example tree has nine categories and three classes. We will not\n",
      "make this distinction, however, but will use the words “category” and “class” interchangeably\n",
      "to refer to what Quinlan calls “class.”\n",
      "73\n",
      "\n",
      "\n",
      "466. 74\n",
      "CHAPTER 6. DECISION TREES\n",
      "T1\n",
      "T2\n",
      "T3\n",
      "T4\n",
      "T4\n",
      "T4\n",
      "3\n",
      "1\n",
      "3\n",
      "2\n",
      "1\n",
      "2\n",
      "3\n",
      "2\n",
      "1\n",
      "Figure 6.1: A Decision Tree\n",
      "d. We might have two classes or more than two. If we have two classes and\n",
      "binary inputs, the tree implements a Boolean function, and is called a\n",
      "Boolean decision tree.\n",
      "It is straightforward to represent the function implemented by a univariate\n",
      "Boolean decision tree in DNF form. The DNF form implemented by such a tree\n",
      "can be obtained by tracing down each path leading to a tip node corresponding\n",
      "to an output value of 1, forming the conjunction of the tests along this path,\n",
      "and then taking the disjunction of these conjunctions. We show an example in\n",
      "Fig. 6.2. In drawing univariate decision trees, each non-leaf node is depicted by\n",
      "a single attribute. If the attribute has value 0 in the input pattern, we branch\n",
      "left; if it has value 1, we branch right.\n",
      "The k-DL class of Boolean functions can be implemented by a multivariate\n",
      "decision tree having the (highly unbalanced) form shown in Fig. 6.3. Each test,\n",
      "ci, is a term of size k or less. The vi all have values of 0 or 1.\n",
      "6.2\n",
      "Supervised Learning of Univariate Decision\n",
      "Trees\n",
      "Several systems for learning decision trees have been proposed.\n",
      "Prominent\n",
      "among these are ID3 and its new version, C4.5 [Quinlan, 1986, Quinlan, 1993],\n",
      "and CART [Breiman, et al., 1984] We discuss here only batch methods, al-\n",
      "though incremental ones have also been proposed [Utgoﬀ, 1989].\n",
      "\n",
      "\n",
      "467. 6.2. SUPERVISED LEARNING OF UNIVARIATE DECISION TREES\n",
      "75\n",
      "x3\n",
      "x2\n",
      "x4\n",
      "x1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "x3x2\n",
      "x3x2\n",
      "x3x4\n",
      "x3x4x1\n",
      "x3x4x1\n",
      "f = x3x2 + x3x4x1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "Figure 6.2: A Decision Tree Implementing a DNF Function\n",
      "6.2.1\n",
      "Selecting the Type of Test\n",
      "As usual, we have n features or attributes. If the attributes are binary, the\n",
      "tests are simply whether the attribute’s value is 0 or 1. If the attributes are\n",
      "categorical, but non-binary, the tests might be formed by dividing the attribute\n",
      "values into mutually exclusive and exhaustive subsets. A decision tree with such\n",
      "tests is shown in Fig. 6.4. If the attributes are numeric, the tests might involve\n",
      "“interval tests,” for example 7 ≤xi ≤13.2.\n",
      "6.2.2\n",
      "Using Uncertainty Reduction to Select Tests\n",
      "The main problem in learning decision trees for the binary-attribute case is\n",
      "selecting the order of the tests.\n",
      "For categorical and numeric attributes, we\n",
      "must also decide what the tests should be (besides selecting the order). Several\n",
      "techniques have been tried; the most popular one is at each stage to select that\n",
      "test that maximally reduces an entropy-like measure.\n",
      "We show how this technique works for the simple case of tests with binary\n",
      "outcomes. Extension to multiple-outcome tests is straightforward computation-\n",
      "ally but gives poor results because entropy is always decreased by having more\n",
      "outcomes.\n",
      "The entropy or uncertainty still remaining about the class of a pattern—\n",
      "knowing that it is in some set, Ξ, of patterns is deﬁned as:\n",
      "H(Ξ) = −\n",
      "X\n",
      "i\n",
      "p(i|Ξ) log2 p(i|Ξ)\n",
      "\n",
      "\n",
      "468. 76\n",
      "CHAPTER 6. DECISION TREES\n",
      "cq\n",
      "cq-1\n",
      "ci\n",
      "1\n",
      "vn\n",
      "vn-1\n",
      "vi\n",
      "v1\n",
      "Figure 6.3: A Decision Tree Implementing a Decision List\n",
      "where p(i|Ξ) is the probability that a pattern drawn at random from Ξ belongs\n",
      "to class i, and the summation is over all of the classes. We want to select tests at\n",
      "each node such that as we travel down the decision tree, the uncertainty about\n",
      "the class of a pattern becomes less and less.\n",
      "Since we do not in general have the probabilities p(i|Ξ), we estimate them by\n",
      "sample statistics. Although these estimates might be errorful, they are never-\n",
      "theless useful in estimating uncertainties. Let ˆp(i|Ξ) be the number of patterns\n",
      "in Ξ belonging to class i divided by the total number of patterns in Ξ. Then an\n",
      "estimate of the uncertainty is:\n",
      "ˆH(Ξ) = −\n",
      "X\n",
      "i\n",
      "ˆp(i|Ξ) log2 ˆp(i|Ξ)\n",
      "For simplicity, from now on we’ll drop the “hats” and use sample statistics as\n",
      "if they were real probabilities.\n",
      "If we perform a test, T, having k possible outcomes on the patterns in Ξ, we\n",
      "will create k subsets, Ξ1, Ξ2, . . . , Ξk. Suppose that ni of the patterns in Ξ are in\n",
      "Ξi for i = 1, ..., k. (Some ni may be 0.) If we knew that T applied to a pattern\n",
      "in Ξ resulted in the j-th outcome (that is, we knew that the pattern was in Ξj),\n",
      "the uncertainty about its class would be:\n",
      "H(Ξj) = −\n",
      "X\n",
      "i\n",
      "p(i|Ξj) log2 p(i|Ξj)\n",
      "and the reduction in uncertainty (beyond knowing only that the pattern was in\n",
      "Ξ) would be:\n",
      "\n",
      "\n",
      "469. 6.2. SUPERVISED LEARNING OF UNIVARIATE DECISION TREES\n",
      "77\n",
      "x3 = a, b, c, or d \n",
      "{a, c}\n",
      "{b}\n",
      "x1 = e, b, or d \n",
      "{e,b}\n",
      "{d}\n",
      "x4 = a, e, f, or g\n",
      "{a, g}\n",
      "{e, f}\n",
      "x2 = a, or g\n",
      "{a}\n",
      "{g}\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "{d}\n",
      "2\n",
      "Figure 6.4: A Decision Tree with Categorical Attributes\n",
      "H(Ξ) −H(Ξj)\n",
      "Of course we cannot say that the test T is guaranteed always to produce that\n",
      "amount of reduction in uncertainty because we don’t know that the result of\n",
      "the test will be the j-th outcome. But we can estimate the average uncertainty\n",
      "over all the Ξj, by:\n",
      "E[HT (Ξ)] =\n",
      "X\n",
      "j\n",
      "p(Ξj)H(Ξj)\n",
      "where by HT (Ξ) we mean the average uncertainty after performing test T on\n",
      "the patterns in Ξ, p(Ξj) is the probability that the test has outcome j, and the\n",
      "sum is taken from 1 to k. Again, we don’t know the probabilities p(Ξj), but we\n",
      "can use sample values. The estimate ˆp(Ξj) of p(Ξj) is just the number of those\n",
      "patterns in Ξ that have outcome j divided by the total number of patterns in\n",
      "Ξ. The average reduction in uncertainty achieved by test T (applied to patterns\n",
      "in Ξ) is then:\n",
      "RT (Ξ) = H(Ξ) −E[HT (Ξ)]\n",
      "An important family of decision tree learning algorithms selects for the root\n",
      "of the tree that test that gives maximum reduction of uncertainty, and then\n",
      "applies this criterion recursively until some termination condition is met (which\n",
      "we shall discuss in more detail later). The uncertainty calculations are particu-\n",
      "larly simple when the tests have binary outcomes and when the attributes have\n",
      "\n",
      "\n",
      "470. 78\n",
      "CHAPTER 6. DECISION TREES\n",
      "binary values. We’ll give a simple example to illustrate how the test selection\n",
      "mechanism works in that case.\n",
      "Suppose we want to use the uncertainty-reduction method to build a decision\n",
      "tree to classify the following patterns:\n",
      "pattern\n",
      "class\n",
      "(0, 0, 0)\n",
      "0\n",
      "(0, 0, 1)\n",
      "0\n",
      "(0, 1, 0)\n",
      "0\n",
      "(0, 1, 1)\n",
      "0\n",
      "(1, 0, 0)\n",
      "0\n",
      "(1, 0, 1)\n",
      "1\n",
      "(1, 1, 0)\n",
      "0\n",
      "(1, 1, 1)\n",
      "1\n",
      "What single test, x1, x2, or x3, should be performed ﬁrst? The illustration in\n",
      "Fig. 6.5 gives geometric intuition about the problem.\n",
      "x1\n",
      "x2\n",
      "x3\n",
      "The test x1\n",
      "Figure 6.5: Eight Patterns to be Classiﬁed by a Decision Tree\n",
      "The initial uncertainty for the set, Ξ, containing all eight points is:\n",
      "H(Ξ) = −(6/8) log2(6/8) −(2/8) log2(2/8) = 0.81\n",
      "Next, we calculate the uncertainty reduction if we perform x1 ﬁrst. The left-\n",
      "hand branch has only patterns belonging to class 0 (we call them the set Ξl), and\n",
      "the right-hand-branch (Ξr) has two patterns in each class. So, the uncertainty\n",
      "of the left-hand branch is:\n",
      "\n",
      "\n",
      "471. 6.3. NETWORKS EQUIVALENT TO DECISION TREES\n",
      "79\n",
      "Hx1(Ξl) = −(4/4) log2(4/4) −(0/4) log2(0/4) = 0\n",
      "And the uncertainty of the right-hand branch is:\n",
      "Hx1(Ξr) = −(2/4) log2(2/4) −(2/4) log2(2/4) = 1\n",
      "Half of the patterns “go left” and half “go right” on test x1. Thus, the average\n",
      "uncertainty after performing the x1 test is:\n",
      "1/2Hx1(Ξl) + 1/2Hx1(Ξr) = 0.5\n",
      "Therefore the uncertainty reduction on Ξ achieved by x1 is:\n",
      "Rx1(Ξ) = 0.81 −0.5 = 0.31\n",
      "By similar calculations, we see that the test x3 achieves exactly the same\n",
      "uncertainty reduction, but x2 achieves no reduction whatsoever.\n",
      "Thus, our\n",
      "“greedy” algorithm for selecting a ﬁrst test would select either x1 or x3. Suppose\n",
      "x1 is selected. The uncertainty-reduction procedure would select x3 as the next\n",
      "test. The decision tree that this procedure creates thus implements the Boolean\n",
      "function: f = x1x3.\n",
      "See [Quinlan, 1986, sect. 4] for\n",
      "another example.\n",
      "6.2.3\n",
      "Non-Binary Attributes\n",
      "If the attributes are non-binary, we can still use the uncertainty-reduction tech-\n",
      "nique to select tests. But now, in addition to selecting an attribute, we must\n",
      "select a test on that attribute. Suppose for example that the value of an at-\n",
      "tribute is a real number and that the test to be performed is to set a threshold\n",
      "and to test to see if the number is greater than or less than that threshold. In\n",
      "principle, given a set of labeled patterns, we can measure the uncertainty reduc-\n",
      "tion for each test that is achieved by every possible threshold (there are only\n",
      "a ﬁnite number of thresholds that give diﬀerent test results if there are only\n",
      "a ﬁnite number of training patterns). Similarly, if an attribute is categorical\n",
      "(with a ﬁnite number of categories), there are only a ﬁnite number of mutually\n",
      "exclusive and exhaustive subsets into which the values of the attribute can be\n",
      "split. We can calculate the uncertainty reduction for each split.\n",
      "6.3\n",
      "Networks Equivalent to Decision Trees\n",
      "Since univariate Boolean decision trees are implementations of DNF functions,\n",
      "they are also equivalent to two-layer, feedforward neural networks. We show\n",
      "an example in Fig. 6.6. The decision tree at the left of the ﬁgure implements\n",
      "\n",
      "\n",
      "472. 80\n",
      "CHAPTER 6. DECISION TREES\n",
      "the same function as the network at the right of the ﬁgure. Of course, when\n",
      "implemented as a network, all of the features are evaluated in parallel for any\n",
      "input pattern, whereas when implemented as a decision tree only those features\n",
      "on the branch traveled down by the input pattern need to be evaluated. The\n",
      "decision-tree induction methods discussed in this chapter can thus be thought of\n",
      "as particular ways to establish the structure and the weight values for networks.\n",
      "X\n",
      "x1\n",
      "x2\n",
      "x3\n",
      "x4\n",
      "terms\n",
      "-1\n",
      "+1\n",
      "disjunction\n",
      "x3x2\n",
      "x3x4x1\n",
      "+1\n",
      "-1\n",
      "+1\n",
      "f\n",
      "1.5\n",
      "0.5\n",
      "x3\n",
      "x2\n",
      "x4\n",
      "x1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "x3x2\n",
      "x3x2\n",
      "x3x4\n",
      "x3x4x1\n",
      "x3x4x1\n",
      "f = x3x2 + x3x4x1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "Figure 6.6: A Univariate Decision Tree and its Equivalent Network\n",
      "Multivariate decision trees with linearly separable functions at each node can\n",
      "also be implemented by feedforward networks—in this case three-layer ones. We\n",
      "show an example in Fig. 6.7 in which the linearly separable functions, each im-\n",
      "plemented by a TLU, are indicated by L1, L2, L3, and L4. Again, the ﬁnal layer\n",
      "has ﬁxed weights, but the weights in the ﬁrst two layers must be trained. Dif-\n",
      "ferent approaches to training procedures have been discussed by [Brent, 1990],\n",
      "by [John, 1995], and (for a special case) by [Marchand & Golea, 1993].\n",
      "6.4\n",
      "Overﬁtting and Evaluation\n",
      "6.4.1\n",
      "Overﬁtting\n",
      "In supervised learning, we must choose a function to ﬁt the training set from\n",
      "among a set of hypotheses.\n",
      "We have already showed that generalization is\n",
      "impossible without bias.\n",
      "When we know a priori that the function we are\n",
      "trying to guess belongs to a small subset of all possible functions, then, even\n",
      "with an incomplete set of training samples, it is possible to reduce the subset\n",
      "of functions that are consistent with the training set suﬃciently to make useful\n",
      "guesses about the value of the function for inputs not in the training set. And,\n",
      "\n",
      "\n",
      "473. 6.4. OVERFITTING AND EVALUATION\n",
      "81\n",
      "L1\n",
      "L2\n",
      "L3\n",
      "L4\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "X\n",
      "L1\n",
      "L2\n",
      "L3\n",
      "L4\n",
      "conjunctions\n",
      "L1L2\n",
      "L1 L3 L4\n",
      "<\n",
      "+\n",
      "+\n",
      "+\n",
      "disjunction\n",
      "<\n",
      "f\n",
      "Figure 6.7: A Multivariate Decision Tree and its Equivalent Network\n",
      "the larger the training set, the more likely it is that even a randomly selected\n",
      "consistent function will have appropriate outputs for patterns not yet seen.\n",
      "However, even with bias, if the training set is not suﬃciently large compared\n",
      "with the size of the hypothesis space, there will still be too many consistent\n",
      "functions for us to make useful guesses, and generalization performance will be\n",
      "poor. When there are too many hypotheses that are consistent with the training\n",
      "set, we say that we are overﬁtting the training data. Overﬁtting is a problem\n",
      "that we must address for all learning methods.\n",
      "Since a decision tree of suﬃcient size can implement any Boolean function\n",
      "there is a danger of overﬁtting—especially if the training set is small. That\n",
      "is, even if the decision tree is synthesized to classify all the members of the\n",
      "training set correctly, it might perform poorly on new patterns that were not\n",
      "used to build the decision tree. Several techniques have been proposed to avoid\n",
      "overﬁtting, and we shall examine some of them here. They make use of methods\n",
      "for estimating how well a given decision tree might generalize—methods we shall\n",
      "describe next.\n",
      "6.4.2\n",
      "Validation Methods\n",
      "The most straightforward way to estimate how well a hypothesized function\n",
      "(such as a decision tree) performs on a test set is to test it on the test set! But,\n",
      "if we are comparing several learning systems (for example, if we are comparing\n",
      "diﬀerent decision trees) so that we can select the one that performs the best on\n",
      "the test set, then such a comparison amounts to “training on the test data.”\n",
      "True, training on the test data enlarges the training set, with a consequent ex-\n",
      "pected improvement in generalization, but there is still the danger of overﬁtting\n",
      "if we are comparing several diﬀerent learning systems. Another technique is to\n",
      "\n",
      "\n",
      "474. 82\n",
      "CHAPTER 6. DECISION TREES\n",
      "split the training set—using (say) two-thirds for training and the other third\n",
      "for estimating generalization performance. But splitting reduces the size of the\n",
      "training set and thereby increases the possibility of overﬁtting. We next describe\n",
      "some validation techniques that attempt to avoid these problems.\n",
      "Cross-Validation\n",
      "In cross-validation, we divide the training set Ξ into K mutually exclusive and\n",
      "exhaustive equal-sized subsets: Ξ1, . . . , ΞK. For each subset, Ξi, train on the\n",
      "union of all of the other subsets, and empirically determine the error rate, εi,\n",
      "on Ξi. (The error rate is the number of classiﬁcation errors made on Ξi divided\n",
      "by the number of patterns in Ξi.) An estimate of the error rate that can be\n",
      "expected on new patterns of a classiﬁer trained on all the patterns in Ξ is then\n",
      "the average of the εi.\n",
      "Leave-one-out Validation\n",
      "Leave-one-out validation is the same as cross validation for the special case in\n",
      "which K equals the number of patterns in Ξ, and each Ξi consists of a single\n",
      "pattern. When testing on each Ξi, we simply note whether or not a mistake\n",
      "was made.\n",
      "We count the total number of mistakes and divide by K to get\n",
      "the estimated error rate. This type of validation is, of course, more expensive\n",
      "computationally, but useful when a more accurate estimate of the error rate for\n",
      "a classiﬁer is needed.\n",
      "Describe “bootstrapping” also\n",
      "[Efron, 1982].\n",
      "6.4.3\n",
      "Avoiding Overﬁtting in Decision Trees\n",
      "Near the tips of a decision tree there may be only a few patterns per node.\n",
      "For these nodes, we are selecting a test based on a very small sample, and thus\n",
      "we are likely to be overﬁtting. This problem can be dealt with by terminating\n",
      "the test-generating procedure before all patterns are perfectly split into their\n",
      "separate categories. That is, a leaf node may contain patterns of more than one\n",
      "class, but we can decide in favor of the most numerous class. This procedure\n",
      "will result in a few errors but often accepting a small number of errors on the\n",
      "training set results in fewer errors on a testing set.\n",
      "This behavior is illustrated in Fig. 6.8.\n",
      "One can use cross-validation techniques to determine when to stop splitting\n",
      "nodes. If the cross validation error increases as a consequence of a node split,\n",
      "then don’t split. One has to be careful about when to stop, though, because\n",
      "underﬁtting usually leads to more errors on test sets than does overﬁtting. There\n",
      "is a general rule that the lowest error-rate attainable by a sub-tree of a fully\n",
      "expanded tree can be no less than 1/2 of the error rate of the fully expanded\n",
      "tree [Weiss & Kulikowski, 1991, page 126].\n",
      "\n",
      "\n",
      "475. 6.4. OVERFITTING AND EVALUATION\n",
      "83\n",
      "(From Weiss, S., and Kulikowski, C., Computer Systems that Learn,\n",
      "Morgan Kaufmann, 1991)\n",
      "training errors\n",
      "validation errors\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "0\n",
      "0\n",
      "Error Rate\n",
      "Number of Terminal\n",
      "Nodes\n",
      "Iris Data Decision Tree\n",
      "Figure 6.8: Determining When Overﬁtting Begins\n",
      "Rather than stopping the growth of a decision tree, one might grow it to\n",
      "its full size and then prune away leaf nodes and their ancestors until cross-\n",
      "validation accuracy no longer increases. This technique is called post-pruning.\n",
      "Various techniques for pruning are discussed in [Weiss & Kulikowski, 1991].\n",
      "6.4.4\n",
      "Minimum-Description Length Methods\n",
      "An important tree-growing and pruning technique is based on the minimum-\n",
      "description-length (MDL) principle. (MDL is an important idea that extends\n",
      "beyond decision-tree methods [Rissanen, 1978].) The idea is that the simplest\n",
      "decision tree that can predict the classes of the training patterns is the best\n",
      "one. Consider the problem of transmitting just the labels of a training set of\n",
      "patterns, assuming that the receiver of this information already has the ordered\n",
      "set of patterns.\n",
      "If there are m patterns, each labeled by one of R classes,\n",
      "one could transmit a list of m R-valued numbers. Assuming equally probable\n",
      "classes, this transmission would require m log2 R bits. Or, one could transmit a\n",
      "decision tree that correctly labelled all of the patterns. The number of bits that\n",
      "this transmission would require depends on the technique for encoding decision\n",
      "trees and on the size of the tree. If the tree is small and accurately classiﬁes\n",
      "all of the patterns, it might be more economical to transmit the tree than to\n",
      "transmit the labels directly. In between these extremes, we might transmit a\n",
      "tree plus a list of labels of all the patterns that the tree misclassiﬁes.\n",
      "In general, the number of bits (or description length of the binary encoded\n",
      "message) is t + d, where t is the length of the message required to transmit\n",
      "the tree, and d is the length of the message required to transmit the labels of\n",
      "\n",
      "\n",
      "476. 84\n",
      "CHAPTER 6. DECISION TREES\n",
      "the patterns misclassiﬁed by the tree. In a sense, that tree associated with the\n",
      "smallest value of t + d is the best or most economical tree. The MDL method\n",
      "is one way of adhering to the Occam’s razor principle.\n",
      "Quinlan and Rivest [Quinlan & Rivest, 1989] have proposed techniques for\n",
      "encoding decision trees and lists of exception labels and for calculating the\n",
      "description length (t+d) of these trees and labels. They then use the description\n",
      "length as a measure of quality of a tree in two ways:\n",
      "a. In growing a tree, they use the reduction in description length to select\n",
      "tests (instead of reduction in uncertainty).\n",
      "b. In pruning a tree after it has been grown to zero error, they prune away\n",
      "those nodes (starting at the tips) that achieve a decrease in the description\n",
      "length.\n",
      "These techniques compare favorably with the uncertainty-reduction method,\n",
      "although they are quite sensitive to the coding schemes used.\n",
      "6.4.5\n",
      "Noise in Data\n",
      "Noise in the data means that one must inevitably accept some number of\n",
      "errors—depending on the noise level. Refusal to tolerate errors on the training\n",
      "set when there is noise leads to the problem of “ﬁtting the noise.” Dealing with\n",
      "noise, then, requires accepting some errors at the leaf nodes just as does the\n",
      "fact that there are a small number of patterns at leaf nodes.\n",
      "6.5\n",
      "The Problem of Replicated Subtrees\n",
      "Decision trees are not the most economical means of implementing some Boolean\n",
      "functions. Consider, for example, the function f = x1x2 +x3x4. A decision tree\n",
      "for this function is shown in Fig. 6.9. Notice the replicated subtrees shown\n",
      "circled. The DNF-form equivalent to the function implemented by this decision\n",
      "tree is f = x1x2 + x1x2x3x4 + x1x3x4. This DNF form is non-minimal (in the\n",
      "number of disjunctions) and is equivalent to f = x1x2 + x3x4.\n",
      "The need for replication means that it takes longer to learn the tree and\n",
      "that subtrees replicated further down the tree must be learned using a smaller\n",
      "training subset. This problem is sometimes called the fragmentation problem.\n",
      "Several approaches might be suggested for dealing with fragmenta-\n",
      "tion.\n",
      "One is to attempt to build a decision graph instead of a tree\n",
      "[Oliver, Dowe, & Wallace, 1992, Kohavi, 1994]. A decision graph that imple-\n",
      "ments the same decisions as that of the decision tree of Fig. 6.9 is shown in Fig.\n",
      "6.10.\n",
      "Another approach is to use multivariate (rather than univariate tests at each\n",
      "node). In our example of learning f = x1x2 + x3x4, if we had a test for x1x2\n",
      "\n",
      "\n",
      "477. 6.6. THE PROBLEM OF MISSING ATTRIBUTES\n",
      "85\n",
      "x1\n",
      "x3\n",
      "x2\n",
      "1\n",
      "0\n",
      "x4\n",
      "0\n",
      "1\n",
      "x3\n",
      "0\n",
      "x4\n",
      "0\n",
      "1\n",
      "Figure 6.9: A Decision Tree with Subtree Replication\n",
      "and a test for x3x4, the decision tree could be much simpliﬁed, as shown in Fig.\n",
      "6.11. Several researchers have proposed techniques for learning decision trees in\n",
      "which the tests at each node are linearly separable functions. [John, 1995] gives\n",
      "a nice overview (with several citations) of learning such linear discriminant trees\n",
      "and presents a method based on “soft entropy.”\n",
      "A third method for dealing with the replicated subtree problem involves ex-\n",
      "tracting propositional “rules” from the decision tree. The rules will have as an-\n",
      "tecedents the conjunctions that lead down to the leaf nodes, and as consequents\n",
      "the name of the class at the corresponding leaf node. An example rule from the\n",
      "tree with the repeating subtree of our example would be: x1 ∧¬x2 ∧x3 ∧x4 ⊃1.\n",
      "Quinlan [Quinlan, 1987] discusses methods for reducing a set of rules to a sim-\n",
      "pler set by 1) eliminating from the antecedent of each rule any “unnecessary”\n",
      "conjuncts, and then 2) eliminating “unnecessary” rules. A conjunct or rule is\n",
      "determined to be unnecessary if its elimination has little eﬀect on classiﬁcation\n",
      "accuracy—as determined by a chi-square test, for example. After a rule set is\n",
      "processed, it might be the case that more than one rule is “active” for any given\n",
      "pattern, and care must be taken that the active rules do not conﬂict in their\n",
      "decision about the class of a pattern.\n",
      "\n",
      "\n",
      "478. 86\n",
      "CHAPTER 6. DECISION TREES\n",
      "x1\n",
      "x3\n",
      "x2\n",
      "1\n",
      "0\n",
      "x4\n",
      "0\n",
      "1\n",
      "Figure 6.10: A Decision Graph\n",
      "6.6\n",
      "The Problem of Missing Attributes\n",
      "To be added.\n",
      "6.7\n",
      "Comparisons\n",
      "Several experimenters have compared decision-tree, neural-net, and nearest-\n",
      "neighbor classiﬁers on a wide variety of problems.\n",
      "For a comparison of\n",
      "neural nets versus decision trees, for example, see [Dietterich, et al., 1990,\n",
      "Shavlik, Mooney, & Towell, 1991, Quinlan, 1994].\n",
      "In their StatLog project,\n",
      "[Taylor, Michie, & Spiegalhalter, 1994] give thorough comparisons of several\n",
      "machine learning algorithms on several diﬀerent types of problems. There seems\n",
      "x1x2\n",
      "1\n",
      "0\n",
      "x3x4\n",
      "1\n",
      "Figure 6.11: A Multivariate Decision Tree\n",
      "\n",
      "\n",
      "479. 6.8. BIBLIOGRAPHICAL AND HISTORICAL REMARKS\n",
      "87\n",
      "to be no single type of classiﬁer that is best for all problems. And, there do\n",
      "not seem to be any general conclusions that would enable one to say which\n",
      "classiﬁer method is best for which sorts of classiﬁcation problems, although\n",
      "[Quinlan, 1994] does provide some intuition about properties of problems that\n",
      "might render them ill suited for decision trees, on the one hand, or backpropa-\n",
      "gation, on the other.\n",
      "6.8\n",
      "Bibliographical and Historical Remarks\n",
      "To be added.\n",
      "\n",
      "\n",
      "480. 88\n",
      "CHAPTER 6. DECISION TREES\n",
      "\n",
      "\n",
      "481. Chapter 7\n",
      "Inductive Logic\n",
      "Programming\n",
      "There are many diﬀerent representational forms for functions of input vari-\n",
      "ables. So far, we have seen (Boolean) algebraic expressions, decision trees, and\n",
      "neural networks, plus other computational mechanisms such as techniques for\n",
      "computing nearest neighbors.\n",
      "Of course, the representation most important\n",
      "in computer science is a computer program. For example, a Lisp predicate of\n",
      "binary-valued inputs computes a Boolean function of those inputs. Similarly, a\n",
      "logic program (whose ordinary application is to compute bindings for variables)\n",
      "can also be used simply to decide whether or not a predicate has value True\n",
      "(T) or False (F). For example, the Boolean exclusive-or (odd parity) function\n",
      "of two variables can be computed by the following logic program:\n",
      "Parity(x,y) :- True(x), ¬ True(y)\n",
      ":- True(y), ¬ True(x)\n",
      "We follow Prolog syntax (see, for example, [Mueller & Page, 1988]), except that\n",
      "our convention is to write variables as strings beginning with lower-case letters\n",
      "and predicates as strings beginning with upper-case letters. The unary function\n",
      "“True” returns T if and only if the value of its argument is T. (We now think\n",
      "of Boolean functions and arguments as having values of T and F instead of 0\n",
      "and 1.) Programs will be written in “typewriter” font.\n",
      "In this chapter, we consider the matter of learning logic programs given\n",
      "a set of variable values for which the logic program should return T (the\n",
      "positive instances) and a set of variable values for which it should return\n",
      "F (the negative instances). The subspecialty of machine learning that deals\n",
      "with learning logic programs is called inductive logic programming (ILP)\n",
      "[Lavraˇc & Dˇzeroski, 1994]. As with any learning problem, this one can be quite\n",
      "complex and intractably diﬃcult unless we constrain it with biases of some sort.\n",
      "89\n",
      "\n",
      "\n",
      "482. 90\n",
      "CHAPTER 7. INDUCTIVE LOGIC PROGRAMMING\n",
      "In ILP, there are a variety of possible biases (called language biases). One might\n",
      "restrict the program to Horn clauses, not allow recursion, not allow functions,\n",
      "and so on.\n",
      "As an example of an ILP problem, suppose we are trying to induce a func-\n",
      "tion Nonstop(x,y), that is to have value T for pairs of cities connected by a\n",
      "non-stop air ﬂight and F for all other pairs of cities. We are given a training set\n",
      "consisting of positive and negative examples. As positive examples, we might\n",
      "have (A,B), (A, A1), and some other pairs; as negative examples, we might\n",
      "have (A1, A2), and some other pairs. In ILP, we usually have additional infor-\n",
      "mation about the examples, called “background knowledge.” In our air-ﬂight\n",
      "problem, the background information might be such ground facts as Hub(A),\n",
      "Hub(B), Satellite(A1,A), plus others. (Hub(A) is intended to mean that the\n",
      "city denoted by A is a hub city, and Satellite(A1,A) is intended to mean that\n",
      "the city denoted by A1 is a satellite of the city denoted by A.) From these train-\n",
      "ing facts, we want to induce a program Nonstop(x,y), written in terms of the\n",
      "background relations Hub and Satellite, that has value T for all the positive\n",
      "instances and has value F for all the negative instances. Depending on the exact\n",
      "set of examples, we might induce the program:\n",
      "Nonstop(x,y) :- Hub(x), Hub(y)\n",
      ":- Satellite(x,y)\n",
      ":- Satellite(y,x)\n",
      "which would have value T if both of the two cities were hub cities or if one were\n",
      "a satellite of the other. As with other learning problems, we want the induced\n",
      "program to generalize well; that is, if presented with arguments not represented\n",
      "in the training set (but for which we have the needed background knowledge),\n",
      "we would like the function to guess well.\n",
      "7.1\n",
      "Notation and Deﬁnitions\n",
      "In evaluating logic programs in ILP, we implicitly append the background facts\n",
      "to the program and adopt the usual convention that a program has value T for\n",
      "a set of inputs if and only if the program interpreter returns T when actually\n",
      "running the program (with background facts appended) on those inputs; oth-\n",
      "erwise it has value F. Using the given background facts, the program above\n",
      "would return T for input (A, A1), for example. If a logic program, π, returns\n",
      "T for a set of arguments X, we say that the program covers the arguments and\n",
      "write covers(π, X). Following our terminology introduced in connection with\n",
      "version spaces, we will say that a program is suﬃcient if it covers all of the\n",
      "positive instances and that it is necessary if it does not cover any of the neg-\n",
      "ative instances. (That is, a program implements a suﬃcient condition that a\n",
      "training instance is positive if it covers all of the positive training instances; it\n",
      "\n",
      "\n",
      "483. 7.2. A GENERIC ILP ALGORITHM\n",
      "91\n",
      "implements a necessary condition if it covers none of the negative instances.) In\n",
      "the noiseless case, we want to induce a program that is both suﬃcient and nec-\n",
      "essary, in which case we will call it consistent. With imperfect (noisy) training\n",
      "sets, we might relax this criterion and settle for a program that covers all but\n",
      "some fraction of the positive instances while allowing it to cover some fraction\n",
      "of the negative instances. We illustrate these deﬁnitions schematically in Fig.\n",
      "7.1.\n",
      "<\n",
      "<\n",
      "<\n",
      "<\n",
      "<\n",
      "<\n",
      "<\n",
      "/1 is a necessary program\n",
      "/2 is a sufficient program\n",
      "/3 is a consistent program\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "<\n",
      "<\n",
      "A positive instance\n",
      " covered by /2 and /3\n",
      "Figure 7.1: Suﬃcient, Necessary, and Consistent Programs\n",
      "As in version spaces, if a program is suﬃcient but not necessary it can be\n",
      "made to cover fewer examples by specializing it. Conversely, if it is necessary\n",
      "but not suﬃcient, it can be made to cover more examples by generalizing it.\n",
      "Suppose we are attempting to induce a logic program to compute the relation\n",
      "ρ. The most general logic program, which is certainly suﬃcient, is the one that\n",
      "has value T for all inputs, namely a single clause with an empty body, [ρ :-\n",
      "], which is called a fact in Prolog. The most special logic program, which is\n",
      "certainly necessary, is the one that has value F for all inputs, namely [ρ :-\n",
      "F\n",
      "]. Two of the many diﬀerent ways to search for a consistent logic program\n",
      "are: 1) start with [ρ :-\n",
      "] and specialize until the program is consistent, or 2)\n",
      "start with [ρ :- F\n",
      "] and generalize until the program is consistent. We will\n",
      "be discussing a method that starts with [ρ :-\n",
      "], specializes until the program\n",
      "is necessary (but might no longer be suﬃcient), then reachieves suﬃciency in\n",
      "stages by generalizing—ensuring within each stage that the program remains\n",
      "necessary (by specializing).\n",
      "7.2\n",
      "A Generic ILP Algorithm\n",
      "Since the primary operators in our search for a consistent program are special-\n",
      "ization and generalization, we must next discuss those operations. There are\n",
      "\n",
      "\n",
      "484. 92\n",
      "CHAPTER 7. INDUCTIVE LOGIC PROGRAMMING\n",
      "three major ways in which a logic program might be generalized:\n",
      "a. Replace some terms in a program clause by variables. (Readers familiar\n",
      "with substitutions in the predicate calculus will note that this process is\n",
      "the inverse of substitution.)\n",
      "b. Remove literals from the body of a clause.\n",
      "c. Add a clause to the program\n",
      "Analogously, there are three ways in which a logic program might be specialized:\n",
      "a. Replace some variables in a program clause by terms (a substitution).\n",
      "b. Add literals to the body of a clause.\n",
      "c. Remove a clause from the program\n",
      "We will be presenting an ILP learning method that adds clauses to a program\n",
      "when generalizing and that adds literals to the body of a clause when special-\n",
      "izing. When we add a clause, we will always add the clause [ρ :- ] and then\n",
      "specialize it by adding literals to the body. Thus, we need only describe the\n",
      "process for adding literals.\n",
      "Clauses can be partially ordered by the specialization relation. In general,\n",
      "clause c1 is more special than clause c2 if c2 |= c1. A special case, which is what\n",
      "we use here, is that a clause c1 is more special than a clause c2 if the set of\n",
      "literals in the body of c2 is a subset of those in c1. This ordering relation can\n",
      "be used in a structure of partially ordered clauses, called the reﬁnement graph,\n",
      "that is similar to a version space. Clause c1 is an immediate successor of clause\n",
      "c2 in this graph if and only if clause c1 can be obtained from clause c2 by adding\n",
      "a literal to the body of c2. A reﬁnement graph then tells us the ways in which\n",
      "we can specialize a clause by adding a literal to it.\n",
      "Of course there are unlimited possible literals we might add to the body of\n",
      "a clause. Practical ILP systems restrict the literals in various ways. Typical\n",
      "allowed additions are:\n",
      "a. Literals used in the background knowledge.\n",
      "b. Literals whose arguments are a subset of those in the head of the clause.\n",
      "c. Literals that introduce a new distinct variable diﬀerent from those in the\n",
      "head of the clause.\n",
      "d. A literal that equates a variable in the head of the clause with another\n",
      "such variable or with a term mentioned in the background knowledge.\n",
      "(This possibility is equivalent to forming a specialization by making a\n",
      "substitution.)\n",
      "\n",
      "\n",
      "485. 7.2. A GENERIC ILP ALGORITHM\n",
      "93\n",
      "e. A literal that is the same (except for its arguments) as that in the head\n",
      "of the clause. (This possibility admits recursive programs, which are dis-\n",
      "allowed in some systems.)\n",
      "We can illustrate these possibilities using our air-ﬂight example. We start\n",
      "with the program [Nonstop(x,y) :-\n",
      "]. The literals used in the background\n",
      "knowledge are Hub and Satellite. Thus the literals that we might consider\n",
      "adding are:\n",
      "Hub(x)\n",
      "Hub(y)\n",
      "Hub(z)\n",
      "Satellite(x,y)\n",
      "Satellite(y,x)\n",
      "Satellite(x,z)\n",
      "Satellite(z,y)\n",
      "(x = y)\n",
      "(If recursive programs are allowed, we could also add the literals Nonstop(x,z)\n",
      "and Nonstop(z,y).) These possibilities are among those illustrated in the re-\n",
      "ﬁnement graph shown in Fig. 7.2. Whatever restrictions on additional literals\n",
      "are imposed, they are all syntactic ones from which the successors in the reﬁne-\n",
      "ment graph are easily computed. ILP programs that follow the approach we\n",
      "are discussing (of specializing clauses by adding a literal) thus have well deﬁned\n",
      "methods of computing the possible literals to add to a clause.\n",
      "Now we are ready to write down a simple generic algorithm for inducing a\n",
      "logic program, π for inducing a relation ρ. We are given a training set, Ξ of\n",
      "argument sets some known to be in the relation ρ and some not in ρ; Ξ+ are\n",
      "the positive instances, and Ξ−are the negative instances. The algorithm has\n",
      "an outer loop in which it successively adds clauses to make π more and more\n",
      "suﬃcient. It has an inner loop for constructing a clause, c, that is more and\n",
      "more necessary and in which it refers only to a subset, Ξcur, of the training\n",
      "instances. (The positive instances in Ξcur will be denoted by Ξ+\n",
      "cur, and the\n",
      "negative ones by Ξ−\n",
      "cur.) The algorithm is also given background relations and\n",
      "the means for adding literals to a clause. It uses a logic program interpreter to\n",
      "compute whether or not the program it is inducing covers training instances.\n",
      "The algorithm can be written as follows:\n",
      "Generic ILP Algorithm\n",
      "(Adapted from [Lavraˇc & Dˇzeroski, 1994, p. 60].)\n",
      "\n",
      "\n",
      "486. 94\n",
      "CHAPTER 7. INDUCTIVE LOGIC PROGRAMMING\n",
      "Nonstop(x,y) :-\n",
      "Nonstop(x,y) :-\n",
      "   Hub(x)\n",
      "Nonstop(x,y) :-\n",
      "   Satellite(x,y)\n",
      "Nonstop(x,y) :-\n",
      "   (x = y)\n",
      ". . .\n",
      ". . .\n",
      ". . .\n",
      ". . .\n",
      "Nonstop(x,y) :- Hub(x), Hub(y)\n",
      ". . .\n",
      ". . .\n",
      ". . .\n",
      "Figure 7.2: Part of a Reﬁnement Graph\n",
      "Initialize Ξcur := Ξ.\n",
      "Initialize π := empty set of clauses.\n",
      "repeat [The outer loop works to make π suﬃcient.]\n",
      "Initialize c := ρ : −.\n",
      "repeat [The inner loop makes c necessary.]\n",
      "Select a literal l to add to c. [This is a nondeterministic choice point.]\n",
      "Assign c := c, l.\n",
      "until c is necessary. [That is, until c covers no negative instances in Ξcur.]\n",
      "Assign π := π, c. [We add the clause c to the program.]\n",
      "Assign Ξcur := Ξcur −(the positive instances in Ξcur covered by π).\n",
      "until π is suﬃcient.\n",
      "(The termination tests for the inner and outer loops can be relaxed as appro-\n",
      "priate for the case of noisy instances.)\n",
      "7.3\n",
      "An Example\n",
      "We illustrate how the algorithm works by returning to our example of airline\n",
      "ﬂights. Consider the portion of an airline route map, shown in Fig. 7.3. Cities\n",
      "A, B, and C are “hub” cities, and we know that there are nonstop ﬂights between\n",
      "all hub cities (even those not shown on this portion of the route map). The other\n",
      "\n",
      "\n",
      "487. 7.3. AN EXAMPLE\n",
      "95\n",
      "cities are “satellites” of one of the hubs, and we know that there are nonstop\n",
      "ﬂights between each satellite city and its hub. The learning program is given a\n",
      "set of positive instances, Ξ+, of pairs of cities between which there are nonstop\n",
      "ﬂights and a set of negative instances, Ξ−, of pairs of cities between which there\n",
      "are not nonstop ﬂights. Ξ+ contains just the pairs:\n",
      "{< A, B >, < A, C >, < B, C >, < B, A >, < C, A >, < C, B >,\n",
      "< A, A1 >, < A, A2 >, < A1, A >, < A2, A >, < B, B1 >, < B, B2 >,\n",
      "< B1, B >, < B2, B >, < C, C1 >, < C, C2 >, < C1, C >, < C2, C >}\n",
      "For our example, we will assume that Ξ−contains all those pairs of cities shown\n",
      "in Fig. 7.3 that are not in Ξ+ (a type of closed-world assumption). These are:\n",
      "{< A, B1 >, < A, B2 >, < A, C1 >, < A, C2 >, < B, C1 >, < B, C2 >,\n",
      "< B, A1 >, < B, A2 >, < C, A1 >, < C, A2 >, < C, B1 >, < C, B2 >,\n",
      "< B1, A >, < B2, A >, < C1, A >, < C2, A >, < C1, B >, < C2, B >,\n",
      "< A1, B >, < A2, B >, < A1, C >, < A2, C >, < B1, C >, < B2, C >}\n",
      "There may be other cities not shown on this map, so the training set does not\n",
      "necessarily exhaust all the cities.\n",
      "A\n",
      "B\n",
      "C\n",
      "C1\n",
      "C2\n",
      "B1\n",
      "B2\n",
      "A1\n",
      "A2\n",
      "Figure 7.3: Part of an Airline Route Map\n",
      "We want the learning program to induce a program for computing the value\n",
      "of the relation Nonstop. The training set, Ξ, can be thought of as a partial\n",
      "\n",
      "\n",
      "488. 96\n",
      "CHAPTER 7. INDUCTIVE LOGIC PROGRAMMING\n",
      "description of this relation in extensional form—it explicitly names some pairs\n",
      "in the relation and some pairs not in the relation.\n",
      "We desire to learn the\n",
      "Nonstop relation as a logic program in terms of the background relations, Hub\n",
      "and Satellite, which are also given in extensional form. Doing so will give us\n",
      "a more compact, intensional, description of the relation, and this description\n",
      "could well generalize usefully to other cities not mentioned in the map.\n",
      "We assume the learning program has the following extensional deﬁnitions of\n",
      "the relations Hub and Satellite:\n",
      "Hub\n",
      "{< A >, < B >, < C >}\n",
      "All other cities mentioned in the map are assumed not in the relation Hub. We\n",
      "will use the notation Hub(x) to express that the city named x is in the relation\n",
      "Hub.\n",
      "Satellite\n",
      "{< A1, A, >, < A2, A >, < B1, B >, < B2, B >, < C1, C >, < C2, C >}\n",
      "All other pairs of cities mentioned in the map are not in the relation Satellite.\n",
      "We will use the notation Satellite(x,y) to express that the pair < x, y > is\n",
      "in the relation Satellite.\n",
      "Knowing that the predicate Nonstop is a two-place predicate, the inner loop\n",
      "of our algorithm initializes the ﬁrst clause to Nonstop(x,y) :-\n",
      ". This clause\n",
      "is not necessary because it covers all the negative examples (since it covers all\n",
      "examples). So we must add a literal to its (empty) body. Suppose (selecting\n",
      "a literal from the reﬁnement graph) the algorithm adds Hub(x). The following\n",
      "positive instances in Ξ are covered by Nonstop(x,y) :- Hub(x):\n",
      "{< A, B >, < A, C >, < B, C >, < B, A >, < C, A >, < C, B >,\n",
      "< A, A1 >, < A, A2 >, < B, B1 >, < B, B2 >, < C, C1 >, < C, C2 >}\n",
      "To compute this covering, we interpret the logic program Nonstop(x,y) :-\n",
      "Hub(x) for all pairs of cities in Ξ, using the pairs given in the background\n",
      "relation Hub as ground facts. The following negative instances are also covered:\n",
      "\n",
      "\n",
      "489. 7.3. AN EXAMPLE\n",
      "97\n",
      "{< A, B1 >, < A, B2 >, < A, C1 >, < A, C2 >, < C, A1 >, < C, A2 >,\n",
      "< C, B1 >, < C, B2 >, < B, A1 >, < B, A2 >, < B, C1 >, < B, C2 >}\n",
      "Thus, the clause is not yet necessary and another literal must be added. Sup-\n",
      "pose we next add Hub(y).\n",
      "The following positive instances are covered by\n",
      "Nonstop(x,y) :- Hub(x), Hub(y):\n",
      "{< A, B >, < A, C >, < B, C >, < B, A >, < C, A >, < C, B >}\n",
      "There are no longer any negative instances in Ξ covered so the clause\n",
      "Nonstop(x,y) :- Hub(x), Hub(y) is necessary, and we can terminate the ﬁrst\n",
      "pass through the inner loop.\n",
      "But the program, π, consisting of just this clause is not suﬃcient. These\n",
      "positive instances are not covered by the clause:\n",
      "{< A, A1 >, < A, A2 >, < A1, A >, < A2, A >, < B, B1 >, < B, B2 >,\n",
      "< B1, B >, < B2, B >, < C, C1 >, < C, C2 >, < C1, C >, < C2, C >}\n",
      "The positive instances that were covered by Nonstop(x,y) :- Hub(x), Hub(y)\n",
      "are removed from Ξ to form the Ξcur to be used in the next pass through the\n",
      "inner loop. Ξcur consists of all the negative instances in Ξ plus the positive\n",
      "instances (listed above) that are not yet covered. In order to attempt to cover\n",
      "them, the inner loop creates another clause c, initially set to Nonstop(x,y)\n",
      ":- . This clause covers all the negative instances, and so we must add liter-\n",
      "als to make it necessary. Suppose we add the literal Satellite(x,y). The\n",
      "clause Nonstop(x,y) :- Satellite(x,y) covers no negative instances, so it is\n",
      "necessary. It does cover the following positive instances in Ξcur:\n",
      "{< A1, A >, < A2, A >, < B1, B >, < B2, B >, < C1, C >, < C2, C >}\n",
      "These instances are removed from Ξcur for the next pass through the inner loop.\n",
      "The program now contains two clauses:\n",
      "Nonstop(x,y) :- Hub(x), Hub(y)\n",
      ":- Satellite(x,y)\n",
      "This program is not yet suﬃcient since it does not cover the following positive\n",
      "instances:\n",
      "{< A, A1 >, < A, A2 >, < B, B1 >, < B, B2 >, < C, C1 >, < C, C2 >}\n",
      "\n",
      "\n",
      "490. 98\n",
      "CHAPTER 7. INDUCTIVE LOGIC PROGRAMMING\n",
      "During the next pass through the inner loop, we add the clause Nonstop(x,y)\n",
      ":- Satellite(y,x). This clause is necessary, and since the program containing\n",
      "all three clauses is now suﬃcient, the procedure terminates with:\n",
      "Nonstop(x,y) :- Hub(x), Hub(y)\n",
      ":- Satellite(x,y)\n",
      ":- Satellite(y,x)\n",
      "Since each clause is necessary, and the whole program is suﬃcient, the pro-\n",
      "gram is also consistent with all instances of the training set. Note that this\n",
      "program can be applied (perhaps with good generalization) to other cities be-\n",
      "sides those in our partial map—so long as we can evaluate the relations Hub and\n",
      "Satellite for these other cities. In the next section, we show how the technique\n",
      "can be extended to use recursion on the relation we are inducing. With that\n",
      "extension, the method can be used to induce more general logic programs.\n",
      "7.4\n",
      "Inducing Recursive Programs\n",
      "To induce a recursive program, we allow the addition of a literal having the\n",
      "same predicate letter as that in the head of the clause. Various mechanisms\n",
      "must be used to ensure that such a program will terminate; one such is to make\n",
      "sure that the new literal has diﬀerent variables than those in the head literal.\n",
      "The process is best illustrated with another example. Our example continues\n",
      "the one using the airline map, but we make the map somewhat simpler in order\n",
      "to reduce the size of the extensional relations used. Consider the map shown\n",
      "in Fig. 7.4. Again, B and C are hub cities, B1 and B2 are satellites of B, C1\n",
      "and C2 are satellites of C. We have introduced two new cities, B3 and C3. No\n",
      "ﬂights exist between these cities and any other cities—perhaps there are only\n",
      "bus routes as shown by the grey lines in the map.\n",
      "We now seek to learn a program for Canfly(x,y) that covers only those\n",
      "pairs of cities that can be reached by one or more nonstop ﬂights. The relation\n",
      "Canfly is satisﬁed by the following pairs of postive instances:\n",
      "{< B1, B >, < B1, B2 >, < B1, C >, < B1, C1 >, < B1, C2 >,\n",
      "< B, B1 >, < B2, B1 >, < C, B1 >, < C1, B1 >, < C2, B1 >,\n",
      "< B2, B >, < B2, C >, < B2, C1 >, < B2, C2 >, < B, B2 >,\n",
      "< C, B2 >, < C1, B2 >, < C2, B2 >, < B, C >, < B, C1 >,\n",
      "< B, C2 >, < C, B >, < C1, B >, < C2, B >, < C, C1 >,\n",
      "< C, C2 >, < C1, C >, < C2, C >, < C1, C2 >, < C2, C1 >}\n",
      "\n",
      "\n",
      "491. 7.4. INDUCING RECURSIVE PROGRAMS\n",
      "99\n",
      "B\n",
      "C\n",
      "C1\n",
      "C2\n",
      "B1\n",
      "B2\n",
      "B3\n",
      "C3\n",
      "Figure 7.4: Another Airline Route Map\n",
      "Using a closed-world assumption on our map, we take the negative instances of\n",
      "Canfly to be:\n",
      "{< B3, B2 >, < B3, B >, < B3, B1 >, < B3, C >, < B3, C1 >,\n",
      "< B3, C2 >, < B3, C3 >, < B2, B3 >, < B, B3 >, < B1, B3 >,\n",
      "< C, B3 >, < C1, B3 >, < C2, B3 >, < C3, B3 >, < C3, B2 >,\n",
      "< C3, B >, < C3, B1 >, < C3, C >, < C3, C1 >, < C3, C2 >,\n",
      "< B2, C3 >, < B, C3 >, < B1, C3 >, < C, C3 >, < C1, C3 >,\n",
      "< C2, C3 >}\n",
      "We will induce Canfly(x,y) using the extensionally deﬁned background\n",
      "relation Nonstop given earlier (modiﬁed as required for our reduced airline map)\n",
      "and Canfly itself (recursively).\n",
      "As before, we start with the empty program and proceed to the inner loop\n",
      "to construct a clause that is necessary. Suppose that the inner loop adds the\n",
      "background literal Nonstop(x,y). The clause Canfly(x,y) :- Nonstop(x,y)\n",
      "is necessary; it covers no negative instances. But it is not suﬃcient because it\n",
      "does not cover the following positive instances:\n",
      "{< B1, B2 >, < B1, C >, < B1, C1 >, < B1, C2 >, < B2, B1 >,\n",
      "< C, B1 >, < C1, B1 >, < C2, B1 >, < B2, C >, < B2, C1 >,\n",
      "< B2, C2 >, < C, B2 >, < C1, B2 >, < C2, B2 >, < B, C1 >,\n",
      "\n",
      "\n",
      "492. 100\n",
      "CHAPTER 7. INDUCTIVE LOGIC PROGRAMMING\n",
      "< B, C2 >, < C1, B >, < C2, B >, < C1, C2 >, < C2, C1 >}\n",
      "Thus, we must add another clause to the program. In the inner loop, we ﬁrst\n",
      "create the clause Canfly(x,y) :- Nonstop(x,z) which introduces the new\n",
      "variable z. We digress brieﬂy to describe how a program containing a clause\n",
      "with unbound variables in its body is interpreted.\n",
      "Suppose we try to inter-\n",
      "pret it for the positive instance Canfly(B1,B2). The interpreter attempts to\n",
      "establish Nonstop(B1,z) for some z. Since Nonstop(B1, B), for example, is\n",
      "a background fact, the interpreter returns T—which means that the instance\n",
      "< B1, B2 > is covered.\n",
      "Suppose now, we attempt to interpret the clause\n",
      "for the negative instance Canfly(B3,B). The interpreter attempts to estab-\n",
      "lish Nonstop(B3,z) for some z. There are no background facts that match, so\n",
      "the clause does not cover < B3, B >. Using the interpreter, we see that the\n",
      "clause Canfly(x,y) :- Nonstop(x,z) covers all of the positive instances not\n",
      "already covered by the ﬁrst clause, but it also covers many negative instances\n",
      "such as < B2, B3 >, and < B, B3 >. So the inner loop must add another literal.\n",
      "This time, suppose it adds Canfly(z,y) to yield the clause Canfly(x,y) :-\n",
      "Nonstop(x,z), Canfly(z,y). This clause is necessary; no negative instances\n",
      "are covered. The program is now suﬃcient and consistent; it is:\n",
      "Canfly(x,y) :- Nonstop(x,y)\n",
      ":- Nonstop(x,z), Canfly(z,y)\n",
      "7.5\n",
      "Choosing Literals to Add\n",
      "One of the ﬁrst practical ILP systems was Quinlan’s FOIL [Quinlan, 1990]. A\n",
      "major problem involves deciding how to select a literal to add in the inner loop\n",
      "(from among the literals that are allowed). In FOIL, Quinlan suggested that\n",
      "candidate literals can be compared using an information-like measure—similar\n",
      "to the measures used in inducing decision trees. A measure that gives the same\n",
      "comparison as does Quinlan’s is based on the amount by which adding a literal\n",
      "increases the odds that an instance drawn at random from those covered by the\n",
      "new clause is a positive instance beyond what these odds were before adding\n",
      "the literal.\n",
      "Let p be an estimate of the probability that an instance drawn at random\n",
      "from those covered by a clause before adding the literal is a positive instance.\n",
      "That is, p =(number of positive instances covered by the clause)/(total number\n",
      "of instances covered by the clause). It is convenient to express this probability\n",
      "in “odds form.” The odds, o, that a covered instance is positive is deﬁned to\n",
      "be o = p/(1 −p). Expressing the probability in terms of the odds, we obtain\n",
      "p = o/(1 + o).\n",
      "\n",
      "\n",
      "493. 7.6. RELATIONSHIPS BETWEEN ILP AND DECISION TREE INDUCTION101\n",
      "After selecting a literal, l, to add to a clause, some of the instances previously\n",
      "covered are still covered; some of these are positive and some are negative. Let\n",
      "pl denote the probability that an instance drawn at random from the instances\n",
      "covered by the new clause (with l added) is positive. The odds will be denoted\n",
      "by ol.\n",
      "We want to select a literal, l, that gives maximal increase in these\n",
      "odds.\n",
      "That is, if we deﬁne λl = ol/o, we want a literal that gives a high\n",
      "value of λl. Specializing the clause in such a way that it fails to cover many of\n",
      "the negative instances previously covered but still covers most of the positive\n",
      "instances previously covered will result in a high value of λl. (It turns out that\n",
      "the value of Quinlan’s information theoretic measure increases monotonically\n",
      "with λl, so we could just as well use the latter instead.)\n",
      "Besides ﬁnding a literal with a high value of λl, Quinlan’s FOIL system also\n",
      "restricts the choice to literals that:\n",
      "a) contain at least one variable that has already been used,\n",
      "b) place further restrictions on the variables if the literal selected has the\n",
      "same predicate letter as the literal being induced (in order to prevent inﬁnite\n",
      "recursion), and\n",
      "c) survive a pruning test based on the values of λl for those literals selected\n",
      "so far.\n",
      "We refer the reader to Quinlan’s paper for further discussion of these points.\n",
      "Quinlan also discusses post-processing pruning methods and presents experi-\n",
      "mental results of the method applied to learning recursive relations on lists, on\n",
      "learning rules for chess endgames and for the card game Eleusis, and for some\n",
      "other standard tasks mentioned in the machine learning literature.\n",
      "The\n",
      "reader\n",
      "should\n",
      "also\n",
      "refer\n",
      "to\n",
      "[Pazzani & Kibler, 1992,\n",
      "Lavraˇc & Dˇzeroski, 1994, Muggleton, 1991, Muggleton, 1992].\n",
      "Discuss preprocessing,\n",
      "postprocessing, bottom-up\n",
      "methods, and LINUS.\n",
      "7.6\n",
      "Relationships Between ILP and Decision\n",
      "Tree Induction\n",
      "The generic ILP algorithm can also be understood as a type of decision tree\n",
      "induction.\n",
      "Recall the problem of inducing decision trees when the values of\n",
      "attributes are categorical.\n",
      "When splitting on a single variable, the split at\n",
      "each node involves asking to which of several mutually exclusive and exhaustive\n",
      "subsets the value of a variable belongs. For example, if a node tested the variable\n",
      "xi, and if xi could have values drawn from {A, B, C, D, E, F}, then one possible\n",
      "split (among many) might be according to whether the value of xi had as value\n",
      "one of {A, B, C} or one of {D, E, F}.\n",
      "It is also possible to make a multi-variate split—testing the values of two or\n",
      "more variables at a time. With categorical variables, an n-variable split would\n",
      "be based on which of several n-ary relations the values of the variables satisﬁed.\n",
      "For example, if a node tested the variables xi and xj, and if xi and xj both\n",
      "could have values drawn from {A, B, C, D, E, F}, then one possible binary split\n",
      "\n",
      "\n",
      "494. 102\n",
      "CHAPTER 7. INDUCTIVE LOGIC PROGRAMMING\n",
      "(among many) might be according to whether or not < xi, xj > satisﬁed the\n",
      "relation {< A, C >, < C, D >}. (Note that our subset method of forming single-\n",
      "variable splits could equivalently have been framed using 1-ary relations—which\n",
      "are usually called properties.)\n",
      "In this framework, the ILP problem is as follows: We are given a training set,\n",
      "Ξ, of positively and negatively labeled patterns whose components are drawn\n",
      "from a set of variables {x, y, z, . . .}. The positively labeled patterns in Ξ form an\n",
      "extensional deﬁnition of a relation, R. We are also given background relations,\n",
      "R1, . . . , Rk, on various subsets of these variables. (That is, we are given sets\n",
      "of tuples that are in these relations.)\n",
      "We desire to construct an intensional\n",
      "deﬁnition of R in terms of the R1, . . . , Rk, such that all of the positively labeled\n",
      "patterns in Ξ are satisﬁed by R and none of the negatively labeled patterns\n",
      "are. The intensional deﬁnition will be in terms of a logic program in which the\n",
      "relation R is the head of a set of clauses whose bodies involve the background\n",
      "relations.\n",
      "The generic ILP algorithm can be understood as decision tree induction,\n",
      "where each node of the decision tree is itself a sub-decision tree, and each sub-\n",
      "decision tree consists of nodes that make binary splits on several variables using\n",
      "the background relations, Ri. Thus we will speak of a top-level decision tree\n",
      "and various sub-decision trees. (Actually, our decision trees will be decision\n",
      "lists—a special case of decision trees, but we will refer to them as trees in our\n",
      "discussions.)\n",
      "In broad outline, the method for inducing an intensional version of the rela-\n",
      "tion R is illustrated by considering the decision tree shown in Fig. 7.5. In this\n",
      "diagram, the patterns in Ξ are ﬁrst ﬁltered through the decision tree in top-\n",
      "level node 1. The background relation R1 is satisﬁed by some of these patterns;\n",
      "these are ﬁltered to the right (to relation R2), and the rest are ﬁltered to the\n",
      "left (more on what happens to these later). Right-going patterns are ﬁltered\n",
      "through a sequence of relational tests until only positively labeled patterns sat-\n",
      "isfy the last relation—in this case R3. That is, the subset of patterns satisfying\n",
      "all the relations, R1, R2, and R3 contains only positive instances from Ξ. (We\n",
      "might say that this combination of tests is necessary. They correspond to the\n",
      "clause created in the ﬁrst pass through the inner loop of the generic ILP algo-\n",
      "rithm.) Let us call the subset of patterns satisfying these relations, Ξ1; these\n",
      "satisfy Node 1 at the top level. All other patterns, that is {Ξ −Ξ1} = Ξ2 are\n",
      "ﬁltered to the left by Node 1.\n",
      "Ξ2 is then ﬁltered by top-level Node 2 in much the same manner, so that\n",
      "Node 2 is satisﬁed only by the positively labeled samples in Ξ2. We continue\n",
      "ﬁltering through top-level nodes until only the negatively labeled patterns fail to\n",
      "satisfy a top node. In our example, Ξ4 contains only negatively labeled patterns\n",
      "and the union of Ξ1 and Ξ3 contains all the positively labeled patterns. The\n",
      "relation, R, that distinguishes positive from negative patterns in Ξ is then given\n",
      "in terms of the following logic program:\n",
      "R :- R1, R2, R3\n",
      "\n",
      "\n",
      "495. 7.6. RELATIONSHIPS BETWEEN ILP AND DECISION TREE INDUCTION103\n",
      "R1\n",
      "R2\n",
      "R3\n",
      "T\n",
      "T\n",
      "T\n",
      "F\n",
      "F\n",
      "F\n",
      "T\n",
      "F\n",
      "R4\n",
      "R5\n",
      "T\n",
      "T\n",
      "F\n",
      "F\n",
      "T\n",
      "F\n",
      "U\n",
      "U1\n",
      "U2 = U < U1\n",
      "U3\n",
      "U4= U2 < U3\n",
      "Node 1\n",
      "Node 2\n",
      "(only positive\n",
      "instances\n",
      "satisfy all three\n",
      "tests)\n",
      "(only positivel\n",
      "instances satisfy\n",
      "these two tests)\n",
      "(only negative\n",
      "instances)\n",
      "Figure 7.5: A Decision Tree for ILP\n",
      ":- R4, R5\n",
      "If we apply this sort of decision-tree induction procedure to the problem\n",
      "of generating a logic program for the relation Nonstop (refer to Fig. 7.3), we\n",
      "obtain the decision tree shown in Fig. 7.6. The logic program resulting from\n",
      "this decision tree is the same as that produced by the generic ILP algorithm.\n",
      "In setting up the problem, the training set, Ξ can be expressed as a set of 2-\n",
      "dimensional vectors with components x and y. The values of these components\n",
      "range over the cities {A, B, C, A1, A2, B1, B2, C1, C2} except (for simplicity)\n",
      "we do not allow patterns in which x and y have the same value. As before, the\n",
      "relation, Nonstop, contains the following pairs of cities, which are the positive\n",
      "instances:\n",
      "{< A, B >, < A, C >, < B, C >, < B, A >, < C, A >, < C, B >,\n",
      "< A, A1 >, < A, A2 >, < A1, A >, < A2, A >, < B, B1 >, < B, B2 >,\n",
      "< B1, B >, < B2, B >, < C, C1 >, < C, C2 >, < C1, C >, < C2, C >}\n",
      "All other pairs of cities named in the map of Fig. 7.3 (using the closed world\n",
      "assumption) are not in the relation Nonstop and thus are negative instances.\n",
      "Because the values of x and y are categorical, decision-tree induction would\n",
      "be a very diﬃcult task—involving as it does the need to invent relations on\n",
      "\n",
      "\n",
      "496. 104\n",
      "CHAPTER 7. INDUCTIVE LOGIC PROGRAMMING\n",
      "x and y to be used as tests. But with the background relations, Ri (in this\n",
      "case Hub and Satellite), the problem is made much easier. We select these\n",
      "relations in the same way that we select literals; from among the available tests,\n",
      "we make a selection based on which leads to the largest value of λRi.\n",
      "7.7\n",
      "Bibliographical and Historical Remarks\n",
      "To be added.\n",
      "\n",
      "\n",
      "497. 7.7. BIBLIOGRAPHICAL AND HISTORICAL REMARKS\n",
      "105\n",
      "Hub(x)\n",
      "T\n",
      "F\n",
      "U\n",
      "Node 1\n",
      "(top level)\n",
      "{<A,B>, <A,C>,\n",
      "<B,C>, <B,A>,\n",
      "<C,A>, <C,B>}\n",
      "Hub(y)\n",
      "T\n",
      "T\n",
      "F\n",
      "Node 2\n",
      "(top level)\n",
      "Satellite(x,y)\n",
      "F\n",
      "T\n",
      "T\n",
      "{<A1,A>, <A2,A>, <B1,B>,\n",
      "<B2,B>, <C1,C>, <C2,C>}\n",
      "F\n",
      "{<A,A1>, <A,A2>,<B,B1>,\n",
      "<B,B2>,  <C,C1>, <C,C2>}\n",
      "Satellite(y,x)\n",
      "F\n",
      "F\n",
      "T\n",
      "Node 3\n",
      "(top level)\n",
      "T\n",
      "{Only negative instances}\n",
      "(Only positive instances)\n",
      "(Only positive instances)\n",
      "(Only positive instances)\n",
      "F\n",
      "Figure 7.6: A Decision Tree for the Airline Route Problem\n",
      "\n",
      "\n",
      "498. 106\n",
      "CHAPTER 7. INDUCTIVE LOGIC PROGRAMMING\n",
      "\n",
      "\n",
      "499. Chapter 8\n",
      "Computational Learning\n",
      "Theory\n",
      "In chapter one we posed the problem of guessing a function given a set of\n",
      "sample inputs and their values. We gave some intuitive arguments to support\n",
      "the claim that after seeing only a small fraction of the possible inputs (and\n",
      "their values) that we could guess almost correctly the values of most subsequent\n",
      "inputs—if we knew that the function we were trying to guess belonged to an\n",
      "appropriately restricted subset of functions. That is, a given training set of\n",
      "sample patterns might be adequate to allow us to select a function, consistent\n",
      "with the labeled samples, from among a restricted set of hypotheses such that\n",
      "with high probability the function we select will be approximately correct (small\n",
      "probability of error) on subsequent samples drawn at random according to the\n",
      "same distribution from which the labeled samples were drawn.\n",
      "This insight\n",
      "led to the theory of probably approximately correct (PAC) learning—initially\n",
      "developed by Leslie Valiant [Valiant, 1984]. We present here a brief description\n",
      "of the theory for the case of Boolean functions. [Dietterich, 1990, Haussler, 1988,\n",
      "Haussler, 1990] give nice surveys of the important results.\n",
      "Other overviews?\n",
      "8.1\n",
      "Notation and Assumptions for PAC Learn-\n",
      "ing Theory\n",
      "We assume a training set Ξ of n-dimensional vectors, Xi, i = 1, . . . , m, each\n",
      "labeled (by 1 or 0) according to a target function, f, which is unknown to\n",
      "the learner. The probability of any given vector X being in Ξ, or later being\n",
      "presented to the learner, is P(X).\n",
      "The probability distribution, P, can be\n",
      "arbitrary. (In the literature of PAC learning theory, the target function is usually\n",
      "called the target concept and is denoted by c, but to be consistent with our\n",
      "previous notation we will continue to denote it by f.) Our problem is to guess\n",
      "107\n",
      "\n",
      "\n",
      "500. 108\n",
      "CHAPTER 8. COMPUTATIONAL LEARNING THEORY\n",
      "a function, h(X), based on the labeled samples in Ξ. In PAC theory such a\n",
      "guessed function is called the hypothesis. We assume that the target function\n",
      "is some element of a set of functions, C. We also assume that the hypothesis,\n",
      "h, is an element of a set, H, of hypotheses, which includes the set, C, of target\n",
      "functions. H is called the hypothesis space.\n",
      "In general, h won’t be identical to f, but we can strive to have the value of\n",
      "h(X) = the value of f(X) for most X’s. That is, we want h to be approximately\n",
      "correct. To quantify this notion, we deﬁne the error of h, εh, as the probability\n",
      "that an X drawn randomly according to P will be misclassiﬁed:\n",
      "εh =\n",
      "X\n",
      "[X:h(X)̸=f(X)]\n",
      "P(X)\n",
      "Boldface symbols need to be\n",
      "smaller when they are subscripts in\n",
      "math environments.\n",
      "We say that h is approximately (except for ε ) correct if εh ≤ε, where ε is the\n",
      "accuracy parameter.\n",
      "Suppose we are able to ﬁnd an h that classiﬁes all m randomly drawn training\n",
      "samples correctly; that is, h is consistent with this randomly selected training\n",
      "set, Ξ.\n",
      "If m is large enough, will such an h be approximately correct (and\n",
      "for what value of ε)? On some training occasions, using m randomly drawn\n",
      "training samples, such an h might turn out to be approximately correct (for a\n",
      "given value of ε), and on others it might not. We say that h is probably (except\n",
      "for δ) approximately correct (PAC) if the probability that it is approximately\n",
      "correct is greater than 1−δ, where δ is the conﬁdence parameter. We shall show\n",
      "that if m is greater than some bound whose value depends on ε and δ, such an\n",
      "h is guaranteed to be probably approximately correct.\n",
      "In general, we say that a learning algorithm PAC-learns functions from C in\n",
      "terms of H iﬀfor every function fϵ C, it outputs a hypothesis hϵ H, such that\n",
      "with probability at least (1 −δ), εh ≤ε. Such a hypothesis is called probably\n",
      "(except for δ) approximately (except for ε) correct.\n",
      "We want learning algorithms that are tractable, so we want an algorithm\n",
      "that PAC-learns functions in polynomial time. This can only be done for certain\n",
      "classes of functions. If there are a ﬁnite number of hypotheses in a hypothesis\n",
      "set (as there are for many of the hypothesis sets we have considered), we could\n",
      "always produce a consistent hypothesis from this set by testing all of them\n",
      "against the training data. But if there are an exponential number of hypotheses,\n",
      "that would take exponential time.\n",
      "We seek training methods that produce\n",
      "consistent hypotheses in less time. The time complexities for various hypothesis\n",
      "sets have been determined, and these are summarized in a table to be presented\n",
      "later.\n",
      "A class, C, is polynomially PAC learnable in terms of H provided there exists\n",
      "a polynomial-time learning algorithm (polynomial in the number of samples\n",
      "needed, m, in the dimension, n, in 1/ε, and in 1/δ) that PAC-learns functions\n",
      "in C in terms of H.\n",
      "Initial work on PAC assumed H = C, but it was later shown that some func-\n",
      "tions cannot be polynomially PAC-learned under such an assumption (assuming\n",
      "\n",
      "\n",
      "501. 8.2. PAC LEARNING\n",
      "109\n",
      "P ̸= NP)—but can be polynomially PAC-learned if H is a strict superset of C!\n",
      "Also our deﬁnition does not specify the distribution, P, from which patterns\n",
      "are drawn nor does it say anything about the properties of the learning algo-\n",
      "rithm. Since C and H do not have to be identical, we have the further restrictive\n",
      "deﬁnition:\n",
      "A properly PAC-learnable class is a class C for which there exists an algorithm\n",
      "that polynomially PAC-learns functions from C in terms of C.\n",
      "8.2\n",
      "PAC Learning\n",
      "8.2.1\n",
      "The Fundamental Theorem\n",
      "Suppose our learning algorithm selects some h randomly from among those that\n",
      "are consistent with the values of f on the m training patterns. The probability\n",
      "that the error of this randomly selected h is greater than some ε, with h consis-\n",
      "tent with the values of f(X) for m instances of X (drawn according to arbitrary\n",
      "P), is less than or equal to |H|e−εm, where |H| is the number of hypotheses in\n",
      "H. We state this result as a theorem [Blumer, et al., 1987]:\n",
      "Theorem 8.1 (Blumer, et al.) Let H be any set of hypotheses, Ξ be a set of\n",
      "m ≥1 training examples drawn independently according to some distribution\n",
      "P, f be any classiﬁcation function in H, and ε > 0. Then, the probability that\n",
      "there exists a hypothesis h consistent with f for the members of Ξ but with error\n",
      "greater than ε is at most |H|e−εm.\n",
      "Proof:\n",
      "Consider the set of all hypotheses, {h1, h2, . . . , hi, . . . , hS}, in H, where S =\n",
      "|H|. The error for hi is εhi= the probability that hi will classify a pattern in\n",
      "error (that is, diﬀerently than f would classify it). The probability that hi will\n",
      "classify a pattern correctly is (1−εhi). A subset, HB, of H will have error greater\n",
      "than ε. We will call the hypotheses in this subset bad. The probability that any\n",
      "particular one of these bad hypotheses, say hb, would classify a pattern correctly\n",
      "is (1−εhb). Since εhb > ε, the probability that hb (or any other bad hypothesis)\n",
      "would classify a pattern correctly is less than (1 −ε). The probability that it\n",
      "would classify all m independently drawn patterns correctly is then less than\n",
      "(1 −ε)m.\n",
      "That is,\n",
      "prob[hb classiﬁes all m patterns correctly |hb ϵ HB] ≤(1 −ε)m.\n",
      "prob[some h ϵ HB classiﬁes all m patterns correctly]\n",
      "= P\n",
      "hb ϵ HB prob[hb classiﬁes all m patterns correctly |hb ϵ HB]\n",
      "≤K(1 −ε)m, where K = |HB|.\n",
      "\n",
      "\n",
      "502. 110\n",
      "CHAPTER 8. COMPUTATIONAL LEARNING THEORY\n",
      "That is,\n",
      "prob[there is a bad hypothesis that classiﬁes all m patterns correctly]\n",
      "≤K(1 −ε)m.\n",
      "Since K ≤|H| and (1 −ε)m ≤e−εm, we have:\n",
      "prob[there is a bad hypothesis that classiﬁes all m patterns correctly]\n",
      "= prob[there is a hypothesis with error > ε and that classiﬁes all m patterns\n",
      "correctly] ≤|H|e−εm.\n",
      "QED\n",
      "A corollary of this theorem is:\n",
      "Corollary 8.2 Given m ≥(1/ε)(ln |H| + ln(1/δ)) independent samples, the\n",
      "probability that there exists a hypothesis in H that is consistent with f on these\n",
      "samples and has error greater than ε is at most δ.\n",
      "Proof: We are to ﬁnd a bound on m that guarantees that\n",
      "prob[there is a hypothesis with error > ε and that classiﬁes all m patterns\n",
      "correctly] ≤δ.\n",
      "Thus, using the result of the theorem, we must show that\n",
      "|H|e−εm ≤δ. Taking the natural logarithm of both sides yields:\n",
      "ln |H| −εm ≤ln δ\n",
      "or\n",
      "m ≥(1/ε)(ln |H| + ln(1/δ))\n",
      "QED\n",
      "This corollary is important for two reasons. First it clearly states that we\n",
      "can select any hypothesis consistent with the m samples and be assured that\n",
      "with probability (1 −δ) its error will be less than ε. Also, it shows that in\n",
      "order for m to increase no more than polynomially with n, |H| can be no larger\n",
      "than 2O(nk). No class larger than that can be guaranteed to be properly PAC\n",
      "learnable.\n",
      "Here is a possible point of confusion: The bound given in the corollary is\n",
      "an upper bound on the value of m needed to guarantee polynomial probably ap-\n",
      "proximately correct learning. Values of m greater than that bound are suﬃcient\n",
      "(but might not be necessary). We will present a lower (necessary) bound later\n",
      "in the chapter.\n",
      "\n",
      "\n",
      "503. 8.2. PAC LEARNING\n",
      "111\n",
      "8.2.2\n",
      "Examples\n",
      "Terms\n",
      "Let H be the set of terms (conjunctions of literals). Then, |H| = 3n, and\n",
      "m ≥(1/ε)(ln(3n) + ln(1/δ))\n",
      "≥(1/ε)(1.1n + ln(1/δ))\n",
      "Note that the bound on m increases only polynomially with n, 1/ε, and 1/δ.\n",
      "For n = 50, ε = 0.01 and δ = 0.01, m ≥5, 961 guarantees PAC learnability.\n",
      "In order to show that terms are properly PAC learnable, we additionally\n",
      "have to show that one can ﬁnd in time polynomial in m and n a hypothesis\n",
      "h consistent with a set of m patterns labeled by the value of a term.\n",
      "The\n",
      "following procedure for ﬁnding such a consistent hypothesis requires O(nm)\n",
      "steps (adapted from [Dietterich, 1990, page 268]):\n",
      "We are given a training sequence, Ξ, of m examples. Find the ﬁrst pattern,\n",
      "say X1, in that list that is labeled with a 1.\n",
      "Initialize a Boolean function,\n",
      "h, to the conjunction of the n literals corresponding to the values of the n\n",
      "components of X1. (Components with value 1 will have corresponding positive\n",
      "literals; components with value 0 will have corresponding negative literals.) If\n",
      "there are no patterns labeled by a 1, we exit with the null concept (h ≡0 for\n",
      "all patterns). Then, for each additional pattern, Xi, that is labeled with a 1,\n",
      "we delete from h any Boolean variables appearing in Xi with a sign diﬀerent\n",
      "from their sign in h. After processing all the patterns labeled with a 1, we check\n",
      "all of the patterns labeled with a 0 to make sure that none of them is assigned\n",
      "value 1 by h. If, at any stage of the algorithm, any patterns labeled with a 0\n",
      "are assigned a 1 by h, then there exists no term that consistently classiﬁes the\n",
      "patterns in Ξ, and we exit with failure. Otherwise, we exit with h.\n",
      "Change this paragraph if this\n",
      "algorithm was presented in Chapter\n",
      "Three.\n",
      "As an example, consider the following patterns, all labeled with a 1 (from\n",
      "[Dietterich, 1990]):\n",
      "(0, 1, 1, 0)\n",
      "(1, 1, 1, 0)\n",
      "(1, 1, 0, 0)\n",
      "After processing the ﬁrst pattern, we have h = x1x2x3x4; after processing the\n",
      "second pattern, we have h = x2x3x4; ﬁnally, after the third pattern, we have\n",
      "h = x2x4.\n",
      "Linearly Separable Functions\n",
      "Let H be the set of all linearly separable functions. Then, |H| ≤2n2, and\n",
      "\n",
      "\n",
      "504. 112\n",
      "CHAPTER 8. COMPUTATIONAL LEARNING THEORY\n",
      "m ≥(1/ε)\n",
      "\u0000n2 ln 2 + ln(1/δ)\n",
      "\u0001\n",
      "Again, note that the bound on m increases only polynomially with n, 1/ε, and\n",
      "1/δ.\n",
      "For n = 50, ε = 0.01 and δ = 0.01, m ≥173, 748 guarantees PAC learnabil-\n",
      "ity.\n",
      "To show that linearly separable functions are properly PAC learnable, we\n",
      "would have additionally to show that one can ﬁnd in time polynomial in m and\n",
      "n a hypothesis h consistent with a set of m labeled linearly separable patterns.\n",
      "Linear programming is polynomial.\n",
      "8.2.3\n",
      "Some Properly PAC-Learnable Classes\n",
      "Some properly PAC-learnable classes of functions are given in the following\n",
      "table. (Adapted from [Dietterich, 1990,\n",
      "pages 262 and 268] which also gives\n",
      "references to proofs of some of the time complexities.)\n",
      "H\n",
      "|H|\n",
      "Time Complexity\n",
      "P. Learnable?\n",
      "terms\n",
      "3n\n",
      "polynomial\n",
      "yes\n",
      "k-term DNF\n",
      "2O(kn)\n",
      "NP-hard\n",
      "no\n",
      "(k disjunctive terms)\n",
      "k-DNF\n",
      "2O(nk)\n",
      "polynomial\n",
      "yes\n",
      "(a disjunction of k-sized terms)\n",
      "k-CNF\n",
      "2O(nk)\n",
      "polynomial\n",
      "yes\n",
      "(a conjunction of k-sized clauses)\n",
      "k-DL\n",
      "2O(nkk lg n)\n",
      "polynomial\n",
      "yes\n",
      "(decision lists with k-sized terms)\n",
      "lin. sep.\n",
      "2O(n2)\n",
      "polynomial\n",
      "yes\n",
      "lin. sep. with (0,1) weights\n",
      "?\n",
      "NP-hard\n",
      "no\n",
      "k-2NN\n",
      "?\n",
      "NP-hard\n",
      "no\n",
      "DNF\n",
      "22n\n",
      "polynomial\n",
      "no\n",
      "(all Boolean functions)\n",
      "(Members of the class k-2NN are two-layer, feedforward neural networks with\n",
      "exactly k hidden units and one output unit.)\n",
      "Summary:\n",
      "In order to show that a class of functions is Properly PAC-\n",
      "Learnable :\n",
      "a. Show that there is an algorithm that produces a consistent hypothesis on\n",
      "m n-dimensional samples in time polynomial in m and n.\n",
      "b. Show that the sample size, m, needed to ensure PAC learnability is polyno-\n",
      "mial (or better) in (1/ε), (1/δ), and n by showing that ln |H| is polynomial\n",
      "or better in the number of dimensions.\n",
      "\n",
      "\n",
      "505. 8.3. THE VAPNIK-CHERVONENKIS DIMENSION\n",
      "113\n",
      "As hinted earlier, sometimes enlarging the class of hypotheses makes learning\n",
      "easier. For example, the table above shows that k-CNF is PAC learnable, but\n",
      "k-term-DNF is not. And yet, k-term-DNF is a subclass of k-CNF! So, even if\n",
      "the target function were in k-term-DNF, one would be able to ﬁnd a hypothesis\n",
      "in k-CNF that is probably approximately correct for the target function. Sim-\n",
      "ilarly, linearly separable functions implemented by TLUs whose weight values\n",
      "are restricted to 0 and 1 are not properly PAC learnable, whereas unrestricted\n",
      "linearly separable functions are. It is possible that enlarging the space of hy-\n",
      "potheses makes ﬁnding one that is consistent with the training examples easier.\n",
      "An interesting question is whether or not the class of functions in k-2NN is poly-\n",
      "nomially PAC learnable if the hypotheses are drawn from k′-2NN with k′ > k.\n",
      "(At the time of writing, this matter is still undecided.)\n",
      "Although PAC learning theory is a powerful analytic tool, it (like complexity\n",
      "theory) deals mainly with worst-case results. The fact that the class of two-\n",
      "layer, feedforward neural networks is not polynomially PAC learnable is more an\n",
      "attack on the theory than it is on the networks, which have had many successful\n",
      "applications. As [Baum, 1994, page 416-17] says: “ . . . humans are capable of\n",
      "learning in the natural world. Therefore, a proof within some model of learning\n",
      "that learning is not feasible is an indictment of the model. We should examine\n",
      "the model to see what constraints can be relaxed and made more realistic.”\n",
      "8.3\n",
      "The Vapnik-Chervonenkis Dimension\n",
      "8.3.1\n",
      "Linear Dichotomies\n",
      "Consider a set, H, of functions, and a set, Ξ, of (unlabeled) patterns.\n",
      "One\n",
      "measure of the expressive power of a set of hypotheses, relative to Ξ, is its\n",
      "ability to make arbitrary classiﬁcations of the patterns in Ξ.1 If there are m\n",
      "patterns in Ξ, there are 2m diﬀerent ways to divide these patterns into two\n",
      "disjoint and exhaustive subsets. We say there are 2m diﬀerent dichotomies of\n",
      "Ξ. If Ξ were to include all of the 2n Boolean patterns, for example, there are\n",
      "22n ways to dichotomize them, and (of course) the set of all possible Boolean\n",
      "functions dichotomizes them in all of these ways. But a subset, H, of the Boolean\n",
      "functions might not be able to dichotomize an arbitrary set, Ξ, of m Boolean\n",
      "patterns in all 2m ways. In general (that is, even in the non-Boolean case), we\n",
      "say that if a subset, H, of functions can dichotomize a set, Ξ, of m patterns in\n",
      "all 2m ways, then H shatters Ξ.\n",
      "As an example, consider a set Ξ of m patterns in the n-dimensional space,\n",
      "Rn. (That is, the n components of these patterns are real numbers.) We deﬁne\n",
      "a linear dichotomy as one implemented by an (n−1)-dimensional hyperplane in\n",
      "the n-dimensional space. How many linear dichotomies of m patterns in n di-\n",
      "mensions are there? For example, as shown in Fig. 8.1, there are 14 dichotomies\n",
      "1And, of course, if a hypothesis drawn from a set that could make arbitrary classiﬁcations\n",
      "of a set of training patterns, there is little likelihood that such a hypothesis will generalize\n",
      "well beyond the training set.\n",
      "\n",
      "\n",
      "506. 114\n",
      "CHAPTER 8. COMPUTATIONAL LEARNING THEORY\n",
      "of four points in two dimensions (each separating line yields two dichotomies\n",
      "depending on whether the points on one side of the line are classiﬁed as 1 or 0).\n",
      "(Note that even though there are an inﬁnite number of hyperplanes, there are,\n",
      "nevertheless, only a ﬁnite number of ways in which hyperplanes can dichotomize\n",
      "a ﬁnite number of patterns. Small movements of a hyperplane typically do not\n",
      "change the classiﬁcations of any patterns.)\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "14 dichotomies of 4 points in 2 dimensions\n",
      "5\n",
      "6\n",
      "7\n",
      "Figure 8.1: Dichotomizing Points in Two Dimensions\n",
      "The number of dichotomies achievable by hyperplanes depends on how the\n",
      "patterns are disposed.\n",
      "For the maximum number of linear dichotomies, the\n",
      "points must be in what is called general position. For m > n, we say that a set\n",
      "of m points is in general position in an n-dimensional space if and only if no\n",
      "subset of (n+1) points lies on an (n−1)-dimensional hyperplane. When m ≤n,\n",
      "a set of m points is in general position if no (m −2)-dimensional hyperplane\n",
      "contains the set. Thus, for example, a set of m ≥4 points is in general position\n",
      "in a three-dimensional space if no four of them lie on a (two-dimensional) plane.\n",
      "We will denote the number of linear dichotomies of m points in general position\n",
      "in an n-dimensional space by the expression ΠL(m, n).\n",
      "It is not too diﬃcult to verify that:\n",
      "Include the derivation.\n",
      "ΠL(m, n) = 2\n",
      "n\n",
      "X\n",
      "i=0\n",
      "C(m −1, i)\n",
      "for m > n, and\n",
      "= 2m\n",
      "for m ≤n\n",
      "\n",
      "\n",
      "507. 8.3. THE VAPNIK-CHERVONENKIS DIMENSION\n",
      "115\n",
      "where C(m −1, i) is the binomial coeﬃcient\n",
      "(m−1)!\n",
      "(m−1−i)!i!.\n",
      "The table below shows some values for ΠL(m, n).\n",
      "m\n",
      "n\n",
      "(no. of patterns)\n",
      "(dimension)\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "3\n",
      "6\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "4\n",
      "8\n",
      "14\n",
      "16\n",
      "16\n",
      "16\n",
      "5\n",
      "10\n",
      "22\n",
      "30\n",
      "32\n",
      "32\n",
      "6\n",
      "12\n",
      "32\n",
      "52\n",
      "62\n",
      "64\n",
      "7\n",
      "14\n",
      "44\n",
      "84\n",
      "114\n",
      "126\n",
      "8\n",
      "16\n",
      "58\n",
      "128\n",
      "198\n",
      "240\n",
      "Note that the class of linear dichotomies shatters the m patterns if m ≤n + 1.\n",
      "The bold-face entries in the table correspond to the highest values of m for\n",
      "which linear dichotomies shatter m patterns in n dimensions.\n",
      "8.3.2\n",
      "Capacity\n",
      "Let Pm,n = ΠL(m,n)\n",
      "2m\n",
      "= the probability that a randomly selected dichotomy (out\n",
      "of the 2m possible dichotomies of m patterns in n dimensions) will be linearly\n",
      "separable. In Fig. 8.2 we plot Pλ(n+1),n versus λ and n, where λ = m/(n + 1).\n",
      "Note that for large n (say n > 30) how quickly Pm,n falls from 1 to 0 as\n",
      "m goes above 2(n + 1). For m < 2(n + 1), any dichotomy of the m points is\n",
      "almost certainly linearly separable. But for m > 2(n + 1), a randomly selected\n",
      "dichotomy of the m points is almost certainly not linearly separable. For this\n",
      "reason m = 2(n + 1) is called the capacity of a TLU [Cover, 1965]. Unless the\n",
      "number of training patterns exceeds the capacity, the fact that a TLU separates\n",
      "those training patterns according to their labels means nothing in terms of how\n",
      "well that TLU will generalize to new patterns. There is nothing special about\n",
      "a separation found for m < 2(n + 1) patterns—almost any dichotomy of those\n",
      "patterns would have been linearly separable. To make sure that the separation\n",
      "found is forced by the training set and thus generalizes well, it has to be the\n",
      "case that there are very few linearly separable functions that would separate\n",
      "the m training patterns.\n",
      "Analogous results about the generalizing abilities of neural networks have\n",
      "been developed by [Baum & Haussler, 1989] and given intuitive and experimen-\n",
      "tal justiﬁcation in [Baum, 1994, page 438]:\n",
      "“The results seemed to indicate the following heuristic rule holds. If\n",
      "M examples [can be correctly classiﬁed by] a net with W weights (for\n",
      "M >> W), the net will make a fraction ε of errors on new examples\n",
      "chosen from the same [uniform] distribution where ε = W/M.”\n",
      "\n",
      "\n",
      "508. 116\n",
      "CHAPTER 8. COMPUTATIONAL LEARNING THEORY\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "0\n",
      "0.25\n",
      "0.5\n",
      "0.75\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "0\n",
      "25\n",
      ".5\n",
      "75\n",
      "1\n",
      "Ph(n + 1), n\n",
      "h\n",
      "n\n",
      "Figure 8.2: Probability that a Random Dichotomy is Linearly Separable\n",
      "8.3.3\n",
      "A More General Capacity Result\n",
      "Corollary 7.2 gave us an expression for the number of training patterns suﬃcient\n",
      "to guarantee a required level of generalization—assuming that the function we\n",
      "were guessing was a function belonging to a class of known and ﬁnite cardinality.\n",
      "The capacity result just presented applies to linearly separable functions for non-\n",
      "binary patterns. We can extend these ideas to general dichotomies of non-binary\n",
      "patterns.\n",
      "In general, let us denote the maximum number of dichotomies of any set\n",
      "of m n-dimensional patterns by hypotheses in H as ΠH(m, n). The number of\n",
      "dichotomies will, of course, depend on the disposition of the m points in the\n",
      "n-dimensional space; we take ΠH(m, n) to be the maximum over all possible\n",
      "arrangements of the m points. (In the case of the class of linearly separable\n",
      "functions, the maximum number is achieved when the m points are in general\n",
      "position.) For each class, H, there will be some maximum value of m for which\n",
      "ΠH(m, n) = 2m, that is, for which H shatters the m patterns. This maximum\n",
      "number is called the Vapnik-Chervonenkis (VC) dimension and is denoted by\n",
      "VCdim(H) [Vapnik & Chervonenkis, 1971].\n",
      "We saw that for the class of linear dichotomies, VCdim(Linear) = (n + 1).\n",
      "As another example, let us calculate the VC dimension of the hypothesis space\n",
      "of single intervals on the real line—used to classify points on the real line. We\n",
      "show an example of how points on the line might be dichotomized by a single\n",
      "interval in Fig. 8.3. The set Ξ could be, for example, {0.5, 2.5, - 2.3, 3.14}, and\n",
      "one of the hypotheses in our set would be [1, 4.5]. This hypothesis would label\n",
      "the points 2.5 and 3.14 with a 1 and the points - 2.3 and 0.5 with a 0. This\n",
      "\n",
      "\n",
      "509. 8.3. THE VAPNIK-CHERVONENKIS DIMENSION\n",
      "117\n",
      "set of hypotheses (single intervals on the real line) can arbitrarily classify any\n",
      "two points. But no single interval can classify three points such that the outer\n",
      "two are classiﬁed as 1 and the inner one as 0. Therefore the VC dimension of\n",
      "single intervals on the real line is 2. As soon as we have many more than 2\n",
      "training patterns on the real line and provided we know that the classiﬁcation\n",
      "function we are trying to guess is a single interval, then we begin to have good\n",
      "generalization.\n",
      "Figure 8.3: Dichotomizing Points by an Interval\n",
      "The VC dimension is a useful measure of the expressive power of a hypothesis\n",
      "set. Since any dichotomy of VCdim(H) or fewer patterns in general position in n\n",
      "dimensions can be achieved by some hypothesis in H, we must have many more\n",
      "than VCdim(H) patterns in the training set in order that a hypothesis consistent\n",
      "with the training set is suﬃciently constrained to imply good generalization.\n",
      "Our examples have shown that the concept of VC dimension is not restricted\n",
      "to Boolean functions.\n",
      "8.3.4\n",
      "Some Facts and Speculations About the VC Dimen-\n",
      "sion\n",
      "• If there are a ﬁnite number, |H|, of hypotheses in H, then:\n",
      "VCdim(H) ≤log(|H|)\n",
      "• The VC dimension of terms in n dimensions is n.\n",
      "• Suppose we generalize our example that used a hypothesis set of single\n",
      "intervals on the real line. Now let us consider an n-dimensional feature\n",
      "space and tests of the form Li ≤xi ≤Hi. We allow only one such test per\n",
      "dimension. A hypothesis space consisting of conjunctions of these tests\n",
      "(called axis-parallel hyper-rectangles) has VC dimension bounded by:\n",
      "n ≤VCdim ≤2n\n",
      "• As we have already seen, TLUs with n inputs have a VC dimension of\n",
      "n + 1.\n",
      "• [Baum, 1994, page 438] gives experimental evidence for the proposition\n",
      "that “ . . . multilayer [neural] nets have a VC dimension roughly equal to\n",
      "their total number of [adjustable] weights.”\n",
      "\n",
      "\n",
      "510. 118\n",
      "CHAPTER 8. COMPUTATIONAL LEARNING THEORY\n",
      "8.4\n",
      "VC Dimension and PAC Learning\n",
      "There are two theorems that connect the idea of VC dimension with PAC learn-\n",
      "ing [Blumer, et al., 1990]. We state these here without proof.\n",
      "Theorem 8.3 (Blumer, et al.) A hypothesis space H is PAC learnable iﬀit\n",
      "has ﬁnite VC dimension.\n",
      "Theorem 8.4 A set of hypotheses, H, is properly PAC learnable if:\n",
      "a. m ≥(1/ε) max [4 lg(2/δ), 8 VCdim lg(13/ε)], and\n",
      "b. if there is an algorithm that outputs a hypothesis h ϵ H consistent with the\n",
      "training set in polynomial (in m and n) time.\n",
      "The second of these two theorems improves the bound on the number of\n",
      "training patterns needed for linearly separable functions to one that is linear\n",
      "in n. In our previous example of how many training patterns were needed to\n",
      "ensure PAC learnability of a linearly separable function if n = 50, ε = 0.01, and\n",
      "δ = 0.01, we obtained m ≥173, 748. Using the Blumer, et al. result we would\n",
      "get m ≥52, 756.\n",
      "As another example of the second theorem, let us take H to be the set of\n",
      "closed intervals on the real line. The VC dimension is 2 (as shown previously).\n",
      "With n = 50, ε = 0.01, and δ = 0.01, m ≥16, 551 ensures PAC learnability.\n",
      "There is also a theorem that gives a lower (necessary) bound on the number\n",
      "of training patterns required for PAC learning [Ehrenfeucht, et al., 1988]:\n",
      "Theorem 8.5 Any\n",
      "PAC\n",
      "learning\n",
      "algorithm\n",
      "must\n",
      "examine\n",
      "at\n",
      "least\n",
      "Ω(1/ε lg(1/δ) + VCdim(H)) training patterns.\n",
      "The\n",
      "diﬀerence\n",
      "between\n",
      "the\n",
      "lower\n",
      "and\n",
      "upper\n",
      "bounds\n",
      "is\n",
      "O(log(1/ε)VCdim(H)/ε).\n",
      "8.5\n",
      "Bibliographical and Historical Remarks\n",
      "To be added.\n",
      "\n",
      "\n",
      "511. Chapter 9\n",
      "Unsupervised Learning\n",
      "9.1\n",
      "What is Unsupervised Learning?\n",
      "Consider the various sets of points in a two-dimensional space illustrated in Fig.\n",
      "9.1. The ﬁrst set (a) seems naturally partitionable into two classes, while the\n",
      "second (b) seems diﬃcult to partition at all, and the third (c) is problematic.\n",
      "Unsupervised learning uses procedures that attempt to ﬁnd natural partitions\n",
      "of patterns. There are two stages:\n",
      "• Form an R-way partition of a set Ξ of unlabeled training patterns (where\n",
      "the value of R, itself, may need to be induced from the patterns). The\n",
      "partition separates Ξ into R mutually exclusive and exhaustive subsets,\n",
      "Ξ1, . . . , ΞR, called clusters.\n",
      "• Design a classiﬁer based on the labels assigned to the training patterns by\n",
      "the partition.\n",
      "We will explain shortly various methods for deciding how many clusters there\n",
      "should be and for separating a set of patterns into that many clusters. We can\n",
      "base some of these methods, and their motivation, on minimum-description-\n",
      "length (MDL) principles. In that setting, we assume that we want to encode\n",
      "a description of a set of points, Ξ, into a message of minimal length.\n",
      "One\n",
      "encoding involves a description of each point separately; other, perhaps shorter,\n",
      "encodings might involve a description of clusters of points together with how\n",
      "each point in a cluster can be described given the cluster it belongs to. The\n",
      "speciﬁc techniques described in this chapter do not explicitly make use of MDL\n",
      "principles, but the MDL method has been applied with success. One of the\n",
      "MDL-based methods, Autoclass II [Cheeseman, et al., 1988] discovered a new\n",
      "classiﬁcation of stars based on the properties of infrared sources.\n",
      "Another type of unsupervised learning involves ﬁnding hierarchies of par-\n",
      "titionings or clusters of clusters. A hierarchical partition is one in which Ξ is\n",
      "119\n",
      "\n",
      "\n",
      "512. 120\n",
      "CHAPTER 9. UNSUPERVISED LEARNING\n",
      "a)  two clusters\n",
      "b) one cluster\n",
      "c) ?\n",
      "Figure 9.1: Unlabeled Patterns\n",
      "divided into mutually exclusive and exhaustive subsets, Ξ1, . . . , ΞR; each set,\n",
      "Ξi, (i = 1, . . . , R) is divided into mutually exclusive and exhaustive subsets,\n",
      "and so on. We show an example of such a hierarchical partition in Fig. 9.2.\n",
      "The hierarchical form is best displayed as a tree, as shown in Fig. 9.3. The tip\n",
      "nodes of the tree can further be expanded into their individual pattern elements.\n",
      "One application of such hierarchical partitions is in organizing individuals into\n",
      "taxonomic hierarchies such as those used in botany and zoology.\n",
      "9.2\n",
      "Clustering Methods\n",
      "9.2.1\n",
      "A Method Based on Euclidean Distance\n",
      "Most of the unsupervised learning methods use a measure of similarity between\n",
      "patterns in order to group them into clusters. The simplest of these involves\n",
      "deﬁning a distance between patterns. For patterns whose features are numeric,\n",
      "the distance measure can be ordinary Euclidean distance between two points in\n",
      "an n-dimensional space.\n",
      "There is a simple, iterative clustering method based on distance.\n",
      "It can\n",
      "be described as follows. Suppose we have R randomly chosen cluster seekers,\n",
      "C1, . . . , CR. These are points in an n-dimensional space that we want to adjust\n",
      "so that they each move toward the center of one of the clusters of patterns.\n",
      "We present the (unlabeled) patterns in the training set, Ξ, to the algorithm\n",
      "\n",
      "\n",
      "513. 9.2. CLUSTERING METHODS\n",
      "121\n",
      "U11\n",
      "U12\n",
      "U21\n",
      "U22\n",
      "U23\n",
      "U31\n",
      "U32\n",
      "U11 F U12 = U1\n",
      "U21 F U22 F U23 = U2\n",
      "U31 F U32 = U3\n",
      "U1 F U2 F U3 = U\n",
      "Figure 9.2: A Hierarchy of Clusters\n",
      "one-by-one. For each pattern, Xi, presented, we ﬁnd that cluster seeker, Cj,\n",
      "that is closest to Xi and move it closer to Xi:\n",
      "Cj ←−(1 −αj)Cj + αjXi\n",
      "where αj is a learning rate parameter for the j-th cluster seeker; it determines\n",
      "how far Cj is moved toward Xi.\n",
      "Reﬁnements on this procedure make the cluster seekers move less far as\n",
      "training proceeds. Suppose each cluster seeker, Cj, has a mass, mj, equal to\n",
      "the number of times that it has moved. As a cluster seeker’s mass increases it\n",
      "moves less far towards a pattern. For example, we might set αj = 1/(1 + mj)\n",
      "and use the above rule together with mj ←−mj +1. With this adjustment rule,\n",
      "a cluster seeker is always at the center of gravity (sample mean) of the set of\n",
      "patterns toward which it has so far moved. Intuitively, if a cluster seeker ever\n",
      "gets within some reasonably well clustered set of patterns (and if that cluster\n",
      "seeker is the only one so located), it will converge to the center of gravity of\n",
      "that cluster.\n",
      "\n",
      "\n",
      "514. 122\n",
      "CHAPTER 9. UNSUPERVISED LEARNING\n",
      "U\n",
      "U2\n",
      "U11\n",
      "U12\n",
      "U31\n",
      "U32\n",
      "U21\n",
      "U22\n",
      "U23\n",
      "U1\n",
      "U3\n",
      "Figure 9.3: Displaying a Hierarchy as a Tree\n",
      "Once the cluster seekers have converged, the classiﬁer implied by the now-\n",
      "labeled patterns in Ξ can be based on a Voronoi partitioning of the space (based\n",
      "on distances to the various cluster seekers). This kind of classiﬁcation, an ex-\n",
      "ample of which is shown in Fig. 9.4, can be implemented by a linear machine.\n",
      "Georgy Fedoseevich Voronoi, was a\n",
      "Russian mathematician who lived\n",
      "from 1868 to 1909.\n",
      "When basing partitioning on distance, we seek clusters whose patterns are\n",
      "as close together as possible. We can measure the badness, V , of a cluster of\n",
      "patterns, {Xi}, by computing its sample variance deﬁned by:\n",
      "V = (1/K)\n",
      "X\n",
      "i\n",
      "(Xi −M)2\n",
      "where M is the sample mean of the cluster, which is deﬁned to be:\n",
      "M = (1/K)\n",
      "X\n",
      "i\n",
      "Xi\n",
      "and K is the number of points in the cluster.\n",
      "We would like to partition a set of patterns into clusters such that the sum of\n",
      "the sample variances (badnesses) of these clusters is small. Of course if we have\n",
      "one cluster for each pattern, the sample variances will all be zero, so we must\n",
      "arrange that our measure of the badness of a partition must increase with the\n",
      "number of clusters. In this way, we can seek a trade-oﬀbetween the variances of\n",
      "\n",
      "\n",
      "515. 9.2. CLUSTERING METHODS\n",
      "123\n",
      "C1\n",
      "C2\n",
      "C3\n",
      "Separating boundaries\n",
      "Figure 9.4: Minimum-Distance Classiﬁcation\n",
      "the clusters and the number of them in a way somewhat similar to the principle\n",
      "of minimal description length discussed earlier.\n",
      "Elaborations of our basic cluster-seeking procedure allow the number of clus-\n",
      "ter seekers to vary depending on the distances between them and depending on\n",
      "the sample variances of the clusters. For example, if the distance, dij, between\n",
      "two cluster seekers, Ci and Cj, ever falls below some threshold ε, then we can\n",
      "replace them both by a single cluster seeker placed at their center of gravity\n",
      "(taking into account their respective masses). In this way we can decrease the\n",
      "overall badness of a partition by reducing the number of clusters for compara-\n",
      "tively little penalty in increased variance.\n",
      "On the other hand, if any of the cluster seekers, say Ci, deﬁnes a cluster\n",
      "whose sample variance is larger than some amount δ, then we can place a new\n",
      "cluster seeker, Cj, at some random location somewhat adjacent to Ci and reset\n",
      "the masses of both Ci and Cj to zero. In this way the badness of the par-\n",
      "tition might ultimately decrease by decreasing the total sample variance with\n",
      "comparatively little penalty for the additional cluster seeker. The values of the\n",
      "parameters ε and δ are set depending on the relative weights given to sample\n",
      "variances and numbers of clusters.\n",
      "In distance-based methods, it is important to scale the components of the\n",
      "pattern vectors. The variation of values along some dimensions of the pattern\n",
      "vector may be much diﬀerent than that of other dimensions. One commonly\n",
      "used technique is to compute the standard deviation (i.e., the square root of the\n",
      "variance) of each of the components over the entire training set and normalize\n",
      "the values of the components so that their adjusted standard deviations are\n",
      "equal.\n",
      "\n",
      "\n",
      "516. 124\n",
      "CHAPTER 9. UNSUPERVISED LEARNING\n",
      "9.2.2\n",
      "A Method Based on Probabilities\n",
      "Suppose we have a partition of the training set, Ξ, into R mutually exclusive\n",
      "and exhaustive clusters, C1, . . . , CR. We can decide to which of these clusters\n",
      "some arbitrary pattern, X, should be assigned by selecting the Ci for which\n",
      "the probability, p(Ci|X), is largest, providing p(Ci|X) is larger than some ﬁxed\n",
      "threshold, δ. As we saw earlier, we can use Bayes rule and base our decision on\n",
      "maximizing p(X|Ci)p(Ci). Assuming conditional independence of the pattern\n",
      "components, xi, the quantity to be maximized is:\n",
      "S(X, Ci) = p(x1|Ci)p(x2|Ci) · · · p(xn|Ci)p(Ci)\n",
      "The p(xj|Ci) can be estimated from the sample statistics of the patterns in the\n",
      "clusters and then used in the above expression. (Recall the linear form that this\n",
      "formula took in the case of binary-valued components.)\n",
      "We call S(X, Ci) the similarity of X to a cluster, Ci, of patterns. Thus, we\n",
      "assign X to the cluster to which it is most similar, providing the similarity is\n",
      "larger than δ.\n",
      "Just as before, we can deﬁne the sample mean of a cluster, Ci, to be:\n",
      "Mi = (1/Ki)\n",
      "X\n",
      "Xjϵ Ci\n",
      "Xj\n",
      "where Ki is the number of patterns in Ci.\n",
      "We can base an iterative clustering algorithm on this measure of similarity\n",
      "[Mahadevan & Connell, 1992]. It can be described as follows:\n",
      "a. Begin with a set of unlabeled patterns Ξ and an empty list, L, of clusters.\n",
      "b. For the next pattern, X, in Ξ, compute S(X, Ci) for each cluster, Ci.\n",
      "(Initially, these similarities are all zero.)\n",
      "Suppose the largest of these\n",
      "similarities is S(X, Cmax).\n",
      "(a) If S(X, Cmax) > δ, assign X to Cmax. That is,\n",
      "Cmax ←−Cmax ∪{X}\n",
      "Update the sample statistics p(x1|Cmax), p(x2|Cmax), . . . , p(xn|Cmax),\n",
      "and p(Cmax) to take the new pattern into account. Go to 3.\n",
      "(b) If S(X, Cmax) ≤δ, create a new cluster, Cnew = {X} and add Cnew\n",
      "to L. Go to 3.\n",
      "c. Merge any existing clusters, Ci and Cj if (Mi −Mj)2 < ε. Compute\n",
      "new sample statistics p(x1|Cmerge), p(x2|Cmerge), . . . , p(xn|Cmerge), and\n",
      "p(Cmerge) for the merged cluster, Cmerge = Ci ∪Cj.\n",
      "\n",
      "\n",
      "517. 9.3. HIERARCHICAL CLUSTERING METHODS\n",
      "125\n",
      "d. If the sample statistics of the clusters have not changed during an entire\n",
      "iteration through Ξ, then terminate with the clusters in L; otherwise go\n",
      "to 2.\n",
      "The value of the parameter δ controls the number of clusters. If δ is high,\n",
      "there will be a large number of clusters with few patterns in each cluster. For\n",
      "small values of δ, there will be a small number of clusters with many patterns in\n",
      "each cluster. Similarly, the larger the value of ε, the smaller the number clusters\n",
      "that will be found.\n",
      "Designing a classiﬁer based on the patterns labeled by the partitioning is\n",
      "straightforward. We assign any pattern, X, to that category that maximizes\n",
      "S(X, Ci).\n",
      "Mention “k-means and “EM”\n",
      "methods.\n",
      "9.3\n",
      "Hierarchical Clustering Methods\n",
      "9.3.1\n",
      "A Method Based on Euclidean Distance\n",
      "Suppose we have a set, Ξ, of unlabeled training patterns. We can form a hi-\n",
      "erarchical classiﬁcation of the patterns in Ξ by a simple agglomerative method.\n",
      "(The description of this algorithm is based on an unpublished manuscript by\n",
      "Pat Langley.) Our description here gives the general idea; we leave it to the\n",
      "reader to generate a precise algorithm.\n",
      "We ﬁrst compute the Euclidean distance between all pairs of patterns in Ξ.\n",
      "(Again, appropriate scaling of the dimensions is assumed.) Suppose the smallest\n",
      "distance is between patterns Xi and Xj. We collect Xi and Xj into a cluster,\n",
      "C, eliminate Xi and Xj from Ξ and replace them by a cluster vector, C, equal\n",
      "to the average of Xi and Xj. Next we compute the Euclidean distance again\n",
      "between all pairs of points in Ξ. If the smallest distance is between pairs of\n",
      "patterns, we form a new cluster, C, as before and replace the pair of patterns\n",
      "in Ξ by their average. If the shortest distance is between a pattern, Xi, and\n",
      "a cluster vector, Cj (representing a cluster, Cj), we form a new cluster, C,\n",
      "consisting of the union of Cj and {Xi}. In this case, we replace Cj and Xi\n",
      "in Ξ by their (appropriately weighted) average and continue. If the shortest\n",
      "distance is between two cluster vectors, Ci and Cj, we form a new cluster, C,\n",
      "consisting of the union of Ci and Cj. In this case, we replace Ci and Cj by their\n",
      "(appropriately weighted) average and continue. Since we reduce the number of\n",
      "points in Ξ by one each time, we ultimately terminate with a tree of clusters\n",
      "rooted in the cluster containing all of the points in the original training set.\n",
      "An example of how this method aggregates a set of two dimensional patterns\n",
      "is shown in Fig. 9.5. The numbers associated with each cluster indicate the order\n",
      "in which they were formed. These clusters can be organized hierarchically in a\n",
      "binary tree with cluster 9 as root, clusters 7 and 8 as the two descendants of the\n",
      "root, and so on. A ternary tree could be formed instead if one searches for the\n",
      "three points in Ξ whose triangle deﬁned by those patterns has minimal area.\n",
      "\n",
      "\n",
      "518. 126\n",
      "CHAPTER 9. UNSUPERVISED LEARNING\n",
      "1\n",
      "2\n",
      "3\n",
      "5\n",
      "4\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "Figure 9.5: Agglommerative Clustering\n",
      "9.3.2\n",
      "A Method Based on Probabilities\n",
      "A probabilistic quality measure for partitions\n",
      "We can develop a measure of the goodness of a partitioning based on how\n",
      "accurately we can guess a pattern given only what partition it is in. Suppose\n",
      "we are given a partitioning of Ξ into R classes, C1, . . . , CR. As before, we can\n",
      "compute the sample statistics p(xi|Ck) which give probability values for each\n",
      "component given the class assigned to it by the partitioning.\n",
      "Suppose each\n",
      "component xi of X can take on the values vij, where the index j steps over the\n",
      "domain of that component. We use the notation pi(vij|Ck) = probability(xi =\n",
      "vij|Ck).\n",
      "Suppose we use the following probabilistic guessing rule about the values\n",
      "of the components of a vector X given only that it is in class k. Guess that\n",
      "xi = vij with probability pi(vij|Ck). Then, the probability that we guess the\n",
      "i-th component correctly is:\n",
      "X\n",
      "j\n",
      "probability(guess is vij)pi(vij|Ck) =\n",
      "X\n",
      "j\n",
      "[pi(vij|Ck)]2\n",
      "The average number of (the n) components whose values are guessed correctly\n",
      "by this method is then given by the sum of these probabilities over all of the\n",
      "components of X:\n",
      "X\n",
      "i\n",
      "X\n",
      "j\n",
      "[pi(vij|Ck)]2\n",
      "\n",
      "\n",
      "519. 9.3. HIERARCHICAL CLUSTERING METHODS\n",
      "127\n",
      "Given our partitioning into R classes, the goodness measure, G, of this parti-\n",
      "tioning is the average of the above expression over all classes:\n",
      "G =\n",
      "X\n",
      "k\n",
      "p(Ck)\n",
      "X\n",
      "i\n",
      "X\n",
      "j\n",
      "[pi(vij|Ck)]2\n",
      "where p(Ck) is the probability that a pattern is in class Ck. In order to penalize\n",
      "this measure for having a large number of classes, we divide it by R to get an\n",
      "overall “quality” measure of a partitioning:\n",
      "Z = (1/R)\n",
      "X\n",
      "k\n",
      "p(Ck)\n",
      "X\n",
      "i\n",
      "X\n",
      "j\n",
      "[pi(vij|Ck)]2\n",
      "We give an example of the use of this measure for a trivially simple\n",
      "clustering of the four three-dimensional patterns shown in Fig.\n",
      "9.6.\n",
      "There\n",
      "are several diﬀerent partitionings.\n",
      "Let’s evaluate Z values for the follow-\n",
      "ing ones: P1 = {a, b, c, d}, P2 = {{a, b}, {c, d}}, P3 = {{a, c}, {b, d}}, and\n",
      "P4 = {{a}, {b}, {c}, {d}}. The ﬁrst, P1, puts all of the patterns into a single\n",
      "cluster. The sample probabilities pi(vi1 = 1) and pi(vi0 = 0) are all equal to 1/2\n",
      "for each of the three components. Summing over the values of the components\n",
      "(0 and 1) gives (1/2)2 + (1/2)2 = 1/2. Summing over the three components\n",
      "gives 3/2. Averaging over all of the clusters (there is just one) also gives 3/2.\n",
      "Finally, dividing by the number of clusters produces the ﬁnal Z value of this\n",
      "partition, Z(P1) = 3/2.\n",
      "The second partition, P2, gives the following sample probabilities:\n",
      "p1(v11 = 1|C1) = 1\n",
      "p2(v21 = 1|C1) = 1/2\n",
      "p3(v31 = 1|C1) = 1\n",
      "Summing over the values of the components (0 and 1) gives (1)2 + (0)2 = 1 for\n",
      "component 1, (1/2)2 + (1/2)2 = 1/2 for component 2, and (1)2 + (0)2 = 1 for\n",
      "component 3. Summing over the three components gives 2 1/2 for class 1. A\n",
      "similar calculation also gives 2 1/2 for class 2. Averaging over the two clusters\n",
      "also gives 2 1/2. Finally, dividing by the number of clusters produces the ﬁnal\n",
      "Z value of this partition, Z(P2) = 1 1/4, not quite as high as Z(P1).\n",
      "Similar calculations yield Z(P3) = 1 and Z(P4) = 3/4, so this method of\n",
      "evaluating partitions would favor placing all patterns in a single cluster.\n",
      "\n",
      "\n",
      "520. 128\n",
      "CHAPTER 9. UNSUPERVISED LEARNING\n",
      "x2\n",
      "x3\n",
      "x1\n",
      "a\n",
      "b\n",
      "c\n",
      "d\n",
      "Figure 9.6: Patterns in 3-Dimensional Space\n",
      "An iterative method for hierarchical clustering\n",
      "Evaluating all partitionings of m patterns and then selecting the best would be\n",
      "computationally intractable. The following iterative method is based on a hi-\n",
      "erarchical clustering procedure called COBWEB [Fisher, 1987]. The procedure\n",
      "grows a tree each node of which is labeled by a set of patterns. At the end\n",
      "of the process, the root node contains all of the patterns in Ξ. The successors\n",
      "of the root node will contain mutually exclusive and exhaustive subsets of Ξ.\n",
      "In general, the successors of a node, η, are labeled by mutually exclusive and\n",
      "exhaustive subsets of the pattern set labelling node η. The tips of the tree will\n",
      "contain singleton sets. The method uses Z values to place patterns at the vari-\n",
      "ous nodes; sample statistics are used to update the Z values whenever a pattern\n",
      "is placed at a node. The algorithm is as follows:\n",
      "a. We start with a tree whose root node contains all of the patterns in Ξ\n",
      "and a single empty successor node. We arrange that at all times dur-\n",
      "ing the process every non-empty node in the tree has (besides any other\n",
      "successors) exactly one empty successor.\n",
      "b. Select a pattern Xi in Ξ (if there are no more patterns to select, terminate).\n",
      "c. Set µ to the root node.\n",
      "d. For each of the successors of µ (including the empty successor!), calculate\n",
      "the best host for Xi. A best host is determined by tentatively placing\n",
      "Xi in one of the successors and calculating the resulting Z value for each\n",
      "\n",
      "\n",
      "521. 9.3. HIERARCHICAL CLUSTERING METHODS\n",
      "129\n",
      "one of these ways of accomodating Xi. The best host corresponds to the\n",
      "assignment with the highest Z value.\n",
      "e. If the best host is an empty node, η, we place Xi in η, generate an empty\n",
      "successor node of η, generate an empty sibling node of η, and go to 2.\n",
      "f. If the best host is a non-empty, singleton (tip) node, η, we place Xi in η,\n",
      "create one successor node of η containing the singleton pattern that was\n",
      "in η, create another successor node of η containing Xi, create an empty\n",
      "successor node of η, create empty successor nodes of the new non-empty\n",
      "successors of η, and go to 2.\n",
      "g. If the best host is a non-empty, non-singleton node, η, we place Xi in η,\n",
      "set µ to η, and go to 4.\n",
      "This process is rather sensitive to the order in which patterns are presented.\n",
      "To make the ﬁnal classiﬁcation tree less order dependent, the COBWEB proce-\n",
      "dure incorporates node merging and splitting.\n",
      "Node merging:\n",
      "It may happen that two nodes having the same parent could be merged with\n",
      "an overall increase in the quality of the resulting classiﬁcation performed by the\n",
      "successors of that parent. Rather than try all pairs to merge, a good heuristic\n",
      "is to attempt to merge the two best hosts. When such a merging improves the\n",
      "Z value, a new node containing the union of the patterns in the merged nodes\n",
      "replaces the merged nodes, and the two nodes that were merged are installed\n",
      "as successors of the new node.\n",
      "Node splitting:\n",
      "A heuristic for node splitting is to consider replacing the best host among a\n",
      "group of siblings by that host’s successors. This operation is performed only if\n",
      "it increases the Z value of the classiﬁcation performed by a group of siblings.\n",
      "Example results from COBWEB\n",
      "We mention two experiments with COBWEB. In the ﬁrst, the program at-\n",
      "tempted to ﬁnd two categories (we will call them Class 1 and Class 2) of United\n",
      "States Senators based on their votes (yes or no) on six issues. After the clus-\n",
      "ters were established, the majority vote in each class was computed. These are\n",
      "shown in the table below.\n",
      "Issue\n",
      "Class 1\n",
      "Class 2\n",
      "Toxic Waste\n",
      "yes\n",
      "no\n",
      "Budget Cuts\n",
      "yes\n",
      "no\n",
      "SDI Reduction\n",
      "no\n",
      "yes\n",
      "Contra Aid\n",
      "yes\n",
      "no\n",
      "Line-Item Veto\n",
      "yes\n",
      "no\n",
      "MX Production\n",
      "yes\n",
      "no\n",
      "\n",
      "\n",
      "522. 130\n",
      "CHAPTER 9. UNSUPERVISED LEARNING\n",
      "In the second experiment, the program attempted to classify soybean dis-\n",
      "eases based on various characteristics. COBWEB grouped the diseases in the\n",
      "taxonomy shown in Fig. 9.7.\n",
      "N0\n",
      "soybean\n",
      "diseases\n",
      "N1\n",
      "  Diaporthe\n",
      "Stem Canker\n",
      "N2\n",
      "Charcoal\n",
      "     Rot\n",
      "N3\n",
      "N31\n",
      "Rhizoctonia\n",
      "       Rot\n",
      "N32\n",
      "Phytophthora\n",
      "       Rot\n",
      "Figure 9.7: Taxonomy Induced for Soybean Diseases\n",
      "9.4\n",
      "Bibliographical and Historical Remarks\n",
      "To be added.\n",
      "\n",
      "\n",
      "523. Chapter 10\n",
      "Temporal-Diﬀerence\n",
      "Learning\n",
      "10.1\n",
      "Temporal Patterns and Prediction Prob-\n",
      "lems\n",
      "In this chapter, we consider problems in which we wish to learn to predict the\n",
      "future value of some quantity, say z, from an n-dimensional input pattern, X.\n",
      "In many of these problems, the patterns occur in temporal sequence, X1, X2,\n",
      ".\n",
      ".\n",
      "., Xi, Xi+1, . . ., Xm, and are generated by a dynamical process.\n",
      "The\n",
      "components of Xi are features whose values are available at time, t = i. We\n",
      "distinguish two kinds of prediction problems. In one, we desire to predict the\n",
      "value of z at time t = i + 1 based on input Xi for every i. For example, we\n",
      "might wish to predict some aspects of tomorrow’s weather based on a set of\n",
      "measurements made today. In the other kind of prediction problem, we desire\n",
      "to make a sequence of predictions about the value of z at some ﬁxed time, say\n",
      "t = m + 1, based on each of the Xi, i = 1, . . . , m. For example, we might wish\n",
      "to make a series of predictions about some aspect of the weather on next New\n",
      "Year’s Day, based on measurements taken every day before New Year’s. Sutton\n",
      "[Sutton, 1988] has called this latter problem, multi-step prediction, and that is\n",
      "the problem we consider here. In multi-step prediction, we might expect that\n",
      "the prediction accuracy should get better and better as i increases toward m.\n",
      "10.2\n",
      "Supervised and Temporal-Diﬀerence Meth-\n",
      "ods\n",
      "A training method that naturally suggests itself is to use the actual value of\n",
      "z at time m + 1 (once it is known) in a supervised learning procedure using a\n",
      "131\n",
      "\n",
      "\n",
      "524. 132\n",
      "CHAPTER 10. TEMPORAL-DIFFERENCE LEARNING\n",
      "sequence of training patterns, {X1, X2, . . ., Xi, Xi+1, . . ., Xm}. That is, we\n",
      "seek to learn a function, f, such that f(Xi) is as close as possible to z for each i.\n",
      "Typically, we would need a training set, Ξ, consisting of several such sequences.\n",
      "We will show that a method that is better than supervised learning for some\n",
      "important problems is to base learning on the diﬀerence between f(Xi+1) and\n",
      "f(Xi) rather than on the diﬀerence between z and f(Xi). Such methods involve\n",
      "what is called temporal-diﬀerence (TD) learning.\n",
      "We assume that our prediction, f(X), depends on a vector of modiﬁable\n",
      "weights, W. To make that dependence explicit, we write f(X, W).\n",
      "For su-\n",
      "pervised learning, we consider procedures of the following type: For each Xi,\n",
      "the prediction f(Xi, W) is computed and compared to z, and the learning rule\n",
      "(whatever it is) computes the change, (∆Wi), to be made to W. Then, taking\n",
      "into account the weight changes for each pattern in a sequence all at once after\n",
      "having made all of the predictions with the old weight vector, we change W as\n",
      "follows:\n",
      "W ←−W +\n",
      "m\n",
      "X\n",
      "i=1\n",
      "(∆W)i\n",
      "Whenever we are attempting to minimize the squared error between z and\n",
      "f(Xi, W) by gradient descent, the weight-changing rule for each pattern is:\n",
      "(∆W)i = c(z −fi) ∂fi\n",
      "∂W\n",
      "where c is a learning rate parameter, fi is our prediction of z, f(Xi, W),\n",
      "at time t = i, and\n",
      "∂fi\n",
      "∂W is, by deﬁnition, the vector of partial derivatives\n",
      "( ∂fi\n",
      "∂w1 , . . . , ∂fi\n",
      "∂wi , . . . , ∂fi\n",
      "∂wn ) in which the wi are the individual components of W.\n",
      "(The expression\n",
      "∂fi\n",
      "∂W is sometimes written ∇Wfi.) The reader will recall that\n",
      "we used an equivalent expression for (∆W)i in deriving the backpropagation\n",
      "formulas used in training multi-layer neural networks.\n",
      "The Widrow-Hoﬀrule results when f(X, W) = X • W. Then:\n",
      "(∆W)i = c(z −fi)Xi\n",
      "An interesting form for (∆W)i can be developed if we note that\n",
      "(z −fi) =\n",
      "m\n",
      "X\n",
      "k=i\n",
      "(fk+1 −fk)\n",
      "where we deﬁne fm+1 = z. Substituting in our formula for (∆W)i yields:\n",
      "(∆W)i = c(z −fi) ∂fi\n",
      "∂W\n",
      "\n",
      "\n",
      "525. 10.2. SUPERVISED AND TEMPORAL-DIFFERENCE METHODS\n",
      "133\n",
      "= c ∂fi\n",
      "∂W\n",
      "m\n",
      "X\n",
      "k=i\n",
      "(fk+1 −fk)\n",
      "In this form, instead of using the diﬀerence between a prediction and the value\n",
      "of z, we use the diﬀerences between successive predictions—thus the phrase\n",
      "temporal-diﬀerence (TD) learning.\n",
      "In the case when f(X, W) = X • W, the temporal diﬀerence form of the\n",
      "Widrow-Hoﬀrule is:\n",
      "(∆W)i = cXi\n",
      "m\n",
      "X\n",
      "k=i\n",
      "(fk+1 −fk)\n",
      "One reason for writing (∆W)i in temporal-diﬀerence form is to permit an\n",
      "interesting generalization as follows:\n",
      "(∆W)i = c ∂fi\n",
      "∂W\n",
      "m\n",
      "X\n",
      "k=i\n",
      "λ(k−i)(fk+1 −fk)\n",
      "where 0 < λ ≤1. Here, the λ term gives exponentially decreasing weight to\n",
      "diﬀerences later in time than t = i. When λ = 1, we have the same rule with\n",
      "which we began—weighting all diﬀerences equally, but as λ →0, we weight only\n",
      "the (fi+1 −fi) diﬀerence. With the λ term, the method is called TD(λ).\n",
      "It is interesting to compare the two extreme cases:\n",
      "For TD(0):\n",
      "(∆W)i = c(fi+1 −fi) ∂fi\n",
      "∂W\n",
      "For TD(1):\n",
      "(∆W)i = c(z −fi) ∂fi\n",
      "∂W\n",
      "Both extremes can be handled by the same learning mechanism; only the error\n",
      "term is diﬀerent. In TD(0), the error is the diﬀerence between successive predic-\n",
      "tions, and in TD(1), the error is the diﬀerence between the ﬁnally revealed value\n",
      "of z and the prediction. Intermediate values of λ take into account diﬀerently\n",
      "weighted diﬀerences between future pairs of successive predictions.\n",
      "Only TD(1) can be considered a pure supervised learning procedure, sensitive\n",
      "to the ﬁnal value of z provided by the teacher. For λ < 1, we have various degrees\n",
      "of unsupervised learning, in which the prediction function strives to make each\n",
      "prediction more like successive ones (whatever they might be). We shall soon\n",
      "see that these unsupervised procedures result in better learning than do the\n",
      "supervised ones for an important class of problems.\n",
      "\n",
      "\n",
      "526. 134\n",
      "CHAPTER 10. TEMPORAL-DIFFERENCE LEARNING\n",
      "10.3\n",
      "Incremental Computation of the (∆W)i\n",
      "We can rewrite our formula for (∆W)i, namely\n",
      "(∆W)i = c ∂fi\n",
      "∂W\n",
      "m\n",
      "X\n",
      "k=i\n",
      "λ(k−i)(fk+1 −fk)\n",
      "to allow a type of incremental computation. First we write the expression for\n",
      "the weight change rule that takes into account all of the (∆W)i:\n",
      "W ←−W +\n",
      "m\n",
      "X\n",
      "i=1\n",
      "c ∂fi\n",
      "∂W\n",
      "m\n",
      "X\n",
      "k=i\n",
      "λ(k−i)(fk+1 −fk)\n",
      "Interchanging the order of the summations yields:\n",
      "W ←−W +\n",
      "m\n",
      "X\n",
      "k=1\n",
      "c\n",
      "k\n",
      "X\n",
      "i=1\n",
      "λ(k−i)(fk+1 −fk) ∂fi\n",
      "∂W\n",
      "= W +\n",
      "m\n",
      "X\n",
      "k=1\n",
      "c(fk+1 −fk)\n",
      "k\n",
      "X\n",
      "i=1\n",
      "λ(k−i) ∂fi\n",
      "∂W\n",
      "Interchanging the indices k and i ﬁnally yields:\n",
      "W ←−W +\n",
      "m\n",
      "X\n",
      "i=1\n",
      "c(fi+1 −fi)\n",
      "i\n",
      "X\n",
      "k=1\n",
      "λ(i−k) ∂fk\n",
      "∂W\n",
      "If, as earlier, we want to use an expression of the form W ←−W+Pm\n",
      "i=1(∆W)i,\n",
      "we see that we can write:\n",
      "(∆W)i = c(fi+1 −fi)\n",
      "i\n",
      "X\n",
      "k=1\n",
      "λ(i−k) ∂fk\n",
      "∂W\n",
      "Now, if we let ei = Pi\n",
      "k=1 λ(i−k) ∂fk\n",
      "∂W, we can develop a computationally eﬃcient\n",
      "recurrence equation for ei+1 as follows:\n",
      "ei+1 =\n",
      "i+1\n",
      "X\n",
      "k=1\n",
      "λ(i+1−k) ∂fk\n",
      "∂W\n",
      "= ∂fi+1\n",
      "∂W +\n",
      "i\n",
      "X\n",
      "k=1\n",
      "λ(i+1−k) ∂fk\n",
      "∂W\n",
      "\n",
      "\n",
      "527. 10.4. AN EXPERIMENT WITH TD METHODS\n",
      "135\n",
      "= ∂fi+1\n",
      "∂W + λei\n",
      "Rewriting (∆W)i in these terms, we obtain:\n",
      "(∆W)i = c(fi+1 −fi)ei\n",
      "where:\n",
      "e1 = ∂f1\n",
      "∂W\n",
      "e2 = ∂f2\n",
      "∂W + λe1\n",
      "etc.\n",
      "Quoting Sutton [Sutton, 1988, page 15] (about a diﬀerent equation, but the\n",
      "quote applies equally well to this one):\n",
      "“. . . this equation can be computed incrementally, because each\n",
      "(∆W)i depends only on a pair of successive predictions and on the\n",
      "[weighted] sum of all past values for ∂fi\n",
      "∂W. This saves substantially on\n",
      "memory, because it is no longer necessary to individually remember\n",
      "all past values of\n",
      "∂fi\n",
      "∂W.”\n",
      "10.4\n",
      "An Experiment with TD Methods\n",
      "TD prediction methods [especially TD(0)] are well suited to situations in which\n",
      "the patterns are generated by a dynamic process. In that case, sequences of\n",
      "temporally presented patterns contain important information that is ignored\n",
      "by a conventional supervised method such as the Widrow-Hoﬀrule.\n",
      "Sutton\n",
      "[Sutton, 1988, page 19] gives an interesting example involving a random walk,\n",
      "which we repeat here. In Fig. 10.1, sequences of vectors, X, are generated as\n",
      "follows: We start with vector XD; the next vector in the sequence is equally\n",
      "likely to be one of the adjacent vectors in the diagram. If the next vector is\n",
      "XC (or XE), the next one after that is equally likely to be one of the vectors\n",
      "adjacent to XC (or XE). When XB is in the sequence, it is equally likely that\n",
      "the sequence terminates with z = 0 or that the next vector is XC. Similarly,\n",
      "when XF is in the sequence, it is equally likely that the sequence terminates\n",
      "with z = 1 or that the next vector is XE. Thus the sequences are random, but\n",
      "they always start with XD. Some sample sequences are shown in the ﬁgure.\n",
      "\n",
      "\n",
      "528. 136\n",
      "CHAPTER 10. TEMPORAL-DIFFERENCE LEARNING\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "z = 0\n",
      "z = 1\n",
      "XB\n",
      "XC\n",
      "XD\n",
      "XE\n",
      "XF\n",
      "Typical Sequences:\n",
      "XDXCXDXEXF  1\n",
      "XDXCXBXCXDXEXDXEXF  1\n",
      "XDXEXDXCXB  0\n",
      "Figure 10.1: A Markov Process\n",
      "This random walk is an example of a Markov process; transitions from state i\n",
      "to state j occur with probabilities that depend only on i and j.\n",
      "Given a set of sequences generated by this process as a training set, we want\n",
      "to be able to predict the value of z for each X in a test sequence. We assume\n",
      "that the learning system does not know the transition probabilities.\n",
      "For his experiments with this process, Sutton used a linear predictor, that\n",
      "is f(X, W) = X • W. The learning problem is to ﬁnd a weight vector, W, that\n",
      "minimizes the mean-squared error between z and the predicted value of z. Given\n",
      "the ﬁve diﬀerent values that X can take on, we have the following predictions:\n",
      "f(XB) = w1, f(XC) = w2, f(XD) = w3, f(XE) = w4, f(XF ) = w5, where\n",
      "wi is the i-th component of the weight vector. (Note that the values of the\n",
      "predictions are not limited to 1 or 0—even though z can only have one of\n",
      "those values—because we are minimizing mean-squared error.) After training,\n",
      "these predictions will be compared with the optimal ones—given the transition\n",
      "probabilities.\n",
      "The experimental setup was as follows: ten random sequences were generated\n",
      "using the transition probabilities. Each of these sequences was presented in turn\n",
      "to a TD(λ) method for various values of λ. Weight vector increments, (∆W)i,\n",
      "were computed after each pattern presentation but no weight changes were\n",
      "made until all ten sequences were presented. The weight vector increments were\n",
      "summed after all ten sequences were presented, and this sum was used to change\n",
      "the weight vector to be used for the next pass through the ten sequences. This\n",
      "process was repeated over and over (using the same training sequences) until\n",
      "(quoting Sutton) “the procedure no longer produced any signiﬁcant changes in\n",
      "the weight vector. For small c, the weight vector always converged in this way,\n",
      "\n",
      "\n",
      "529. 10.4. AN EXPERIMENT WITH TD METHODS\n",
      "137\n",
      "and always to the same ﬁnal value [for 100 diﬀerent training sets of ten random\n",
      "sequences], independent of its initial value.” (Even though, for ﬁxed, small c,\n",
      "the weight vector always converged to the same vector, it might converge to a\n",
      "somewhat diﬀerent vector for diﬀerent values of c.)\n",
      "After convergence, the predictions made by the ﬁnal weight vector are com-\n",
      "pared with the optimal predictions made using the transition probabilities.\n",
      "These optimal predictions are simply p(z = 1|X). We can compute these proba-\n",
      "bilities to be 1/6, 1/3, 1/2, 2/3, and 5/6 for XB, XC, XD, XE, XF , respectively.\n",
      "The root-mean-squared diﬀerences between the best learned predictions (over\n",
      "all c) and these optimal ones are plotted in Fig. 10.2 for seven diﬀerent values\n",
      "of λ. (For each data point, the standard error is approximately σ = 0.01.)\n",
      "0.10\n",
      "0.12\n",
      "0.14\n",
      "0.16\n",
      "0.18\n",
      "0.20\n",
      "0.0 0.1\n",
      "0.3\n",
      "0.5\n",
      "0.7\n",
      "0.9 1.0\n",
      "h\n",
      "Error using\n",
      "best c\n",
      "Widrow-Hoff\n",
      "TD(1)\n",
      "TD(0)\n",
      "(Adapted from Sutton, p. 20, 1988)\n",
      "Figure 10.2: Prediction Errors for TD(λ)\n",
      "Notice that the Widrow-Hoﬀprocedure does not perform as well as other\n",
      "versions of TD(λ) for λ < 1! Quoting [Sutton, 1988, page 21]:\n",
      "“This result contradicts conventional wisdom. It is well known that,\n",
      "under repeated presentations, the Widrow-Hoﬀprocedure minimizes\n",
      "the RMS error between its predictions and the actual outcomes in\n",
      "the training set ([Widrow & Stearns, 1985]). How can it be that this\n",
      "optimal method peformed worse than all the TD methods for λ <\n",
      "1? The answer is that the Widrow-Hoﬀprocedure only minimizes\n",
      "error on the training set; it does not necessarily minimize error for\n",
      "future experience. [Later] we prove that in fact it is linear TD(0)\n",
      "that converges to what can be considered the optimal estimates for\n",
      "\n",
      "\n",
      "530. 138\n",
      "CHAPTER 10. TEMPORAL-DIFFERENCE LEARNING\n",
      "matching future experience—those consistent with the maximum-\n",
      "likelihood estimate of the underlying Markov process.”\n",
      "10.5\n",
      "Theoretical Results\n",
      "It is possible to analyze the performance of the linear-prediction TD(λ) methods\n",
      "on Markov processes. We state some theorems here without proof.\n",
      "Theorem 10.1 (Sutton, page 24, 1988) For any absorbing Markov chain,\n",
      "and for any linearly independent set of observation vectors {Xi} for the non-\n",
      "terminal states, there exists an ε > 0 such that for all positive c < ε and for any\n",
      "initial weight vector, the predictions of linear TD(0) (with weight updates after\n",
      "each sequence) converge in expected value to the optimal (maximum likelihood)\n",
      "predictions of the true process.\n",
      "Even though the expected values of the predictions converge, the predictions\n",
      "themselves do not converge but vary around their expected values depending on\n",
      "their most recent experience. Sutton conjectures that if c is made to approach\n",
      "0 as training progresses, the variance of the predictions will approach 0 also.\n",
      "Dayan [Dayan, 1992] has extended the result of Theorem 9.1 to TD(λ) for\n",
      "arbitrary λ between 0 and 1. (Also see [Dayan & Sejnowski, 1994].)\n",
      "10.6\n",
      "Intra-Sequence Weight Updating\n",
      "Our standard weight updating rule for TD(λ) methods is:\n",
      "W ←−W +\n",
      "m\n",
      "X\n",
      "i=1\n",
      "c(fi+1 −fi)\n",
      "i\n",
      "X\n",
      "k=1\n",
      "λ(i−k) ∂fk\n",
      "∂W\n",
      "where the weight update occurs after an entire sequence is observed. To make\n",
      "the method truly incremental (in analogy with weight updating rules for neural\n",
      "nets), it would be desirable to change the weight vector after every pattern\n",
      "presentation. The obvious extension is:\n",
      "Wi+1 ←−Wi + c(fi+1 −fi)\n",
      "i\n",
      "X\n",
      "k=1\n",
      "λ(i−k) ∂fk\n",
      "∂W\n",
      "where fi+1 is computed before making the weight change; that is, fi+1 =\n",
      "f(Xi+1, Wi). But that would make fi = f(Xi, Wi−1), and such a rule would\n",
      "make the prediction diﬀerence, namely (fi+1 −fi), sensitive both to changes in\n",
      "X and changes in W and could lead to instabilities. Instead, we modify the rule\n",
      "so that, for every pair of predictions, fi+1 = f(Xi+1, Wi) and fi = f(Xi, Wi).\n",
      "This version of the rule has been used in practice with excellent results.\n",
      "\n",
      "\n",
      "531. 10.6. INTRA-SEQUENCE WEIGHT UPDATING\n",
      "139\n",
      "For TD(0) and linear predictors, the rule is:\n",
      "Wi+1 = Wi + c(fi+1 −fi)Xi\n",
      "The rule is implemented as follows:\n",
      "a. Initialize the weight vector, W, arbitrarily.\n",
      "b. For i = 1, ..., m, do:\n",
      "(a) fi ←−Xi • W\n",
      "(We compute fi anew each time through rather than use the value\n",
      "of fi+1 the previous time through.)\n",
      "(b) fi+1 ←−Xi+1 • W\n",
      "(c) di+1 ←−fi+1 −fi\n",
      "(d) W ←−W + c di+1Xi\n",
      "(If fi were computed again with this changed weight vector, its value\n",
      "would be closer to fi+1 as desired.)\n",
      "The linear TD(0) method can be regarded as a technique for training a\n",
      "very simple network consisting of a single dot product unit (and no threshold\n",
      "or sigmoid function). TD methods can also be used in combination with back-\n",
      "propagation to train neural networks. For TD(0) we change the network weights\n",
      "according to the expression:\n",
      "Wi+1 = Wi + c(fi+1 −fi) ∂fi\n",
      "∂W\n",
      "The only change that must be made to the standard backpropagation weight-\n",
      "changing rule is that the diﬀerence term between the desired output and the\n",
      "output of the unit in the ﬁnal (k-th) layer, namely (d −f (k)), must be replaced\n",
      "by a diﬀerence term between successive outputs, (fi+1 −fi). This change has a\n",
      "direct eﬀect only on the expression for δ(k) which becomes:\n",
      "δ(k) = 2(f ′(k) −f (k))f (k)(1 −f (k))\n",
      "where f ′(k) and f (k) are two successive outputs of the network.\n",
      "The weight changing rule for the i-th weight vector in the j-th layer of weights\n",
      "has the same form as before, namely:\n",
      "W(j)\n",
      "i\n",
      "←−W(j)\n",
      "i\n",
      "+ cδ(j)\n",
      "i\n",
      "X(j−1)\n",
      "where the δ(j)\n",
      "i\n",
      "are given recursively by:\n",
      "δ(j)\n",
      "i\n",
      "= f (j)\n",
      "i\n",
      "(1 −f (j)\n",
      "i\n",
      ")\n",
      "mj+1\n",
      "X\n",
      "l=1\n",
      "δ(j+1)\n",
      "l\n",
      "w(j+1)\n",
      "il\n",
      "and w(j+1)\n",
      "il\n",
      "is the l-th component of the i-th weight vector in the (j +1)-th layer\n",
      "of weights. Of course, here also it is assumed that f ′(k) and f (k) are computed\n",
      "using the same weights and then the weights are changed. In the next section\n",
      "we shall see an interesting example of this application of TD learning.\n",
      "\n",
      "\n",
      "532. 140\n",
      "CHAPTER 10. TEMPORAL-DIFFERENCE LEARNING\n",
      "10.7\n",
      "An Example Application: TD-gammon\n",
      "A program called TD-gammon [Tesauro, 1992] learns to play backgammon by\n",
      "training a neural network via temporal-diﬀerence methods. The structure of\n",
      "the neural net, and its coding is as shown in Fig. 10.3. The network is trained\n",
      "to minimize the error between actual payoﬀand estimated payoﬀ, where the\n",
      "actual payoﬀis deﬁned to be df = p1 + 2p2 −p3 −2p4, and the pi are the actual\n",
      "probabilities of the various outcomes as deﬁned in the ﬁgure.\n",
      ". . .\n",
      "p3 = pr(black wins)\n",
      "p4 = pr(black gammons)\n",
      "p1 = pr(white wins)\n",
      "p2 = pr(white gammons)\n",
      "estimated payoff:\n",
      "d = p1 + 2p2 < p3 < 2p4\n",
      "no. of white\n",
      "on cell 1\n",
      "no. on bar,\n",
      "off board,\n",
      "and who\n",
      "moves\n",
      "198 inputs\n",
      "1\n",
      "2\n",
      "3\n",
      "# > 3\n",
      ". . .\n",
      "up to 40 hidden units\n",
      "2 x 24\n",
      "cells\n",
      "4 output units\n",
      "hidden and output units are sigmoids\n",
      "learning rate:  c = 0.1; initial weights chosen\n",
      "randomly between <0.5 and +0.5.\n",
      "estimated probabilities:\n",
      "Figure 10.3: The TD-gammon Network\n",
      "TD-gammon learned by using the network to select that move that results\n",
      "in the best predicted payoﬀ. That is, at any stage of the game some ﬁnite set of\n",
      "moves is possible and these lead to the set, {X}, of new board positions. Each\n",
      "member of this set is evaluated by the network, and the one with the largest\n",
      "\n",
      "\n",
      "533. 10.8. BIBLIOGRAPHICAL AND HISTORICAL REMARKS\n",
      "141\n",
      "predicted payoﬀis selected if it is white’s move (and the smallest if it is black’s).\n",
      "The move is made, and the network weights are adjusted to make the predicted\n",
      "payoﬀfrom the original position closer to that of the resulting position.\n",
      "The weight adjustment procedure combines temporal-diﬀerence (TD(λ))\n",
      "learning with backpropagation.\n",
      "If dt is the network’s estimate of the payoﬀ\n",
      "at time t (before a move is made), and dt+1 is the estimate at time t + 1 (after\n",
      "a move is made), the weight adjustment rule is:\n",
      "∆Wt = c(dt+1 −dt)\n",
      "t\n",
      "X\n",
      "k=1\n",
      "λt−k ∂dk\n",
      "∂W\n",
      "where Wt is a vector of all weights in the network at time t, and\n",
      "∂dk\n",
      "∂W is the\n",
      "gradient of dk in this weight space. (For a layered, feedforward network, such\n",
      "as that of TD-gammon, the weight changes for the weight vectors in each layer\n",
      "can be expressed in the usual manner.)\n",
      "To make the special cases clear, recall that for TD(0), the network would be\n",
      "trained so that, for all t, its output, dt, for input Xt tended toward its expected\n",
      "output, dt+1, for input Xt+1. For TD(1), the network would be trained so that,\n",
      "for all t, its output, dt, for input Xt tended toward the expected ﬁnal payoﬀ,\n",
      "df, given that input. The latter case is the same as the Widrow-Hoﬀrule.\n",
      "After about 200,000 games the following results were obtained. TD-gammon\n",
      "(with 40 hidden units, λ = 0.7, and c = 0.1) won 66.2% of 10,000 games against\n",
      "SUN Microsystems Gammontool and 55% of 10,000 games against a neural\n",
      "network trained using expert moves. Commenting on a later version of TD-\n",
      "gammon, incorporating special features as inputs, Tesauro said: “It appears to\n",
      "be the strongest program ever seen by this author.”\n",
      "10.8\n",
      "Bibliographical and Historical Remarks\n",
      "To be added.\n",
      "\n",
      "\n",
      "534. 142\n",
      "CHAPTER 10. TEMPORAL-DIFFERENCE LEARNING\n",
      "\n",
      "\n",
      "535. Chapter 11\n",
      "Delayed-Reinforcement\n",
      "Learning\n",
      "11.1\n",
      "The General Problem\n",
      "Imagine a robot that exists in an environment in which it can sense and act.\n",
      "Suppose (as an extreme case) that it has no idea about the eﬀects of its actions.\n",
      "That is, it doesn’t know how acting will change its sensory inputs. Along with\n",
      "its sensory inputs are “rewards,” which it occasionally receives. How should it\n",
      "choose its actions so as to maximize its rewards over the long run? To maximize\n",
      "rewards, it will need to be able to predict how actions change inputs, and in\n",
      "particular, how actions lead to rewards.\n",
      "We formalize the problem in the following way: The robot exists in an\n",
      "environment consisting of a set, S, of states. We assume that the robot’s sensory\n",
      "apparatus constructs an input vector, X, from the environment, which informs\n",
      "the robot about which state the environment is in. For the moment, we will\n",
      "assume that the mapping from states to vectors is one-to-one, and, in fact, will\n",
      "use the notation X to refer to the state of the environment as well as to the\n",
      "input vector. When presented with an input vector, the robot decides which\n",
      "action from a set, A, of actions to perform. Performing the action produces an\n",
      "eﬀect on the environment—moving it to a new state. The new state results in\n",
      "the robot perceiving a new input vector, and the cycle repeats. We assume a\n",
      "discrete time model; the input vector at time t = i is Xi, the action taken at\n",
      "that time is ai, and the expected reward, ri, received at t = i depends on the\n",
      "action taken and on the state, that is ri = r(Xi, ai). The learner’s goal is to ﬁnd\n",
      "a policy, π(X), that maps input vectors to actions in such a way that maximizes\n",
      "rewards accumulated over time. This type of learning is called reinforcement\n",
      "learning. The learner must ﬁnd the policy by trial and error; it has no initial\n",
      "knowledge of the eﬀects of its actions. The situation is as shown in Fig. 11.1.\n",
      "143\n",
      "\n",
      "\n",
      "536. 144\n",
      "CHAPTER 11. DELAYED-REINFORCEMENT LEARNING\n",
      "Xi\n",
      "ri\n",
      "Learner\n",
      "Environment\n",
      "(reward)\n",
      "(state)\n",
      "(action)\n",
      "ai\n",
      "Figure 11.1: Reinforcement Learning\n",
      "11.2\n",
      "An Example\n",
      "A “grid world,” such as the one shown in Fig. 11.2 is often used to illustrate\n",
      "reinforcement learning. Imagine a robot initially in cell (2,3). The robot receives\n",
      "input vector (x1, x2) telling it what cell it is in; it is capable of four actions,\n",
      "n, e, s, w moving the robot one cell up, right, down, or left, respectively. It is\n",
      "rewarded one negative unit whenever it bumps into the wall or into the blocked\n",
      "cells. For example, if the input to the robot is (1,3), and the robot chooses\n",
      "action w, the next input to the robot is still (1,3) and it receives a reward of\n",
      "−1. If the robot lands in the cell marked G (for goal), it receives a reward of\n",
      "+10. Let’s suppose that whenever the robot lands in the goal cell and gets its\n",
      "reward, it is immediately transported out to some random cell, and the quest\n",
      "for reward continues.\n",
      "A policy for our robot is a speciﬁcation of what action to take for every one\n",
      "of its inputs, that is, for every one of the cells in the grid. For example, a com-\n",
      "ponent of such a policy would be “when in cell (3,1), move right.” An optimal\n",
      "policy is a policy that maximizes long-term reward. One way of displaying a\n",
      "policy for our grid-world robot is by an arrow in each cell indicating the direc-\n",
      "tion the robot should move when in that cell. In Fig. 11.3, we show an optimal\n",
      "policy displayed in this manner. In this chapter we will describe methods for\n",
      "learning optimal policies based on reward values received by the learner.\n",
      "\n",
      "\n",
      "537. 11.3. TEMPORAL DISCOUNTING AND OPTIMAL POLICIES\n",
      "145\n",
      "R\n",
      "G\n",
      "1 2 3 4\n",
      "5 6 7\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "Figure 11.2: A Grid World\n",
      "11.3\n",
      "Temporal Discounting and Optimal Poli-\n",
      "cies\n",
      "In delayed reinforcement learning, one often assumes that rewards in the distant\n",
      "future are not as valuable as are more immediate rewards. This preference can\n",
      "be accomodated by a temporal discount factor, 0 ≤γ < 1. The present value of\n",
      "a reward, ri, occuring i time units in the future, is taken to be γiri. Suppose\n",
      "we have a policy π(X) that maps input vectors into actions, and let rπ(X)\n",
      "i\n",
      "be\n",
      "the reward that will be received on the i-th time step after one begins executing\n",
      "policy π starting in state X. Then the total reward accumulated over all time\n",
      "steps by policy π beginning in state X is:\n",
      "V π(X) =\n",
      "∞\n",
      "X\n",
      "i=0\n",
      "γirπ(X)\n",
      "i\n",
      "One reason for using a temporal discount factor is so that the above sum will\n",
      "be ﬁnite. An optimal policy is one that maximizes V π(X) for all inputs, X.\n",
      "In general, we want to consider the case in which the rewards, ri, are random\n",
      "variables and in which the eﬀects of actions on environmental states are random.\n",
      "In Markovian environments, for example, the probability that action a in state\n",
      "Xi will lead to state Xj is given by a transition probability p[Xj|Xi, a]. Then,\n",
      "we will want to maximize expected future reward and would deﬁne V π(X) as:\n",
      "V π(X) = E\n",
      "\" ∞\n",
      "X\n",
      "i=0\n",
      "γirπ(X)\n",
      "i\n",
      "#\n",
      "In either case, we call V π(X) the value of policy π for input X.\n",
      "\n",
      "\n",
      "538. 146\n",
      "CHAPTER 11. DELAYED-REINFORCEMENT LEARNING\n",
      "R\n",
      "G\n",
      "1 2 3 4\n",
      "5 6 7\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "Figure 11.3: An Optimal Policy in the Grid World\n",
      "If the action prescribed by π taken in state X leads to state X′ (randomly\n",
      "according to the transition probabilities), then we can write V π(X) in terms of\n",
      "V π(X′) as follows:\n",
      "V π(X) = r[X, π(X)] + γ\n",
      "X\n",
      "X′\n",
      "p[X′|X, π(X)]V π(X′)\n",
      "where (in summary):\n",
      "γ = the discount factor,\n",
      "V π(X) = the value of state X under policy π,\n",
      "r[X, π(X)] = the expected immediate reward received when we execute the\n",
      "action prescribed by π in state X, and\n",
      "p[X′|X, π(X)] = the probability that the environment transitions to state\n",
      "X′ when we execute the action prescribed by π in state X.\n",
      "In other words, the value of state X under policy π is the expected value of\n",
      "the immediate reward received when executing the action recommended by π\n",
      "plus the average value (under π) of all of the states accessible from X.\n",
      "For an optimal policy, π∗(and no others!), we have the famous “optimality\n",
      "equation:”\n",
      "V π∗(X) = max\n",
      "a\n",
      "\"\n",
      "r(X, a) + γ\n",
      "X\n",
      "X′\n",
      "p[X′|X, a]V π∗(X′)\n",
      "#\n",
      "The theory of dynamic programming (DP) [Bellman, 1957, Ross, 1983] assures\n",
      "us that there is at least one optimal policy, π∗, that satisﬁes this equation. DP\n",
      "\n",
      "\n",
      "539. 11.4. Q-LEARNING\n",
      "147\n",
      "also provides methods for calculating V π∗(X) and at least one π∗, assuming\n",
      "that we know the average rewards and the transition probabilities. If we knew\n",
      "the transition probabilities, the average rewards, and V π∗(X) for all X and a,\n",
      "then it would be easy to implement an optimal policy. We would simply select\n",
      "that a that maximizes r(X, a) + γ P\n",
      "X′ p[X′|X, a]V π∗(X′). That is,\n",
      "π∗(X) = arg max\n",
      "a\n",
      "\"\n",
      "r(X, a) + γ\n",
      "X\n",
      "X′\n",
      "p[X′|X, a]V π∗(X′)\n",
      "#\n",
      "But, of course, we are assuming that we do not know these average rewards nor\n",
      "the transition probabilities, so we have to ﬁnd a method that eﬀectively learns\n",
      "them.\n",
      "If we had a model of actions, that is, if we knew for every state, X, and\n",
      "action a, which state, X′ resulted, then we could use a method called value\n",
      "iteration to ﬁnd an optimal policy. Value iteration works as follows: We begin\n",
      "by assigning, randomly, an estimated value ˆV (X) to every state, X. On the i-th\n",
      "step of the process, suppose we are at state Xi (that is, our input on the i-th\n",
      "step is Xi), and that the estimated value of state Xi on the i-th step is ˆVi(Xi).\n",
      "We then select that action a that maximizes the estimated value of the predicted\n",
      "subsequent state. Suppose this subsequent state having the highest estimated\n",
      "value is X′\n",
      "i. Then we update the estimated value, ˆVi(Xi), of state Xi as follows:\n",
      "ˆVi(X) = (1 −ci) ˆVi−1(X) + ci\n",
      "h\n",
      "ri + γ ˆVi−1(X′\n",
      "i)\n",
      "i\n",
      "if X = Xi,\n",
      "= ˆVi−1(X)\n",
      "otherwise.\n",
      "We see that this adjustment moves the value of ˆVi(Xi) an increment (depend-\n",
      "ing on ci) closer to\n",
      "h\n",
      "ri + γ ˆVi(X′\n",
      "i)\n",
      "i\n",
      ". Assuming that ˆVi(X′\n",
      "i) is a good estimate for\n",
      "Vi(X′\n",
      "i), then this adjustment helps to make the two estimates more consistent.\n",
      "Providing that 0 < ci < 1 and that we visit each state inﬁnitely often, this\n",
      "process of value iteration will converge to the optimal values.\n",
      "Discuss synchronous dynamic\n",
      "programming, asynchronous\n",
      "dynamic programming, and policy\n",
      "iteration.\n",
      "11.4\n",
      "Q-Learning\n",
      "Watkins [Watkins, 1989] has proposed a technique that he calls incremental\n",
      "dynamic programming. Let a; π stand for the policy that chooses action a once,\n",
      "and thereafter chooses actions according to policy π. We deﬁne:\n",
      "Qπ(X, a) = V a;π(X)\n",
      "\n",
      "\n",
      "540. 148\n",
      "CHAPTER 11. DELAYED-REINFORCEMENT LEARNING\n",
      "Then the optimal value from state X is given by:\n",
      "V π∗(X) = max\n",
      "a\n",
      "Qπ∗(X, a)\n",
      "This equation holds only for an optimal policy, π∗. The optimal policy is given\n",
      "by:\n",
      "π∗(X) = arg max\n",
      "a\n",
      "Qπ∗(X, a)\n",
      "Note that if an action a makes Qπ(X, a) larger than V π(X), then we can improve\n",
      "π by changing it so that π(X) = a. Making such a change is the basis for a\n",
      "powerful learning rule that we shall describe shortly.\n",
      "Suppose action a in state X leads to state X′. Then using the deﬁnitions of\n",
      "Q and V , it is easy to show that:\n",
      "Qπ(X, a) = r(X, a) + γE[V π(X′)]\n",
      "where r(X, a) is the average value of the immediate reward received when we\n",
      "execute action a in state X. For an optimal policy (and no others), we have\n",
      "another version of the optimality equation in terms of Q values:\n",
      "Qπ∗(X, a) = max\n",
      "a\n",
      "h\n",
      "r(X, a) + γE\n",
      "h\n",
      "Qπ∗(X′, a)\n",
      "ii\n",
      "for all actions, a, and states, X. Now, if we had the optimal Q values (for all\n",
      "a and X), then we could implement an optimal policy simply by selecting that\n",
      "action that maximized r(X, a) + γE\n",
      "\u0002\n",
      "Qπ∗(X′, a)\n",
      "\u0003\n",
      ".\n",
      "That is,\n",
      "π∗(X) = arg max\n",
      "a\n",
      "h\n",
      "r(X, a) + γE\n",
      "h\n",
      "Qπ∗(X′, a)\n",
      "ii\n",
      "Watkins’ proposal amounts to a TD(0) method of learning the Q values.\n",
      "We quote (with minor notational changes) from [Watkins & Dayan, 1992, page\n",
      "281]:\n",
      "“In Q-Learning, the agent’s experience consists of a sequence of dis-\n",
      "tinct stages or episodes. In the i-th episode, the agent:\n",
      "• observes its current state Xi,\n",
      "• selects [using the method described below] and performs an\n",
      "action ai,\n",
      "• observes the subsequent state X′\n",
      "i,\n",
      "• receives an immediate reward ri, and\n",
      "\n",
      "\n",
      "541. 11.4. Q-LEARNING\n",
      "149\n",
      "• adjusts its Qi−1 values using a learning factor ci, according to:\n",
      "Qi(X, a) = (1 −ci)Qi−1(X, a) + ci[ri + γVi−1(X′\n",
      "i)]\n",
      "if X = Xi and a = ai,\n",
      "= Qi−1(X, a)\n",
      "otherwise,\n",
      "where\n",
      "Vi−1(X′) = max\n",
      "b\n",
      "[Qi−1(X′, b)]\n",
      "is the best the agent thinks it can do from state X′. . . . The\n",
      "initial Q values, Q0(X, a), for all states and actions are assumed\n",
      "given.”\n",
      "Using the current Q values, Qi(X, a), the agent always selects that action\n",
      "that maximizes Qi(X, a).\n",
      "Note that only the Q value corresponding to the\n",
      "state just exited and the action just taken is adjusted. And that Q value is\n",
      "adjusted so that it is closer (by an amount determined by ci) to the sum of\n",
      "the immediate reward plus the discounted maximum (over all actions) of the Q\n",
      "values of the state just entered. If we imagine the Q values to be predictions of\n",
      "ultimate (inﬁnite horizon) total reward, then the learning procedure described\n",
      "above is exactly a TD(0) method of learning how to predict these Q values.\n",
      "Q learning strengthens the usual TD methods, however, because TD (applied\n",
      "to reinforcement problems using value iteration) requires a one-step lookahead,\n",
      "using a model of the eﬀects of actions, whereas Q learning does not.\n",
      "A convenient notation (proposed by [Schwartz, 1993]) for representing the\n",
      "change in Q value is:\n",
      "Q(X, a)\n",
      "β\n",
      "←−r + γV (X′)\n",
      "where Q(X, a) is the new Q value for input X and action a, r is the immediate\n",
      "reward when action a is taken in response to input X, V (X′) is the maximum\n",
      "(over all actions) of the Q value of the state next reached when action a is taken\n",
      "from state X, and β is the fraction of the way toward which the new Q value,\n",
      "Q(X, a), is adjusted to equal r + γV (X′).\n",
      "Watkins and Dayan [Watkins & Dayan, 1992] prove that, under certain con-\n",
      "ditions, the Q values computed by this learning procedure converge to optimal\n",
      "ones (that is, to ones on which an optimal policy can be based).\n",
      "We deﬁne ni(X, a) as the index (episode number) of the i-th time that action\n",
      "a is tried in state X. Then, we have:\n",
      "\n",
      "\n",
      "542. 150\n",
      "CHAPTER 11. DELAYED-REINFORCEMENT LEARNING\n",
      "Theorem 11.1 (Watkins and Dayan) For Markov problems with states {X}\n",
      "and actions {a}, and given bounded rewards |rn| ≤R, learning rates 0 ≤cn < 1,\n",
      "and\n",
      "∞\n",
      "X\n",
      "i=0\n",
      "cni(X,a) = ∞,\n",
      "∞\n",
      "X\n",
      "i=0\n",
      "\u0002\n",
      "cni(X,a)\n",
      "\u00032 < ∞\n",
      "for all X and a, then\n",
      "Qn(X, a) →Q∗\n",
      "n(X, a) as n →∞, for all X and a, with probability 1, where\n",
      "Q∗\n",
      "n(X, a) corresponds to the Q values of an optimal policy.\n",
      "Again, we quote from [Watkins & Dayan, 1992, page 281]:\n",
      "“The most important condition implicit in the convergence theorem\n",
      ". . . is that the sequence of episodes that forms the basis of learning\n",
      "must include an inﬁnite number of episodes for each starting state\n",
      "and action. This may be considered a strong condition on the way\n",
      "states and actions are selected—however, under the stochastic con-\n",
      "ditions of the theorem, no method could be guaranteed to ﬁnd an\n",
      "optimal policy under weaker conditions. Note, however, that the\n",
      "episodes need not form a continuous sequence—that is the X′ of one\n",
      "episode need not be the X of the next episode.”\n",
      "The relationships among Q learning, dynamic programming, and control\n",
      "are very well described in [Barto, Bradtke, & Singh, 1994]. Q learning is best\n",
      "thought of as a stochastic approximation method for calculating the Q values.\n",
      "Although the deﬁnition of the optimal Q values for any state depends recursively\n",
      "on expected values of the Q values for subsequent states (and on the expected\n",
      "values of rewards), no expected values are explicitly computed by the procedure.\n",
      "Instead, these values are approximated by iterative sampling using the actual\n",
      "stochastic mechanism that produces successor states.\n",
      "11.5\n",
      "Discussion, Limitations, and Extensions of\n",
      "Q-Learning\n",
      "11.5.1\n",
      "An Illustrative Example\n",
      "The Q-learning procedure requires that we maintain a table of Q(X, a) values\n",
      "for all state-action pairs. In the grid world that we described earlier, such a\n",
      "table would not be excessively large. We might start with random entries in the\n",
      "table; a portion of such an intial table might be as follows:\n",
      "\n",
      "\n",
      "543. 11.5. DISCUSSION, LIMITATIONS, AND EXTENSIONS OF Q-LEARNING151\n",
      "X\n",
      "a\n",
      "Q(X, a)\n",
      "r(X, a)\n",
      "(2,3)\n",
      "w\n",
      "7\n",
      "0\n",
      "(2,3)\n",
      "n\n",
      "4\n",
      "0\n",
      "(2,3)\n",
      "e\n",
      "3\n",
      "0\n",
      "(2,3)\n",
      "s\n",
      "6\n",
      "0\n",
      "(1,3)\n",
      "w\n",
      "4\n",
      "-1\n",
      "(1,3)\n",
      "n\n",
      "5\n",
      "0\n",
      "(1,3)\n",
      "e\n",
      "2\n",
      "0\n",
      "(1,3)\n",
      "s\n",
      "4\n",
      "0\n",
      "Suppose the robot is in cell (2,3). The maximum Q value occurs for a = w, so the\n",
      "robot moves west to cell (1,3)—receiving no immediate reward. The maximum\n",
      "Q value in cell (1,3) is 5, and the learning mechanism attempts to make the\n",
      "value of Q((2, 3), w) closer to the discounted value of 5 plus the immediate\n",
      "reward (which was 0 in this case). With a learning rate parameter c = 0.5\n",
      "and γ = 0.9, the Q value of Q((2, 3), w) is adjusted from 7 to 5.75. No other\n",
      "changes are made to the table at this episode. The reader might try this learning\n",
      "procedure on the grid world with a simple computer program. Notice that an\n",
      "optimal policy might not be discovered if some cells are not visited nor some\n",
      "actions not tried frequently enough.\n",
      "The learning problem faced by the agent is to associate speciﬁc actions with\n",
      "speciﬁc input patterns. Q learning gradually reinforces those actions that con-\n",
      "tribute to positive rewards by increasing the associated Q values. Typically, as\n",
      "in this example, rewards occur somewhat after the actions that lead to them—\n",
      "hence the phrase delayed-reinforcement learning. One can imagine that better\n",
      "and better approximations to the optimal Q values gradually propagate back\n",
      "from states producing rewards toward all of the other states that the agent fre-\n",
      "quently visits. With random Q values to begin, the agent’s actions amount to a\n",
      "random walk through its space of states. Only when this random walk happens\n",
      "to stumble into rewarding states does Q learning begin to produce Q values\n",
      "that are useful, and, even then, the Q values have to work their way outward\n",
      "from these rewarding states. The general problem of associating rewards with\n",
      "state-action pairs is called the temporal credit assignment problem—how should\n",
      "credit for a reward be apportioned to the actions leading up to it? Q learning is,\n",
      "to date, the most successful technique for temporal credit assignment, although\n",
      "a related method, called the bucket brigade algorithm, has been proposed by\n",
      "[Holland, 1986].\n",
      "Learning problems similar to that faced by the agent in our grid world have\n",
      "been thoroughly studied by Sutton who has proposed an architecture, called\n",
      "DYNA, for solving them [Sutton, 1990]. DYNA combines reinforcement learning\n",
      "with planning. Sutton characterizes planning as learning in a simulated world\n",
      "that models the world that the agent inhabits. The agent’s model of the world\n",
      "is obtained by Q learning in its actual world, and planning is accomplished by\n",
      "Q learning in its model of the world.\n",
      "We should note that the learning problem faced by our grid-world robot\n",
      "\n",
      "\n",
      "544. 152\n",
      "CHAPTER 11. DELAYED-REINFORCEMENT LEARNING\n",
      "could be modiﬁed to have several places in the grid that give positive rewards.\n",
      "This possibility presents an interesting way to generalize the classical notion of\n",
      "a “goal” in AI planning systems—even in those that do no learning. Instead of\n",
      "representing a goal as a condition to be achieved, we represent a “goal struc-\n",
      "ture” as a set of rewards to be given for achieving various conditions. Then,\n",
      "the generalized “goal” becomes maximizing discounted future reward instead of\n",
      "simply achieving some particular condition. This generalization can be made to\n",
      "encompass so-called goals of maintenance and goals of avoidance. The exam-\n",
      "ple presented above included avoiding bumping into the grid-world boundary.\n",
      "A goal of maintenance, of a particular state, could be expressed in terms of a\n",
      "reward that was earned whenever the agent was in that state and performed an\n",
      "action that transitioned back to that state in one step.\n",
      "11.5.2\n",
      "Using Random Actions\n",
      "When the next pattern presentation in a sequence of patterns is the one caused\n",
      "by the agent’s own action in response to the last pattern, we have what is called\n",
      "an on-line learning method. In Watkins and Dayan’s terminology, in on-line\n",
      "learning the episodes form a continous sequence. As already mentioned, the\n",
      "convergence theorem for Q learning does not require on-line learning; indeed,\n",
      "special precautions must be taken to ensure that on-line learning meets the\n",
      "conditions of the theorem.\n",
      "If on-line learning discovers some good paths to\n",
      "rewards, the agent may ﬁxate on these and never discover a policy that leads\n",
      "to a possibly greater long-term reward. In reinforcement learning phraseology,\n",
      "this problem is referred to as the problem of exploitation (of already learned\n",
      "behavior) versus exploration (of possibly better behavior).\n",
      "One way to force exploration is to perform occasional random actions (in-\n",
      "stead of that single action prescribed by the current Q values). For example,\n",
      "in the grid-world problem, one could imagine selecting an action randomly ac-\n",
      "cording to a probability distribution over the actions (n, e, s, and w).\n",
      "This\n",
      "distribution, in turn, could depend on the Q values. For example, we might\n",
      "ﬁrst ﬁnd that action prescribed by the Q values and then choose that action\n",
      "with probability 1/2, choose the two orthogonal actions with probability 3/16\n",
      "each, and choose the opposite action with probability 1/8. This policy might be\n",
      "modiﬁed by “simulated annealing” which would gradually increase the probabil-\n",
      "ity of the action prescribed by the Q values more and more as time goes on. This\n",
      "strategy would favor exploration at the beginning of learning and exploitation\n",
      "later.\n",
      "Other methods, also, have been proposed for dealing with exploration, in-\n",
      "cluding making unvisited states intrinsically rewarding and using an “interval\n",
      "estimate,” which is related to the uncertainty in the estimate of a state’s value\n",
      "[Kaelbling, 1993].\n",
      "\n",
      "\n",
      "545. 11.5. DISCUSSION, LIMITATIONS, AND EXTENSIONS OF Q-LEARNING153\n",
      "11.5.3\n",
      "Generalizing Over Inputs\n",
      "For large problems it would be impractical to maintain a table like that used\n",
      "in our grid-world example. Various researchers have suggested mechanisms for\n",
      "computing Q values, given pattern inputs and actions. One method that sug-\n",
      "gests itself is to use a neural network. For example, consider the simple linear\n",
      "machine shown in Fig. 11.4.\n",
      "X\n",
      ". . .\n",
      ". . .\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "trainable weights\n",
      "Y\n",
      "Wi\n",
      "R dot product units\n",
      "Q(ai, X) = X . Wi\n",
      "Q(a1, X)\n",
      "Q(a2, X)\n",
      "Q(aR, X)\n",
      "Figure 11.4: A Net that Computes Q Values\n",
      "Such a neural net could be used by an agent that has R actions to select\n",
      "from. The Q values (as a function of the input pattern X and the action ai) are\n",
      "computed as dot products of weight vectors (one for each action) and the input\n",
      "vector. Weight adjustments are made according to a TD(0) procedure to bring\n",
      "the Q value for the action last selected closer to the sum of the immediate reward\n",
      "(if any) and the (discounted) maximum Q value for the next input pattern.\n",
      "If the optimum Q values for the problem (whatever they might be) are more\n",
      "complex than those that can be computed by a linear machine, a layered neural\n",
      "network might be used. Sigmoid units in the ﬁnal layer would compute Q values\n",
      "in the range 0 to 1. The TD(0) method for updating Q values would then have to\n",
      "be combined with a multi-layer weight-changing rule, such as backpropagation.\n",
      "Networks of this sort are able to aggregate diﬀerent input vectors into regions\n",
      "for which the same action should be performed. This kind of aggregation is an\n",
      "example of what has been called structural credit assignment. Combining TD(λ)\n",
      "and backpropagation is a method for dealing with both the temporal and the\n",
      "structural credit assignment problems.\n",
      "\n",
      "\n",
      "546. 154\n",
      "CHAPTER 11. DELAYED-REINFORCEMENT LEARNING\n",
      "Interesting examples of delayed-reinforcement training of simulated and\n",
      "actual robots requiring structural credit assignment have been reported by\n",
      "[Lin, 1992, Mahadevan & Connell, 1992].\n",
      "11.5.4\n",
      "Partially Observable States\n",
      "So far, we have identiﬁed the input vector, X, with the actual state of the envi-\n",
      "ronment. When the input vector results from an agent’s perceptual apparatus\n",
      "(as we assume it does), there is no reason to suppose that it uniquely identiﬁes\n",
      "the environmental state. Because of inevitable perceptual limitations, several\n",
      "diﬀerent environmental states might give rise to the same input vector. This\n",
      "phenomenon has been referred to as perceptual aliasing. With perceptual alias-\n",
      "ing, we can no longer guarantee that Q learning will result in even useful action\n",
      "policies, let alone optimal ones. Several researchers have attempted to deal with\n",
      "this problem using a variety of methods including attempting to model “hid-\n",
      "den” states by using internal memory [Lin, 1993]. That is, if some aspect of\n",
      "the environment cannot be sensed currently, perhaps it was sensed once and\n",
      "can be remembered by the agent. When such is the case, we no longer have a\n",
      "Markov problem; that is, the next X vector, given any action, may depend on\n",
      "a sequence of previous ones rather than just the immediately preceding one. It\n",
      "might be possible to reinstate a Markov framework (over the X’s) if X includes\n",
      "not only current sensory precepts but information from the agent’s memory.\n",
      "11.5.5\n",
      "Scaling Problems\n",
      "Several diﬃculties have so far prohibited wide application of reinforcement learn-\n",
      "ing to large problems. (The TD-gammon program, mentioned in the last chap-\n",
      "ter, is probably unique in terms of success on a high-dimensional problem.)\n",
      "We have already touched on some diﬃculties; these and others are summarized\n",
      "below with references to attempts to overcome them.\n",
      "a. Exploration versus exploitation.\n",
      "• use random actions\n",
      "• favor states not visited recently\n",
      "• separate the learning phase from the use phase\n",
      "• employ a teacher to guide exploration\n",
      "b. Slow time to convergence\n",
      "• combine learning with prior knowledge; use estimates of Q values\n",
      "(rather than random values) initially\n",
      "• use a hierarchy of actions; learn primitive actions ﬁrst and freeze the\n",
      "useful sequences into macros and then learn how to use the macros\n",
      "\n",
      "\n",
      "547. 11.6. BIBLIOGRAPHICAL AND HISTORICAL REMARKS\n",
      "155\n",
      "• employ a teacher; use graded “lessons”—starting near the rewards\n",
      "and then backing away, and use examples of good behavior [Lin, 1992]\n",
      "• use more eﬃcient computations; e.g. do several updates per episode\n",
      "[Moore & Atkeson, 1993]\n",
      "c. Large state spaces\n",
      "• use hand-coded features\n",
      "• use neural networks\n",
      "• use nearest-neighbor methods [Moore, 1990]\n",
      "d. Temporal discounting problems. Using small γ can make the learner too\n",
      "greedy for present rewards and indiﬀerent to the future; but using large γ\n",
      "slows down learning.\n",
      "• use a learning method based on average rewards [Schwartz, 1993]\n",
      "e. No “transfer” of learning . What is learned depends on the reward struc-\n",
      "ture; if the rewards change, learning has to start over.\n",
      "• Separate the learning into two parts: learn an “action model” which\n",
      "predicts how actions change states (and is constant over all prob-\n",
      "lems), and then learn the “values” of states by reinforcement learn-\n",
      "ing for each diﬀerent set of rewards. Sometimes the reinforcement\n",
      "learning part can be replaced by a “planner” that uses the action\n",
      "model to produce plans to achieve goals.\n",
      "Also see other articles in the special issue on reinforcement learning: Machine\n",
      "Learning, 8, May, 1992.\n",
      "11.6\n",
      "Bibliographical and Historical Remarks\n",
      "To be added.\n",
      "\n",
      "\n",
      "548. 156\n",
      "CHAPTER 11. DELAYED-REINFORCEMENT LEARNING\n",
      "\n",
      "\n",
      "549. Chapter 12\n",
      "Explanation-Based\n",
      "Learning\n",
      "12.1\n",
      "Deductive Learning\n",
      "In the learning methods studied so far, typically the training set does not ex-\n",
      "haust the version space. Using logical terminology, we could say that the classi-\n",
      "ﬁer’s output does not logically follow from the training set. In this sense, these\n",
      "methods are inductive. In logic, a deductive system is one whose conclusions\n",
      "logically follow from a set of input facts, if the system is sound.1\n",
      "To contrast inductive with deductive systems in a logical setting, suppose\n",
      "we have a set of facts (the training set) that includes the following formulas:\n",
      "{Round(Obj1), Round(Obj2), Round(Obj3), Round(Obj4),\n",
      "Ball(Obj1), Ball(Obj2), Ball(Obj3), Ball(Obj4)}\n",
      "A learning system that forms the conclusion (∀x)[Ball(x) ⊃Round(x)] is in-\n",
      "ductive.\n",
      "This conclusion may be useful (if there are no facts of the form\n",
      "Ball(σ) ∧¬Round(σ)), but it does not logically follow from the facts. On the\n",
      "other hand, if we had the facts Green(Obj5) and Green(Obj5) ⊃Round(Obj5),\n",
      "then we could logically conclude Round(Obj5). Making this conclusion and sav-\n",
      "ing it is an instance of deductive learning—a topic we study in this chapter.\n",
      "Suppose that some logical proposition, φ, logically follows from some set of\n",
      "facts, ∆. Under what circumstances might we say that the process of deducing\n",
      "φ from ∆results in our learning φ? In a sense, we implicitly knew φ all along,\n",
      "since it was inherent in knowing ∆. Yet, φ might not be obvious given ∆, and\n",
      "1Logical reasoning systems that are not sound, for example those using non-monotonic\n",
      "reasoning, themselves might produce inductive conclusions that do not logically follow from\n",
      "the input facts.\n",
      "157\n",
      "\n",
      "\n",
      "550. 158\n",
      "CHAPTER 12. EXPLANATION-BASED LEARNING\n",
      "the deduction process to establish φ might have been arduous. Rather than have\n",
      "to deduce φ again, we might want to save it, perhaps along with its deduction,\n",
      "in case it is needed later. Shouldn’t that process count as learning? Dietterich\n",
      "[Dietterich, 1990] has called this type of learning speed-up learning.\n",
      "Strictly speaking, speed-up learning does not result in a system being able to\n",
      "make decisions that, in principle, could not have been made before the learning\n",
      "took place. Speed-up learning simply makes it possible to make those decisions\n",
      "more eﬃciently.\n",
      "But, in practice, this type of learning might make possible\n",
      "certain decisions that might otherwise have been infeasible.\n",
      "To take an extreme case, a chess player can be said to learn chess even though\n",
      "optimal play is inherent in the rules of chess. On the surface, there seems to be\n",
      "no real diﬀerence between the experience-based hypotheses that a chess player\n",
      "makes about what constitutes good play and the kind of learning we have been\n",
      "studying so far.\n",
      "As another example, suppose we are given some theorems about geometry\n",
      "and are asked to prove that the sum of the angles of a right triangle is 180\n",
      "degrees. Let us further suppose that the proof we constructed did not depend\n",
      "on the given triangle being a right triangle; in that case we can learn a more\n",
      "general fact. The learning technique that we are going to study next is related\n",
      "to this example.\n",
      "It is called explanation-based learning (EBL). EBL can be\n",
      "thought of as a process in which implicit knowledge is converted into explicit\n",
      "knowledge.\n",
      "In EBL, we specialize parts of a domain theory to explain a particular ex-\n",
      "ample, then we generalize the explanation to produce another element of the\n",
      "domain theory that will be useful on similar examples. This process is illustrated\n",
      "in Fig. 12.1.\n",
      "12.2\n",
      "Domain Theories\n",
      "Two types of information were present in the inductive methods we have studied:\n",
      "the information inherent in the training samples and the information about the\n",
      "domain that is implied by the “bias” (for example, the hypothesis set from which\n",
      "we choose functions). The learning methods are successful only if the hypothesis\n",
      "set is appropriate for the problem. Typically, the smaller the hypothesis set (that\n",
      "is, the more a priori information we have about the function being sought), the\n",
      "less dependent we are on information being supplied by a training set (that\n",
      "is, fewer samples). A priori information about a problem can be expressed in\n",
      "several ways. The methods we have studied so far restrict the hypotheses in a\n",
      "rather direct way. A less direct method involves making assertions in a logical\n",
      "language about the property we are trying to learn. A set of such assertions is\n",
      "usually called a “domain theory.”\n",
      "Suppose, for example, that we wanted to classify people according to whether\n",
      "or not they were good credit risks. We might represent a person by a set of\n",
      "properties (income, marital status, type of employment, etc.), assemble such\n",
      "\n",
      "\n",
      "551. 12.3. AN EXAMPLE\n",
      "159\n",
      "Domain\n",
      "Theory\n",
      "Example\n",
      "(X is P)\n",
      "Prove: X is P\n",
      "specialize\n",
      "Explanation\n",
      "(Proof)\n",
      "generalize\n",
      "A New Domain Rule:\n",
      "Things \"like\" X are P\n",
      "Y is like X\n",
      "Complex Proof\n",
      "Process\n",
      "Trivial  Proof\n",
      "Y is P\n",
      "Figure 12.1: The EBL Process\n",
      "data about people who are known to be good and bad credit risks and train a\n",
      "classiﬁer to make decisions. Or, we might go to a loan oﬃcer of a bank, ask him\n",
      "or her what sorts of things s/he looks for in making a decision about a loan,\n",
      "encode this knowledge into a set of rules for an expert system, and then use\n",
      "the expert system to make decisions. The knowledge used by the loan oﬃcer\n",
      "might have originated as a set of “policies” (the domain theory), but perhaps the\n",
      "application of these policies were specialized and made more eﬃcient through\n",
      "experience with the special cases of loans made in his or her district.\n",
      "12.3\n",
      "An Example\n",
      "To make our discussion more concrete, let’s consider the following fanciful exam-\n",
      "ple. We want to ﬁnd a way to classify robots as “robust” or not. The attributes\n",
      "that we use to represent a robot might include some that are relevant to this\n",
      "decision and some that are not.\n",
      "\n",
      "\n",
      "552. 160\n",
      "CHAPTER 12. EXPLANATION-BASED LEARNING\n",
      "Suppose we have a domain theory of logical sentences that taken together,\n",
      "help to deﬁne whether or not a robot can be classiﬁed as robust. (The same\n",
      "domain theory may be useful for several other purposes also, but among other\n",
      "things, it describes the concept “robust.”)\n",
      "In this example, let’s suppose that our domain theory includes the sentences:\n",
      "Fixes(u, u) ⊃Robust(u)\n",
      "(An individual that can ﬁx itself is robust.)\n",
      "Sees(x, y) ∧Habile(x) ⊃Fixes(x, y)\n",
      "(A habile individual that can see another entity can ﬁx that entity.)\n",
      "Robot(w) ⊃Sees(w, w)\n",
      "(All robots can see themselves.)\n",
      "R2D2(x) ⊃Habile(x)\n",
      "(R2D2-class individuals are habile.)\n",
      "C3PO(x) ⊃Habile(x)\n",
      "(C3PO-class individuals are habile.)\n",
      ". . .\n",
      "(By convention, variables are assumed to be universally quantiﬁed.) We could\n",
      "use theorem-proving methods operating on this domain theory to conclude\n",
      "whether certain robots are robust. These methods might be computationally\n",
      "quite expensive because extensive search may have to be performed to derive a\n",
      "conclusion. But after having found a proof for some particular robot, we might\n",
      "be able to derive some new sentence whose use allows a much faster conclusion.\n",
      "We next show how such a new rule might be derived in this example. Suppose\n",
      "we are given a number of facts about Num5, such as:\n",
      "Robot(Num5)\n",
      "R2D2(Num5)\n",
      "Age(Num5, 5)\n",
      "Manufacturer(Num5, GR)\n",
      ". . .\n",
      "\n",
      "\n",
      "553. 12.3. AN EXAMPLE\n",
      "161\n",
      "Fixes(u, u) => Robust(u)\n",
      "Robust(Num5)\n",
      "Fixes(Num5, Num5)\n",
      "Sees(Num5,Num5)\n",
      "Habile(Num5)\n",
      "Sees(x,y) & Habile(x)\n",
      "              => Fixes(x,y)\n",
      "Robot(w)\n",
      "     => Sees(w,w)\n",
      "Robot(Num5)\n",
      "R2D2(x)\n",
      "         => Habile(x)\n",
      "R2D2(Num5)\n",
      "Figure 12.2: A Proof Tree\n",
      "We are also told that Robust(Num5) is true, but we nevertheless attempt to\n",
      "ﬁnd a proof of that assertion using these facts about Num5 and the domain\n",
      "theory. The facts about Num5 correspond to the features that we might use\n",
      "to represent Num5. In this example, not all of them are relevant to a decision\n",
      "about Robust(Num5). The relevant ones are those used or needed in proving\n",
      "Robust(Num5) using the domain theory. The proof tree in Fig. 12.2 is one that\n",
      "a typical theorem-proving system might produce.\n",
      "In the language of EBL, this proof is an explanation for the fact\n",
      "Robust(Num5). We see from this explanation that the only facts about Num5\n",
      "that were used were Robot(Num5) and R2D2(Num5). In fact, we could con-\n",
      "struct the following rule from this explanation:\n",
      "Robot(Num5) ∧R2D2(Num5) ⊃Robust(Num5)\n",
      "The explanation has allowed us to prune some attributes about Num5 that are\n",
      "irrelevant (at least for deciding Robust(Num5)). This type of pruning is the ﬁrst\n",
      "sense in which an explanation is used to generalize the classiﬁcation problem.\n",
      "([DeJong & Mooney, 1986] call this aspect of explanation-based learning feature\n",
      "elimination.) But the rule we extracted from the explanation applies only to\n",
      "Num5. There might be little value in learning that rule since it is so speciﬁc.\n",
      "Can it be generalized so that it can be applied to other individuals as well?\n",
      "\n",
      "\n",
      "554. 162\n",
      "CHAPTER 12. EXPLANATION-BASED LEARNING\n",
      "Examination of the proof shows that the same proof structure, using the\n",
      "same sentences from the domain theory, could be used independently of whether\n",
      "we are talking about Num5 or some other individual. We can generalize the\n",
      "proof by a process that replaces constants in the tip nodes of the proof tree\n",
      "with variables and works upward—using uniﬁcation to constrain the values of\n",
      "variables as needed to obtain a proof.\n",
      "In this example, we replace Robot(Num5) by Robot(r) and R2D2(Num5)\n",
      "by R2D2(s) and redo the proof—using the explanation proof as a template.\n",
      "Note that we use diﬀerent values for the two diﬀerent occurrences of Num5 at\n",
      "the tip nodes. Doing so sometimes results in more general, but nevertheless\n",
      "valid rules. We now apply the rules used in the proof in the forward direction,\n",
      "keeping track of the substitutions imposed by the most general uniﬁers used in\n",
      "the proof. (Note that we always substitute terms that are already in the tree for\n",
      "variables in rules.) This process results in the generalized proof tree shown in\n",
      "Fig. 12.3. Note that the occurrence of Sees(r, r) as a node in the tree forces the\n",
      "uniﬁcation of x with y in the domain rule, Sees(x, y)∧Habile(y) ⊃Fixes(x, y).\n",
      "The substitutions are then applied to the variables in the tip nodes and the root\n",
      "node to yield the general rule: Robot(r) ∧R2D2(r) ⊃Robust(r).\n",
      "This rule is the end result of EBL for this example.\n",
      "The process\n",
      "by which Num5 in this example was generalized to a variable is what\n",
      "[DeJong & Mooney, 1986] call identity elimination (the precise identity of Num5\n",
      "turned out to be irrelevant). (The generalization process described in this ex-\n",
      "ample is based on that of [DeJong & Mooney, 1986] and diﬀers from that of\n",
      "[Mitchell, et al., 1986]. It is also similar to that used in [Fikes, et al., 1972].)\n",
      "Clearly, under certain assumptions, this general rule is more easily used to con-\n",
      "clude Robust about an individual than the original proof process was.\n",
      "It is important to note that we could have derived the general rule from the\n",
      "domain theory without using the example. (In the literature, doing so is called\n",
      "static analysis [Etzioni, 1991].) In fact, the example told us nothing new other\n",
      "than what it told us about Num5. The sole role of the example in this instance\n",
      "of EBL was to provide a template for a proof to help guide the generalization\n",
      "process. Basing the generalization process on examples helps to insure that we\n",
      "learn rules matched to the distribution of problems that occur.\n",
      "There are a number of qualiﬁcations and elaborations about EBL that need\n",
      "to be mentioned.\n",
      "12.4\n",
      "Evaluable Predicates\n",
      "The domain theory includes a number of predicates other than the one occuring\n",
      "in the formula we are trying to prove and other than those that might custom-\n",
      "arily be used to describe an individual. One might note, for example, that if we\n",
      "used Habile(Num5) to describe Num5, the proof would have been shorter. Why\n",
      "didn’t we? The situation is analogous to that of using a data base augmented\n",
      "by logical rules. In the latter application, the formulas in the actual data base\n",
      "\n",
      "\n",
      "555. 12.4. EVALUABLE PREDICATES\n",
      "163\n",
      "Robust(r)\n",
      "Fixes(r, r)\n",
      "Sees(r,r)\n",
      "Habile(s)\n",
      "Robot(r)\n",
      "R2D2(s)\n",
      "{r/w}\n",
      "{s/x}\n",
      "{r/x, r/y, r/s}\n",
      "{r/u}\n",
      "Robot(w)\n",
      "     => Sees(w,w)\n",
      "R2D2(x)\n",
      "         => Habile(x)\n",
      "Sees(x,y) & Habile(x)\n",
      "              => Fixes(x,y)\n",
      "Fixes(u, u) => Robust(u)\n",
      "becomes R2D2(r) after\n",
      "applying {r/s}\n",
      "Figure 12.3: A Generalized Proof Tree\n",
      "are “extensional,” and those in the logical rules are “intensional.” This usage\n",
      "reﬂects the fact that the predicates in the data base part are deﬁned by their\n",
      "extension—we explicitly list all the tuples sastisfying a relation.\n",
      "The logical\n",
      "rules serve to connect the data base predicates with higher level abstractions\n",
      "that are described (if not deﬁned) by the rules. We typically cannot look up\n",
      "the truth values of formulas containing these intensional predicates; they have\n",
      "to be derived using the rules and the database.\n",
      "The EBL process assumes something similar. The domain theory is useful\n",
      "for connecting formulas that we might want to prove with those whose truth\n",
      "values can be “looked up” or otherwise evaluated. In the EBL literature, such\n",
      "formulas satisfy what is called the operationality criterion.\n",
      "Perhaps another\n",
      "analogy might be to neural networks. The evaluable predicates correspond to\n",
      "the components of the input pattern vector; the predicates in the domain theory\n",
      "correspond to the hidden units. Finding the new rule corresponds to ﬁnding a\n",
      "simpler expression for the formula to be proved in terms only of the evaluable\n",
      "predicates.\n",
      "\n",
      "\n",
      "556. 164\n",
      "CHAPTER 12. EXPLANATION-BASED LEARNING\n",
      "12.5\n",
      "More General Proofs\n",
      "Examining the domain theory for our example reveals that an alternative rule\n",
      "might have been:\n",
      "Robot(u) ∧C3PO(u) ⊃Robust(u).\n",
      "Such a rule might\n",
      "have resulted if we were given {C3PO(Num6), Robot(Num6), . . .} and proved\n",
      "Robust(Num6).\n",
      "After considering these two examples (Num5 and Num6),\n",
      "the question arises, do we want to generalize the two rules to something like:\n",
      "Robot(u)∧[C3PO(u)∨R2D2(u)] ⊃Robust(u)? Doing so is an example of what\n",
      "[DeJong & Mooney, 1986] call structural generalization (via disjunctive augmen-\n",
      "tation ).\n",
      "Adding disjunctions for every alternative proof can soon become cumbersome\n",
      "and destroy any eﬃciency advantage of EBL. In our example, the eﬃciency\n",
      "might be retrieved if there were another evaluable predicate, say, Bionic(u) such\n",
      "that the domain theory also contained R2D2(x) ⊃Bionic(x) and C3PO(x) ⊃\n",
      "Bionic(x). After seeing a number of similar examples, we might be willing to\n",
      "induce the formula Bionic(u) ⊃[C3PO(u) ∨R2D2(u)] in which case the rule\n",
      "with the disjunction could be replaced with Robot(u) ∧Bionic(u) ⊃Robust(u).\n",
      "12.6\n",
      "Utility of EBL\n",
      "It is well known in theorem proving that the complexity of ﬁnding a proof\n",
      "depends both on the number of formulas in the domain theory and on the depth\n",
      "of the shortest proof. Adding a new rule decreases the depth of the shortest\n",
      "proof but it also increases the number of formulas in the domain theory. In\n",
      "realistic applications, the added rules will be relevant for some tasks and not for\n",
      "others. Thus, it is unclear whether the overall utility of the new rules will turn\n",
      "out to be positive. EBL methods have been applied in several settings, usually\n",
      "with positive utility. (See [Minton, 1990] for an analysis).\n",
      "12.7\n",
      "Applications\n",
      "There have been several applications of EBL methods. We mention two here,\n",
      "namely the formation of macro-operators in automatic plan generation and\n",
      "learning how to control search.\n",
      "12.7.1\n",
      "Macro-Operators in Planning\n",
      "In automatic planning systems, eﬃciency can sometimes be enhanced by chain-\n",
      "ing together a sequence of operators into macro-operators. We show an exam-\n",
      "ple of a process for creating macro-operators based on techniques explored by\n",
      "[Fikes, et al., 1972].\n",
      "Referring to Fig. 12.4, consider the problem of ﬁnding a plan for a robot in\n",
      "room R1 to fetch a box, B1, by going to an adjacent room, R2, and pushing it\n",
      "\n",
      "\n",
      "557. 12.7. APPLICATIONS\n",
      "165\n",
      "back to R1. The goal for the robot is INROOM(B1, R1), and the facts that\n",
      "are true in the initial state are listed in the ﬁgure.\n",
      "R1\n",
      "R2\n",
      "R3\n",
      "D1\n",
      "D2\n",
      "B1\n",
      "Initial State:\n",
      "INROOM(ROBOT, R1)\n",
      "INROOM(B1,R2)\n",
      "CONNECTS(D1,R1,R2)\n",
      "CONNECTS(D1,R2,R1)\n",
      ". . .\n",
      "Figure 12.4: Initial State of a Robot Problem\n",
      "We will construct the plan from a set of STRIPS operators that include:\n",
      "GOTHRU(d, r1, r2)\n",
      "Preconditions: INROOM(ROBOT, r1), CONNECTS(d, r1, r2)\n",
      "Delete list: INROOM(ROBOT, r1)\n",
      "Add list: INROOM(ROBOT, r2)\n",
      "PUSHTHRU(b, d, r1, r2)\n",
      "Preconditions: INROOM(ROBOT, r1), CONNECTS(d, r1, r2), INROOM(b, r1)\n",
      "Delete list: INROOM(ROBOT, r1), INROOM(b, r1)\n",
      "Add list: INROOM(ROBOT, r2), INROOM(b, r2)\n",
      "A backward-reasoning STRIPS system might produce the plan shown in\n",
      "Fig. 12.5. We show there the main goal and the subgoals along a solution path.\n",
      "(The conditions in each subgoal that are true in the initial state are shown\n",
      "underlined.) The preconditions for this plan, true in the initial state, are:\n",
      "INROOM(ROBOT, R1)\n",
      "\n",
      "\n",
      "558. 166\n",
      "CHAPTER 12. EXPLANATION-BASED LEARNING\n",
      "CONNECTS(D1, R1, R2)\n",
      "CONNECTS(D1, R2, R1)\n",
      "INROOM(B1, R2)\n",
      "Saving this speciﬁc plan, valid only for the speciﬁc constants it mentions, would\n",
      "not be as useful as would be saving a more general one. We ﬁrst generalize\n",
      "these preconditions by substituting variables for constants. We then follow the\n",
      "structure of the speciﬁc plan to produce the generalized plan shown in Fig. 12.6\n",
      "that achieves INROOM(b1, r4). Note that the generalized plan does not require\n",
      "pushing the box back to the place where the robot started. The preconditions\n",
      "for the generalized plan are:\n",
      "INROOM(ROBOT, r1)\n",
      "CONNECTS(d1, r1, r2)\n",
      "CONNECTS(d2, r2, r4)\n",
      "INROOM(b, r4)\n",
      "INROOM(B1,R1)\n",
      "PUSHTHRU(B1,d,r1,R1)\n",
      "INROOM(ROBOT, r1),\n",
      "CONNECTS(d, r1, R1),\n",
      "INROOM(B1, r1)\n",
      "INROOM(ROBOT, R2),\n",
      "CONNECTS(D1, R2, R1),\n",
      "INROOM(B1, R2)\n",
      "{R2/r1,\n",
      "D1/d}\n",
      "GOTHRU(d2, r3, R2)\n",
      "INROOM(ROBOT, r3),\n",
      "CONNECTS(d2, r3, R2),\n",
      "CONNECTS(D1, R2, R1),\n",
      "INROOM(B1, R2)\n",
      "{R1/r3, D1/d2}\n",
      "INROOM(ROBOT, R1),\n",
      "CONNECTS(D1, R1, R2),\n",
      "CONNECTS(D1, R2, R1),\n",
      "INROOM(B1, R2)\n",
      "R1\n",
      "R2\n",
      "R3\n",
      "D1\n",
      "D2\n",
      "GOTHRU(D1,R1,R2)\n",
      "PUSHTHRU(B1,D1,R2,R1)\n",
      "B1\n",
      "PLAN:\n",
      "Figure 12.5: A Plan for the Robot Problem\n",
      "Another related technique that chains together sequences of operators to\n",
      "form more general ones is the chunking mechanism in Soar [Laird, et al., 1986].\n",
      "\n",
      "\n",
      "559. 12.7. APPLICATIONS\n",
      "167\n",
      "INROOM(b1,r4)\n",
      "PUSHTHRU(b1,d2,r2,r4)\n",
      "INROOM(ROBOT, r2),\n",
      "CONNECTS(d1, r1, r2),\n",
      "CONNECTS(d2, r2, r4),\n",
      "INROOM(b1, r4)\n",
      "GOTHRU(d1, r1, r2)\n",
      "INROOM(ROBOT, r1),\n",
      "CONNECTS(d1, r1, r2),\n",
      "CONNECTS(d2, r2, r4),\n",
      "INROOM(b1, r4)\n",
      "Figure 12.6: A Generalized Plan\n",
      "12.7.2\n",
      "Learning Search Control Knowledge\n",
      "Besides their use in creating macro-operators, EBL methods can be used to\n",
      "improve the eﬃciency of planning in another way also. In his system called\n",
      "PRODIGY, Minton proposed using EBL to learn eﬀective ways to control\n",
      "search [Minton, 1988]. PRODIGY is a STRIPS-like system that solves planning\n",
      "problems in the blocks-world, in a simple mobile robot world, and in job-shop\n",
      "scheduling. PRODIGY has a domain theory involving both the domain of the\n",
      "problem and a simple (meta) theory about planning. Its meta theory includes\n",
      "statements about whether a control choice about a subgoal to work on, an oper-\n",
      "ator to apply, etc. either succeeds or fails. After producing a plan, it analyzes its\n",
      "successful and its unsuccessful choices and attempts to explain them in terms\n",
      "of its domain theory. Using an EBL-like process, it is able to produce useful\n",
      "control rules such as:\n",
      "\n",
      "\n",
      "560. 168\n",
      "CHAPTER 12. EXPLANATION-BASED LEARNING\n",
      "IF (AND (CURRENT −NODE node)\n",
      "(CANDIDATE −GOAL node (ON x y))\n",
      "(CANDIDATE −GOAL node (ON y z)))\n",
      "THEN (PREFER GOAL (ON y z) TO (ON x y))\n",
      "PRODIGY keeps statistics on how often these learned rules are used, their\n",
      "savings (in time to ﬁnd plans), and their cost of application. It saves only the\n",
      "rules whose utility, thus measured, is judged to be high. Minton [Minton, 1990]\n",
      "has shown that there is an overall advantage of using these rules (as against not\n",
      "having any rules and as against hand-coded search control rules).\n",
      "12.8\n",
      "Bibliographical and Historical Remarks\n",
      "To be added.\n",
      "\n",
      "\n",
      "561. Bibliography\n",
      "[Acorn & Walden, 1992] Acorn, T., and Walden, S., “SMART: Support Man-\n",
      "agement Automated Reasoning Technology for COMPAQ Customer Ser-\n",
      "vice,” Proc. Fourth Annual Conf. on Innovative Applications of Artiﬁcial\n",
      "Intelligence, Menlo Park, CA: AAAI Press, 1992.\n",
      "[Aha, 1991] Aha, D., Kibler, D., and Albert, M., “Instance-Based Learning\n",
      "Algorithms,” Machine Learning, 6, 37-66, 1991.\n",
      "[Anderson & Bower, 1973] Anderson, J. R., and Bower, G. H., Human Asso-\n",
      "ciative Memory, Hillsdale, NJ: Erlbaum, 1973.\n",
      "[Anderson, 1958] Anderson, T. W., An Introduction to Multivariate Statistical\n",
      "Analysis, New York: John Wiley, 1958.\n",
      "[Barto, Bradtke, & Singh, 1994] Barto, A., Bradtke, S., and Singh, S., “Learn-\n",
      "ing to Act Using Real-Time Dynamic Programming,” to appear in Ar-\n",
      "tiﬁcial Intelligence, 1994.\n",
      "[Baum & Haussler, 1989] Baum, E, and Haussler, D., “What Size Net Gives\n",
      "Valid Generalization?” Neural Computation, 1, pp. 151-160, 1989.\n",
      "[Baum, 1994] Baum, E., “When Are k-Nearest Neighbor and Backpropagation\n",
      "Accurate for Feasible-Sized Sets of Examples?” in Hanson, S., Drastal,\n",
      "G., and Rivest, R., (eds.), Computational Learning Theory and Natural\n",
      "Learning Systems, Volume 1: Constraints and Prospects, pp. 415-442,\n",
      "Cambridge, MA: MIT Press, 1994.\n",
      "[Bellman, 1957] Bellman, R. E., Dynamic Programming, Princeton: Princeton\n",
      "University Press, 1957.\n",
      "[Blumer, et al., 1987] Blumer, A., et al., “Occam’s Razor,” Info. Process. Lett.,\n",
      "vol 24, pp. 377-80, 1987.\n",
      "[Blumer, et al., 1990] Blumer,\n",
      "A.,\n",
      "et al.,\n",
      "“Learnability and the Vapnik-\n",
      "Chervonenkis Dimension,” JACM, 1990.\n",
      "[Bollinger & Duﬃe, 1988] Bollinger, J., and Duﬃe, N., Computer Control of\n",
      "Machines and Processes, Reading, MA: Addison-Wesley, 1988.\n",
      "169\n",
      "\n",
      "\n",
      "562. 170\n",
      "BIBLIOGRAPHY\n",
      "[Brain, et al., 1962] Brain, A. E., et al., “Graphical Data Processing Research\n",
      "Study and Experimental Investigation,” Report No. 8 (pp. 9-13) and No.\n",
      "9 (pp. 3-10), Contract DA 36-039 SC-78343, SRI International, Menlo\n",
      "Park, CA, June 1962 and September 1962.\n",
      "[Breiman, et al., 1984] Breiman, L., Friedman, J., Olshen, R., and Stone, C.,\n",
      "Classiﬁcation and Regression Trees, Monterey, CA: Wadsworth, 1984.\n",
      "[Brent, 1990] Brent, R. P., “Fast Training Algorithms for Multi-Layer Neural\n",
      "Nets,” Numerical Analysis Project Manuscript NA-90-03, Computer Sci-\n",
      "ence Department, Stanford University, Stanford, CA 94305, March 1990.\n",
      "[Bryson & Ho 1969] Bryson, A., and Ho, Y.-C., Applied Optimal Control, New\n",
      "York: Blaisdell.\n",
      "[Buchanan & Wilkins, 1993] Buchanan, B. and Wilkins, D., (eds.), Readings in\n",
      "Knowledge Acquisition and Learning, San Francisco: Morgan Kaufmann,\n",
      "1993.\n",
      "[Carbonell, 1983] Carbonell, J., “Learning by Analogy,” in Machine Learning:\n",
      "An Artiﬁcial Intelligence Approach, Michalski, R., Carbonell, J., and\n",
      "Mitchell, T., (eds.), San Francisco: Morgan Kaufmann, 1983.\n",
      "[Cheeseman, et al., 1988] Cheeseman, P., et al., “AutoClass: A Bayesian Clas-\n",
      "siﬁcation System,” Proc. Fifth Intl. Workshop on Machine Learning,\n",
      "Morgan Kaufmann, San Mateo, CA, 1988. Reprinted in Shavlik, J. and\n",
      "Dietterich, T., Readings in Machine Learning, Morgan Kaufmann, San\n",
      "Francisco, pp. 296-306, 1990.\n",
      "[Cover & Hart, 1967] Cover, T., and Hart, P., “Nearest Neighbor Pattern Clas-\n",
      "siﬁcation,” IEEE Trans. on Information Theory, 13, 21-27, 1967.\n",
      "[Cover, 1965] Cover, T., “Geometrical and Statistical Properties of Systems\n",
      "of Linear Inequalities with Applications in Pattern Recognition,” IEEE\n",
      "Trans. Elec. Comp., EC-14, 326-334, June, 1965.\n",
      "[Dasarathy, 1991] Dasarathy, B. V., Nearest Neighbor Pattern Classiﬁcation\n",
      "Techniques, IEEE Computer Society Press, 1991.\n",
      "[Dayan & Sejnowski, 1994] Dayan, P., and Sejnowski, T., “TD(λ) Converges\n",
      "with Probability 1,” Machine Learning, 14, pp. 295-301, 1994.\n",
      "[Dayan, 1992] Dayan, P., “The Convergence of TD(λ) for General λ,” Machine\n",
      "Learning, 8, 341-362, 1992.\n",
      "[DeJong & Mooney, 1986] DeJong, G., and Mooney, R., “Explanation-Based\n",
      "Learning: An Alternative View,” Machine Learning, 1:145-176, 1986.\n",
      "Reprinted in Shavlik, J. and Dietterich, T., Readings in Machine Learn-\n",
      "ing, San Francisco: Morgan Kaufmann, 1990, pp 452-467.\n",
      "\n",
      "\n",
      "563. BIBLIOGRAPHY\n",
      "171\n",
      "[Dietterich & Bakiri, 1991] Dietterich, T. G., and Bakiri, G., “Error-Correcting\n",
      "Output Codes:\n",
      "A General Method for Improving Multiclass Induc-\n",
      "tive Learning Programs,” Proc. Ninth Nat. Conf. on A.I., pp. 572-577,\n",
      "AAAI-91, MIT Press, 1991.\n",
      "[Dietterich, et al., 1990] Dietterich, T., Hild, H., and Bakiri, G., “A Compara-\n",
      "tive Study of ID3 and Backpropagation for English Text-to-Speech Map-\n",
      "ping,” Proc. Seventh Intl. Conf. Mach. Learning, Porter, B. and Mooney,\n",
      "R. (eds.), pp. 24-31, San Francisco: Morgan Kaufmann, 1990.\n",
      "[Dietterich, 1990] Dietterich, T., “Machine Learning,” Annu. Rev. Comput.\n",
      "Sci., 4:255-306, Palo Alto: Annual Reviews Inc., 1990.\n",
      "[Duda & Fossum, 1966] Duda, R. O., and Fossum, H., “Pattern Classiﬁcation\n",
      "by Iteratively Determined Linear and Piecewise Linear Discriminant\n",
      "Functions,” IEEE Trans. on Elect. Computers, vol. EC-15, pp. 220-232,\n",
      "April, 1966.\n",
      "[Duda & Hart, 1973] Duda, R. O., and Hart, P.E., Pattern Classiﬁcation and\n",
      "Scene Analysis, New York: Wiley, 1973.\n",
      "[Duda, 1966] Duda, R. O., “Training a Linear Machine on Mislabeled Patterns,”\n",
      "SRI Tech. Report prepared for ONR under Contract 3438(00), SRI In-\n",
      "ternational, Menlo Park, CA, April 1966.\n",
      "[Efron, 1982] Efron, B., The Jackknife, the Bootstrap and Other Resampling\n",
      "Plans, Philadelphia: SIAM, 1982.\n",
      "[Ehrenfeucht, et al., 1988] Ehrenfeucht, A., et al., “A General Lower Bound on\n",
      "the Number of Examples Needed for Learning,” in Proc. 1988 Workshop\n",
      "on Computational Learning Theory, pp. 110-120, San Francisco: Morgan\n",
      "Kaufmann, 1988.\n",
      "[Etzioni, 1991] Etzioni,\n",
      "O.,\n",
      "“STATIC:\n",
      "A\n",
      "Problem-Space\n",
      "Compiler\n",
      "for\n",
      "PRODIGY,” Proc. of Ninth National Conf. on Artiﬁcial Intelligence,\n",
      "pp. 533-540, Menlo Park: AAAI Press, 1991.\n",
      "[Etzioni, 1993] Etzioni, O., “A Structural Theory of Explanation-Based Learn-\n",
      "ing,” Artiﬁcial Intelligence, 60:1, pp. 93-139, March, 1993.\n",
      "[Evans & Fisher, 1992] Evans, B., and Fisher, D., Process Delay Analyses Using\n",
      "Decision-Tree Induction, Tech. Report CS92-06, Department of Com-\n",
      "puter Science, Vanderbilt University, TN, 1992.\n",
      "[Fahlman & Lebiere, 1990] Fahlman,\n",
      "S.,\n",
      "and Lebiere,\n",
      "C.,\n",
      "“The Cascade-\n",
      "Correlation Learning Architecture,” in Touretzky, D., (ed.), Advances in\n",
      "Neural Information Processing Systems, 2, pp. 524-532, San Francisco:\n",
      "Morgan Kaufmann, 1990.\n",
      "\n",
      "\n",
      "564. 172\n",
      "BIBLIOGRAPHY\n",
      "[Fayyad, et al., 1993] Fayyad, U. M., Weir, N., and Djorgovski, S., “SKICAT:\n",
      "A Machine Learning System for Automated Cataloging of Large Scale\n",
      "Sky Surveys,” in Proc. Tenth Intl. Conf. on Machine Learning, pp. 112-\n",
      "119, San Francisco: Morgan Kaufmann, 1993. (For a longer version of\n",
      "this paper see: Fayyad, U. Djorgovski, G., and Weir, N., “Automating\n",
      "the Analysis and Cataloging of Sky Surveys,” in Fayyad, U., et al.(eds.),\n",
      "Advances in Knowledge Discovery and Data Mining, Chapter 19, pp.\n",
      "471ﬀ., Cambridge: The MIT Press, March, 1996.)\n",
      "[Feigenbaum, 1961] Feigenbaum, E. A., “The Simulation of Verbal Learning Be-\n",
      "havior,” Proceedings of the Western Joint Computer Conference, 19:121-\n",
      "132, 1961.\n",
      "[Fikes, et al., 1972] Fikes, R., Hart, P., and Nilsson, N., “Learning and Execut-\n",
      "ing Generalized Robot Plans,” Artiﬁcial Intelligence, pp 251-288, 1972.\n",
      "Reprinted in Shavlik, J. and Dietterich, T., Readings in Machine Learn-\n",
      "ing, San Francisco: Morgan Kaufmann, 1990, pp 468-486.\n",
      "[Fisher, 1987] Fisher, D., “Knowledge Acquisition via Incremental Conceptual\n",
      "Clustering,” Machine Learning, 2:139-172, 1987. Reprinted in Shavlik,\n",
      "J. and Dietterich, T., Readings in Machine Learning, San Francisco:\n",
      "Morgan Kaufmann, 1990, pp. 267–283.\n",
      "[Friedman, et al., 1977] Friedman, J. H., Bentley, J. L., and Finkel, R. A., “An\n",
      "Algorithm for Finding Best Matches in Logarithmic Expected Time,”\n",
      "ACM Trans. on Math. Software, 3(3):209-226, September 1977.\n",
      "[Fu, 1994] Fu,\n",
      "L.,\n",
      "Neural Networks in Artiﬁcial Intelligence,\n",
      "New York:\n",
      "McGraw-Hill, 1994.\n",
      "[Gallant, 1986] Gallant, S. I., “Optimal Linear Discriminants,” in Eighth Inter-\n",
      "national Conf. on Pattern Recognition, pp. 849-852, New York: IEEE,\n",
      "1986.\n",
      "[Genesereth & Nilsson, 1987] Genesereth, M., and Nilsson, N., Logical Founda-\n",
      "tions of Artiﬁcial Intelligence, San Francisco: Morgan Kaufmann, 1987.\n",
      "[Gluck & Rumelhart, 1989] Gluck, M. and Rumelhart, D., Neuroscience and\n",
      "Connectionist Theory, The Developments in Connectionist Theory, Hills-\n",
      "dale, NJ: Erlbaum Associates, 1989.\n",
      "[Hammerstrom, 1993] Hammerstrom, D., “Neural Networks at Work,” IEEE\n",
      "Spectrum, pp. 26-32, June 1993.\n",
      "[Haussler, 1988] Haussler, D., “Quantifying Inductive Bias: AI Learning Al-\n",
      "gorithms and Valiant’s Learning Framework,” Artiﬁcial Intelligence,\n",
      "36:177-221, 1988. Reprinted in Shavlik, J. and Dietterich, T., Readings in\n",
      "Machine Learning, San Francisco: Morgan Kaufmann, 1990, pp. 96-107.\n",
      "\n",
      "\n",
      "565. BIBLIOGRAPHY\n",
      "173\n",
      "[Haussler, 1990] Haussler, D., “Probably Approximately Correct Learning,”\n",
      "Proc. Eighth Nat. Conf. on AI, pp. 1101-1108. Cambridge, MA: MIT\n",
      "Press, 1990.\n",
      "[Hebb, 1949] Hebb, D. O., The Organization of Behaviour, New York: John\n",
      "Wiley, 1949.\n",
      "[Hertz, Krogh, & Palmer, 1991] Hertz, J., Krogh, A, and Palmer, R., Introduc-\n",
      "tion to the Theory of Neural Computation, Lecture Notes, vol. 1, Santa\n",
      "Fe Inst. Studies in the Sciences of Complexity, New York: Addison-\n",
      "Wesley, 1991.\n",
      "[Hirsh, 1994] Hirsh, H., “Generalizing Version Spaces,” Machine Learning, 17,\n",
      "5-45, 1994.\n",
      "[Holland, 1975] Holland, J., Adaptation in Natural and Artiﬁcial Systems, Ann\n",
      "Arbor: The University of Michigan Press, 1975. (Second edition printed\n",
      "in 1992 by MIT Press, Cambridge, MA.)\n",
      "[Holland, 1986] Holland, J. H., “Escaping Brittleness; The Possibilities of\n",
      "General-Purpose Learning Algorithms Applied to Parallel Rule-Based\n",
      "Systems.” In Michalski, R., Carbonell, J., and Mitchell, T. (eds.) , Ma-\n",
      "chine Learning: An Artiﬁcial Intelligence Approach, Volume 2, chapter\n",
      "20, San Francisco: Morgan Kaufmann, 1986.\n",
      "[Hunt, Marin, & Stone, 1966] Hunt, E., Marin, J., and Stone, P., Experiments\n",
      "in Induction, New York: Academic Press, 1966.\n",
      "[Jabbour, K., et al., 1987] Jabbour, K., et al., “ALFA: Automated Load Fore-\n",
      "casting Assistant,” Proc. of the IEEE Pwer Engineering Society Summer\n",
      "Meeting, San Francisco, CA, 1987.\n",
      "[John, 1995] John, G., “Robust Linear Discriminant Trees,” Proc. of the Conf.\n",
      "on Artiﬁcial Intelligence and Statistics, Ft. Lauderdale, FL, January,\n",
      "1995.\n",
      "[Kaelbling, 1993] Kaelbling, L. P., Learning in Embedded Systems, Cambridge,\n",
      "MA: MIT Press, 1993.\n",
      "[Kohavi, 1994] Kohavi, R., “Bottom-Up Induction of Oblivious Read-Once De-\n",
      "cision Graphs,” Proc. of European Conference on Machine Learning\n",
      "(ECML-94), 1994.\n",
      "[Kolodner, 1993] Kolodner, J., Case-Based Reasoning, San Francisco: Morgan\n",
      "Kaufmann, 1993.\n",
      "[Koza, 1992] Koza, J., Genetic Programming: On the Programming of Comput-\n",
      "ers by Means of Natural Selection, Cambridge, MA: MIT Press, 1992.\n",
      "[Koza, 1994] Koza,\n",
      "J.,\n",
      "Genetic Programming II: Automatic Discovery of\n",
      "Reusable Programs, Cambridge, MA: MIT Press, 1994.\n",
      "\n",
      "\n",
      "566. 174\n",
      "BIBLIOGRAPHY\n",
      "[Laird, et al., 1986] Laird, J., Rosenbloom, P., and Newell, A., “Chunking in\n",
      "Soar: The Anatomy of a General Learning Mechanism,” Machine Learn-\n",
      "ing, 1, pp. 11-46, 1986. Reprinted in Buchanan, B. and Wilkins, D.,\n",
      "(eds.), Readings in Knowledge Acquisition and Learning, pp. 518-535,\n",
      "Morgan Kaufmann, San Francisco, CA, 1993.\n",
      "[Langley, 1992] Langley, P., “Areas of Application for Machine Learning,” Proc.\n",
      "of Fifth Int’l. Symp. on Knowledge Engineering, Sevilla, 1992.\n",
      "[Langley, 1996] Langley, P., Elements of Machine Learning, San Francisco:\n",
      "Morgan Kaufmann, 1996.\n",
      "[Lavraˇc & Dˇzeroski, 1994] Lavraˇc, N., and Dˇzeroski, S., Inductive Logic Pro-\n",
      "gramming, Chichester, England: Ellis Horwood, 1994.\n",
      "[Lin, 1992] Lin, L., “Self-Improving Reactive Agents Based on Reinforcement\n",
      "Learning, Planning, and Teaching,” Machine Learning, 8, 293-321, 1992.\n",
      "[Lin, 1993] Lin, L., “Scaling Up Reinforcement Learning for Robot Control,”\n",
      "Proc. Tenth Intl. Conf. on Machine Learning, pp. 182-189, San Francisco:\n",
      "Morgan Kaufmann, 1993.\n",
      "[Littlestone, 1988] Littlestone, N., “Learning Quickly When Irrelevant At-\n",
      "tributes Abound: A New Linear-Threshold Algorithm,” Machine Learn-\n",
      "ing 2: 285-318, 1988.\n",
      "[Maass & Tur´an, 1994] Maass, W., and Tur´an, G., “How Fast Can a Thresh-\n",
      "old Gate Learn?,” in Hanson, S., Drastal, G., and Rivest, R., (eds.),\n",
      "Computational Learning Theory and Natural Learning Systems, Volume\n",
      "1: Constraints and Prospects, pp. 381-414, Cambridge, MA: MIT Press,\n",
      "1994.\n",
      "[Mahadevan & Connell, 1992] Mahadevan, S., and Connell, J., “Automatic\n",
      "Programming of Behavior-Based Robots Using Reinforcement Learn-\n",
      "ing,” Artiﬁcial Intelligence, 55, pp. 311-365, 1992.\n",
      "[Marchand & Golea, 1993] Marchand, M., and Golea, M., “On Learning Sim-\n",
      "ple Neural Concepts: From Halfspace Intersections to Neural Decision\n",
      "Lists,” Network, 4:67-85, 1993.\n",
      "[McCulloch & Pitts, 1943] McCulloch, W. S., and Pitts, W. H., “A Logical Cal-\n",
      "culus of the Ideas Immanent in Nervous Activity,” Bulletin of Mathe-\n",
      "matical Biophysics, Vol. 5, pp. 115-133, Chicago: University of Chicago\n",
      "Press, 1943.\n",
      "[Michie, 1992] Michie, D., “Some Directions in Machine Intelligence,” unpub-\n",
      "lished manuscript, The Turing Institute, Glasgow, Scotland, 1992.\n",
      "[Minton, 1988] Minton,\n",
      "S.,\n",
      "Learning\n",
      "Search\n",
      "Control\n",
      "Knowledge:\n",
      "An\n",
      "Explanation-Based Approach, Kluwer Academic Publishers, Boston,\n",
      "MA, 1988.\n",
      "\n",
      "\n",
      "567. BIBLIOGRAPHY\n",
      "175\n",
      "[Minton, 1990] Minton, S., “Quantitative Results Concerning the Utility of\n",
      "Explanation-Based Learning,” Artiﬁcial Intelligence, 42, pp. 363-392,\n",
      "1990. Reprinted in Shavlik, J. and Dietterich, T., Readings in Machine\n",
      "Learning, San Francisco: Morgan Kaufmann, 1990, pp. 573-587.\n",
      "[Mitchell, et al., 1986] Mitchell, T., et al., “Explanation-Based Generalization:\n",
      "A Unifying View,” Machine Learning, 1:1, 1986. Reprinted in Shavlik,\n",
      "J. and Dietterich, T., Readings in Machine Learning, San Francisco:\n",
      "Morgan Kaufmann, 1990, pp. 435-451.\n",
      "[Mitchell, 1982] Mitchell, T., “Generalization as Search,” Artiﬁcial Intelligence,\n",
      "18:203-226, 1982. Reprinted in Shavlik, J. and Dietterich, T., Readings in\n",
      "Machine Learning, San Francisco: Morgan Kaufmann, 1990, pp. 96–107.\n",
      "[Moore & Atkeson, 1993] Moore, A., and Atkeson, C., “Prioritized Sweeping:\n",
      "Reinforcement Learning with Less Data and Less Time,” Machine Learn-\n",
      "ing, 13, pp. 103-130, 1993.\n",
      "[Moore, et al., 1994] Moore, A. W., Hill, D. J., and Johnson, M. P., “An Em-\n",
      "pirical Investigation of Brute Force to Choose Features, Smoothers, and\n",
      "Function Approximators,” in Hanson, S., Judd, S., and Petsche, T.,\n",
      "(eds.), Computational Learning Theory and Natural Learning Systems,\n",
      "Vol. 3, Cambridge: MIT Press, 1994.\n",
      "[Moore, 1990] Moore, A., Eﬃcient Memory-based Learning for Robot Control,\n",
      "PhD. Thesis; Technical Report No. 209, Computer Laboratory, Univer-\n",
      "sity of Cambridge, October, 1990.\n",
      "[Moore, 1992] Moore, A., “Fast, Robust Adaptive Control by Learning Only\n",
      "Forward Models,” in Moody, J., Hanson, S., and Lippman, R., (eds.),\n",
      "Advances in Neural Information Processing Systems 4, San Francisco:\n",
      "Morgan Kaufmann, 1992.\n",
      "[Mueller & Page, 1988] Mueller, R. and Page, R., Symbolic Computing with\n",
      "Lisp and Prolog, New York: John Wiley & Sons, 1988.\n",
      "[Muggleton, 1991] Muggleton, S., “Inductive Logic Programming,” New Gen-\n",
      "eration Computing, 8, pp. 295-318, 1991.\n",
      "[Muggleton, 1992] Muggleton, S., Inductive Logic Programming, London: Aca-\n",
      "demic Press, 1992.\n",
      "[Muroga, 1971] Muroga, S., Threshold Logic and its Applications, New York:\n",
      "Wiley, 1971.\n",
      "[Natarjan, 1991] Natarajan, B., Machine Learning: A Theoretical Approach,\n",
      "San Francisco: Morgan Kaufmann, 1991.\n",
      "\n",
      "\n",
      "568. 176\n",
      "BIBLIOGRAPHY\n",
      "[Nilsson, 1965] Nilsson, N. J., “Theoretical and Experimental Investigations in\n",
      "Trainable Pattern-Classifying Systems,” Tech. Report No. RADC-TR-\n",
      "65-257, Final Report on Contract AF30(602)-3448, Rome Air Develop-\n",
      "ment Center (Now Rome Laboratories), Griﬃss Air Force Base, New\n",
      "York, September, 1965.\n",
      "[Nilsson, 1990] Nilsson, N. J., The Mathematical Foundations of Learning Ma-\n",
      "chines, San Francisco: Morgan Kaufmann, 1990. (This book is a reprint\n",
      "of Learning Machines:\n",
      "Foundations of Trainable Pattern-Classifying\n",
      "Systems, New York: McGraw-Hill, 1965.)\n",
      "[Oliver, Dowe, & Wallace, 1992] Oliver, J., Dowe, D., and Wallace, C., “Infer-\n",
      "ring Decision Graphs using the Minimum Message Length Principle,”\n",
      "Proc. 1992 Australian Artiﬁcial Intelligence Conference, 1992.\n",
      "[Pagallo & Haussler, 1990] Pagallo, G. and Haussler, D., “Boolean Feature Dis-\n",
      "covery in Empirical Learning,” Machine Learning, vol.5, no.1, pp. 71-99,\n",
      "March 1990.\n",
      "[Pazzani & Kibler, 1992] Pazzani, M., and Kibler, D., “The Utility of Knowl-\n",
      "edge in Inductive Learning,” Machine Learning, 9, 57-94, 1992.\n",
      "[Peterson, 1961] Peterson, W., Error Correcting Codes, New York: John Wiley,\n",
      "1961.\n",
      "[Pomerleau, 1991] Pomerleau, D., “Rapidly Adapting Artiﬁcial Neural Net-\n",
      "works for Autonomous Navigation,” in Lippmann, P., et al. (eds.), Ad-\n",
      "vances in Neural Information Processing Systems, 3, pp. 429-435, San\n",
      "Francisco: Morgan Kaufmann, 1991.\n",
      "[Pomerleau, 1993] Pomerleau, D, Neural Network Perception for Mobile Robot\n",
      "Guidance, Boston: Kluwer Academic Publishers, 1993.\n",
      "[Quinlan & Rivest, 1989] Quinlan, J. Ross, and Rivest, Ron, “Inferring Deci-\n",
      "sion Trees Using the Minimum Description Length Principle,” Informa-\n",
      "tion and Computation, 80:227–248, March, 1989.\n",
      "[Quinlan, 1986] Quinlan, J. Ross, “Induction of Decision Trees,” Machine\n",
      "Learning, 1:81–106, 1986. Reprinted in Shavlik, J. and Dietterich, T.,\n",
      "Readings in Machine Learning, San Francisco: Morgan Kaufmann, 1990,\n",
      "pp. 57–69.\n",
      "[Quinlan, 1987] Quinlan, J. R., “Generating Production Rules from Decision\n",
      "Trees,” In IJCAI-87: Proceedings of the Tenth Intl. Joint Conf. on Ar-\n",
      "tiﬁcial Intelligence, pp. 304-7, San Francisco: Morgan-Kaufmann, 1987.\n",
      "[Quinlan, 1990] Quinlan, J. R., “Learning Logical Deﬁnitions from Relations,”\n",
      "Machine Learning, 5, 239-266, 1990.\n",
      "\n",
      "\n",
      "569. BIBLIOGRAPHY\n",
      "177\n",
      "[Quinlan, 1993] Quinlan, J. Ross, C4.5: Programs for Machine Learning, San\n",
      "Francisco: Morgan Kaufmann, 1993.\n",
      "[Quinlan, 1994] Quinlan, J. R., “Comparing Connectionist and Symbolic Learn-\n",
      "ing Methods,” in Hanson, S., Drastal, G., and Rivest, R., (eds.), Com-\n",
      "putational Learning Theory and Natural Learning Systems, Volume 1:\n",
      "Constraints and Prospects, pp. 445-456,, Cambridge, MA: MIT Press,\n",
      "1994.\n",
      "[Ridgway, 1962] Ridgway, W. C., An Adaptive Logic System with Generalizing\n",
      "Properties, PhD thesis, Tech. Rep. 1556-1, Stanford Electronics Labs.,\n",
      "Stanford, CA, April 1962.\n",
      "[Rissanen, 1978] Rissanen, J., “Modeling by Shortest Data Description,” Auto-\n",
      "matica, 14:465-471, 1978.\n",
      "[Rivest, 1987] Rivest, R. L., “Learning Decision Lists,” Machine Learning, 2,\n",
      "229-246, 1987.\n",
      "[Rosenblatt, 1958] Rosenblatt, F., Principles of Neurodynamics, Washington:\n",
      "Spartan Books, 1961.\n",
      "[Ross, 1983] Ross, S., Introduction to Stochastic Dynamic Programming, New\n",
      "York: Academic Press, 1983.\n",
      "[Rumelhart, Hinton, & Williams, 1986] Rumelhart, D. E., Hinton, G. E., and\n",
      "Williams, R. J., “Learning Internal Representations by Error Propa-\n",
      "gation,” In Rumelhart, D. E., and McClelland, J. L., (eds.) Parallel\n",
      "Distributed Processing, Vol 1, 318–362, 1986.\n",
      "[Russell & Norvig 1995] Russell, S., and Norvig, P., Artiﬁcial Intelligence: A\n",
      "Modern Approach, Englewood Cliﬀs, NJ: Prentice Hall, 1995.\n",
      "[Samuel, 1959] Samuel, A., “Some Studies in Machine Learning Using the Game\n",
      "of Checkers,” IBM Journal of Research and Development, 3:211-229, July\n",
      "1959.\n",
      "[Schwartz, 1993] Schwartz, A., “A Reinforcement Learning Method for Max-\n",
      "imizing Undiscounted Rewards,” Proc. Tenth Intl. Conf. on Machine\n",
      "Learning, pp. 298-305, San Francisco: Morgan Kaufmann, 1993.\n",
      "[Sejnowski, Koch, & Churchland, 1988] Sejnowski, T., Koch, C., and Church-\n",
      "land, P., “Computational Neuroscience,” Science, 241: 1299-1306, 1988.\n",
      "[Shavlik, Mooney, & Towell, 1991] Shavlik, J., Mooney, R., and Towell, G.,\n",
      "“Symbolic and Neural Learning Algorithms: An Experimental Compar-\n",
      "ison,” Machine Learning, 6, pp. 111-143, 1991.\n",
      "[Shavlik & Dietterich, 1990] Shavlik, J. and Dietterich, T., Readings in Ma-\n",
      "chine Learning, San Francisco: Morgan Kaufmann, 1990.\n",
      "\n",
      "\n",
      "570. 178\n",
      "BIBLIOGRAPHY\n",
      "[Sutton & Barto, 1987] Sutton,\n",
      "R. S.,\n",
      "and Barto,\n",
      "A. G.,\n",
      "“A Temporal-\n",
      "Diﬀerence Model of Classical Conditioning,” in Proceedings of the Ninth\n",
      "Annual Conference of the Cognitive Science Society, Hillsdale, NJ: Erl-\n",
      "baum, 1987.\n",
      "[Sutton, 1988] Sutton, R. S., “Learning to Predict by the Methods of Temporal\n",
      "Diﬀerences,” Machine Learning 3: 9-44, 1988.\n",
      "[Sutton, 1990] Sutton, R., “Integrated Architectures for Learning, Planning,\n",
      "and Reacting Based on Approximating Dynamic Programming,” Proc. of\n",
      "the Seventh Intl. Conf. on Machine Learning, pp. 216-224, San Francisco:\n",
      "Morgan Kaufmann, 1990.\n",
      "[Taylor, Michie, & Spiegalhalter, 1994] Taylor, C., Michie, D., and Spiegal-\n",
      "halter, D., Machine Learning, Neural and Statistical Classiﬁcation,\n",
      "Paramount Publishing International.\n",
      "[Tesauro, 1992] Tesauro, G., “Practical Issues in Temporal Diﬀerence Learn-\n",
      "ing,” Machine Learning, 8, nos. 3/4, pp. 257-277, 1992.\n",
      "[Towell & Shavlik, 1992] Towell G., and Shavlik, J., “Interpretation of Artiﬁ-\n",
      "cial Neural Networks: Mapping Knowledge-Based Neural Networks into\n",
      "Rules,” in Moody, J., Hanson, S., and Lippmann, R., (eds.), Advances in\n",
      "Neural Information Processing Systems, 4, pp. 977-984, San Francisco:\n",
      "Morgan Kaufmann, 1992.\n",
      "[Towell, Shavlik, & Noordweier, 1990] Towell, G., Shavlik, J., and Noordweier,\n",
      "M., “Reﬁnement of Approximate Domain Theories by Knowledge-Based\n",
      "Artiﬁcial Neural Networks,” Proc. Eighth Natl., Conf. on Artiﬁcial In-\n",
      "telligence, pp. 861-866, 1990.\n",
      "[Unger, 1989] Unger, S., The Essence of Logic Circuits, Englewood Cliﬀs, NJ:\n",
      "Prentice-Hall, 1989.\n",
      "[Utgoﬀ, 1989] Utgoﬀ, P., “Incremental Induction of Decision Trees,” Machine\n",
      "Learning, 4:161–186, Nov., 1989.\n",
      "[Valiant, 1984] Valiant, L., “A Theory of the Learnable,” Communications of\n",
      "the ACM, Vol. 27, pp. 1134-1142, 1984.\n",
      "[Vapnik & Chervonenkis, 1971] Vapnik, V., and Chervonenkis, A., “On the\n",
      "Uniform Convergence of Relative Frequencies, Theory of Probability and\n",
      "its Applications, Vol. 16, No. 2, pp. 264-280, 1971.\n",
      "[Various Editors, 1989-1994] Advances in Neural Information Processing Sys-\n",
      "tems, vols 1 through 6, San Francisco: Morgan Kaufmann, 1989 -1994.\n",
      "[Watkins & Dayan, 1992] Watkins, C. J. C. H., and Dayan, P., “Technical Note:\n",
      "Q-Learning,” Machine Learning, 8, 279-292, 1992.\n",
      "\n",
      "\n",
      "571. BIBLIOGRAPHY\n",
      "179\n",
      "[Watkins, 1989] Watkins, C. J. C. H., Learning From Delayed Rewards, PhD\n",
      "Thesis, University of Cambridge, England, 1989.\n",
      "[Weiss & Kulikowski, 1991] Weiss, S., and Kulikowski, C., Computer Systems\n",
      "that Learn, San Francisco: Morgan Kaufmann, 1991.\n",
      "[Werbos, 1974] Werbos, P., Beyond Regression: New Tools for Prediction and\n",
      "Analysis in the Behavioral Sciences, Ph.D. Thesis, Harvard University,\n",
      "1974.\n",
      "[Widrow & Lehr, 1990] Widrow, B., and Lehr, M. A., “30 Years of Adaptive\n",
      "Neural Networks: Perceptron, Madaline and Backpropagation,” Proc.\n",
      "IEEE, vol. 78, no. 9, pp. 1415-1442, September, 1990.\n",
      "[Widrow & Stearns, 1985] Widrow, B., and Stearns, S., Adaptive Signal Pro-\n",
      "cessing, Englewood Cliﬀs, NJ: Prentice-Hall.\n",
      "[Widrow, 1962] Widrow, B., “Generalization and Storage in Networks of Ada-\n",
      "line Neurons,” in Yovits, Jacobi, and Goldstein (eds.), Self-organizing\n",
      "Systems—1962, pp. 435-461, Washington, DC: Spartan Books, 1962.\n",
      "[Winder, 1961] Winder, R., “Single Stage Threshold Logic,” Proc. of the AIEE\n",
      "Symp. on Switching Circuits and Logical Design, Conf. paper CP-60-\n",
      "1261, pp. 321-332, 1961.\n",
      "[Winder, 1962] Winder, R., Threshold Logic, PhD Dissertation, Princeton Uni-\n",
      "versity, Princeton, NJ, 1962.\n",
      "[Wnek, et al., 1990] Wnek, J., et al., “Comparing Learning Paradigms via Di-\n",
      "agrammatic Visualization,” in Proc. Fifth Intl. Symp. on Methodologies\n",
      "for Intelligent Systems, pp. 428-437, 1990. (Also Tech. Report MLI90-2,\n",
      "University of Illinois at Urbana-Champaign.)\n",
      "\n",
      "\n",
      "572. Top 100 Machine Learning Questions & Answers  \n",
      "Steve Nouri \n",
      " \n",
      " \n",
      " \n",
      "Q1 Explain the difference between supervised and unsupervised machine\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "learning? \n",
      "In supervised machine learning algorithms, we have to provide labeled data, for example,\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "prediction of stock market prices, whereas in unsupervised we need not have labeled data, for\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "example, classification of emails into spam and non-spam. \n",
      "Q2 What are the parametric models? Give an example. \n",
      "Parametric models are those with a finite number of parameters. To predict new data, you only\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "need to know the parameters of the model. Examples include linear regression, logistic\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "regression, and linear SVMs. \n",
      "Non-parametric models are those with an unbounded number of parameters, allowing for more\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "flexibility. To predict new data, you need to know the parameters of the model and the state of\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "the data that has been observed. Examples include decision trees, k-nearest neighbors, and\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "topic models using latent Dirichlet analysis. \n",
      "Q3 What is the difference between classification​ ​and regression? \n",
      "Classification is used to produce discrete results, classification is used to classify data into some\n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "specific categories. For example, classifying emails into spam and non-spam categories. \n",
      "Whereas, We use regression analysis when we are dealing with continuous data, for example\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "predicting stock prices at a certain point in time.  \n",
      " \n",
      "Q4 What Is Overfitting, and How Can You Avoid It?  \n",
      "Overfitting is a situation that occurs when a model learns the training set too well, taking up\n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "random fluctuations in the training data as concepts. These impact the model’s ability to\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "generalize and don’t apply to new data.  \n",
      "When a model is given the training data, it shows 100 percent accuracy—technically a slight\n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      "loss. But, when we use the test data, there may be an error and low efficiency. This condition is\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      "known as overfitting. \n",
      "There are multiple ways of avoiding overfitting, such as: \n",
      "●\n",
      "Regularization. It involves a cost term for the features involved with the objective function \n",
      "●\n",
      "Making a simple model. With lesser variables and parameters, the variance can be\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "reduced  \n",
      "●\n",
      "Cross-validation methods like k-folds can also be used \n",
      "●\n",
      "If some model parameters are likely to cause overfitting, techniques for regularization\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "like LASSO can be used that penalize these parameters \n",
      " \n",
      "Steve Nouri  ​                           ​https://www.linkedin.com/in/stevenouri/ \n",
      "\n",
      "\n",
      "573.  \n",
      " \n",
      " \n",
      "Q5 What is meant by ‘Training set’ and ‘Test Set’? \n",
      "We split the given data set into two different sections namely,’Training set’ and ‘Test Set’. \n",
      "‘Training set’ is the portion of the dataset used to train the model. \n",
      "‘Testing set’ is the portion of the dataset used to test the trained model.  \n",
      "Q6 How Do You Handle Missing or Corrupted Data in a Dataset? \n",
      "One of the easiest ways to handle missing or corrupted data is to drop those rows or columns or\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "replace them entirely with some other value. \n",
      "There are two useful methods in Pandas: \n",
      "●\n",
      "IsNull() and dropna() will help to find the columns/rows with missing data and drop them \n",
      "●\n",
      "Fillna() will replace the wrong values with a placeholder value \n",
      " \n",
      "Q7 Explain Ensemble learning. \n",
      "In ensemble learning, many base models like classifiers and regressors are generated and\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "combined together so that they give better results. It is used when we build component\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "classifiers that are accurate and independent. There are sequential as well as parallel ensemble\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "methods. \n",
      "Q8 Explain the Bias-Variance Tradeoff. \n",
      "Predictive models have a tradeoff between bias (how well the model fits the data) and variance\n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "(how much the model changes based on changes in the inputs). \n",
      "Simpler models are stable (low variance) but they don't get close to the truth (high bias). \n",
      "More complex models are more prone to overfitting (high variance) but they are expressive\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "enough to get close to the truth (low bias). The best model for a given problem usually lies\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "somewhere in the middle. \n",
      "Q9 What is the difference between stochastic gradient descent (SGD) and\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "gradient descent (GD)? \n",
      "Both algorithms are methods for finding a set of parameters that minimize a loss function by\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      "evaluating parameters against data and then making adjustments. \n",
      "In standard gradient descent, you'll evaluate all training samples for each set of parameters.\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "This is akin to taking big, slow steps toward the solution. \n",
      "In stochastic gradient descent, you'll evaluate only 1 training sample for the set of parameters\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "before updating them. This is akin to taking small, quick steps toward the solution. \n",
      " \n",
      "Q10 How Can You Choose a Classifier Based on a Training Set Data Size? \n",
      "When the training set is small, a model that has a right bias and low variance seems to work\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "better because they are less likely to overfit.  \n",
      "Steve Nouri  ​                           ​https://www.linkedin.com/in/stevenouri/ \n",
      "\n",
      "\n",
      "574. For example, Naive Bayes works best when the training set is large. Models with low bias and\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "high variance tend to perform better as they work fine with complex relationships. \n",
      " \n",
      "Q11 What are 3 data preprocessing techniques to handle outliers? \n",
      "1.\n",
      "Winsorize (cap at threshold). \n",
      "2.\n",
      "Transform to reduce skew (using Box-Cox or similar). \n",
      "3.\n",
      "Remove outliers if you're certain they are anomalies or measurement errors. \n",
      " \n",
      "Q12 How much data should you allocate for your training, validation, and test\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "sets? \n",
      "You have to find a balance, and there's no right answer for every problem. \n",
      "If your test set is too small, you'll have an unreliable estimation of model performance\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "(performance statistic will have high variance). If your training set is too small, your actual model\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "parameters will have a high variance. \n",
      "A good rule of thumb is to use an 80/20 train/test split. Then, your train set can be further split\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "into train/validation or into partitions for cross-validation. \n",
      " \n",
      "Q13 What Is a False Positive and False Negative and How Are They Significant? \n",
      "False positives are those cases which wrongly get classified as True but are False.  \n",
      "False negatives are those cases which wrongly get classified as False but are True. \n",
      "In the term ‘False Positive,’ the word ‘Positive’ refers to the ‘Yes’ row of the predicted value in\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "the confusion matrix. The complete term indicates that the system has predicted it as a positive,\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      "  \n",
      " \n",
      "but the actual value is negative. \n",
      " \n",
      "Q14 Explain the difference between L1 and L2 regularization. \n",
      "L2 regularization tends to spread error among all the terms, while L1 is more binary/sparse, with\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      "many variables either being assigned a 1 or 0 in weighting. L1 corresponds to setting a\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Laplacean prior to the terms, while L2 corresponds to a Gaussian prior. \n",
      " \n",
      "Q15 What’s a Fourier transform? \n",
      "A Fourier transform is a generic method to decompose generic functions into a superposition of\n",
      " \n",
      " \n",
      "   \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      "symmetric functions. Or as this more intuitive tutorial puts it, given a smoothie, it’s how we find\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "the recipe. The Fourier transform finds the set of cycle speeds, amplitudes, and phases to\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "match any time signal. A Fourier transform converts a signal from time to frequency domain —\n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "it’s a very common way to extract features from audio signals or other time series such as\n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "sensor data. \n",
      " \n",
      "Q16 What is deep learning, and how does it contrast with other machine learning\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "algorithms? \n",
      "Deep learning is a subset of machine learning that is concerned with neural networks: how to\n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "use backpropagation and certain principles from neuroscience to more accurately model large\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "sets of unlabelled or semi-structured data. In that sense, deep learning represents an\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Steve Nouri  ​                           ​https://www.linkedin.com/in/stevenouri/ \n",
      "\n",
      "\n",
      "575. unsupervised learning algorithm that learns representations of data through the use of neural\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "nets. \n",
      " \n",
      "Q17 What’s the difference between a generative and discriminative model? \n",
      "A generative model will learn categories of data while a discriminative model will simply learn\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "the distinction between different categories of data. Discriminative models will generally\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "outperform generative models on classification tasks. \n",
      "Q18 What Are the Applications of Supervised Machine Learning in Modern\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Businesses? \n",
      "Applications of supervised machine learning include: \n",
      "●\n",
      "Email Spam Detection \n",
      "Here we train the model using historical data that consists of emails categorized as\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "spam or not spam. This labeled information is fed as input to the model. \n",
      "●\n",
      "Healthcare Diagnosis \n",
      "By providing images regarding a disease, a model can be trained to detect if a person is\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "   \n",
      "  \n",
      "suffering from the disease or not. \n",
      "●\n",
      "Sentiment Analysis \n",
      "This refers to the process of using algorithms to mine documents and determine\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "whether they’re positive, neutral, or negative in sentiment.  \n",
      "●\n",
      "Fraud Detection \n",
      "Training the model to identify suspicious patterns, we can detect instances of possible\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "fraud. \n",
      " \n",
      "Q19 What Is Semi-supervised Machine Learning? \n",
      "Supervised learning uses data that is completely labeled, whereas unsupervised learning uses\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "no training data. \n",
      "In the case of semi-supervised learning, the training data contains a small amount of labeled\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "data and a large amount of unlabeled data. \n",
      "Q20. What Are Unsupervised Machine Learning Techniques?  \n",
      "There are two techniques used in unsupervised learning: clustering and association. \n",
      "Clustering \n",
      "●\n",
      "Clustering problems involve data to be divided into subsets. These subsets, also called\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "clusters, contain data that are similar to each other. Different clusters reveal different\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "details about the objects, unlike classification or regression. \n",
      "Association \n",
      "●\n",
      "In an association problem, we identify patterns of associations between different\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "variables or items. \n",
      "●\n",
      "For example, an eCommerce website can suggest other items for you to buy, based on\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "the prior purchases that you have made, spending habits, items in your wishlist, other\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "customers’ purchase habits, and so on. \n",
      "Steve Nouri  ​                           ​https://www.linkedin.com/in/stevenouri/ \n",
      "\n",
      "\n",
      "576.  \n",
      " \n",
      " \n",
      "Q21 What Is ‘naive’ in the Naive Bayes Classifier? \n",
      "The classifier is called ‘naive’ because it makes assumptions that may or may not turn out to be\n",
      " \n",
      "  \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "correct.  \n",
      "The algorithm assumes that the presence of one feature of a class is not related to the presence\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "of any other feature (absolute independence of features), given the class variable. \n",
      "For instance, a fruit may be considered to be a cherry if it is red in color and round in shape,\n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      "    \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "regardless of other features. This assumption may or may not be right (as an apple also\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "matches the description). \n",
      "Q22 Explain Latent Dirichlet Allocation (LDA). \n",
      "Latent Dirichlet Allocation (LDA) is a common method of topic modeling, or classifying\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "documents by subject matter. \n",
      "LDA is a generative model that represents documents as a mixture of topics that each have\n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "their own probability distribution of possible words. \n",
      "The \"Dirichlet\" distribution is simply a distribution of distributions. In LDA, documents are\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "distributions of topics that are distributions of words. \n",
      "Q23 Explain Principle Component Analysis (PCA). \n",
      "PCA is a method for transforming features in a dataset by combining them into uncorrelated\n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "linear combinations. \n",
      "These new features, or principal components, sequentially maximize the variance represented\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "(i.e. the first principal component has the most variance, the second principal component has\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "the second most, and so on). \n",
      "As a result, PCA is useful for dimensionality reduction because you can set an arbitrary\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "variance cutoff. \n",
      " \n",
      "Q24 What’s the F1 score? How would you use it? \n",
      "The F1 score is a measure of a model’s performance. It is a weighted average of the precision\n",
      " \n",
      " \n",
      "   \n",
      " \n",
      "  \n",
      " \n",
      "    \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "and recall of a model, with results tending to 1 being the best, and those tending to 0 being the\n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      "worst. You would use it in classification tests where true negatives don’t matter much. \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Steve Nouri  ​                           ​https://www.linkedin.com/in/stevenouri/ \n",
      "\n",
      "\n",
      "577.  \n",
      " \n",
      "Q25 When should you use classification over regression? \n",
      "Classification produces discrete values and dataset to strict categories, while regression gives\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "you continuous results that allow you to better distinguish differences between individual points.\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "You would use classification over regression if you wanted your results to reflect the\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "belongingness of data points in your dataset to certain explicit categories (ex: If you wanted to\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      "know whether a name was male or female rather than just how correlated they were with male\n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "and female names.) \n",
      " \n",
      "Q26 How do you ensure you’re not overfitting with a model? \n",
      "This is a simple restatement of a fundamental problem in machine learning: the possibility of\n",
      " \n",
      "  \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "overfitting training data and carrying the noise of that data through to the test set, thereby\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "providing inaccurate generalizations. \n",
      "There are three main methods to avoid overfitting: \n",
      "1- Keep the model simpler: reduce variance by taking into account fewer variables and\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "parameters, thereby removing some of the noise in the training data. \n",
      "2- Use cross-validation techniques such as k-folds cross-validation. \n",
      "3- Use regularization techniques such as LASSO that penalize certain model parameters if\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      "they’re likely to cause overfitting. \n",
      " \n",
      "Q27 How Will You Know Which Machine Learning Algorithm to Choose for Your\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Classification Problem? \n",
      "While there is no fixed rule to choose an algorithm for a classification problem, you can follow\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "these guidelines: \n",
      "●\n",
      "If accuracy is a concern, test different algorithms and cross-validate them \n",
      "●\n",
      "If the training dataset is small, use models that have low variance and high bias \n",
      "●\n",
      "If the training dataset is large, use models that have high variance and little bias \n",
      " \n",
      "Q28 How Do You Design an Email Spam Filter? \n",
      "Building a spam filter involves the following process: \n",
      "●\n",
      "The email spam filter will be fed with thousands of emails  \n",
      "●\n",
      "Each of these emails already has a label: ‘spam’ or ‘not spam.’ \n",
      "●\n",
      "The supervised machine learning algorithm will then determine which type of emails are\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "being marked as spam based on spam words like the lottery, free offer, no money, full\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "refund, etc. \n",
      "●\n",
      "The next time an email is about to hit your inbox, the spam filter will use statistical\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "analysis and algorithms like Decision Trees and SVM to determine how likely the email\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "is spam \n",
      "●\n",
      "If the likelihood is high, it will label it as spam, and the email won’t hit your inbox \n",
      "Steve Nouri  ​                           ​https://www.linkedin.com/in/stevenouri/ \n",
      "\n",
      "\n",
      "578. ●\n",
      "Based on the accuracy of each model, we will use the algorithm with the highest\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "accuracy after testing all the models \n",
      "Q29 What evaluation approaches would you work to gauge the effectiveness of a\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      "machine learning model? \n",
      "You would first split the dataset into training and test sets, or perhaps use cross-validation\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "techniques to further segment the dataset into composite sets of training and test sets within the\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "data. You should then implement a choice selection of performance metrics: here is a fairly\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      "comprehensive list. You could use measures such as the F1 score, the accuracy, and the\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "confusion matrix. What’s important here is to demonstrate that you understand the nuances of\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "how a model is measured and how to choose the right performance measures for the right\n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "situations. \n",
      " \n",
      "Q30 How would you implement a recommendation system for our company’s\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "users? \n",
      "A lot of machine learning interview questions of this type will involve the implementation of\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "machine learning models to a company’s problems. You’ll have to research the company and its\n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "industry in-depth, especially the revenue drivers the company has, and the types of users the\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "company takes on in the context of the industry it’s in. \n",
      " \n",
      "Q31 Explain bagging. \n",
      "Bagging, or Bootstrap Aggregating, is an ensemble method in which the dataset is first divided\n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      "into multiple subsets through resampling. \n",
      "Then, each subset is used to train a model, and the final predictions are made through voting or\n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "averaging the component models. \n",
      "Bagging is performed in parallel. \n",
      "Q32 What is the ROC Curve and what is AUC (a.k.a. AUROC)? \n",
      "The ROC (receiver operating characteristic) the performance plot for binary classifiers of True\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Positive Rate (y-axis) vs. False Positive Rate (x- \n",
      "axis). \n",
      "AUC is the area under the ROC curve, and it's a common performance metric for evaluating\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "binary classification models. \n",
      "It's equivalent to the expected probability that a uniformly drawn random positive is ranked\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "before a uniformly drawn random negative. \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Steve Nouri  ​                           ​https://www.linkedin.com/in/stevenouri/ \n",
      "\n",
      "\n",
      "579.  \n",
      "Q33 Why is Area Under ROC Curve (AUROC) better than raw accuracy as an\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "out-of-sample evaluation metric? \n",
      "AUROC is robust to class imbalance, unlike raw accuracy. \n",
      "For example, if you want to detect a type of cancer that's prevalent in only 1% of the population,\n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "you can build a model that achieves 99% accuracy by simply classifying everyone has\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "cancer-free. \n",
      "Q34 What are the advantages and disadvantages of neural networks? \n",
      "Advantages​: Neural networks (specifically deep NNs) have led to performance breakthroughs\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "for unstructured datasets such as images, audio, and video. Their incredible flexibility allows\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "them to learn patterns that no other ML algorithm can learn. \n",
      "Disadvantages​: However, they require a large amount of training data to converge. It's also\n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "difficult to pick the right architecture, and the internal \"hidden\" layers are incomprehensible. \n",
      "Q35 Define Precision and Recall. \n",
      "Precision \n",
      "●\n",
      "Precision is the ratio of several events you can correctly recall to the total number of\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "events you recall (mix of correct and wrong recalls). \n",
      "●\n",
      "Precision = (True Positive) / (True Positive + False Positive) \n",
      "Recall \n",
      "●\n",
      "A recall is the ratio of a number of events you can recall the number of total events. \n",
      "●\n",
      "Recall = (True Positive) / (True Positive + False Negative) \n",
      " \n",
      "Q36 What Is Decision Tree Classification? \n",
      "A decision tree builds classification (or regression) models as a tree structure, with datasets\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "broken up into ever-smaller subsets while developing the decision tree, literally in a tree-like\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      "way with branches and nodes. Decision trees can handle both categorical and numerical data.  \n",
      " \n",
      "Q37  What Is Pruning in Decision Trees, and How Is It Done? \n",
      "Pruning is a technique in machine learning that reduces the size of decision trees. It reduces the\n",
      "   \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      "complexity of the final classifier, and hence improves predictive accuracy by the reduction of\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "overfitting.  \n",
      "Pruning can occur in: \n",
      "●\n",
      "Top-down fashion. It will traverse nodes and trim subtrees starting at the root \n",
      "●\n",
      "Bottom-up fashion. It will begin at the leaf nodes \n",
      "There is a popular pruning algorithm called reduced error pruning, in which: \n",
      "●\n",
      "Starting at the leaves, each node is replaced with its most popular class \n",
      "●\n",
      "If the prediction accuracy is not affected, the change is kept \n",
      "●\n",
      "There is an advantage of simplicity and speed \n",
      " \n",
      "Steve Nouri  ​                           ​https://www.linkedin.com/in/stevenouri/ \n",
      "\n",
      "\n",
      "580.  \n",
      " \n",
      "Q38 What Is a Recommendation System? \n",
      "Anyone who has used Spotify or shopped at Amazon will recognize a recommendation system:\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      "It’s an information filtering system that predicts what a user might want to hear or see based on\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "choice patterns provided by the user. \n",
      "Q39 What Is Kernel SVM? \n",
      "Kernel SVM is the abbreviated version of the kernel support vector machine. Kernel methods\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "are a class of algorithms for pattern analysis, and the most common one is the kernel SVM. \n",
      " \n",
      "Q40 What Are Some Methods of Reducing Dimensionality? \n",
      "You can reduce dimensionality by combining features with feature engineering, removing\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "collinear features, or using algorithmic dimensionality reduction. \n",
      "Now that you have gone through these machine learning interview questions, you must have\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "got an idea of your strengths and weaknesses in this domain. \n",
      "Q41 What Are the Three Stages of Building a Model in Machine Learning? \n",
      "The three stages of building a machine learning model are: \n",
      "●\n",
      "Model Building Choose a suitable algorithm for the model and train it according to the\n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      "requirement  \n",
      "●\n",
      "Model Testing​ Check the accuracy of the model through the test data  \n",
      "●\n",
      "Applying the Mode ​Make the required changes after testing and use the final model for\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "real-time projects. Here, it’s important to remember that once in a while, the model\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      "needs to be checked to make sure it’s working correctly. It should be modified to make\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "sure that it is up-to-date. \n",
      " \n",
      "Q42 How is KNN different from k-means clustering? \n",
      "K-Nearest Neighbors is a supervised classification algorithm, while k-means clustering is an\n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "unsupervised clustering algorithm. While the mechanisms may seem similar at first, what this\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "really means is that in order for K-Nearest Neighbors to work, you need labeled data you want\n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "to classify an unlabeled point into (thus the nearest neighbor part). K-means clustering requires\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "only a set of unlabeled points and a threshold: the algorithm will take unlabeled points and\n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "gradually learn how to cluster them into groups by computing the mean of the distance between\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "different points. \n",
      " \n",
      "Q43 Mention the difference between Data Mining and Machine learning? \n",
      "Machine learning relates to the study, design, and development of the algorithms that give\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "computers the capability to learn without being explicitly programmed. While data mining can\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "be defined as the process in which the unstructured data tries to extract knowledge or unknown\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "interesting patterns.  During this processing machine, learning algorithms are used. \n",
      "Steve Nouri  ​                           ​https://www.linkedin.com/in/stevenouri/ \n",
      "\n",
      "\n",
      "581.  \n",
      "Q44 What are the different Algorithm techniques in Machine Learning? \n",
      "The different types of techniques in Machine Learning are \n",
      "●\n",
      "Supervised Learning \n",
      "●\n",
      "Unsupervised Learning \n",
      "●\n",
      "Semi-supervised Learning \n",
      "●\n",
      "Reinforcement Learning \n",
      "●\n",
      "Transduction \n",
      "●\n",
      "Learning to Learn \n",
      " \n",
      "Q45 You are given a data set. The data set has missing values that spread along 1\n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      "standard deviation from the median. What percentage of data would remain\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "unaffected? Why? \n",
      "This question has enough hints for you to start thinking! Since the data is spread across the\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "median, let’s assume it’s a normal distribution. We know, in a normal distribution, ~68% of the\n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "data lies in 1 standard deviation from mean (or mode, median), which leaves ~32% of the data\n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "unaffected. Therefore, ~32% of the data would remain unaffected by missing values. \n",
      " \n",
      "Q46 What are PCA, KPCA, and ICA used for? \n",
      "PCA (Principal Components Analysis), KPCA ( Kernel-based Principal Component Analysis)\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "and ICA ( Independent Component Analysis) are important feature extraction techniques used\n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "for dimensionality reduction. \n",
      " \n",
      "Q47 What are support vector machines? \n",
      "Support vector machines are supervised learning algorithms used for classification and\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "regression analysis. \n",
      " \n",
      "Q48 What is batch statistical learning? \n",
      "Statistical learning techniques allow learning a function or predictor from a set of observed data\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "that can make predictions about unseen or future data. These techniques provide guarantees\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "on the performance of the learned predictor on the future unseen data based on a statistical\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      "assumption on the data generating process. \n",
      " \n",
      "Q49 What is the bias-variance decomposition of classification error in the\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "ensemble method? \n",
      "The expected error of a learning algorithm can be decomposed into bias and variance. A bias\n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      "term measures how closely the average classifier produced by the learning algorithm matches\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "the target function. The variance term measures how much the learning algorithm’s prediction\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "fluctuates for different training sets. \n",
      " \n",
      " \n",
      " \n",
      "Steve Nouri  ​                           ​https://www.linkedin.com/in/stevenouri/ \n",
      "\n",
      "\n",
      "582.  \n",
      "Q50 When is Ridge regression favorable over Lasso regression? \n",
      "You can quote ISLR’s authors Hastie, Tibshirani who asserted that, in the presence of few\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "variables with medium / large sized effect, use lasso regression. In presence of many variables\n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "with small/medium-sized effects, use ridge regression. \n",
      "Conceptually, we can say, lasso regression (L1) does both variable selection and parameter\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "shrinkage, whereas Ridge regression only does parameter shrinkage and end up including all\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "the coefficients in the model. In the presence of correlated variables, ridge regression might be\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "the preferred choice. Also, ridge regression works best in situations where the least square\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "estimates have higher variance. Therefore, it depends on our model objective. \n",
      " \n",
      "Q51 You’ve built a random forest model with 10000 trees. You got delighted after\n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "getting training error as 0.00. But, the validation error is 34.23. What is going on?\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Haven’t you trained your model perfectly? \n",
      "The model has overfitted. Training error 0.00 means the classifier has mimicked the training\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "data patterns to an extent, that they are not available in the unseen data. Hence, when this\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "classifier was run on an unseen sample, it couldn’t find those patterns and returned predictions\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "with higher error. In a random forest, it happens when we use a larger number of trees than\n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "necessary. Hence, to avoid this situation, we should tune the number of trees using\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "cross-validation. \n",
      " \n",
      "Q50 What is a convex hull? \n",
      "In the case of linearly separable data, the convex hull represents the outer boundaries of the\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "two groups of data points. Once the convex hull is created, we get maximum margin hyperplane\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "(MMH) as a perpendicular bisector between two convex hulls. MMH is the line which attempts to\n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "create the greatest separation between two groups. \n",
      " \n",
      "Q51 What do you understand by Type I vs Type II error? \n",
      "Type I error is committed when the null hypothesis is true and we reject it, also known as a\n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      "‘False Positive’. Type II error is committed when the null hypothesis is false and we accept it,\n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "also known as ‘False Negative’. \n",
      "In the context of the confusion matrix, we can say Type I error occurs when we classify a value\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      "as positive (1) when it is actually negative (0). Type II error occurs when we classify a value as\n",
      " \n",
      " \n",
      " \n",
      "   \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      "negative (0) when it is actually positive(1). \n",
      " \n",
      "Q52. In k-means or kNN, we use euclidean distance to calculate the distance\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "between nearest neighbors. Why not manhattan distance? \n",
      "We don’t use manhattan distance because it calculates distance horizontally or vertically only. It\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      "has dimension restrictions. On the other hand, the euclidean metric can be used in any space to\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      "calculate distance. Since the data points can be present in any dimension, euclidean distance is\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      "a more viable option. \n",
      "Steve Nouri  ​                           ​https://www.linkedin.com/in/stevenouri/ \n",
      "\n",
      "\n",
      "583. Example: Think of a chessboard, the movement made by a bishop or a rook is calculated by\n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      "  \n",
      "  \n",
      " \n",
      " \n",
      "manhattan distance because of their respective vertical & horizontal movements. \n",
      " \n",
      "Q53 Do you suggest that treating a categorical variable as a continuous variable\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      "would result in a better predictive model? \n",
      "For better predictions, the categorical variable can be considered as a continuous variable only\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      "when the variable is ordinal in nature. \n",
      " \n",
      "Q54 OLS is to linear regression. The maximum likelihood is logistic regression.\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Explain the statement. \n",
      "OLS and Maximum likelihood are the methods used by the respective regression methods to\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "approximate the unknown parameter (coefficient) value. In simple words, \n",
      "Ordinary least square(OLS) is a method used in linear regression which approximates the\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "parameters resulting in minimum distance between actual and predicted values. Maximum\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Likelihood helps in choosing the values of parameters which maximizes the likelihood that the\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "parameters are most likely to produce observed data. \n",
      " \n",
      "Q55 When does regularization becomes necessary in Machine Learning? \n",
      "Regularization becomes necessary when the model begins to overfit/underfit. This technique\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "introduces a cost term for bringing in more features with the objective function. Hence, it tries to\n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      "push the coefficients for many variables to zero and hence reduce the cost term. This helps to\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "reduce model complexity so that the model can become better at predicting (generalizing). \n",
      " \n",
      "Q56 What is Linear Regression? \n",
      "Linear Regression is a supervised Machine Learning algorithm. It is used to find the linear\n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "relationship between the dependent and the independent variables for predictive analysis. \n",
      "Q57 What is the Variance Inflation Factor? \n",
      "Variance Inflation Factor (VIF) is the estimate of the volume of multicollinearity in a collection of\n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "   \n",
      " \n",
      " \n",
      "many regression variables. \n",
      "VIF = Variance of the model / Variance of the model with a single independent variable \n",
      "We have to calculate this ratio for every independent variable. If VIF is high, then it shows the\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      "  \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      "high collinearity of the independent variables. \n",
      " \n",
      "Q58 We know that one hot encoding increases the dimensionality of a dataset,\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      "but label encoding doesn’t. How? \n",
      "When we use ​one-hot encoding​, there is an increase in the dimensionality of a dataset. The\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      "reason for the increase in dimensionality is that, for every class in the categorical variables, it\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      "forms a different variable. \n",
      "Steve Nouri  ​                           ​https://www.linkedin.com/in/stevenouri/ \n",
      "\n",
      "\n",
      "584.  \n",
      "Q59 What is a Decision Tree? \n",
      "A decision tree is used to explain the sequence of actions that must be performed to get the\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "desired output. It is a hierarchical diagram that shows the actions. \n",
      "Q60 What is the Binarizing of data? How to Binarize? \n",
      "In most of the Machine Learning Interviews, apart from theoretical questions, interviewers focus\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "on the implementation part. So, this ML Interview Questions focused on the implementation of\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "the theoretical concepts. \n",
      "Converting data into binary values on the basis of threshold values is known as the binarizing of\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "data. The values that are less than the threshold are set to 0 and the values that are greater\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "than the threshold are set to 1. This process is useful when we have to perform feature\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "engineering, and we can also use it for adding unique features. \n",
      " \n",
      "Q61 What is cross-validation? \n",
      "Cross-validation is essentially a technique used to assess how well a model performs on a new\n",
      "  \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      "independent dataset. The simplest example of cross-validation is when you split your data into\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "two groups: training data and testing data, where you use the training data to build the model\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "and the testing data to test the model. \n",
      " \n",
      "Q62 When would you use random forests Vs SVM and why? \n",
      "There are a couple of reasons why a random forest is a better choice of the model than a\n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      "support vector machine: \n",
      "●\n",
      "Random forests allow you to determine the feature importance. SVM’s can’t do this. \n",
      "●\n",
      "Random forests are much quicker and simpler to build than an SVM. \n",
      "●\n",
      "For multi-class classification problems, SVMs require a one-vs-rest method, which is\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "less scalable and more memory intensive. \n",
      " \n",
      "Q63 What are the drawbacks of a linear model? \n",
      "There are a couple of drawbacks of a linear model: \n",
      "●\n",
      "A linear model holds some strong assumptions that may not be true in the application. It\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      "  \n",
      "assumes a linear relationship, multivariate normality, no or little multicollinearity, no\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "auto-correlation, and homoscedasticity \n",
      "●\n",
      "A linear model can’t be used for discrete or binary outcomes. \n",
      "●\n",
      "You can’t vary the model flexibility of a linear model. \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Steve Nouri  ​                           ​https://www.linkedin.com/in/stevenouri/ \n",
      "\n",
      "\n",
      "585.  \n",
      "Q64 Do you think 50 small decision trees are better than a large one? Why? \n",
      "Another way of asking this question is “Is a random forest a better model than a decision tree?”\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      "  \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      "And the answer is yes because a random forest is an ensemble method that takes many weak\n",
      " \n",
      " \n",
      "  \n",
      " \n",
      "  \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "decision trees to make a strong learner. Random forests are more accurate, more robust, and\n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "less prone to overfitting. \n",
      " \n",
      "Q65 What is a kernel? Explain the kernel trick \n",
      "A kernel is a way of computing the dot product of two vectors 𝐱x and 𝐲y in some (possibly very\n",
      " \n",
      "   \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      "high dimensional) feature space, which is why kernel functions are sometimes called\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "“generalized dot product”  \n",
      "The kernel trick is a method of using a linear classifier to solve a non-linear problem by\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "transforming linearly inseparable data to linearly separable ones in a higher dimension. \n",
      "Q66 State the differences between causality and correlation? \n",
      "Causality applies to situations where one action, say X, causes an outcome, say Y, whereas\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Correlation is just relating one action (X) to another action(Y) but X does not necessarily cause\n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Y. \n",
      "Q67 What is the exploding gradient problem while using the backpropagation\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "technique? \n",
      "When large error gradients accumulate and result in large changes in the neural network\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "weights during training, it is called the exploding gradient problem. The values of weights can\n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "become so large as to overflow and result in NaN values. This makes the model unstable and\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "the learning of the model to stall just like the vanishing gradient problem. \n",
      "Q68 What do you mean by Associative Rule Mining (ARM)? \n",
      "Associative Rule Mining is one of the techniques to discover patterns in data like features\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "(dimensions) which occur together and features (dimensions) which are correlated. \n",
      " \n",
      "Q69 What is Marginalisation? Explain the process. \n",
      "Marginalizationarginalisation is summing the probability of a random variable X given the joint\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      "probability distribution of X with other variables. It is an application of the law of total probability. \n",
      " \n",
      "Q70 Why is the rotation of components so important in Principle Component\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Analysis(PCA)? \n",
      "Rotation in PCA is very important as it maximizes the separation within the variance obtained by\n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "all the components because of which interpretation of components would become easier. If the\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      "components are not rotated, then we need extended components to describe the variance of\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "the components. \n",
      " \n",
      "Steve Nouri  ​                           ​https://www.linkedin.com/in/stevenouri/ \n",
      "\n",
      "\n",
      "586.  \n",
      "Q71 What is the difference between regularization and normalisation?  \n",
      "Normalisation adjusts the data; regularisation adjusts the prediction function. If your data is on\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      "  \n",
      " \n",
      "very different scales (especially low to high), you would want to normalise the data. Alter each\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "column to have compatible basic statistics. This can be helpful to make sure there is no loss of\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      "accuracy. One of the goals of model training is to identify the signal and ignore the noise if the\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      "model is given free rein to minimize error, there is a possibility of suffering from overfitting.\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Regularization imposes some control on this by providing simpler fitting functions over complex\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "ones. \n",
      "Q72 When does the linear regression line stop rotating or finds an optimal spot where it\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      "is fitted on data?  \n",
      "A place where the highest RSquared value is found, is the place where the line comes to rest.\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "RSquared represents the amount of variance captured by the virtual linear regression line with\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "respect to the total variance captured by the dataset. \n",
      "Q73 How does the SVM algorithm deal with self-learning?  \n",
      "SVM has a learning rate and expansion rate which takes care of this. The learning rate\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "compensates or penalises the hyperplanes for making all the wrong moves and expansion rate\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "deals with finding the maximum separation area between classes. \n",
      "Q74 How do you handle outliers in the data? \n",
      "Outlier is an observation in the data set that is far away from other observations in the data set.\n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      "We can discover outliers using tools and functions like box plot, scatter plot, Z-Score, IQR score\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "etc. and then handle them based on the visualization we have got. To handle outliers, we can\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "cap at some threshold, use transformations to reduce skewness of the data and remove outliers\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "if they are anomalies or errors. \n",
      "Q75 Name and define techniques used to find similarities in the recommendation system.  \n",
      "Pearson\n",
      "correlation and Cosine correlation are techniques used to find similarities in\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "recommendation systems. \n",
      "Q76 Why would you Prune your tree? \n",
      "In the context of data science or AIML, pruning refers to the process of reducing redundant\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "branches of a decision tree. Decision Trees are prone to overfitting, pruning the tree helps to\n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "reduce the size and minimizes the chances of overfitting. Pruning involves turning branches of a\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      "decision tree into leaf nodes and removing the leaf nodes from the original branch. It serves as\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      "a tool to perform the tradeoff. \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Steve Nouri  ​                           ​https://www.linkedin.com/in/stevenouri/ \n",
      "\n",
      "\n",
      "587.  \n",
      "Q77 Mention some of the EDA Techniques? \n",
      "Exploratory Data Analysis (EDA) helps analysts to understand the data better and forms the\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "foundation of better models.  \n",
      "Visualization \n",
      "●\n",
      "Univariate visualization \n",
      "●\n",
      "Bivariate visualization \n",
      "●\n",
      "Multivariate visualization \n",
      "Missing Value Treatmen​t – Replace missing values with Either Mean/Median \n",
      "Outlier Detection – Use Boxplot to identify the distribution of Outliers, then Apply IQR to set the\n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "boundary for IQR \n",
      " \n",
      "Q78 What is data augmentation? Can you give some examples? \n",
      "Data augmentation is a technique for synthesizing new data by modifying existing data in such a\n",
      " \n",
      "   \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      "  \n",
      "way that the target is not changed, or it is changed in a known way. \n",
      "CV is one of the fields where data augmentation is very useful. There are many modifications\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "that we can do to images: \n",
      "●\n",
      "Resize \n",
      "●\n",
      "Horizontal or vertical flip \n",
      "●\n",
      "Rotate \n",
      "●\n",
      "Add noise \n",
      "●\n",
      "Deform \n",
      "●\n",
      "Modify colors \n",
      "Each problem needs a customized data augmentation pipeline. For example, on OCR, doing\n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "flips will change the text and won’t be beneficial; however, resizes and small rotations may help. \n",
      "Q79 What is Inductive Logic Programming in Machine Learning (ILP)? \n",
      "Inductive Logic Programming (ILP) is a subfield of machine learning which uses logic\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "programming representing background knowledge and examples. \n",
      "Q80 What is the difference between inductive machine learning and deductive machine\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "learning? \n",
      "The difference between inductive machine learning and deductive machine learning are as\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "follows: machine-learning where the model learns by examples from a set of observed\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "instances to draw a generalized conclusion whereas in deductive learning the model first draws\n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "the conclusion and then the conclusion is drawn. \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Steve Nouri  ​                           ​https://www.linkedin.com/in/stevenouri/ \n",
      "\n",
      "\n",
      "588.  \n",
      "Q81 Difference between machine learning and deep learning \n",
      "Machine learning is a branch of computer science and a method to implement artificial\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "intelligence. This technique provides the ability to automatically learn and improve from\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "experiences without being explicitly programmed. \n",
      "Deep learning can be said as a subset of machine learning. It is mainly based on the artificial\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      "   \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "neural network where data is taken as an input and the technique makes intuitive decisions\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "using the artificial neural network. \n",
      " \n",
      "Q82 What Are The Steps Involved In Machine Learning Project? \n",
      "As you plan for doing a machine learning project. There are several important steps you must\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "follow to achieve a good working model and they are data collection, data preparation, choosing\n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "a machine learning model, training the model, model evaluation, parameter tuning and lastly\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "prediction. \n",
      " \n",
      "Q83 Differences between Artificial Intelligence and Machine Learning? \n",
      "Artificial intelligence is a broader prospect than machine learning. Artificial intelligence mimics\n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "the cognitive functions of the human brain. The purpose of AI is to carry out a task in an\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      "intelligent manner based on algorithms. On the other hand, machine learning is a subclass of\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "   \n",
      " \n",
      " \n",
      "artificial intelligence. To develop an autonomous machine in such a way so that it can learn\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      "without being explicitly programmed is the goal of machine learning. \n",
      " \n",
      "Q84 Steps Needed to Choose the Appropriate Machine Learning Algorithm for\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "your Classification problem. \n",
      "Firstly, you need to have a clear picture of your data, your constraints, and your problems\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "before\n",
      "heading\n",
      "towards\n",
      "different\n",
      "machine learning algorithms. Secondly, you have to\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "understand which type and kind of data you have because it plays a primary role in deciding\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "which algorithm you have to use. \n",
      "Following this step is the data categorization step, which is a two-step process – categorization\n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "   \n",
      " \n",
      "  \n",
      " \n",
      "by input and categorization by output. The next step is to understand your constraints; that is,\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "what is your data storage capacity? How fast the prediction has to be? etc. \n",
      "Finally, find the available machine learning algorithms and implement them wisely. Along with\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "that, also try to optimize the hyperparameters which can be done in three ways – grid search,\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      "random search, and Bayesian optimization. \n",
      " \n",
      " \n",
      " \n",
      "Steve Nouri  ​                           ​https://www.linkedin.com/in/stevenouri/ \n",
      "\n",
      "\n",
      "589.  \n",
      "Q85 Explain Backpropagation in Machine Learning. \n",
      "A very important question for your machine learning interview. Backpropagation ​is the\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "algorithm for computing artificial neural networks (ANN). It is used by the gradient descent\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "optimization that exploits the chain rule. By calculating the gradient of the loss function, the\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "weight of the neurons is adjusted to a certain value. To train a multi-layered neural network is\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      "  \n",
      "the prime motivation of backpropagation so that it can learn the appropriate internal\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "demonstrations. This will help them learn to map any input to its respective output arbitrarily. \n",
      " \n",
      "Q86 What is the Convex Function? \n",
      "This question is very often asked in machine learning interviews. A convex function is a\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "continuous function, and the value of the midpoint at every interval in its given domain is less\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      "than the numerical mean of the values at the two ends of the interval. \n",
      " \n",
      "Q87 What’s the Relationship between True Positive Rate and Recall? \n",
      "The True positive rate in machine learning is the percentage of the positives that have been\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "properly acknowledged, and recall is just the count of the results that have been correctly\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "identified and are relevant. Therefore, they are the same things, just having different names. It is\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "   \n",
      "also known as sensitivity. \n",
      " \n",
      "Q88 List some Tools for Parallelizing Machine Learning Algorithms. \n",
      "Although this question may seem very easy, make sure not to skip this one because it is also\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "   \n",
      " \n",
      "very closely related to artificial intelligence and thereby, AI interview questions. Almost all\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "machine learning algorithms are easy to serialize. Some of the basic tools for parallelizing are\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Matlab, Weka, R, Octave, or the Python-based sci-kit learn. \n",
      " \n",
      "Q89 What do you mean by Genetic Programming? \n",
      "Genetic Programming (GP) is almost similar to an Evolutionary Algorithm, a subset of machine\n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      "learning. Genetic programming software systems implement an algorithm that uses random\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "mutation, a fitness function, crossover, and multiple generations of evolution to resolve a\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "user-defined task. The genetic programming model is based on testing and choosing the best\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "option among a set of results. \n",
      " \n",
      "Q90 What do you know about Bayesian Networks? \n",
      "Bayesian Networks also referred to as 'belief networks' or 'casual networks', are used to\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "represent the graphical model for probability relationship among a set of variables. \n",
      "For example, a Bayesian network can be used to represent the probabilistic relationships\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "between diseases and symptoms. As per the symptoms, the network can also compute the\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "probabilities of the presence of various diseases. \n",
      "Steve Nouri  ​                           ​https://www.linkedin.com/in/stevenouri/ \n",
      "\n",
      "\n",
      "590. Efficient algorithms can perform inference or learning in Bayesian networks. Bayesian networks\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "which relate the variables (e.g., speech signals or protein sequences) are called dynamic\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Bayesian networks. \n",
      "Q91 Which are the two components of the Bayesian logic program? \n",
      "A Bayesian logic program consists of two components: \n",
      "●\n",
      "Logical It contains a set of Bayesian Clauses, which capture the qualitative structure of\n",
      "  \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "the domain. \n",
      "●\n",
      "Quantitative​ It is used to encode quantitative information about the domain. \n",
      " \n",
      "Q92 How is machine learning used in day-to-day life? \n",
      "Most of the people are already using machine learning in their everyday life. Assume that you\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "are engaging with the internet, you are actually expressing your preferences, likes, dislikes\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "through your searches. All these things are picked up by cookies coming on your computer,\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "from this, the behavior of a user is evaluated. It helps to increase the progress of a user through\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      "  \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      "the internet and provide similar suggestions. \n",
      "The navigation system can also be considered as one of the examples where we are using\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "machine learning to calculate a distance between two places using optimization techniques.\n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Surely, people are going to more engage with machine learning in the near future \n",
      " \n",
      "Q93 Define Sampling. Why do we need it? \n",
      "Answer: Sampling is a process of choosing a subset from a target population that would serve\n",
      " \n",
      "   \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "as its representative. We use the data from the sample to understand the pattern in the\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "community as a whole. Sampling is necessary because often, we can not gather or process the\n",
      " \n",
      "  \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "complete data within a reasonable time. \n",
      " \n",
      "Q94 What does the term decision boundary mean? \n",
      "Answer: A decision boundary or a decision surface is a hypersurface which divides the\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "underlying feature space into two subspaces, one for each class. If the decision boundary is a\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      "   \n",
      "hyperplane, then the classes are linearly separable. \n",
      " \n",
      "Q95 Define entropy? \n",
      "Answer: Entropy is the measure of uncertainty associated with random variable Y. It is the\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      "expected number of bits required to communicate the value of the variable. \n",
      " \n",
      "Q96 Indicate the top intents of machine learning? \n",
      "Answer: The top intents of machine learning are stated below, \n",
      "●\n",
      "The system gets information from the already established computations to give\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "well-founded decisions and outputs. \n",
      "●\n",
      "It locates certain patterns in the data and then makes certain predictions on it to provide\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      "answers on matters. \n",
      " \n",
      "Steve Nouri  ​                           ​https://www.linkedin.com/in/stevenouri/ \n",
      "\n",
      "\n",
      "591. Q97\n",
      "Highlight\n",
      "the\n",
      "differences\n",
      "between\n",
      "the\n",
      "Generative\n",
      "model\n",
      "and\n",
      "the\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Discriminative model? \n",
      "The aim of the Generative model is to generate new samples from the same distribution and\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "new data instances, Whereas, the Discriminative model highlights the differences between\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "different kinds of data instances. It tries to learn directly from the data and then classifies the\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "data. \n",
      " \n",
      "Q98 Identify the most important aptitudes of a machine learning engineer? \n",
      "Machine learning allows the computer to learn itself without being decidedly programmed. It\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      "helps the system to learn from experience and then improve from its mistakes. The intelligence\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "system, which is based on machine learning, can learn from recorded data and past incidents.\n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "In-depth knowledge of statistics, probability, data modelling, programming language, as well as\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "CS, Application of ML Libraries and algorithms, and software design is required to become a\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      "successful machine learning engineer. \n",
      " \n",
      "Q99 What is feature engineering? How do you apply it in the process of\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "modelling? \n",
      "Feature engineering is the process of transforming raw data into features that better represent\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "the underlying problem to the predictive models, resulting in improved model accuracy on\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "unseen data. \n",
      " \n",
      "Q100 How can learning curves help create a better model? \n",
      "Learning curves give the indication of the presence of overfitting or underfitting. \n",
      "In a learning curve, the training error and cross-validating error are plotted against the number\n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "of training data points.  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "References \n",
      " \n",
      "1\n",
      "springboard.com\n",
      "2\n",
      "​simplilearn.com\n",
      "3\n",
      "geeksforgeeks.org\n",
      "4\n",
      "​elitedatascience.com\n",
      "5\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "analyticsvidhya.com\n",
      "6\n",
      "​guru99.com\n",
      "7\n",
      "​intellipaat.com\n",
      "8\n",
      "towardsdatascience.com\n",
      "9\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "mygreatlearning.com 10 ​mindmajix.com 11 toptal.com 12 ​glassdoor.co.in 13 ​udacity.com 14\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "educba.com 15 ​analyticsindiamag.com 16 ​ubuntupit.com 17 ​javatpoint.com 18 quora.com 19\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "hackr.io 20 kaggle.com \n",
      "Steve Nouri  ​                           ​https://www.linkedin.com/in/stevenouri/ \n",
      "\n",
      "\n",
      "592. Steve Nouri                          https://www.linkedin.com/in/stevenouri/ \n",
      "Top 100 NLP Questions \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Steve Nouri \n",
      " \n",
      " \n",
      "Q1. Which of the following techniques can be used for keyword normalization in \n",
      "NLP, the process of converting a keyword into its base form? \n",
      "a. Lemmatization \n",
      "b. Soundex \n",
      "c. Cosine Similarity \n",
      "d. N-grams \n",
      " \n",
      "Answer : a) Lemmatization helps to get to the base form of a word, e.g. are playing -> play, eating \n",
      "-> eat, etc.Other options are meant for different purposes. \n",
      " \n",
      "Q2. Which of the following techniques can be used to compute the distance \n",
      "between two word vectors in NLP? \n",
      "a. Lemmatization \n",
      "b. Euclidean distance \n",
      "c. Cosine Similarity \n",
      "d. N-grams \n",
      " \n",
      "Answer : b) and c) \n",
      "Distance between two word vectors can be computed using Cosine similarity and Euclidean \n",
      "Distance.  Cosine Similarity establishes a cosine angle between the vector of two words. A cosine \n",
      "angle close to each other between two word vectors indicates the words are similar and vice a \n",
      "versa. \n",
      "E.g. cosine angle between two words “Football” and “Cricket” will be closer to 1 as compared to \n",
      "angle between the words “Football” and “New Delhi” \n",
      " \n",
      "Q3. What are the possible features of a text corpus in NLP? \n",
      "a. Count of the word in a document \n",
      "b. Vector notation of the word \n",
      "c. Part of Speech Tag \n",
      "d. Basic Dependency Grammar \n",
      "e. All of the above \n",
      " \n",
      "Answer : e)All of the above can be used as features of the text corpus. \n",
      " \n",
      " \n",
      "Q4. You created a document term matrix on the input data of 20K documents for a \n",
      "Machine learning model. Which of the following can be used to reduce the \n",
      "dimensions of data? \n",
      "\n",
      "\n",
      "593. Steve Nouri                          https://www.linkedin.com/in/stevenouri/ \n",
      "1. Keyword Normalization \n",
      "2. Latent Semantic Indexing \n",
      "3. Latent Dirichlet Allocation \n",
      " \n",
      "a. only 1 \n",
      "b. 2, 3 \n",
      "c. 1, 3 \n",
      "d. 1, 2, 3 \n",
      "  \n",
      "Answer : d) \n",
      " \n",
      "Q5. Which of the text parsing techniques can be used for noun phrase detection, \n",
      "verb phrase detection, subject detection, and object detection in NLP. \n",
      "a. Part of speech tagging \n",
      "b. Skip Gram and N-Gram extraction \n",
      "c. Continuous Bag of Words \n",
      "d. Dependency Parsing and Constituency Parsing \n",
      " \n",
      "Answer : d) \n",
      " \n",
      "Q6. Dissimilarity between words expressed using cosine similarity will have values \n",
      "significantly higher than 0.5 \n",
      "a. True \n",
      "b. False \n",
      " \n",
      "Answer : a) \n",
      " \n",
      "Q7. Which one of the following are keyword Normalization techniques in NLP \n",
      "a.  Stemming \n",
      "b.  Part of Speech \n",
      "c. Named entity recognition \n",
      "d. Lemmatization \n",
      " \n",
      "Answer : a) and d) \n",
      "Part of Speech (POS) and Named Entity Recognition(NER) are not keyword Normalization \n",
      "techniques. Named Entity help you extract Organization, Time, Date, City, etc..type of entities \n",
      "from the given sentence, whereas Part of Speech helps you extract Noun, Verb, Pronoun, \n",
      "adjective, etc..from the given sentence tokens. \n",
      " \n",
      "Q8. Which of the below are NLP use cases? \n",
      "a. Detecting objects from an image \n",
      "b. Facial Recognition \n",
      "c. Speech Biometric \n",
      "d. Text Summarization \n",
      "\n",
      "\n",
      "594. Steve Nouri                          https://www.linkedin.com/in/stevenouri/ \n",
      " \n",
      "Answer : (d) \n",
      "a) And b) are Computer Vision use cases, and c) is Speech use case. \n",
      "Only d) Text Summarization is an NLP use case. \n",
      " \n",
      "Q9. In a corpus of N documents, one randomly chosen document contains a total \n",
      "of T terms and the term “hello” appears K times. \n",
      "What is the correct value for the product of TF (term frequency) and IDF (inverse-document-\n",
      "frequency), if the term “hello” appears in approximately one-third of the total documents? \n",
      "a. KT * Log(3) \n",
      "b. T * Log(3) / K \n",
      "c. K * Log(3) / T \n",
      "d. Log(3) / KT \n",
      " \n",
      "Answer : (c) \n",
      "formula for TF is K/T \n",
      "formula for IDF is log(total docs / no of docs containing “data”) \n",
      "= log(1 / (⅓)) \n",
      "= log (3) \n",
      "Hence correct choice is Klog(3)/T \n",
      " \n",
      "Q10. In NLP, The algorithm decreases the weight for commonly used words and \n",
      "increases the weight for words that are not used very much in a collection of \n",
      "documents \n",
      "a. Term Frequency (TF) \n",
      "b. Inverse Document Frequency (IDF) \n",
      "c. Word2Vec \n",
      "d. Latent Dirichlet Allocation (LDA) \n",
      " \n",
      "Answer : b) \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Q11. In NLP, The process of removing words like “and”, “is”, “a”, “an”, “the” from \n",
      "a sentence is called as \n",
      "a. Stemming \n",
      "b. Lemmatization \n",
      "c. Stop word \n",
      "d. All of the above \n",
      " \n",
      "Answer : c) In Lemmatization, all the stop words such as a, an, the, etc.. are removed. One can \n",
      "also define custom stop words for removal. \n",
      "\n",
      "\n",
      "595. Steve Nouri                          https://www.linkedin.com/in/stevenouri/ \n",
      " \n",
      "Q12. In NLP, The process of converting a sentence or paragraph into tokens is \n",
      "referred to as Stemming \n",
      "a. True \n",
      "b. False \n",
      " \n",
      "Answer : b) The statement describes the process of tokenization and not stemming, hence it is \n",
      "False. \n",
      " \n",
      "Q13. In NLP, Tokens are converted into numbers before giving to any Neural \n",
      "Network \n",
      "a. True \n",
      "b. False \n",
      " \n",
      "Answer : a) In NLP, all words are converted into a number before feeding to a Neural Network. \n",
      " \n",
      "Q14 Identify the odd one out \n",
      "a. nltk \n",
      "b. scikit learn \n",
      "c. SpaCy \n",
      "d. BERT \n",
      " \n",
      "Answer : d) All the ones mentioned are NLP libraries except BERT, which is a word embedding \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Q15 TF-IDF helps you to establish? \n",
      "a. most frequently occurring word in the document \n",
      "b. most important word in the document \n",
      " \n",
      "Answer : b) TF-IDF helps to establish how important a particular word is in the context of the \n",
      "document corpus. TF-IDF takes into account the number of times the word appears in the \n",
      "document and offset by the number of documents that appear in the corpus. \n",
      "● TF is the frequency of term divided by a total number of terms in the document. \n",
      "● IDF is obtained by dividing the total number of documents by the number of documents \n",
      "containing the term and then taking the logarithm of that quotient. \n",
      "\n",
      "\n",
      "596. Steve Nouri                          https://www.linkedin.com/in/stevenouri/ \n",
      "● Tf.idf is then the multiplication of two values TF and IDF. \n",
      " \n",
      "Q16 In NLP, The process of identifying people, an organization from a given \n",
      "sentence, paragraph is called \n",
      "a. Stemming \n",
      "b. Lemmatization \n",
      "c. Stop word removal \n",
      "d. Named entity recognition \n",
      " \n",
      "Answer : d) \n",
      " \n",
      "Q17 Which one of the following is not a pre-processing technique in NLP \n",
      "a. Stemming and Lemmatization \n",
      "b. converting to lowercase \n",
      "c. removing punctuations \n",
      "d. removal of stop words \n",
      "e. Sentiment analysis \n",
      " \n",
      "Answer : e) Sentiment Analysis is not a pre-processing technique. It is done after pre-processing \n",
      "and is an NLP use case. All other listed ones are used as part of statement pre-processing. \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Q18 In text mining, converting text into tokens and then converting them into an \n",
      "integer or floating-point vectors can be done using \n",
      "a. CountVectorizer \n",
      "b.  TF-IDF \n",
      "c. Bag of Words \n",
      "d. NERs \n",
      " \n",
      "Answer : a) CountVectorizer helps do the above, while others are not applicable. \n",
      "text =[“Rahul is an avid writer, he enjoys studying understanding and presenting. He loves to \n",
      "play”] \n",
      "vectorizer = CountVectorizer() \n",
      "vectorizer.fit(text) \n",
      "vector = vectorizer.transform(text) \n",
      "\n",
      "\n",
      "597. Steve Nouri                          https://www.linkedin.com/in/stevenouri/ \n",
      "print(vector.toarray()) \n",
      "output  \n",
      "[[1 1 1 1 2 1 1 1 1 1 1 1 1 1]] \n",
      "The second section of the interview questions covers advanced NLP techniques such as \n",
      "Word2Vec, GloVe  word embeddings, and advanced models such as GPT, ELMo, BERT, XLNET \n",
      "based questions, and explanations. \n",
      " \n",
      "Q19. In NLP, Words represented as vectors are called as Neural Word Embeddings \n",
      "a. True \n",
      "b. False \n",
      "Answer : a) Word2Vec, GloVe based models build word embedding vectors that are \n",
      "multidimensional. \n",
      " \n",
      "Q20. In NLP, Context modeling is supported with which one of the following word \n",
      "embeddings \n",
      "1. a. Word2Vec \n",
      "2. b) GloVe \n",
      "3. c) BERT \n",
      "4. d) All of the above \n",
      "Answer : c) Only BERT (Bidirectional Encoder Representations from Transformer) supports \n",
      "context modelling where the previous and next sentence context is taken into consideration. In \n",
      "Word2Vec, GloVe only word embeddings are considered and previous and next sentence context \n",
      "is not considered. \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Q21. In NLP, Bidirectional context is supported by which of the following \n",
      "embedding \n",
      "a. Word2Vec \n",
      "b. BERT \n",
      "c. GloVe \n",
      "d. All the above \n",
      "Answer : b) Only BERT provides a bidirectional context. The BERT model uses the previous and \n",
      "the next sentence to arrive at the context.Word2Vec and GloVe are word embeddings, they do \n",
      "not provide any context. \n",
      " \n",
      "Q22. Which one of the following Word embeddings can be custom trained for a \n",
      "specific subject in NLP \n",
      "a. Word2Vec \n",
      "b. BERT \n",
      "\n",
      "\n",
      "598. Steve Nouri                          https://www.linkedin.com/in/stevenouri/ \n",
      "c. GloVe \n",
      "d. All the above \n",
      " \n",
      "Answer : b) BERT allows Transform Learning on the existing pre-trained models and hence can \n",
      "be custom trained for the given specific subject, unlike Word2Vec and GloVe where existing word \n",
      "embeddings can be used, no transfer learning on text is possible. \n",
      " \n",
      "Q23. Word embeddings capture multiple dimensions of data and are represented \n",
      "as vectors \n",
      "a. True \n",
      "b. False \n",
      " \n",
      "Answer : a) \n",
      " \n",
      "Q24. In NLP, Word embedding vectors help establish distance between two tokens \n",
      "a. True \n",
      "b. False \n",
      " \n",
      "Answer : a) One can use Cosine similarity to establish distance between two vectors represented \n",
      "through Word Embeddings \n",
      " \n",
      "Q25. Language Biases are introduced due to historical data used during training of \n",
      "word embeddings, which one amongst the below is not an example of bias \n",
      "a. New Delhi is to India, Beijing is to China \n",
      "b. Man is to Computer, Woman is to Homemaker \n",
      " \n",
      "Answer : a) \n",
      "Statement b) is a bias as it buckets Woman into Homemaker, whereas statement a) is not a \n",
      "biased statement. \n",
      " \n",
      "Q26. Which of the following will be a better choice to address NLP use cases such \n",
      "as semantic similarity, reading comprehension, and common sense reasoning \n",
      "a. ELMo \n",
      "b. Open AI’s GPT \n",
      "c. ULMFit \n",
      " \n",
      "Answer :  b) Open AI’s GPT is able to learn complex pattern in data by using the Transformer \n",
      "models Attention mechanism and hence is more suited for complex use cases such as semantic \n",
      "similarity, reading comprehensions, and common sense reasoning. \n",
      " \n",
      "Q27. Transformer architecture was first introduced with? \n",
      "a. GloVe \n",
      "b. BERT \n",
      "\n",
      "\n",
      "599. Steve Nouri                          https://www.linkedin.com/in/stevenouri/ \n",
      "c. Open AI’s GPT \n",
      "d. ULMFit \n",
      " \n",
      "Answer : c) ULMFit has an LSTM based Language modeling architecture. This got replaced into \n",
      "Transformer architecture with Open AI’s GPT \n",
      " \n",
      "Q28. Which of the following architecture can be trained faster and needs less \n",
      "amount of training data \n",
      "a. LSTM based Language Modelling \n",
      "b. Transformer architecture \n",
      " \n",
      "Answer :  b) Transformer architectures were supported from GPT onwards and were faster to \n",
      "train and needed less amount of data for training too. \n",
      " \n",
      "Q29. Same word can have multiple word embeddings possible with ____________? \n",
      "a. GloVe \n",
      "b. Word2Vec \n",
      "c. ELMo \n",
      "d. nltk \n",
      " \n",
      "Answer : c) EMLo word embeddings supports same word with multiple embeddings, this helps \n",
      "in using the same word in a different context and thus captures the context than just meaning of \n",
      "the word unlike in GloVe and Word2Vec. Nltk is not a word embedding. \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Q30 For a given token, its input representation is the sum of embedding from the \n",
      "token, segment and position embedding \n",
      "a. ELMo \n",
      "b. GPT \n",
      "c. BERT \n",
      "d. ULMFit \n",
      " \n",
      "Answer :  c) BERT uses token, segment and position embedding. \n",
      " \n",
      " \n",
      "Q31. Trains two independent LSTM language model left to right and right to left and \n",
      "shallowly concatenates them \n",
      "a. GPT \n",
      "b. BERT \n",
      "c. ULMFit \n",
      "d. ELMo \n",
      "\n",
      "\n",
      "600. Steve Nouri                          https://www.linkedin.com/in/stevenouri/ \n",
      " \n",
      "Answer : d) ELMo tries to train two independent LSTM language models (left to right and right to \n",
      "left) and concatenates the results to produce word embedding. \n",
      " \n",
      "Q32. Uses unidirectional language model for producing word embedding \n",
      "a. BERT \n",
      "b. GPT \n",
      "c. ELMo \n",
      "d. Word2Vec \n",
      " \n",
      "Answer : b) GPT is a unidirectional model and word embedding are produced by training on \n",
      "information flow from left to right. ELMo is bidirectional but shallow. Word2Vec provides simple \n",
      "word embedding. \n",
      " \n",
      "Q33. In this architecture, the relationship between all words in a sentence is \n",
      "modelled irrespective of their position. Which architecture is this? \n",
      "a. OpenAI GPT \n",
      "b. ELMo \n",
      "c. BERT \n",
      "d. ULMFit \n",
      " \n",
      "Answer : c)BERT Transformer architecture models the relationship between each word and all \n",
      "other words in the sentence to generate attention scores. These attention scores are later used \n",
      "as weights for a weighted average of all words’ representations which is fed into a fully-connected \n",
      "network to generate a new representation. \n",
      " \n",
      "Q34. List 10 use cases to be solved using NLP techniques? \n",
      "● Sentiment Analysis \n",
      "● Language Translation (English to German, Chinese to English, etc..) \n",
      "● Document Summarization \n",
      "● Question Answering \n",
      "● Sentence Completion \n",
      "● Attribute extraction (Key information extraction from the documents) \n",
      "● Chatbot interactions \n",
      "● Topic classification \n",
      "● Intent extraction \n",
      "● Grammar or Sentence correction \n",
      "● Image captioning \n",
      "● Document Ranking \n",
      "● Natural Language inference \n",
      " \n",
      "Q35. Transformer model pays attention to the most important word in Sentence \n",
      "a. True \n",
      "b. False \n",
      "\n",
      "\n",
      "601. Steve Nouri                          https://www.linkedin.com/in/stevenouri/ \n",
      "Answer : a) Attention mechanisms in the Transformer model are used to model the relationship \n",
      "between all words and also provide weights to the most important word. \n",
      " \n",
      "Q36. Which NLP model gives the best accuracy amongst the following? \n",
      "a. BERT \n",
      "b. XLNET \n",
      "c. GPT-2 \n",
      "d. ELMo \n",
      " \n",
      "Answer : b) XLNET has given best accuracy amongst all the models. It has outperformed BERT \n",
      "on 20 tasks and achieves state of art results on 18 tasks including sentiment analysis, question \n",
      "answering, natural language inference, etc. \n",
      " \n",
      "Q37. Permutation Language models is a feature of \n",
      "a. BERT \n",
      "b. EMMo \n",
      "c. GPT \n",
      "d. XLNET \n",
      " \n",
      "Answer : d) XLNET provides permutation-based language modelling and is a key difference from \n",
      "BERT. In permutation language modeling, tokens are predicted in a random manner and not \n",
      "sequential. The order of prediction is not necessarily left to right and can be right to left. The \n",
      "original order of words is not changed but a prediction can be random.  \n",
      "The conceptual difference between BERT and XLNET can be seen from the following diagram. \n",
      " \n",
      "Q38. Transformer XL uses relative positional embedding \n",
      "a. True \n",
      "b. False \n",
      "a) Instead of embedding having to represent the absolute position of a word, Transformer XL uses \n",
      "an embedding to encode the relative distance between the words. This embedding is used to \n",
      "compute the attention score between any 2 words that could be separated by n words before or \n",
      "after. \n",
      " \n",
      "Q39. What is Naive Bayes algorithm, When we can use this algorithm in NLP? \n",
      "Naive Bayes algorithm is a collection of classifiers which works on the principles of the Bayes’ \n",
      "theorem. This series of NLP model forms a family of algorithms that can be used for a wide range \n",
      "of classification tasks including sentiment prediction, filtering of spam, classifying documents and \n",
      "more. \n",
      "Naive Bayes algorithm converges faster and requires less training data. Compared to other \n",
      "discriminative models like logistic regression, Naive Bayes model it takes lesser time to train. This \n",
      "algorithm is perfect for use while working with multiple classes and text classification where the \n",
      "data is dynamic and changes frequently. \n",
      " \n",
      "Q40. Explain Dependency Parsing in NLP? \n",
      "\n",
      "\n",
      "602. Steve Nouri                          https://www.linkedin.com/in/stevenouri/ \n",
      "Dependency Parsing, also known as Syntactic parsing in NLP is a process of assigning syntactic \n",
      "structure to a sentence and identifying its dependency parses. This process is crucial to \n",
      "understand the correlations between the “head” words in the syntactic structure. \n",
      "The process of dependency parsing can be a little complex considering how any sentence can \n",
      "have more than one dependency parses. Multiple parse trees are known as ambiguities. \n",
      "Dependency parsing needs to resolve these ambiguities in order to effectively assign a syntactic \n",
      "structure to a sentence. \n",
      "Dependency parsing can be used in the semantic analysis of a sentence apart from the syntactic \n",
      "structuring. \n",
      " \n",
      "Q41. What is text Summarization? \n",
      "Text summarization is the process of shortening a long piece of text with its meaning and effect \n",
      "intact. Text summarization intends to create a summary of any given piece of text and outlines \n",
      "the main points of the document. This technique has improved in recent times and is capable of \n",
      "summarizing volumes of text successfully. \n",
      "Text summarization has proved to a blessing since machines can summarise large volumes of \n",
      "text in no time which would otherwise be really time-consuming. There are two types of text \n",
      "summarization: \n",
      "● Extraction-based summarization \n",
      "● Abstraction-based summarization \n",
      " \n",
      " \n",
      "Q42. What is NLTK? How is it different from Spacy? \n",
      "NLTK or Natural Language Toolkit is a series of libraries and programs that are used for symbolic \n",
      "and statistical natural language processing. This toolkit contains some of the most powerful \n",
      "libraries that can work on different ML techniques to break down and understand human \n",
      "language. NLTK is used for Lemmatization, Punctuation, Character count, Tokenization, and \n",
      "Stemming. The difference between NLTK and Spacey are as follows: \n",
      "● While NLTK has a collection of programs to choose from, Spacey contains only the best-\n",
      "suited algorithm for a problem in its toolkit \n",
      "● NLTK supports a wider range of languages compared to Spacey (Spacey supports only 7 \n",
      "languages) \n",
      "● While Spacey has an object-oriented library, NLTK has a string processing library \n",
      "● Spacey can support word vectors while NLTK cannot \n",
      " \n",
      "Q43. What is information extraction? \n",
      "Information extraction in the context of Natural Language Processing refers to the technique of \n",
      "extracting structured information automatically from unstructured sources to ascribe meaning to \n",
      "it. This can include extracting information regarding attributes of entities, relationship between \n",
      "different entities and more. The various models of information extraction includes: \n",
      "● Tagger Module \n",
      "● Relation Extraction Module \n",
      "● Fact Extraction Module \n",
      "● Entity Extraction Module \n",
      "\n",
      "\n",
      "603. Steve Nouri                          https://www.linkedin.com/in/stevenouri/ \n",
      "● Sentiment Analysis Module \n",
      "● Network Graph Module \n",
      "● Document Classification & Language Modeling Module \n",
      " \n",
      "Q44. What is Bag of Words? \n",
      "Bag of Words is a commonly used model that depends on word frequencies or occurrences to \n",
      "train a classifier. This model creates an occurrence matrix for documents or sentences \n",
      "irrespective of its grammatical structure or word order.  \n",
      " \n",
      "Q45. What is Pragmatic Ambiguity in NLP? \n",
      "Pragmatic ambiguity refers to those words which have more than one meaning and their use in \n",
      "any sentence can depend entirely on the context. Pragmatic ambiguity can result in multiple \n",
      "interpretations of the same sentence. More often than not, we come across sentences which have \n",
      "words with multiple meanings, making the sentence open to interpretation. This multiple \n",
      "interpretation causes ambiguity and is known as Pragmatic ambiguity in NLP. \n",
      " \n",
      "Q46. What is a Masked Language Model? \n",
      "Masked language models help learners to understand deep representations in downstream tasks \n",
      "by taking an output from the corrupt input. This model is often used to predict the words to be \n",
      "used in a sentence. \n",
      "Q48. What are the best NLP Tools? \n",
      "Some of the best NLP tools from open sources are: \n",
      "● SpaCy \n",
      "● TextBlob \n",
      "● Textacy \n",
      "● Natural language Toolkit \n",
      "● Retext \n",
      "● NLP.js \n",
      "● Stanford NLP \n",
      "● CogcompNLP \n",
      " \n",
      "Q49. What is POS tagging? \n",
      "Parts of speech tagging better known as POS tagging refers to the process of identifying specific \n",
      "words in a document and group them as part of speech, based on its context. POS tagging is also \n",
      "known as grammatical tagging since it involves understanding grammatical structures and \n",
      "identifying the respective component. \n",
      "POS tagging is a complicated process since the same word can be different parts of speech \n",
      "depending on the context. The same generic process used for word mapping is quite ineffective \n",
      "for POS tagging because of the same reason. \n",
      " \n",
      "Q50. What is NES? \n",
      "Name entity recognition is more commonly known as NER is the process of identifying specific \n",
      "entities in a text document which are more informative and have a unique context. These often \n",
      "denote places, people, organisations, and more. Even though it seems like these entities are \n",
      "\n",
      "\n",
      "604. Steve Nouri                          https://www.linkedin.com/in/stevenouri/ \n",
      "proper nouns, the NER process is far from identifying just the nouns. In fact, NER involves entity \n",
      "chunking or extraction wherein entities are segmented to categorise them under different \n",
      "predefined classes. This step further helps in extracting information. \n",
      " \n",
      "Q51 Explain the Masked Language Model? \n",
      "Masked language modelling is the process in which the output is taken from the corrupted input. \n",
      "This model helps the learners to master the deep representations in downstream tasks. You can \n",
      "predict a word from the other words of the sentence using this model. \n",
      " \n",
      "Q52 What is pragmatic analysis in NLP? \n",
      "Pragmatic Analysis: It deals with outside word knowledge, which means knowledge that is \n",
      "external to the documents and/or queries. Pragmatics analysis that focuses on what was \n",
      "described is reinterpreted by what it actually meant, deriving the various aspects of language that \n",
      "require real-world knowledge. \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Q53 What is perplexity in NLP? \n",
      "The word \"perplexed\" means \"puzzled\" or \"confused\", thus Perplexity in general means the \n",
      "inability to tackle something complicated and a problem that is not specified. Therefore, Perplexity \n",
      "in NLP is a way to determine the extent of uncertainty in predicting some text. \n",
      "In NLP, perplexity is a way of evaluating language models. Perplexity can be high and low; Low \n",
      "perplexity is ethical because the inability to deal with any complicated problem is less while high \n",
      "perplexity is terrible because the failure to deal with a complicated is high. \n",
      " \n",
      "Q54 What is ngram in NLP? \n",
      "N-gram in NLP is simply a sequence of n words, and we also conclude the sentences which \n",
      "appeared more frequently, for example, let us consider the progression of these three words: \n",
      "● New York (2 gram) \n",
      "● The Golden Compass (3 gram) \n",
      "● She was there in the hotel (4 gram) \n",
      "Now from the above sequence, we can easily conclude that sentence (a) appeared more \n",
      "frequently than the other two sentences, and the last sentence(c) is not seen that often. Now if \n",
      "we assign probability in the occurrence of an n-gram, then it will be advantageous. It would help \n",
      "in making next-word predictions and in spelling error corrections. \n",
      " \n",
      "Q55 Explain differences between AI, Machine Learning and NLP \n",
      "\n",
      "\n",
      "605. Steve Nouri                          https://www.linkedin.com/in/stevenouri/ \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Q56 Why self-attention is awesome? \n",
      "“In terms of computational complexity, self-attention layers are faster than recurrent layers when \n",
      "the sequence length n is smaller than the representation dimensionality d, which is most often the \n",
      "case with sentence representations used by state-of-the-art models in machine translations, such \n",
      "as word-piece and byte-pair representations.” — from Attention is all you need \n",
      " \n",
      "Q57 What are stop words? \n",
      "Stop words are said to be useless data for a search engine. Words such as articles, prepositions, \n",
      "etc. are considered as stop words. There are stop words such as was, were, is, am, the, a, an, \n",
      "how, why, and many more. In Natural Language Processing, we eliminate the stop words to \n",
      "understand and analyze the meaning of a sentence. The removal of stop words is one of the most \n",
      "important tasks for search engines. Engineers design the algorithms of search engines in such a \n",
      "way that they ignore the use of stop words. This helps show the relevant search result for a query. \n",
      " \n",
      "Q58 What is Latent Semantic Indexing (LSI)? \n",
      "\n",
      "\n",
      "606. Steve Nouri                          https://www.linkedin.com/in/stevenouri/ \n",
      "Latent semantic indexing is a mathematical technique used to improve the accuracy of the \n",
      "information retrieval process. The design of LSI algorithms allows machines to detect the hidden \n",
      "(latent) correlation between semantics (words). To enhance information understanding, machines \n",
      "generate various concepts that associate with the words of a sentence. \n",
      "The technique used for information understanding is called singular value decomposition. It is \n",
      "generally used to handle static and unstructured data. The matrix obtained for singular value \n",
      "decomposition contains rows for words and columns for documents. This method best suits to \n",
      "identify components and group them according to their types. \n",
      "The main principle behind LSI is that words carry a similar meaning when used in a similar context. \n",
      "Computational LSI models are slow in comparison to other models. However, they are good at \n",
      "contextual awareness that helps improve the analysis and understanding of a text or a document. \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Q60 What are Regular Expressions? \n",
      "A regular expression is used to match and tag words. It consists of a series of characters for \n",
      "matching strings. \n",
      "Suppose, if A and B are regular expressions, then the following are true for them: \n",
      "● If {ɛ} is a regular language, then ɛ is a regular expression for it. \n",
      "● If A and B are regular expressions, then A + B is also a regular expression within the \n",
      "language {A, B}. \n",
      "● If A and B are regular expressions, then the concatenation of A and B (A.B) is a regular \n",
      "expression. \n",
      "● If A is a regular expression, then A* (A occurring multiple times) is also a regular \n",
      "expression. \n",
      " \n",
      "Q61 What are unigrams, bigrams, trigrams, and n-grams in NLP? \n",
      "When we parse a sentence one word at a time, then it is called a unigram. The sentence parsed \n",
      "two words at a time is a bigram. \n",
      "When the sentence is parsed three words at a time, then it is a trigram. Similarly, n-gram refers \n",
      "to the parsing of n words at a time. \n",
      "Example: To understand unigrams, bigrams, and trigrams, you can refer to the below diagram: \n",
      " \n",
      "Q62 What are the steps involved in solving an NLP problem? \n",
      "Below are the steps involved in solving an NLP problem: \n",
      "1. Gather the text from the available dataset or by web scraping \n",
      "\n",
      "\n",
      "607. Steve Nouri                          https://www.linkedin.com/in/stevenouri/ \n",
      "2. Apply stemming and lemmatization for text cleaning \n",
      "3. Apply feature engineering techniques \n",
      "4. Embed using word2vec \n",
      "5. Train the built model using neural networks or other Machine Learning techniques \n",
      "6. Evaluate the model’s performance \n",
      "7. Make appropriate changes in the model \n",
      "8. Deploy the model \n",
      " \n",
      "Q63. There have some various common elements of natural language processing. \n",
      "Those elements are very important for understanding NLP properly, can you please \n",
      "explain the same in details with an example? \n",
      "Answer: \n",
      "There have a lot of components normally using by natural language processing (NLP). Some of \n",
      "the major components are explained below: \n",
      "● Extraction of Entity: It actually identifying and extracting some critical data from the \n",
      "available information which help to segmentation of provided sentence on identifying each \n",
      "entity. It can help in identifying one human that it’s fictional or real, same kind of reality \n",
      "identification for any organization, events or any geographic location etc. \n",
      "● The analysis in a syntactic way: it mainly helps for maintaining ordering properly of the \n",
      "available words. \n",
      "Q64 In the case of processing natural language, we normally mentioned one \n",
      "common terminology NLP and binding every language with the same terminology \n",
      "properly. Please explain in details about this NLP terminology with an example? \n",
      "Answer: \n",
      "This is the basic NLP Interview Questions asked in an interview. There have some several factors \n",
      "available in case of explaining natural language processing. Some of the key factors are given \n",
      "below: \n",
      "● Vectors and Weights: Google Word vectors, length of TF-IDF, varieties documents, word \n",
      "vectors, TF-IDF. \n",
      "● Structure of Text: Named Entities, tagging of part of speech, identifying the head of the \n",
      "sentence. \n",
      "● Analysis of sentiment: Know about the features of sentiment, entities available for the \n",
      "sentiment, sentiment common dictionary. \n",
      "● Classification of Text: Learning supervising, set off a train, set of validation in Dev, Set of \n",
      "define test, a feature of the individual text, LDA. \n",
      "● Reading of Machine Language: Extraction of the possible entity, linking with an individual \n",
      "entity, DBpedia, some libraries like Pikes or FRED. \n",
      " \n",
      "Q65 Explain briefly about word2vec \n",
      "Word2Vec  embeds words in a lower-dimensional vector space using a shallow neural network. \n",
      "The result is a set of word-vectors where vectors close together in vector space have similar \n",
      "meanings based on context, and word-vectors distant to each other have differing meanings. For \n",
      "example, apple and orange would be close together and apple and gravity would be relatively far. \n",
      "\n",
      "\n",
      "608. Steve Nouri                          https://www.linkedin.com/in/stevenouri/ \n",
      "There are two versions of this model based on skip-grams (SG) and continuous-bag-of-words \n",
      "(CBOW). \n",
      " \n",
      "Q66 What are the metrics used to test an NLP model? \n",
      "Accuracy, Precision, Recall and F1. Accuracy is the usual ratio of the prediction to the desired \n",
      "output. But going just be accuracy is naive considering the complexities involved. \n",
      " \n",
      "Q67 What are some ways we can preprocess text input? \n",
      "Here are several preprocessing steps that are commonly used for NLP tasks: \n",
      "● case normalization: we can convert all input to the same case (lowercase or uppercase) \n",
      "as a way of reducing our text to a more canonical form \n",
      "● punctuation/stop word/white space/special characters removal: if we don’t think these \n",
      "words or characters are relevant, we can remove them to reduce the feature space \n",
      "● lemmatizing/stemming: we can also reduce words to their inflectional forms (i.e. walks → \n",
      "walk) to further trim our vocabulary \n",
      "● generalizing irrelevant information: we can replace all numbers with a <NUMBER> token \n",
      "or all names with a <NAME> token \n",
      " \n",
      " \n",
      "Q68 How does the encoder-decoder structure work for language modelling? \n",
      "The encoder-decoder structure is a deep learning model architecture responsible for several state \n",
      "of the art solutions, including Machine Translation. \n",
      "The input sequence is passed to the encoder where it is transformed to a fixed-dimensional vector \n",
      "representation using a neural network. The transformed input is then decoded using another \n",
      "neural network. Then, these outputs undergo another transformation and a softmax layer. The \n",
      "final output is a vector of probabilities over the vocabularies. Meaningful information is extracted \n",
      "based on these probabilities. \n",
      " \n",
      "Q69 What are attention mechanisms and why do we use them? \n",
      "This was a followup to the encoder-decoder question. Only the output from the last time step is \n",
      "passed to the decoder, resulting in a loss of information learned at previous time steps. This \n",
      "information loss is compounded for longer text sequences with more time steps. \n",
      "Attention mechanisms are a function of the hidden weights at each time step. When we use \n",
      "attention in encoder-decoder networks, the fixed-dimensional vector passed to the decoder \n",
      "becomes a function of all vectors outputted in the intermediary steps. \n",
      "Two commonly used attention mechanisms are additive attention and multiplicative attention. As \n",
      "the names suggest, additive attention is a weighted sum while multiplicative attention is a \n",
      "weighted multiplier of the hidden weights. During the training process, the model also learns \n",
      "weights for the attention mechanisms to recognize the relative importance of each time step. \n",
      " \n",
      "Q70 How would you implement an NLP system as a service, and what are some \n",
      "pitfalls you might face in production? \n",
      "This is less of a NLP question than a question for productionizing machine learning models. There \n",
      "are however certain intricacies to NLP models. \n",
      "\n",
      "\n",
      "609. Steve Nouri                          https://www.linkedin.com/in/stevenouri/ \n",
      "Without diving too much into the productionization aspect, an ideal Machine Learning service will \n",
      "have: \n",
      "● endpoint(s) that other business systems can use to make inference \n",
      "● a feedback mechanism for validating model predictions \n",
      "● a database to store predictions and ground truths from the feedback \n",
      "● a workflow orchestrator which will (upon some signal) re-train and load the new model for \n",
      "serving based on the records from the database + any prior training data \n",
      "● some form of model version control to facilitate rollbacks in case of bad deployments \n",
      "● post-production accuracy and error monitoring \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Q71 How can we handle misspellings for text input? \n",
      "By using word embeddings trained over a large corpus (for instance, an extensive web scrape of \n",
      "billions of words), the model vocabulary would include common misspellings by design. The \n",
      "model can then learn the relationship between misspelled and correctly spelled words to \n",
      "recognize their semantic similarity. \n",
      "We can also preprocess the input to prevent misspellings. Terms not found in the model \n",
      "vocabulary can be mapped to the “closest” vocabulary term using: \n",
      "● edit distance between strings \n",
      "● phonetic distance between word pronunciations \n",
      "● keyword distance to catch common typos \n",
      " \n",
      "Q72 Which of the following models can perform tweet classification with regards \n",
      "to context mentioned above? \n",
      "A) Naive Bayes \n",
      "B) SVM \n",
      "C) None of the above \n",
      "Solution: (C) \n",
      "Since, you are given only the data of tweets and no other information, which means there is no \n",
      "target variable present. One cannot train a supervised learning model, both svm and naive bayes \n",
      "are supervised learning techniques. \n",
      " \n",
      "Q73 You have created a document term matrix of the data, treating every tweet as \n",
      "one document. Which of the following is correct, in regards to document term \n",
      "matrix? \n",
      "1. Removal of stopwords from the data will affect the dimensionality of data \n",
      "2. Normalization of words in the data will reduce the dimensionality of data \n",
      "\n",
      "\n",
      "610. Steve Nouri                          https://www.linkedin.com/in/stevenouri/ \n",
      "3. Converting all the words in lowercase will not affect the dimensionality of the data \n",
      "A) Only 1 \n",
      "B) Only 2 \n",
      "C) Only 3 \n",
      "D) 1 and 2 \n",
      "E) 2 and 3 \n",
      "F) 1, 2 and 3 \n",
      "Solution: (D) \n",
      "Choices A and B are correct because stopword removal will decrease the number of features in \n",
      "the matrix, normalization of words will also reduce redundant features, and, converting all words \n",
      "to lowercase will also decrease the dimensionality. \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Q74 Which of the following features can be used for accuracy improvement of a \n",
      "classification model? \n",
      "A) Frequency count of terms \n",
      "B) Vector Notation of sentence \n",
      "C) Part of Speech Tag \n",
      "D) Dependency Grammar \n",
      "E) All of these \n",
      "Solution: (E) \n",
      "All of the techniques can be used for the purpose of engineering features in a model. \n",
      "  \n",
      "Q75 What percentage of the total statements are correct with regards to Topic \n",
      "Modeling? \n",
      "1. It is a supervised learning technique \n",
      "2. LDA (Linear Discriminant Analysis) can be used to perform topic modeling \n",
      "3. Selection of number of topics in a model does not depend on the size of data \n",
      "4. Number of topic terms are directly proportional to size of the data \n",
      "A) 0 \n",
      "B) 25 \n",
      "C) 50 \n",
      "D) 75 \n",
      "E) 100 \n",
      "Solution: (A) \n",
      "LDA is unsupervised learning model, LDA is latent Dirichlet allocation, not Linear discriminant \n",
      "analysis. Selection of the number of topics is directly proportional to the size of the data, while \n",
      "number of topic terms is not directly proportional to the size of the data. Hence none of the \n",
      "statements are correct. \n",
      "  \n",
      "\n",
      "\n",
      "611. Steve Nouri                          https://www.linkedin.com/in/stevenouri/ \n",
      "Q76 In Latent Dirichlet Allocation model for text classification purposes, what does \n",
      "alpha and beta hyperparameter represent- \n",
      "A) Alpha: number of topics within documents, beta: number of terms within topics False \n",
      "B) Alpha: density of terms generated within topics, beta: density of topics generated within terms \n",
      "False \n",
      "C) Alpha: number of topics within documents, beta: number of terms within topics False \n",
      "D) Alpha: density of topics generated within documents, beta: density of terms generated within \n",
      "topics True \n",
      "Solution: (D) \n",
      "Option D is correct \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Q77 What is the problem with ReLu? \n",
      "● Exploding gradient(Solved by gradient clipping) \n",
      "● Dying ReLu — No learning if the activation is 0 (Solved by parametric relu) \n",
      "● Mean and variance of activations is not 0 and 1.(Partially solved by subtracting around 0.5 \n",
      "from activation. Better explained in fastai videos) \n",
      " \n",
      "Q78 What is the difference between learning latent features using SVD and getting \n",
      "embedding vectors using deep network? \n",
      "SVD uses linear combination of inputs while a neural network uses nonlinear combination. \n",
      " \n",
      "Q79 What is the information in the hidden and cell state of LSTM? \n",
      "Hidden stores all the information till that time step and cell state stores particular information that \n",
      "might be needed in the future time step. \n",
      "Number of parameters in an LSTM model with bias \n",
      "4(𝑚h+h²+h) where 𝑚 is input vectors size and h is output vectors size a.k.a. hidden \n",
      "The point to see here is that mh dictates the model size as m>>h. Hence it's important to have a \n",
      "small vocab. \n",
      "Time complexity of LSTM \n",
      "seq_length*hidden² \n",
      "Time complexity of transfomer \n",
      "seq_length²*hidden \n",
      "When hidden size is more than the seq_length(which is normally the case), transfomer is faster \n",
      "than LSTM. \n",
      " \n",
      "Q80 When is self-attention not faster than recurrent layers? \n",
      "When the sequence length is greater than the representation dimensions. This is rare. \n",
      " \n",
      "Q81 What is the benefit of learning rate warm-up? \n",
      "\n",
      "\n",
      "612. Steve Nouri                          https://www.linkedin.com/in/stevenouri/ \n",
      "Learning rate warm-up is a learning rate schedule where you have low (or lower) learning rate at \n",
      "the beginning of training to avoid divergence due to unreliable gradients at the beginning. As the \n",
      "model becomes more stable, the learning rate would increase to speed up convergence. \n",
      " \n",
      "Q82 What’s the difference between hard and soft parameter sharing in multi-task \n",
      "learning? \n",
      "Hard sharing is where we train for all the task at the same time and update our weights using all \n",
      "the losses whereas soft sharing is where we train for one task at a time. \n",
      " \n",
      "Q83 What’s the difference between BatchNorm and LayerNorm? \n",
      "BatchNorm computes the mean and variance at each layer for every minibatch whereas \n",
      "LayerNorm computes the mean and variance for every sample for each layer independently. \n",
      "Batch normalisation allows you to set higher learning rates, increasing speed of training as it \n",
      "reduces the unstability of initial starting weights. \n",
      "Q84 Difference between BatchNorm and LayerNorm? \n",
      "BatchNorm — Compute the mean and var at each layer for every minibatch \n",
      "LayerNorm — Compute the mean and var for every single sample for each layer independently \n",
      " \n",
      "Q85 Why does the transformer block have LayerNorm instead of BatchNorm? \n",
      "Looking at the advantages of LayerNorm, it is robust to batch size and works better as it works at \n",
      "the sample level and not batch level. \n",
      " \n",
      "Q86 What changes would you make to your deep learning code if you knew there \n",
      "are errors in your training data? \n",
      "We can do label smoothening where the smoothening value is based on % error. If any particular \n",
      "class has known error, we can also use class weights to modify the loss. \n",
      " \n",
      "Q87 What are the tricks used in ULMFiT? (Not a great questions but checks the \n",
      "awareness) \n",
      "● LM tuning with task text \n",
      "● Weight dropout \n",
      "● Discriminative learning rates for layers \n",
      "● Gradual unfreezing of layers \n",
      "● Slanted triangular learning rate schedule \n",
      "This can be followed up with a question on explaining how they help. \n",
      " \n",
      "Q88 Tell me a language model which doesn’t use dropout \n",
      "ALBERT v2 — This throws a light on the fact that a lot of assumptions we take for granted are \n",
      "not necessarily true. The regularisation effect of parameter sharing in ALBERT is so strong that \n",
      "dropouts are not needed. (ALBERT v1 had dropouts.) \n",
      " \n",
      "Q89 What are the differences between GPT and GPT-2? (From Lilian Weng) \n",
      "\n",
      "\n",
      "613. Steve Nouri                          https://www.linkedin.com/in/stevenouri/ \n",
      "● Layer normalization was moved to the input of each sub-block, similar to a residual unit of \n",
      "type “building block” (differently from the original type “bottleneck”, it has batch \n",
      "normalization applied before weight layers). \n",
      "● An additional layer normalization was added after the final self-attention block. \n",
      "● A modified initialization was constructed as a function of the model depth. \n",
      "● The weights of residual layers were initially scaled by a factor of 1/√n where n is the \n",
      "number of residual layers. \n",
      "● Use larger vocabulary size and context size. \n",
      " \n",
      "Q90 What are the differences between GPT and BERT? \n",
      " \n",
      "● GPT is not bidirectional and has no concept of masking \n",
      "● BERT adds next sentence prediction task in training and so it also has a segment \n",
      "embedding \n",
      "Q91 What are the differences between BERT and ALBERT v2? \n",
      "● Embedding matrix factorisation(helps in reducing no. of parameters) \n",
      "● No dropout \n",
      "● Parameter sharing(helps in reducing no. of parameters and regularisation) \n",
      "Q92 How does parameter sharing in ALBERT affect the training and inference time? \n",
      "No effect. Parameter sharing just decreases the number of parameters. \n",
      "Q93 How would you reduce the inference time of a trained NN model? \n",
      "● Serve on GPU/TPU/FPGA \n",
      "● 16 bit quantisation and served on GPU with fp16 support \n",
      "● Pruning to reduce parameters \n",
      "● Knowledge distillation (To a smaller transformer model or simple neural network) \n",
      "● Hierarchical softmax/Adaptive softmax \n",
      "● You can also cache results as explained here. \n",
      "Q94 Would you use BPE with classical models? \n",
      "Of course! BPE is a smart tokeniser and it can help us get a smaller vocabulary which can help \n",
      "us find a model with less parameters. \n",
      "Q95 How would you make an arxiv papers search engine? (I was asked — How \n",
      "would you make a plagiarism detector?) \n",
      " \n",
      "Get top k results with TF-IDF similarity and then rank results with \n",
      "● semantic encoding + cosine similarity \n",
      "● a model trained for ranking \n",
      " \n",
      "\n",
      "\n",
      "614. Steve Nouri                          https://www.linkedin.com/in/stevenouri/ \n",
      "Q96 Get top k results with TF-IDF similarity and then rank results with \n",
      "● semantic encoding + cosine similarity \n",
      "● a model trained for ranking \n",
      "Q97 How would you make a sentiment classifier? \n",
      "This is a trick question. The interviewee can say all things such as using transfer learning and \n",
      "latest models but they need to talk about having a neutral class too otherwise you can have really \n",
      "good accuracy/f1 and still, the model will classify everything into positive or negative. \n",
      "The truth is that a lot of news is neutral and so the training needs to have this class. The \n",
      "interviewee should also talk about how he will create a dataset and his training strategies like the \n",
      "selection of language model, language model fine-tuning and using various datasets for multi-\n",
      "task learning. \n",
      "Q98 What is the difference between regular expression and regular grammar? \n",
      "A regular expression is the representation of natural language in the form of mathematical \n",
      "expressions containing a character sequence. On the other hand, regular grammar is the \n",
      "generator of natural language, defining a set of defined rules and syntax which the strings in the \n",
      "natural language must follow. \n",
      " \n",
      "Q99 Why should we use Batch Normalization? \n",
      "Once the interviewer has asked you about the fundamentals of deep learning architectures, they \n",
      "would move on to the key topic of improving your deep learning model’s performance. \n",
      "Batch Normalization is one of the techniques used for reducing the training time of our deep \n",
      "learning algorithm. Just like normalizing our input helps improve our logistic regression model, we \n",
      "can normalize the activations of the hidden layers in our deep learning model as well: \n",
      " \n",
      "Q100 How is backpropagation different in RNN compared to ANN? \n",
      "In Recurrent Neural Networks, we have an additional loop at each node: \n",
      "This loop essentially includes a time component into the network as well. This helps in capturing \n",
      "sequential information from the data, which could not be possible in a generic artificial neural \n",
      "network. \n",
      "This is why the backpropagation in RNN is called Backpropagation through Time, as in \n",
      "backpropagation at each time step. \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\n",
      "\n",
      "615. Top 100 Python Interview Questions You Must Prepare In 2019\n",
      "Last updated on Aug 14,2019\n",
      "379.7K Views\n",
      "Python CertiÕcation is the most sought-after skill in programming domain. In this Python Interview Questions blog, I will\n",
      "introduce you to the most frequently asked questions in Python interviews. Our Python Interview Questions is the one-stop\n",
      "resource from where you can boost your interview preparation. We have 100+ questions on Python Programming basics which\n",
      "will help you with diàerent expertise levels to reap the maximum beneÕt from our blog.\n",
      "Edureka 2019 Tech Career Guide is out! Hottest job roles, precise learning paths, industry outlook & more in the\n",
      "guide. Download now.\n",
      "Let us start by taking a look at some of the most frequently asked Python interview questions,\n",
      "Q1. What is the diàerence between list and tuples in Python?\n",
      "Q2. What are the key features of Python?\n",
      "Q3. What type of language is python?\n",
      "Q4. How is Python an interpreted language?\n",
      "Q5. What is pep 8?\n",
      "Q6. How is memory managed in Python?\n",
      "Q7. What is name space in Python?\n",
      "Q8. What is PYTHON PATH?\n",
      "Q9. What are python modules?\n",
      "Q10. What are local variables and global variables in Python?\n",
      "We have compiled a list of top Python interview questions which are classiÕed into 7 sections, namely:\n",
      "Basic Interview Questions\n",
      "OOPS Interview Questions\n",
      "Basic Python Programs\n",
      "Python Libraries Interview Questions\n",
      "Web Scraping Interview Questions\n",
      "Data Analysis Interview Questions\n",
      "Multiple Choice Questions (MCQ)\n",
      "Before moving ahead, you may go through the recording of Python Interview Questions where our instructor has shared his\n",
      "experience and expertise that will help you to crack any Python Interview:\n",
      "Python Interview Questions And Answers | Python Training | Edureka\n",
      "Aayushi Johari\n",
      "A technophile who likes writing about diàerent technologies and spreading knowledge.\n",
      "myMock Interview Service for Real Tech Jobs\n",
      "\n",
      "Mock interview in latest tech domains i.e JAVA, AI, DEVOPS,etc\n",
      "Get interviewed by leading tech experts\n",
      "Real time assement report and video recording\n",
      "TAKE PYTHON MOCK INTERVIEW\n",
      "\n",
      "\n",
      "Follow me on LinkedIn for more:\n",
      "              Steve Nouri\n",
      "https://www.linkedin.com/in/stevenouri/\n",
      "\n",
      "\n",
      "616. If you have other doubts regarding Python, feel free to post them in our QnA Forum. Our expert team will get back to you at the\n",
      "earliest.\n",
      "Basic Python Interview Questions\n",
      "Q1. What is the diàerence between list and tuples in Python?\n",
      "LIST vs TUPLES\n",
      "LIST\n",
      "TUPLES\n",
      "Lists are mutable i.e they can be edited.\n",
      "Tuples are immutable (tuples are lists which can’t be edited).\n",
      "Lists are slower than tuples.\n",
      "Tuples are faster than list.\n",
      "Syntax: list_1 = [10, ‘Chelsea’, 20]\n",
      "Syntax: tup_1 = (10, ‘Chelsea’ , 20)\n",
      "Q2. What are the key features of Python?\n",
      "Python is an interpreted language. That means that, unlike languages like C and its variants, Python does not need to be\n",
      "compiled before it is run. Other interpreted languages include PHP and Ruby.\n",
      "Python is dynamically typed, this means that you don’t need to state the types of variables when you declare them or\n",
      "anything like that. You can do things like x=111 and then x=\"I'm a string\" without error\n",
      "Python is well suited to object orientated programming in that it allows the deÕnition of classes along with composition\n",
      "and inheritance. Python does not have access speciÕers (like C++’s public, private).\n",
      "In Python, functions are Õrst-class objects. This means that they can be assigned to variables, returned from other\n",
      "functions and passed into functions. Classes are also Õrst class objects\n",
      "Writing Python code is quick but running it is often slower than compiled languages. Fortunately ，Python allows the\n",
      "inclusion of C based extensions so bottlenecks can be optimized away and often are. The numpy package is a good\n",
      "example of this, it’s really quite quick because a lot of the number crunching it does isn’t actually done by Python\n",
      "Python Õnds use in many spheres – web applications, automation, scientiÕc modeling, big data applications and many\n",
      "more. It’s also often used as “glue” code to get other languages and components to play nice.\n",
      "Q3. What type of language is python? Programming or scripting?\n",
      "Ans: Python is capable of scripting, but in general sense, it is considered as a general-purpose programming language. To know\n",
      "more about Scripting, you can refer to the Python Scripting Tutorial.\n",
      "Q4.How is Python an interpreted language?\n",
      "Ans: An interpreted language is any programming language which is not in machine level code before runtime. Therefore,\n",
      "Python is an interpreted language.\n",
      "Q5.What is pep 8?\n",
      "Ans: PEP stands for Python Enhancement Proposal. It is a set of rules that specify how to format Python code for maximum\n",
      "readability.\n",
      "Q6. How is memory managed in Python?\n",
      "Ans: \n",
      "1. Memory management in python is managed by Python private heap space. All Python objects and data structures are\n",
      "located in a private heap. The programmer does not have access to this private heap. The python interpreter takes care of\n",
      "this instead.\n",
      "2. The allocation of heap space for Python objects is done by Python’s memory manager. The core API gives access to some\n",
      "tools for the programmer to code.\n",
      "3. Python also has an inbuilt garbage collector, which recycles all the unused memory and so that it can be made available to\n",
      "the heap space.\n",
      "Q7. What is namespace in Python?\n",
      "Ans: A namespace is a naming system used to make sure that names are unique to avoid naming conÖicts.\n",
      "Q8. What is PYTHONPATH?\n",
      "Python Interview Questions And Answers | Python Interview Preparati\n",
      "Python Interview Questions And Answers | Python Interview Preparati…\n",
      "\n",
      "\n",
      "\n",
      "617. Ans: It is an environment variable which is used when a module is imported. Whenever a module is imported, PYTHONPATH is\n",
      "also looked up to check for the presence of the imported modules in various directories. The interpreter uses it to determine\n",
      "which module to load.\n",
      "Q9. What are python modules? Name some commonly used built-in modules in Python?\n",
      "Ans: Python modules are Õles containing Python code. This code can either be functions classes or variables. A Python module is\n",
      "a .py Õle containing executable code.\n",
      "Some of the commonly used built-in modules are:\n",
      "os\n",
      "sys\n",
      "math\n",
      "random\n",
      "data time\n",
      "JSON\n",
      "Q10.What are local variables and global variables in Python?\n",
      "Global Variables:\n",
      "Variables declared outside a function or in global space are called global variables. These variables can be accessed by any\n",
      "function in the program.\n",
      "Local Variables:\n",
      "Any variable declared inside a function is known as a local variable. This variable is present in the local space and not in the\n",
      "global space.\n",
      "Example:\n",
      " \n",
      "a=2 \n",
      "def add(): \n",
      "b=3 \n",
      "c=a+b \n",
      "print(c) \n",
      "add() \n",
      " \n",
      "Output: 5\n",
      "When you try to access the local variable outside the function add(), it will throw an error.\n",
      "Q11. Is python case sensitive?\n",
      "Ans: Yes. Python is a case sensitive language.\n",
      "Q12.What is type conversion in Python?\n",
      "Ans: Type conversion refers to the conversion of one data type iinto another.\n",
      "int() – converts any data type into integer type\n",
      "Öoat() – converts any data type into Öoat type\n",
      "ord() – converts characters into integer\n",
      "hex() – converts integers to hexadecimal\n",
      "oct() – converts integer to octal\n",
      "tuple() – This function is used to convert to a tuple.\n",
      "set() – This function returns the type after converting to set.\n",
      "list() – This function is used to convert any data type to a list type.\n",
      "dict() – This function is used to convert a tuple of order (key,value) into a dictionary.\n",
      "str() – Used to convert integer into a string.\n",
      "\n",
      "\n",
      "\n",
      "618. complex(real,imag) – This functionconverts real numbers to complex(real,imag) number.\n",
      "Q13. How to install Python on Windows and set path variable?\n",
      "Ans: To install Python on Windows, follow the below steps:\n",
      "Install python from this link: https://www.python.org/downloads/\n",
      "After this, install it on your PC. Look for the location where PYTHON has been installed on your PC using the following\n",
      "command on your command prompt: cmd python. \n",
      "Then go to advanced system settings and add a new variable and name it as PYTHON_NAME and paste the copied path.\n",
      "Look for the path variable, select its value and select ‘edit’.\n",
      "Add a semicolon towards the end of the value if it’s not present and then type %PYTHON_HOME% \n",
      "Q14. Is indentation required in python?\n",
      "Ans: Indentation is necessary for Python. It speciÕes a block of code. All code within loops, classes, functions, etc is speciÕed\n",
      "within an indented block. It is usually done using four space characters. If your code is not indented necessarily, it will not\n",
      "execute accurately and will throw errors as well.\n",
      "Q15. What is the diàerence between Python Arrays and lists?\n",
      "Ans: Arrays and lists, in Python, have the same way of storing data. But, arrays can hold only a single data type elements whereas\n",
      "lists can hold any data type elements.\n",
      "Example:\n",
      "import array as arr \n",
      "My_Array=arr.array('i',[1,2,3,4]) \n",
      "My_list=[1,'abc',1.20] \n",
      "print(My_Array) \n",
      "print(My_list) \n",
      "Output:\n",
      "array(‘i’, [1, 2, 3, 4]) [1, ‘abc’, 1.2]\n",
      "Q16. What are functions in Python?\n",
      "Ans: A function is a block of code which is executed only when it is called. To deÕne a Python function, the def keyword is used.\n",
      "Example:\n",
      "def Newfunc(): \n",
      "print(\"Hi, Welcome to Edureka\") \n",
      "Newfunc(); #calling the function \n",
      "Output: Hi, Welcome to Edureka\n",
      "Q17.What is __init__?\n",
      "Ans: __init__ is a method or constructor in Python. This method is automatically called to allocate memory when a new object/\n",
      "instance of a class is created. All classes have the __init__ method.\n",
      "Here is an example of how to use it.\n",
      "class Employee: \n",
      "def __init__(self, name, age,salary): \n",
      "self.name = name \n",
      "self.age = age \n",
      "self.salary = 20000 \n",
      "E1 = Employee(\"XYZ\", 23, 20000) \n",
      "# E1 is the instance of class Employee. \n",
      "#__init__ allocates memory for E1.  \n",
      "print(E1.name) \n",
      "print(E1.age) \n",
      "print(E1.salary) \n",
      "Output:\n",
      "XYZ\n",
      "\n",
      "\n",
      "\n",
      "619. 23\n",
      "20000\n",
      "Q18.What is a lambda function?\n",
      "Ans: An anonymous function is known as a lambda function. This function can have any number of parameters but, can have\n",
      "just one statement.\n",
      "Example:\n",
      "a = lambda x,y : x+y \n",
      "print(a(5, 6)) \n",
      "Output: 11\n",
      "Q19. What is self in Python?\n",
      "Ans: Self is an instance or an object of a class. In Python, this is explicitly included as the Õrst parameter. However, this is not the\n",
      "case in Java where it’s optional.  It helps to diàerentiate between the methods and attributes of a class with local variables.\n",
      "The self variable in the init method refers to the newly created object while in other methods, it refers to the object whose\n",
      "method was called.\n",
      "Q20. How does break, continue and pass work?\n",
      "Break\n",
      "Allows loop termination when some condition is met and the control is transferred\n",
      "to the next statement.\n",
      "Continue\n",
      "Allows skipping some part of a loop when some speciÕc condition is met and the\n",
      "control is transferred to the beginning of the loop\n",
      "Pass\n",
      "Used when you need some block of code syntactically, but you want to skip its\n",
      "execution. This is basically a null operation. Nothing happens when this is executed.\n",
      "Q21. What does [::-1} do?\n",
      "Ans: [::-1] is used to reverse the order of an array or a sequence.\n",
      "For example:\n",
      "import array as arr \n",
      "My_Array=arr.array('i',[1,2,3,4,5]) \n",
      "My_Array[::-1] \n",
      "Output: array(‘i’, [5, 4, 3, 2, 1])\n",
      "[::-1] reprints a reversed copy of ordered data structures such as an array or a list. the original array or list remains unchanged.\n",
      "Q22. How can you randomize the items of a list in place in Python?\n",
      "Ans: Consider the example shown below:\n",
      "from random import shuffle \n",
      "x = ['Keep', 'The', 'Blue', 'Flag', 'Flying', 'High'] \n",
      "shuffle(x) \n",
      "print(x) \n",
      "The output of the following code is as below.\n",
      "['Flying', 'Keep', 'Blue', 'High', 'The', 'Flag'] \n",
      "Q23. What are python iterators?\n",
      "Ans: Iterators are objects which can be traversed though or iterated upon.\n",
      "Q24. How can you generate random numbers in Python?\n",
      "Ans: Random module is the standard module that is used to generate a random number. The method is deÕned as:\n",
      "import random \n",
      "random.random \n",
      "\n",
      "\n",
      "\n",
      "620. The statement random.random() method return the Öoating point number that is in the range of [0, 1). The function generates\n",
      "random Öoat numbers. The methods that are used with the random class are the bound methods of the hidden instances. The\n",
      "instances of the Random can be done to show the multi-threading programs that creates a diàerent instance of individual\n",
      "threads. The other random generators that are used in this are:\n",
      "1. randrange(a, b): it chooses an integer and deÕne the range in-between [a, b). It returns the elements by selecting it\n",
      "randomly from the range that is speciÕed. It doesn’t build a range object.\n",
      "2. uniform(a, b): it chooses a Öoating point number that is deÕned in the range of [a,b).Iyt returns the Öoating point number\n",
      "3. normalvariate(mean, sdev): it is used for the normal distribution where the mu is a mean and the sdev is a sigma that is\n",
      "used for standard deviation.\n",
      "4. The Random class that is used and instantiated creates an independent multiple random number generators.\n",
      "Q25. What is the diàerence between range & xrange?\n",
      "Ans: For the most part, xrange and range are the exact same in terms of functionality. They both provide a way to generate a list\n",
      "of integers for you to use, however you please. The only diàerence is that range returns a Python list object and x range returns\n",
      "an xrange object.\n",
      "This means that xrange doesn’t actually generate a static list at run-time like range does. It creates the values as you need them\n",
      "with a special technique called yielding. This technique is used with a type of object known as generators. That means that if you\n",
      "have a really gigantic range you’d like to generate a list for, say one billion, xrange is the function to use.\n",
      "This is especially true if you have a really memory sensitive system such as a cell phone that you are working with, as range will\n",
      "use as much memory as it can to create your array of integers, which can result in a Memory Error and crash your program. It’s a\n",
      "memory hungry beast.\n",
      "Q26. How do you write comments in python?\n",
      "Ans: Comments in Python start with a # character. However, alternatively at times, commenting is done using docstrings(strings\n",
      "enclosed within triple quotes).\n",
      "Example:\n",
      "#Comments in Python start like this \n",
      "print(\"Comments in Python start with a #\")\n",
      "Output:  Comments in Python start with a #\n",
      "Q27. What is pickling and unpickling?\n",
      "Ans: Pickle module accepts any Python object and converts it into a string representation and dumps it into a Õle by using dump\n",
      "function, this process is called pickling. While the process of retrieving original Python objects from the stored string\n",
      "representation is called unpickling.\n",
      "Q28. What are the generators in python?\n",
      "Ans: Functions that return an iterable set of items are called generators.\n",
      "Q29. How will you capitalize the Õrst letter of string?\n",
      "Ans: In Python, the capitalize() method capitalizes the Õrst letter of a string. If the string already consists of a capital letter at the\n",
      "beginning, then, it returns the original string.\n",
      "Q30. How will you convert a string to all lowercase?\n",
      "Ans: To convert a string to lowercase, lower() function can be used.\n",
      "Example:\n",
      "stg='ABCD' \n",
      "print(stg.lower()) \n",
      "Output: abcd\n",
      "Q31. How to comment multiple lines in python?\n",
      "Ans: Multi-line comments appear in more than one line. All the lines to be commented are to be preÕxed by a #. You can also a\n",
      "very good shortcut method to comment multiple lines. All you need to do is hold the ctrl key and left click in every place\n",
      "wherever you want to include a # character and type a # just once. This will comment all the lines where you introduced your\n",
      "cursor.\n",
      "Q32.What are docstrings in Python?\n",
      "\n",
      "\n",
      "\n",
      "621. Ans: Docstrings are not actually comments, but, they are documentation strings. These docstrings are within triple quotes.\n",
      "They are not assigned to any variable and therefore, at times, serve the purpose of comments as well.\n",
      "Example:\n",
      "\"\"\" \n",
      "Using docstring as a comment. \n",
      "This code divides 2 numbers \n",
      "\"\"\" \n",
      "x=8 \n",
      "y=4 \n",
      "z=x/y \n",
      "print(z) \n",
      "Output: 2.0\n",
      "Q33. What is the purpose of is, not and in operators?\n",
      "Ans: Operators are special functions. They take one or more values and produce a corresponding result.\n",
      "is: returns true when 2 operands are true  (Example: “a” is ‘a’)\n",
      "not: returns the inverse of the boolean value\n",
      "in: checks if some element is present in some sequence\n",
      "Q34. What is the usage of help() and dir() function in Python?\n",
      "Ans: Help() and dir() both functions are accessible from the Python interpreter and used for viewing a consolidated dump of\n",
      "built-in functions. \n",
      "1. Help() function: The help() function is used to display the documentation string and also facilitates you to see the help\n",
      "related to modules, keywords, attributes, etc.\n",
      "2. Dir() function: The dir() function is used to display the deÕned symbols.\n",
      "Q35. Whenever Python exits, why isn’t all the memory de-allocated?\n",
      "Ans: \n",
      "1. Whenever Python exits, especially those Python modules which are having circular references to other objects or the\n",
      "objects that are referenced from the global namespaces are not always de-allocated or freed.\n",
      "2. It is impossible to de-allocate those portions of memory that are reserved by the C library.\n",
      "3. On exit, because of having its own eÞcient clean up mechanism, Python would try to de-allocate/destroy every other\n",
      "object.\n",
      "Q36. What is a dictionary in Python?\n",
      "Ans: The built-in datatypes in Python is called dictionary. It deÕnes one-to-one relationship between keys and values. Dictionaries\n",
      "contain pair of keys and their corresponding values. Dictionaries are indexed by keys.\n",
      "Let’s take an example:\n",
      "The following example contains some keys. Country, Capital & PM. Their corresponding values are India, Delhi and Modi\n",
      "respectively.\n",
      "dict={'Country':'India','Capital':'Delhi','PM':'Modi'} \n",
      "print dict[Country] \n",
      "India\n",
      "print dict[Capital] \n",
      "Delhi\n",
      "print dict[PM] \n",
      "Modi\n",
      "Q37. How can the ternary operators be used in python?\n",
      "Ans: The Ternary operator is the operator that is used to show the conditional statements. This consists of the true or false\n",
      "values with a statement that has to be evaluated for it.\n",
      "\n",
      "\n",
      "\n",
      "622. Syntax:\n",
      "The Ternary operator will be given as:\n",
      "[on_true] if [expression] else [on_false]x, y = 25, 50big = x if x < y else y\n",
      "Example:\n",
      "The expression gets evaluated like if x<y else y, in this case if x<y is true then the value is returned as big=x and if it is incorrect\n",
      "then big=y will be sent as a result.\n",
      "Q38. What does this mean: *args, **kwargs? And why would we use it?\n",
      "Ans: We use *args when we aren’t sure how many arguments are going to be passed to a function, or if we want to pass a stored\n",
      "list or tuple of arguments to a function. **kwargs is used when we don’t know how many keyword arguments will be passed to a\n",
      "function, or it can be used to pass the values of a dictionary as keyword arguments. The identiÕers args and kwargs are a\n",
      "convention, you could also use *bob and **billy but that would not be wise.\n",
      "Q39. What does len() do?\n",
      "Ans: It is used to determine the length of a string, a list, an array, etc.\n",
      "Example:\n",
      "stg='ABCD' \n",
      "len(stg) \n",
      "Q40. Explain split(), sub(), subn() methods of “re” module in Python.\n",
      "Ans: To modify the strings, Python’s “re” module is providing 3 methods. They are:\n",
      "split() – uses a regex pattern to “split” a given string into a list.\n",
      "sub() – Õnds all substrings where the regex pattern matches and then replace them with a diàerent string\n",
      "subn() – it is similar to sub() and also returns the new string along with the no. of replacements.\n",
      "Q41. What are negative indexes and why are they used?\n",
      "Ans: The sequences in Python are indexed and it consists of the positive as well as negative numbers. The numbers that are\n",
      "positive uses ‘0’ that is uses as Õrst index and ‘1’ as the second index and the process goes on like that.\n",
      "The index for the negative number starts from ‘-1’ that represents the last index in the sequence and ‘-2’ as the penultimate index\n",
      "and the sequence carries forward like the positive number.\n",
      "The negative index is used to remove any new-line spaces from the string and allow the string to except the last character that is\n",
      "given as S[:-1]. The negative index is also used to show the index to represent the string in correct order.\n",
      "Q42. What are Python packages?\n",
      "Ans: Python packages are namespaces containing multiple modules.\n",
      "Q43.How can Õles be deleted in Python?\n",
      "Ans: To delete a Õle in Python, you need to import the OS Module. After that, you need to use the os.remove() function.\n",
      "Example:\n",
      "import os \n",
      "os.remove(\"xyz.txt\") \n",
      "Q44. What are the built-in types of python?\n",
      "Ans: Built-in types in Python are as follows –\n",
      "Integers\n",
      "Floating-point\n",
      "Complex numbers\n",
      "Strings\n",
      "Boolean\n",
      "Built-in functions\n",
      "Q45. What advantages do NumPy arrays oàer over (nested) Python lists?\n",
      "Ans: \n",
      "1. Python’s lists are eÞcient general-purpose containers. They support (fairly) eÞcient insertion, deletion, appending, and\n",
      "concatenation, and Python’s list comprehensions make them easy to construct and manipulate.\n",
      "\n",
      "\n",
      "\n",
      "623. 2. They have certain limitations: they don’t support “vectorized” operations like elementwise addition and multiplication, and\n",
      "the fact that they can contain objects of diàering types mean that Python must store type information for every element,\n",
      "and must execute type dispatching code when operating on each element.\n",
      "3. NumPy is not just more eÞcient; it is also more convenient. You get a lot of vector and matrix operations for free, which\n",
      "sometimes allow one to avoid unnecessary work. And they are also eÞciently implemented.\n",
      "4. NumPy array is faster and You get a lot built in with NumPy, FFTs, convolutions, fast searching, basic statistics, linear\n",
      "algebra, histograms, etc. \n",
      "Q46. How to add values to a python array?\n",
      "Ans: Elements can be added to an array using the append(), extend() and the insert (i,x) functions.\n",
      "Example:\n",
      "a=arr.array('d', [1.1 , 2.1 ,3.1] ) \n",
      "a.append(3.4) \n",
      "print(a) \n",
      "a.extend([4.5,6.3,6.8]) \n",
      "print(a) \n",
      "a.insert(2,3.8) \n",
      "print(a) \n",
      "Output:\n",
      "array(‘d’, [1.1, 2.1, 3.1, 3.4])\n",
      "array(‘d’, [1.1, 2.1, 3.1, 3.4, 4.5, 6.3, 6.8])\n",
      "array(‘d’, [1.1, 2.1, 3.8, 3.1, 3.4, 4.5, 6.3, 6.8])\n",
      "Q47. How to remove values to a python array?\n",
      "Ans: Array elements can be removed using pop() or remove() method. The diàerence between these two functions is that the\n",
      "former returns the deleted value whereas the latter does not.\n",
      "Example:\n",
      "a=arr.array('d', [1.1, 2.2, 3.8, 3.1, 3.7, 1.2, 4.6]) \n",
      "print(a.pop()) \n",
      "print(a.pop(3)) \n",
      "a.remove(1.1) \n",
      "print(a) \n",
      "Output:\n",
      "4.6\n",
      "3.1\n",
      "array(‘d’, [2.2, 3.8, 3.7, 1.2])\n",
      "Q48. Does Python have OOps concepts?\n",
      "Ans: Python is an object-oriented programming language. This means that any program can be solved in python by creating an\n",
      "object model. However, Python can be treated as procedural as well as structural language.\n",
      "Q49. What is the diàerence between deep and shallow copy?\n",
      "Ans: Shallow copy is used when a new instance type gets created and it keeps the values that are copied in the new instance.\n",
      "Shallow copy is used to copy the reference pointers just like it copies the values. These references point to the original objects\n",
      "and the changes made in any member of the class will also aàect the original copy of it. Shallow copy allows faster execution of\n",
      "the program and it depends on the size of the data that is used.\n",
      "Deep copy is used to store the values that are already copied. Deep copy doesn’t copy the reference pointers to the objects. It\n",
      "makes the reference to an object and the new object that is pointed by some other object gets stored. The changes made in the\n",
      "original copy won’t aàect any other copy that uses the object. Deep copy makes execution of the program slower due to making\n",
      "certain copies for each object that is been called.\n",
      "Q50. How is Multithreading achieved in Python?\n",
      "\n",
      "\n",
      "\n",
      "624. Ans: \n",
      "1. Python has a multi-threading package but if you want to multi-thread to speed your code up, then it’s usually not a good\n",
      "idea to use it.\n",
      "2. Python has a construct called the Global Interpreter Lock (GIL). The GIL makes sure that only one of your ‘threads’ can\n",
      "execute at any one time. A thread acquires the GIL, does a little work, then passes the GIL onto the next thread.\n",
      "3. This happens very quickly so to the human eye it may seem like your threads are executing in parallel, but they are really\n",
      "just taking turns using the same CPU core.\n",
      "4. All this GIL passing adds overhead to execution. This means that if you want to make your code run faster then using the\n",
      "threading package often isn’t a good idea.\n",
      "Q51. What is the process of compilation and linking in python?\n",
      "Ans: The compiling and linking allows the new extensions to be compiled properly without any error and the linking can be done\n",
      "only when it passes the compiled procedure. If the dynamic loading is used then it depends on the style that is being provided\n",
      "with the system. The python interpreter can be used to provide the dynamic loading of the conÕguration setup Õles and will\n",
      "rebuild the interpreter.\n",
      "The steps that are required in this as:\n",
      "1. Create a Õle with any name and in any language that is supported by the compiler of your system. For example Õle.c or\n",
      "Õle.cpp\n",
      "2. Place this Õle in the Modules/ directory of the distribution which is getting used.\n",
      "3. Add a line in the Õle Setup.local that is present in the Modules/ directory.\n",
      "4. Run the Õle using spam Õle.o\n",
      "5. After a successful run of this rebuild the interpreter by using the make command on the top-level directory.\n",
      "6. If the Õle is changed then run rebuildMakeÕle by using the command as ‘make MakeÕle’.\n",
      "Q52. What are Python libraries? Name a few of them.\n",
      "Python libraries are a collection of Python packages. Some of the majorly used python libraries are – Numpy, Pandas, Matplotlib,\n",
      "Scikit-learn and many more.\n",
      "Q53. What is split used for?\n",
      "The split() method is used to separate a given string in Python.\n",
      "Example:\n",
      "a=\"edureka python\" \n",
      "print(a.split()) \n",
      "Output:  [‘edureka’, ‘python’]\n",
      "Q54. How to import modules in python?\n",
      "Modules can be imported using the import keyword.  You can import modules in three ways-\n",
      "Example:\n",
      " \n",
      "import array           #importing using the original module name \n",
      "import array as arr    # importing using an alias name \n",
      "from array import *    #imports everything present in the array module \n",
      " \n",
      "OOPS Interview Questions\n",
      "Q55. Explain Inheritance in Python with an example.\n",
      "Ans: Inheritance allows One class to gain all the members(say attributes and methods) of another class. Inheritance provides\n",
      "code reusability, makes it easier to create and maintain an application. The class from which we are inheriting is called super-\n",
      "class and the class that is inherited is called a derived / child class.\n",
      "They are diàerent types of inheritance supported by Python:\n",
      "1. Single Inheritance – where a derived class acquires the members of a single super class.\n",
      "2. Multi-level inheritance – a derived class d1 in inherited from base class base1, and d2 are inherited from base2.\n",
      "3. Hierarchical inheritance – from one base class you can inherit any number of child classes\n",
      "4. Multiple inheritance – a derived class is inherited from more than one base class.\n",
      "Q56. How are classes created in Python? \n",
      "\n",
      "\n",
      "\n",
      "625. Ans: Class in Python is created using the class keyword.\n",
      "Example:\n",
      "class Employee: \n",
      "def __init__(self, name): \n",
      "self.name = name \n",
      "E1=Employee(\"abc\") \n",
      "print(E1.name) \n",
      "Output: abc\n",
      "Q57. What is monkey patching in Python?\n",
      "Ans: In Python, the term monkey patch only refers to dynamic modiÕcations of a class or module at run-time.\n",
      "Consider the below example:\n",
      "# m.py \n",
      "class MyClass: \n",
      "def f(self): \n",
      "print \"f()\" \n",
      "We can then run the monkey-patch testing like this:\n",
      "import m \n",
      "def monkey_f(self): \n",
      "print \"monkey_f()\" \n",
      " \n",
      "m.MyClass.f = monkey_f \n",
      "obj = m.MyClass() \n",
      "obj.f() \n",
      "The output will be as below:\n",
      "monkey_f()\n",
      "As we can see, we did make some changes in the behavior of f() in MyClass using the function we deÕned, monkey_f(), outside of\n",
      "the module m.\n",
      "Q58. Does python support multiple inheritance?\n",
      "Ans: Multiple inheritance means that a class can be derived from more than one parent classes. Python does support multiple\n",
      "inheritance, unlike Java.\n",
      "Q59. What is Polymorphism in Python?\n",
      "Ans: Polymorphism means the ability to take multiple forms. So, for instance, if the parent class has a method named ABC then\n",
      "the child class also can have a method with the same name ABC having its own parameters and variables. Python allows\n",
      "polymorphism.\n",
      "Q60. DeÕne encapsulation in Python?\n",
      "Ans: Encapsulation means binding the code and the data together. A Python class in an example of encapsulation.\n",
      "Q61. How do you do data abstraction in Python?\n",
      "Ans: Data Abstraction is providing only the required details and hiding the implementation from the world. It can be achieved in\n",
      "Python by using interfaces and abstract classes.\n",
      "Q62.Does python make use of access speciÕers?\n",
      "Ans: Python does not deprive access to an instance variable or function. Python lays down the concept of preÕxing the name of\n",
      "the variable, function or method with a single or double underscore to imitate the behavior of protected and private access\n",
      "speciÕers.  \n",
      "Q63. How to create an empty class in Python? \n",
      "Ans: An empty class is a class that does not have any code deÕned within its block. It can be created using the pass keyword.\n",
      "However, you can create objects of this class outside the class itself. IN PYTHON THE PASS command does nothing when its\n",
      "executed. it’s a null statement. \n",
      "For example-\n",
      "\n",
      "\n",
      "\n",
      "626. class a: \n",
      "  &nbsp; pass \n",
      "obj=a() \n",
      "obj.name=\"xyz\" \n",
      "print(\"Name = \",obj.name) \n",
      "Output: \n",
      "Name =  xyz\n",
      "Q64. What does an object() do?\n",
      "Ans: It returns a featureless object that is a base for all classes. Also, it does not take any parameters.\n",
      "Basic Python Programs\n",
      "Q65. Write a program in Python to execute the Bubble sort algorithm.\n",
      "def bs(a):&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# a = name of list \n",
      "  &nbsp; b=len(a)-1&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# minus 1 because we always compare 2 adjacent values \n",
      "  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; \n",
      "  &nbsp; for x in range(b): \n",
      "  &nbsp; &nbsp; &nbsp; for y in range(b-x): \n",
      "  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if a[y]>a[y+1]: \n",
      "  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; a[y],a[y+1]=a[y+1],a[y] \n",
      "  &nbsp; return a \n",
      "a=[32,5,3,6,7,54,87] \n",
      "bs(a) \n",
      "Output:  [3, 5, 6, 7, 32, 54, 87]\n",
      "Q66. Write a program in Python to produce Star triangle.\n",
      "def pyfunc(r): \n",
      "    for x in range(r): \n",
      "        print(' '*(r-x-1)+'*'*(2*x+1))     \n",
      "pyfunc(9) \n",
      "Output:\n",
      "        * \n",
      "       *** \n",
      "      ***** \n",
      "     ******* \n",
      "    ********* \n",
      "   *********** \n",
      "  ************* \n",
      " *************** \n",
      "***************** \n",
      " \n",
      "Q67. Write a program to produce Fibonacci series in Python.\n",
      "\n",
      "\n",
      "\n",
      "627. # Enter number of terms needed&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; \n",
      "&nbsp;#0,1,1,2,3,5.... \n",
      "a=int(input(\"Enter the terms\")) \n",
      "f=0&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; \n",
      "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#first element of series \n",
      "s=1&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; \n",
      "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#second element of series \n",
      "if a<=0: \n",
      "  &nbsp; print(\"The requested series is \n",
      "\",f) \n",
      "else: \n",
      "  &nbsp; print(f,s,end=\" \") \n",
      "  &nbsp; for x in range(2,a): \n",
      "  &nbsp; &nbsp; &nbsp; next=f+s&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; \n",
      "&nbsp; &nbsp;   \n",
      "  &nbsp; &nbsp; &nbsp; print(next,end=\" \") \n",
      "  &nbsp; &nbsp; &nbsp; f=s \n",
      "  &nbsp; &nbsp; &nbsp; s=next</pre> \n",
      " \n",
      "Output: Enter the terms 5 0 1 1 2 3\n",
      "Q68. Write a program in Python to check if a number is prime.\n",
      "a=int(input(\"enter number\"))&nbsp; &nbsp; &nbsp; \n",
      "if a>1: \n",
      "  &nbsp; for x in range(2,a): \n",
      "  &nbsp; &nbsp; &nbsp; if(a%x)==0: \n",
      "  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; print(\"not prime\") \n",
      "  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; break \n",
      "  &nbsp; else: \n",
      "  &nbsp; &nbsp; &nbsp; print(\"Prime\") \n",
      "else: \n",
      "  &nbsp; print(\"not prime\") \n",
      "Output:\n",
      "enter number 3\n",
      "Prime\n",
      "Q69. Write a program in Python to check if a sequence is a Palindrome.\n",
      "a=input(\"enter sequence\") \n",
      "b=a[::-1] \n",
      "if a==b: \n",
      "  &nbsp; print(\"palindrome\") \n",
      "else: \n",
      "  &nbsp; print(\"Not a Palindrome\") \n",
      "Output:\n",
      "enter sequence 323 palindrome\n",
      "Q70. Write a one-liner that will count the number of capital letters in a Õle. Your code should work even if the Õle is too\n",
      "big to Õt in memory.\n",
      "Ans:  Let us Õrst write a multiple line solution and then convert it to one-liner code.\n",
      "with open(SOME_LARGE_FILE) as fh: \n",
      "count = 0 \n",
      "text = fh.read() \n",
      "for character in text: \n",
      "    if character.isupper(): \n",
      "count += 1 \n",
      "We will now try to transform this into a single line.\n",
      "\n",
      "\n",
      "\n",
      "628. count sum(1 for line in fh for character in line if character.isupper()) \n",
      "Q71. Write a sorting algorithm for a numerical dataset in Python.\n",
      "Ans: The following code can be used to sort a list in Python:\n",
      "list = [\"1\", \"4\", \"0\", \"6\", \"9\"] \n",
      "list = [int(i) for i in list] \n",
      "list.sort() \n",
      "print (list) \n",
      "Q72. Looking at the below code, write down the Õnal values of A0, A1, …An.\n",
      "A0 = dict(zip(('a','b','c','d','e'),(1,2,3,4,5))) \n",
      "A1 = range(10)A2 = sorted([i for i in A1 if i in A0]) \n",
      "A3 = sorted([A0[s] for s in A0]) \n",
      "A4 = [i for i in A1 if i in A3] \n",
      "A5 = {i:i*i for i in A1} \n",
      "A6 = [[i,i*i] for i in A1] \n",
      "print(A0,A1,A2,A3,A4,A5,A6) \n",
      "Ans: The following will be the Õnal outputs of A0, A1, … A6\n",
      "A0 = {'a': 1, 'c': 3, 'b': 2, 'e': 5, 'd': 4} # the order may vary \n",
      "A1 = range(0, 10)  \n",
      "A2 = [] \n",
      "A3 = [1, 2, 3, 4, 5] \n",
      "A4 = [1, 2, 3, 4, 5] \n",
      "A5 = {0: 0, 1: 1, 2: 4, 3: 9, 4: 16, 5: 25, 6: 36, 7: 49, 8: 64, 9: 81} \n",
      "A6 = [[0, 0], [1, 1], [2, 4], [3, 9], [4, 16], [5, 25], [6, 36], [7, 49], [8, 64], [9, 81]]\n",
      "Python Libraries Interview Questions\n",
      "Q73. Explain what Flask is and its beneÕts?\n",
      "Ans: Flask is a web microframework for Python based on “Werkzeug, Jinja2 and good intentions” BSD license. Werkzeug and\n",
      "Jinja2 are two of its dependencies. This means it will have little to no dependencies on external libraries.  It makes the framework\n",
      "light while there is a little dependency to update and fewer security bugs.\n",
      "A session basically allows you to remember information from one request to another. In a Öask, a session uses a signed cookie\n",
      "so the user can look at the session contents and modify. The user can modify the session if only it has the secret key\n",
      "Flask.secret_key.\n",
      "Q74. Is Django better than Flask?\n",
      "Ans: Django and Flask map the URL’s or addresses typed in the web browsers to functions in Python. \n",
      "Flask is much simpler compared to Django but, Flask does not do a lot for you meaning you will need to specify the details,\n",
      "whereas Django does a lot for you wherein you would not need to do much work. Django consists of prewritten code, which the\n",
      "user will need to analyze whereas Flask gives the users to create their own code, therefore, making it simpler to understand the\n",
      "code. Technically both are equally good and both contain their own pros and cons.\n",
      "Q75. Mention the diàerences between Django, Pyramid and Flask.\n",
      "Ans: \n",
      "Flask is a “microframework” primarily build for a small application with simpler requirements. In Öask, you have to use\n",
      "external libraries. Flask is ready to use.\n",
      "Pyramid is built for larger applications. It provides Öexibility and lets the developer use the right tools for their project. The\n",
      "developer can choose the database, URL structure, templating style and more. Pyramid is heavy conÕgurable.\n",
      "Django can also be used for larger applications just like Pyramid. It includes an ORM.\n",
      "Q76. Discuss Django architecture.\n",
      "Ans: Django MVT Pattern:\n",
      "\n",
      "\n",
      "\n",
      "629. Figure:  Python Interview Questions – Django Architecture\n",
      "The developer provides the Model, the view and the template then just maps it to a URL and Django does the magic to serve it to\n",
      "the user.\n",
      "Q77. Explain how you can set up the Database in Django.\n",
      "Ans: You can use the command edit mysite/setting.py, it is a normal python module with module level representing Django\n",
      "settings.\n",
      "Django uses SQLite by default; it is easy for Django users as such it won’t require any other type of installation. In the case your\n",
      "database choice is diàerent that you have to the following keys in the  DATABASE ‘default’  item to match your database\n",
      "connection settings.\n",
      "Engines: you can change the database by using ‘django.db.backends.sqlite3’ , ‘django.db.backeneds.mysql’,\n",
      "‘django.db.backends.postgresql_psycopg2’, ‘django.db.backends.oracle’ and so on\n",
      "Name: The name of your database. In the case if you are using SQLite as your database, in that case, database will be a Õle\n",
      "on your computer, Name should be a full absolute path, including the Õle name of that Õle.\n",
      "If you are not choosing SQLite as your database then settings like Password, Host, User, etc. must be added.\n",
      "Django uses SQLite as a default database, it stores data as a single Õle in the Õlesystem. If you do have a database server—\n",
      "PostgreSQL, MySQL, Oracle, MSSQL—and want to use it rather than SQLite, then use your database’s administration tools to\n",
      "create a new database for your Django project. Either way, with your (empty) database in place, all that remains is to tell Django\n",
      "how to use it. This is where your project’s settings.py Õle comes in.\n",
      "We will add the following lines of code to the setting.py Õle:\n",
      "DATABASES = { \n",
      "     'default': { \n",
      "          'ENGINE' : 'django.db.backends.sqlite3', \n",
      "          'NAME' : os.path.join(BASE_DIR, 'db.sqlite3'), \n",
      "     } \n",
      "} \n",
      "Q78. Give an example how you can write a VIEW in Django?\n",
      "Ans: This is how we can use write a view in Django:\n",
      "from django.http import HttpResponse \n",
      "import datetime \n",
      " \n",
      "def Current_datetime(request): \n",
      "     now = datetime.datetime.now() \n",
      "     html = \"<html><body>It is now %s</body></html> % now \n",
      "     return HttpResponse(html) \n",
      "Returns the current date and time, as an HTML document\n",
      "Q79. Mention what the Django templates consist of.\n",
      "Ans: The template is a simple text Õle.   It can create any text-based format like XML, CSV, HTML, etc.   A template contains\n",
      "variables that get replaced with values when the template is evaluated and tags (% tag %) that control the logic of the template.\n",
      "Figure: Python Interview Questions – Django Template\n",
      "\n",
      "\n",
      "\n",
      "630. Q80. Explain the use of session in Django framework?\n",
      "Ans: Django provides a session that lets you store and retrieve data on a per-site-visitor basis. Django abstracts the process of\n",
      "sending and receiving cookies, by placing a session ID cookie on the client side, and storing all the related data on the server side.\n",
      "Figure: Python Interview Questions – Django Framework\n",
      "So the data itself is not stored client side. This is nice from a security perspective.\n",
      "Q81.  List out the inheritance styles in Django.\n",
      "Ans: In Django, there are three possible inheritance styles:\n",
      "1. Abstract Base Classes: This style is used when you only want parent’s class to hold information that you don’t want to type\n",
      "out for each child model.\n",
      "2. Multi-table Inheritance: This style is used If you are sub-classing an existing model and need each model to have its own\n",
      "database table.\n",
      "3. Proxy models: You can use this model, If you only want to modify the Python level behavior of the model, without changing\n",
      "the model’s Õelds.\n",
      "Web Scraping – Python Interview Questions\n",
      "Q82. How To Save An Image Locally Using Python Whose URL Address I Already Know?\n",
      "Ans: We will use the following code to save an image locally from an URL address\n",
      "import urllib.request \n",
      "urllib.request.urlretrieve(\"URL\", \"local-filename.jpg\") \n",
      "Q83. How can you Get the Google cache age of any URL or web page?\n",
      "Ans: Use the following URL format:\n",
      "http://webcache.googleusercontent.com/search?q=cache:URLGOESHERE\n",
      "Be sure to replace “URLGOESHERE” with the proper web address of the page or site whose cache you want to retrieve and see\n",
      "the time for. For example, to check the Google Webcache age of edureka.co you’d use the following URL:\n",
      "http://webcache.googleusercontent.com/search?q=cache:edureka.co\n",
      "Q84. You are required to scrap data from IMDb top 250 movies page. It should only have Õelds movie name, year, and\n",
      "rating.\n",
      "Ans: We will use the following lines of code:\n",
      "\n",
      "\n",
      "\n",
      "631. from bs4 import BeautifulSoup \n",
      " \n",
      "import requests \n",
      "import sys \n",
      " \n",
      "url = 'http://www.imdb.com/chart/top' \n",
      "response = requests.get(url) \n",
      "soup = BeautifulSoup(response.text) \n",
      "tr = soup.findChildren(\"tr\") \n",
      "tr = iter(tr) \n",
      "next(tr) \n",
      " \n",
      "for movie in tr: \n",
      "title = movie.find('td', {'class': 'titleColumn'} ).find('a').contents[0] \n",
      "year = movie.find('td', {'class': 'titleColumn'} ).find('span', {'class': 'secondaryInfo'}).contents[0] \n",
      "rating = movie.find('td', {'class': 'ratingColumn imdbRating'} ).find('strong').contents[0] \n",
      "row = title + ' - ' + year + ' ' + ' ' + rating \n",
      " \n",
      "print(row) \n",
      "The above code will help scrap data from IMDb’s top 250 list\n",
      "Data Analysis – Python Interview Questions\n",
      "Q85. What is map function in Python?\n",
      "Ans: map function executes the function given as the Õrst argument on all the elements of the iterable given as the second\n",
      "argument. If the function given takes in more than 1 arguments, then many iterables are given. #Follow the link to know more\n",
      "similar functions.\n",
      "Q86. Is python numpy better than lists?\n",
      "Ans: We use python numpy array instead of a list because of the below three reasons:\n",
      "1. Less Memory\n",
      "2. Fast\n",
      "3. Convenient\n",
      "For more information on these parameters, you can refer to this section – Numpy Vs List.\n",
      "Q87. How to get indices of N maximum values in a NumPy array?\n",
      "Ans: We can get the indices of N maximum values in a NumPy array using the below code:\n",
      "import numpy as np \n",
      "arr = np.array([1, 3, 2, 4, 5]) \n",
      "print(arr.argsort()[-3:][::-1]) \n",
      "Output\n",
      "[ 4 3 1 ]\n",
      "Q88. How do you calculate percentiles with Python/ NumPy?\n",
      "Ans: We can calculate percentiles with the following code\n",
      "import numpy as np \n",
      "a = np.array([1,2,3,4,5]) \n",
      "p = np.percentile(a, 50) #Returns 50th percentile, e.g. median \n",
      "print(p) \n",
      "Output\n",
      "3\n",
      "Q89. What is the diàerence between NumPy and SciPy?\n",
      "Ans: \n",
      "1. In an ideal world, NumPy would contain nothing but the array data type and the most basic operations: indexing, sorting,\n",
      "reshaping, basic elementwise functions, et cetera.\n",
      "\n",
      "\n",
      "\n",
      "632. 2. All numerical code would reside in SciPy. However, one of NumPy’s important goals is compatibility, so NumPy tries to\n",
      "retain all features supported by either of its predecessors.\n",
      "3. Thus NumPy contains some linear algebra functions, even though these more properly belong in SciPy. In any case, SciPy\n",
      "contains more fully-featured versions of the linear algebra modules, as well as many other numerical algorithms.\n",
      "4. If you are doing scientiÕc computing with python, you should probably install both NumPy and SciPy. Most new features\n",
      "belong in SciPy rather than NumPy.\n",
      "Q90. How do you make 3D plots/visualizations using NumPy/SciPy?\n",
      "Ans: Like 2D plotting, 3D graphics is beyond the scope of NumPy and SciPy, but just as in the 2D case, packages exist that\n",
      "integrate with NumPy. Matplotlib provides basic 3D plotting in the mplot3d subpackage, whereas Mayavi provides a wide range\n",
      "of high-quality 3D visualization features, utilizing the powerful VTK engine.\n",
      "Multiple Choice Questions (MCQ)\n",
      "Q91. Which of the following statements create a dictionary? (Multiple Correct Answers Possible)\n",
      "a) d = {}\n",
      "b) d = {“john”:40, “peter”:45}\n",
      "c) d = {40:”john”, 45:”peter”}\n",
      "d) d = (40:”john”, 45:”50”)\n",
      "Answer: b, c & d. \n",
      "Dictionaries are created by specifying keys and values.\n",
      "Q92. Which one of these is Öoor division?\n",
      "a) /\n",
      "b) //\n",
      "c) %\n",
      "d) None of the mentioned\n",
      "Answer: b) //\n",
      "When both of the operands are integer then python chops out the fraction part and gives you the round oà value, to get the\n",
      "accurate answer use Öoor division. For ex, 5/2 = 2.5 but both of the operands are integer so answer of this expression in python\n",
      "is 2. To get the 2.5 as the answer, use Öoor division using //. So, 5//2 = 2.5\n",
      "Q93. What is the maximum possible length of an identiÕer?\n",
      "a) 31 characters\n",
      "b) 63 characters\n",
      "c) 79 characters\n",
      "d) None of the above\n",
      "Answer: d) None of the above\n",
      "IdentiÕers can be of any length.\n",
      "Q94. Why are local variable names beginning with an underscore discouraged?\n",
      "a) they are used to indicate a private variables of a class\n",
      "b) they confuse the interpreter\n",
      "c) they are used to indicate global variables\n",
      "d) they slow down execution\n",
      "Answer: a) they are used to indicate a private variable of a class\n",
      "As Python has no concept of private variables, leading underscores are used to indicate variables that must not be accessed from\n",
      "outside the class.\n",
      "Q95. Which of the following is an invalid statement?\n",
      "a) abc = 1,000,000\n",
      "b) a b c = 1000 2000 3000\n",
      "c) a,b,c = 1000, 2000, 3000\n",
      "d) a_b_c = 1,000,000\n",
      "Answer: b) a b c = 1000 2000 3000\n",
      "Spaces are not allowed in variable names.\n",
      "Q96. What is the output of the following?\n",
      "\n",
      "\n",
      "\n",
      "633. try: \n",
      "    if '1' != 1: \n",
      "        raise \"someError\" \n",
      "    else: \n",
      "        print(\"someError has not occured\") \n",
      "except \"someError\": \n",
      "    print (\"someError has occured\") \n",
      "a) someError has occured\n",
      "b) someError has not occured\n",
      "c) invalid code\n",
      "d) none of the above\n",
      "Answer: c) invalid code\n",
      "A new exception class must inherit from a BaseException. There is no such inheritance here.\n",
      "Q97. Suppose list1 is [2, 33, 222, 14, 25], What is list1[-1] ?\n",
      "a) Error\n",
      "b) None\n",
      "c) 25\n",
      "d) 2\n",
      "Answer: c) 25\n",
      "The index -1 corresponds to the last index in the list.\n",
      "Q98. To open a Õle c:scores.txt for writing, we use\n",
      "a) outÕle = open(“c:scores.txt”, “r”)\n",
      "b) outÕle = open(“c:scores.txt”, “w”)\n",
      "c) outÕle = open(Õle = “c:scores.txt”, “r”)\n",
      "d) outÕle = open(Õle = “c:scores.txt”, “o”)\n",
      "Answer: b) The location contains double slashes ( ) and w is used to indicate that Õle is being written to.\n",
      "Q99. What is the output of the following?\n",
      "f = None \n",
      " \n",
      "for i in range (5): \n",
      "    with open(\"data.txt\", \"w\") as f: \n",
      "        if i > 2: \n",
      "            break \n",
      " \n",
      "print f.closed \n",
      "a) True\n",
      "b) False\n",
      "c) None\n",
      "d) Error\n",
      "Answer: a) True \n",
      "The WITH statement when used with open Õle guarantees that the Õle object is closed when the with block exits.\n",
      "Q100. When will the else part of try-except-else be executed?\n",
      "a) always\n",
      "b) when an exception occurs\n",
      "c) when no exception occurs\n",
      "d) when an exception occurs into except block\n",
      "Answer: c) when no exception occurs\n",
      "The else part is executed when no exception occurs.\n",
      "I hope this set of Python Interview Questions will help you in preparing for your interviews. All the best!\n",
      "Got a question for us? Please mention it in the comments section and we will get back to you at the earliest.\n",
      "If you wish to learn Python and gain expertise in quantitative analysis, data mining, and the presentation of data to see beyond\n",
      "the numbers by transforming your career into Data Scientist role, check out our interactive, live-online Python CertiÕcation\n",
      "Training. You will use libraries like Pandas, Numpy, Matplotlib, Scipy, Scikit, Pyspark and master the concepts like Python\n",
      "machine learning, scripts, sequence, web scraping and big data analytics leveraging Apache Spark. The training comes with 24*7\n",
      "support to guide you throughout your learning period.\n",
      "\n",
      "\n",
      "\n",
      "634.                                                                                        http://career.guru99.com/\n",
      " \n",
      "Top 40 Python Interview Questions & Answers\n",
      "1) What is Python? What are the benefits of using Python?\n",
      "Python is a programming language with objects, modules, threads, exceptions and automatic\n",
      "memory management. The benefits of pythons are that it is simple and easy, portable,\n",
      "extensible, build-in data structure and it is an open source.\n",
      "2)  What is PEP 8?\n",
      "PEP 8 is a coding convention, a set of recommendation, about how to write your Python code\n",
      "more readable.\n",
      "3) What is pickling and unpickling?\n",
      "Pickle module accepts any Python object and converts it into a string representation and dumps\n",
      "it into a file by using dump function, this process is called pickling.  While the process of\n",
      "retrieving original Python objects from the stored string representation is called unpickling.\n",
      "4) How Python is interpreted?\n",
      "Python language is an interpreted language. Python program runs directly from the source\n",
      "code. It converts the source code that is written by the programmer into an intermediate\n",
      "language, which is again translated into machine language that has to be executed.\n",
      "5) How memory is managed in Python?\n",
      "Python memory is managed by Python private heap space. All Python objects and data\n",
      "structures are located in a private heap. The programmer does not have an access to\n",
      "this private heap and interpreter takes care of this Python private heap.\n",
      "The allocation of Python heap space for Python objects is done by Python memory\n",
      "manager.  The core API gives access to some tools for the programmer to code.\n",
      "Python also have an inbuilt garbage collector, which recycle all the unused memory and\n",
      "frees the memory and makes it available to the heap space.\n",
      "                               1 / 7\n",
      "MUST READ  click >>>>> All AI related Cheatsheets and Tutorials in one place\n",
      "https://www.linkedin.com/pulse/all-cheatsheets-one-place-vipul-patel/\n",
      "\n",
      "\n",
      "635.                                                                                        http://career.guru99.com/\n",
      " \n",
      "6) What are the tools that help to find bugs or perform static analysis?\n",
      "PyChecker is a static analysis tool that detects the bugs in Python source code and warns\n",
      "about the style and complexity of the bug. Pylint is another tool that verifies whether the module\n",
      "meets the coding standard.\n",
      "7) What are Python decorators?\n",
      "A Python decorator is a specific change that we make in Python syntax to alter functions easily.\n",
      "8) What is the difference between list and tuple?\n",
      "The difference between list and tuple is that list is mutable while tuple is not. Tuple can be\n",
      "hashed for e.g as a key for dictionaries.\n",
      "9) How are arguments passed by value or by reference?\n",
      "Everything in Python is an object and all variables hold references to the objects. The\n",
      "references values are according to the functions; as a result you cannot change the value of the\n",
      "references. However, you can change the objects if it is mutable.\n",
      "10) What is Dict and List comprehensions are?\n",
      "They are syntax constructions to ease the creation of a Dictionary or List based on existing\n",
      "iterable.\n",
      "11) What are the built-in type does python provides?\n",
      "There are mutable and Immutable types of Pythons built in types Mutable built-in types\n",
      "List\n",
      "Sets\n",
      "Dictionaries\n",
      "Immutable built-in types\n",
      "                               2 / 7\n",
      "\n",
      "\n",
      "636.                                                                                        http://career.guru99.com/\n",
      " \n",
      "Strings\n",
      "Tuples\n",
      "Numbers\n",
      "12) What is namespace in Python?\n",
      "In Python, every name introduced has a place where it lives and can be hooked for. This is\n",
      "known as namespace. It is like a box where a variable name is mapped to the object placed.\n",
      " Whenever the variable is searched out, this box will be searched, to get corresponding object.\n",
      "13) What is lambda in Python?\n",
      "It is a single expression anonymous function often used as inline function.\n",
      "14) Why lambda forms in python does not have statements?\n",
      "A lambda form in python does not have statements as it is used to make new function object\n",
      "and then return them at runtime.\n",
      "15) What is pass in Python?\n",
      "Pass means, no-operation Python statement, or in other words it is a place holder in compound\n",
      "statement, where there should be a blank left and nothing has to be written there.\n",
      "16) In Python what are iterators?\n",
      "In Python, iterators are used to iterate a group of elements, containers like list.\n",
      "17) What is unittest in Python?\n",
      "A unit testing framework in Python is known as unittest.  It supports sharing of setups,\n",
      "automation testing, shutdown code for tests, aggregation of tests into collections etc.\n",
      "18) In Python what is slicing?\n",
      "A mechanism to select a range of items from sequence types like list, tuple, strings etc. is\n",
      "known as slicing.\n",
      "19) What are generators in Python?\n",
      "The way of implementing iterators are known as generators.  It is a normal function except that\n",
      "it yields expression in the function.\n",
      "20) What is docstring in Python?\n",
      "A Python documentation string is known as docstring, it is a way of documenting Python\n",
      "                               3 / 7\n",
      "\n",
      "\n",
      "637.                                                                                        http://career.guru99.com/\n",
      " \n",
      "functions, modules and classes.\n",
      "21)  How can you copy an object in Python?\n",
      "To copy an object in Python, you can try copy.copy () or copy.deepcopy() for the general case.\n",
      "You cannot copy all objects but most of them.\n",
      "22) What is negative index in Python?\n",
      "Python sequences can be index in positive and negative numbers.   For positive index, 0 is the\n",
      "first index, 1 is the second index and so forth.  For negative index, (-1) is the last index and (-2)\n",
      "is the second last index and so forth.\n",
      "23) How you can convert a number to a string?\n",
      "In order to convert a number into a string, use the inbuilt function str().  If you want a octal or\n",
      "hexadecimal representation, use the inbuilt function oct() or hex().\n",
      "24) What is the difference between Xrange and range?\n",
      "Xrange returns the xrange object while range returns the list, and uses the same memory and\n",
      "no matter what the range size is.\n",
      "25) What is module and package in Python?\n",
      "In Python, module is the way to structure program. Each Python program file is a module, which\n",
      "imports other modules like objects and attributes.\n",
      "The folder of Python program is a package of modules.  A package can have modules or\n",
      "subfolders.\n",
      "26) Mention what are the rules for local and global variables in Python?\n",
      "Local variables: If a variable is assigned a new value anywhere within the function's body, it's\n",
      "assumed to be local.\n",
      "Global variables: Those variables that are only referenced inside a function are implicitly\n",
      "global.\n",
      "27) How can you share global variables across modules?\n",
      "To share global variables across modules within a single program, create a special module.\n",
      "Import the config module in all modules of your application. The module will be available as a\n",
      "global variable across modules.\n",
      "28) Explain how can you make a Python Script executable on Unix?\n",
      "                               4 / 7\n",
      "\n",
      "\n",
      "638.                                                                                        http://career.guru99.com/\n",
      " \n",
      "To make a Python Script executable on Unix, you need to do two things,\n",
      "Script file's mode must be executable and\n",
      "the first line must begin with # ( #!/usr/local/bin/python)\n",
      "29) Explain how to delete a file in Python?\n",
      "By using a command os.remove (filename) or os.unlink(filename)\n",
      "30) Explain how can you generate random numbers in Python?\n",
      "To generate random numbers in Python, you need to import command as\n",
      "import random\n",
      "random.random()\n",
      "This returns a random floating point number in the range [0,1)\n",
      "31) Explain how can you access a module written in Python from C?\n",
      "You can access a module written in Python from C by following method,\n",
      "Module =  =PyImport_ImportModule(\"\");\n",
      "32) Mention the use of // operator in Python?\n",
      "It is a Floor Divisionoperator , which is used for dividing two operands with the result as quotient\n",
      "showing only digits before the decimal point. For instance, 10//5 = 2 and 10.0//5.0 = 2.0.\n",
      "33) Mention five benefits of using Python?\n",
      "Python comprises of a huge standard library for most Internet platforms like Email,\n",
      "HTML, etc.\n",
      "Python does not require explicit memory management as the interpreter itself allocates\n",
      "the memory to new variables and free them automatically\n",
      "Provide easy readability due to use of square brackets\n",
      "Easy-to-learn for beginners\n",
      "Having the built-in data types saves programming time and effort from declaring\n",
      "variables\n",
      "34) Mention the use of the split function in Python?\n",
      "The use of the split function in Python is that it breaks a string into shorter strings using the\n",
      "defined separator. It gives a list of all words present in the string.\n",
      "                               5 / 7\n",
      "\n",
      "\n",
      "639.                                                                                        http://career.guru99.com/\n",
      " \n",
      "35) Explain what is Flask & its benefits?\n",
      "Flask is a web micro framework for Python based on “Werkzeug, Jinja 2 and good intentions”\n",
      "BSD licensed. Werkzeug and jingja are two of its dependencies.\n",
      "Flask is part of the micro-framework. Which means it will have little to no dependencies on\n",
      "external libraries.  It makes the framework light while there is little dependency to update and\n",
      "less security bugs.\n",
      "36) Mention what is the difference between Django, Pyramid, and Flask?\n",
      "Flask is a “microframework” primarily build for a small application with simpler requirements.  In\n",
      "flask, you have to use external libraries.  Flask is ready to use.\n",
      "Pyramid are build for larger applications.  It provides flexibility and lets the developer use the\n",
      "right tools for their project. The developer can choose the database, URL structure, templating\n",
      "style and more. Pyramid is heavy configurable.\n",
      "Like Pyramid, Django can also used for larger applications.  It includes an ORM.\n",
      " \n",
      "37) Mention what is Flask-WTF and what are their features?\n",
      "Flask-WTF offers simple integration with WTForms.  Features include for Flask WTF are\n",
      "Integration with wtforms\n",
      "Secure form with csrf token\n",
      "Global csrf protection\n",
      "Internationalization integration\n",
      "Recaptcha supporting\n",
      "File upload that works with Flask Uploads\n",
      "38) Explain what is the common way for the Flask script to work?\n",
      "The common way for the flask script to work is\n",
      "Either it should be the import path for your application\n",
      "Or the path to a Python file\n",
      " \n",
      "39) Explain how you can access sessions in Flask?\n",
      "A session basically allows you to remember information from one request to another.  In a flask,\n",
      "it uses a signed cookie so the user can look at the session contents and modify. The user can\n",
      "                               6 / 7\n",
      "\n",
      "\n",
      "640.                                                                                        http://career.guru99.com/\n",
      " \n",
      "modify the session if only it has the secret key Flask.secret_key.\n",
      "40) Is Flask an MVC model and if yes give an example showing MVC pattern for your\n",
      "application?\n",
      "Basically, Flask is a minimalistic framework which behaves same as MVC framework. So MVC\n",
      "is a perfect fit for Flask, and the pattern for MVC we will consider for the following example\n",
      " \n",
      "Powered by TCPDF (www.tcpdf.org)\n",
      "                               7 / 7\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for page_number, page in tqdm(pages.items(), total=len(documents)):\n",
    "    print(f\"{page_number}. {pages[page_number]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bb2c1ba808daed",
   "metadata": {},
   "source": [
    "## 2.4 Getting the metadata of each page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7753b375ca7e8f31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:40:03.757120Z",
     "start_time": "2025-02-10T16:40:03.738343Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "653b637d584a4fa7a133c62a0b16576f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/641 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pages_and_metadata = list()\n",
    "for page_number, page in tqdm(pages.items(), total=len(pages)):\n",
    "    metadata = dict()\n",
    "    metadata['page_number'] = page_number\n",
    "    metadata['raw_text'] = page\n",
    "    metadata['number_of_characters'] = len(page)\n",
    "    metadata['number_of_tokens'] = len(page)/4\n",
    "    metadata['number_of_words'] = len(page.split())\n",
    "    pages_and_metadata.append(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdf53efcf6db14a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:40:04.656908Z",
     "start_time": "2025-02-10T16:40:04.643066Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pages_and_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fda5bbeb07d871b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:40:06.939020Z",
     "start_time": "2025-02-10T16:40:06.915569Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>number_of_characters</th>\n",
       "      <th>number_of_tokens</th>\n",
       "      <th>number_of_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>641.00</td>\n",
       "      <td>641.00</td>\n",
       "      <td>641.00</td>\n",
       "      <td>641.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>320.00</td>\n",
       "      <td>1604.09</td>\n",
       "      <td>401.02</td>\n",
       "      <td>263.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>185.19</td>\n",
       "      <td>811.66</td>\n",
       "      <td>202.91</td>\n",
       "      <td>136.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>160.00</td>\n",
       "      <td>1094.00</td>\n",
       "      <td>273.50</td>\n",
       "      <td>180.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>320.00</td>\n",
       "      <td>1601.00</td>\n",
       "      <td>400.25</td>\n",
       "      <td>260.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>480.00</td>\n",
       "      <td>2128.00</td>\n",
       "      <td>532.00</td>\n",
       "      <td>347.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>640.00</td>\n",
       "      <td>3653.00</td>\n",
       "      <td>913.25</td>\n",
       "      <td>843.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  number_of_characters  number_of_tokens  number_of_words\n",
       "count       641.00                641.00            641.00           641.00\n",
       "mean        320.00               1604.09            401.02           263.04\n",
       "std         185.19                811.66            202.91           136.50\n",
       "min           0.00                  3.00              0.75             1.00\n",
       "25%         160.00               1094.00            273.50           180.00\n",
       "50%         320.00               1601.00            400.25           260.00\n",
       "75%         480.00               2128.00            532.00           347.00\n",
       "max         640.00               3653.00            913.25           843.00"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d6f59c1ce38342",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:40:08.516641Z",
     "start_time": "2025-02-10T16:40:08.511511Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of tokens this model is being trained on are: 0.26 million tokens\n"
     ]
    }
   ],
   "source": [
    "print(f\"The number of tokens this model is being trained on are: {df[\"number_of_tokens\"].sum()/1000000:.2f} million tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea25bc7daf4f49bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:40:09.361819Z",
     "start_time": "2025-02-10T16:40:09.356518Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of pages in the database are: 641\n"
     ]
    }
   ],
   "source": [
    "print(f\"The number of pages in the database are: {len(pages_and_metadata)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb4a2f984a78d18",
   "metadata": {},
   "source": [
    "## 2.4 Preprocessing the `raw_text` from metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8103461a031b8c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:41:53.870665Z",
     "start_time": "2025-02-10T16:41:53.856349Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_to_lowercase(text):\n",
    "    new_text = text.lower()\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "522849a9bcea61aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:41:54.463014Z",
     "start_time": "2025-02-10T16:41:54.459723Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    new_text = []\n",
    "    for word in text.split():\n",
    "        if word not in STOP_WORDS:\n",
    "            new_text.append(word)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c2f5226953fb7ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:41:54.848133Z",
     "start_time": "2025-02-10T16:41:54.842801Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "    new_text = re.sub(r\"<!--.*?-->\", \"\", text)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1d03d45cc290e34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:41:55.205639Z",
     "start_time": "2025-02-10T16:41:55.202443Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_newlines(text):\n",
    "    new_text = re.sub(r\"\\n+\", \" \", text)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be43849bb32f99de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:41:55.666352Z",
     "start_time": "2025-02-10T16:41:55.662230Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_multiple_spaces(text):\n",
    "    new_text = text.replace(\"  \", \" \")\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3d5cde9e9b9d9ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:41:56.144254Z",
     "start_time": "2025-02-10T16:41:56.139212Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_comments(text):\n",
    "    new_text = re.sub(r\"<!--.*?-->\", \"\", text)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed85a9aa-2a62-472e-8f54-b32283309103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unnecessary_text(text):\n",
    "    new_text = text.replace(\"answer:\",\"\").replace(\"question\", \"\").replace(\":\",\"\").replace(\"  \",\" \")\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce97b989463a7c01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:41:56.936299Z",
     "start_time": "2025-02-10T16:41:56.922968Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = convert_to_lowercase(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = remove_html_tags(text)\n",
    "    text = remove_newlines(text)\n",
    "    text = remove_multiple_spaces(text)\n",
    "    text = remove_comments(text)\n",
    "    text = remove_unnecessary_text(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2241967daa9316b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:41:57.858803Z",
     "start_time": "2025-02-10T16:41:57.814634Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08e1548ed33448ab9255615094bb9d3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/641 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for page in tqdm(pages_and_metadata, total=len(pages_and_metadata)):\n",
    "    page[\"formatted_text\"] = preprocess_text(page[\"raw_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d92a1fdb3f006122",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:41:59.743703Z",
     "start_time": "2025-02-10T16:41:59.720627Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41aaea503be84a86b2ad7a0cf92d8cc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/641 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data science interview s pdf 1. meant selection bias? selection bias type error arises researcher decides going conduct study. happens selection participants takes place randomly. selection bias referred selection effect. works effectively selection bias taken account, conclusions study wrong. 2. boltzmann machine? boltzmann developed simple learning algorithms allow find important information presented complex regularities data. machines generally optimize quantity weights given problem. learning program works slow networks layers feature detectors. consider restricted boltzmann machines, single algorithm feature detectors faster compared others. 3. difference cluster systematic sampling? cluster sampling technique difficult study target population spread wide area simple random sampling applied. cluster sample probability sample sampling unit collection cluster elements. systematic sampling statistical technique elements selected ordered sampling frame. systematic sampling, list progressed circular manner reach end list, progressed again. best example systematic sampling equal probability method. 4. law large numbers? theorem describes result performing experiment large number times. theorem forms basis frequency-style thinking. says sample means, sample variance sample standard deviation converge trying estimate. 5. eigenvectors eigenvalues? eigenvectors understanding linear transformations. data analysis, usually calculate eigenvectors correlation covariance matrix. eigenvectors directions particular linear transformation acts flipping, compressing stretching. eigenvalue referred strength transformation direction eigenvector factor compression occurs. 6. cite examples false positive false negatives equally important? banking industry giving loans primary source making money time repayment rate good profit, risk huge losses. banks don’t want lose good customers point time, don’t want acquire bad customers. scenario, false positives false negatives important measure.\n",
      "\n",
      "7. logistic regression? state example logistic regression recently. logistic regression referred logit model technique predict binary outcome linear combination predictor variables. example, want predict particular political leader win election not. case, outcome prediction binary i.e. 0 1 (win/lose). predictor variables money spent election campaigning particular candidate, time spent campaigning, etc. 8. role activation function? activation function introduce non-linearity neural network helping learn complex function. neural network able learn linear function linear combination input data. activation function function artificial neuron delivers output based inputs. 9. mean cluster sampling systematic sampling? studying target population spread wide area difficult applying simple random sampling ineffective, technique cluster sampling used. cluster sample probability sample, sampling units collection cluster elements. following technique systematic sampling, elements chosen ordered sampling frame. list advanced circular fashion. way end list reached, progressed start, top, again. 10. explain gradient descent? degree change output function relating changes inputs known gradient. measures change weights respect change error. gradient comprehended slope function. gradient descent refers escalating valley. simply, consider opposed climbing hill. minimization algorithm meant minimizing given activation function. 11. know autoencoders? autoencoders simplistic learning networks transforming inputs outputs minimum possible error. means outputs resulted close inputs. couple layers added input output size layer smaller size pertaining input layer. autoencoder receives unlabeled input encoded reconstructing output. 12. methods data visualizations effectively used? addition giving insights effective efficient manner, data visualization way restricted bar, line stereotypic graphs. data represented visually pleasing manner. thing taken care convey intended insight finding correctly audience. baseline set. innovative creative help come\n",
      "\n",
      "better looking functional dashboards. fine line simple insightful dashboard awesome looking 0 fruitful insight dashboards. 13. common perception visualization? people think visualization charts summary information. drive business lot underlying principles. learning design principles help build effective efficient visualizations tableau prep tool drastically increase time focusing important part. issue tableau is, paid companies need pay leveraging awesome tool. 14. seek help case discrepancies tableau? face issue tableau, try searching tableau community forum. best places queries answered. write query answered hour day. post linkedin follow people. 15. data cleaning essential data science? data cleaning important data science end results outcomes data analysis come existing data useless unimportant need cleaned periodically required. ensures data reliability & accuracy memory freed up. data cleaning reduces data redundancy gives good results data analysis large customer information exists cleaned periodically. businesses like e-commerce, retail, government organizations contain large customer transaction information outdated needs cleaned. depending size data, suitable tools methods clean data database big data environment. different types data existing data source dirty data, clean data, mixed clean dirty data sample clean data. modern data science applications rely machine learning model learner learns existing data. so, existing data cleanly maintained sophisticated good outcomes optimization system. 16. a/b testing data science? answers a/b testing called bucket testing split testing. method comparing testing versions systems applications determine version application performs better. important cases multiple versions shown customers end-users order achieve goals. area data science, a/b testing know variable existing variables order optimize increase outcome goal. a/b testing called design experiment. testing helps establishing cause effect relationship independent dependent variables. testing simply combination design experimentation statistical inference. significance, randomization multiple comparisons key elements a/b testing.\n",
      "\n",
      "significance term significance statistical tests conducted. randomization core component experimental design variables balanced. multiple comparisons way comparing variables case customer interests causes false positives resulting requirement correction confidence level seller area e-commerce. 17. machine learning deployed real world scenarios? scenarios machine learning finds applications real world ecommerce understanding customer churn, deploying targeted advertising, remarketing. search engine ranking pages depending personal preferences searcher finance evaluating investment opportunities & risks, detecting fraudulent transactions medicare designing drugs depending patient’s history needs robotics machine learning handling situations ordinary social media understanding relationships recommending connections extraction information framing s getting answers databases web. 18. power analysis? power analysis vital experimental design. involved process determining sample size needed detecting effect given size cause certain degree assurance. lets deploy specific probability sample size constraint. techniques statistical power analysis sample size estimation widely deployed making statistical judgment accurate evaluates size needed experimental effects practice. power analysis lets understand sample size estimate high low. low sample size authentication provide reliable answers large wastage resources. 19. k-means? select k k-means? k-means clustering termed basic unsupervised learning algorithm. method classifying data certain set clusters called k clusters. deployed grouping data order find similarity data. includes defining k centers, cluster. clusters defined k groups k predefined. k points selected random cluster centers. objects assigned nearest cluster center. objects cluster closely related possible differ possible objects clusters. k-means clustering works large sets data. 20. resampling done? resampling cases estimating accuracy sample statistics subsets accessible data drawing randomly replacement set data points\n",
      "\n",
      "substituting labels data points performing significance tests validating models random subsets (bootstrapping, cross-validation) 21. tools devices help succeed role data scientist? ’s purpose learn programming languages applications candidate knows experience using. answer candidate’s need additional training basic programming languages platforms transferable skills. vital understand cost time money train candidate knowledgeable languages applications required position. 22. want work company data scientist? purpose determine motivation candidate’s choice applying interviewing position. answer reveal inspiration working company drive data scientist. candidate pursuing position passionate data believe company, elements determine candidate’s performance. answers look include interest data mining respect company’s innovative practices desire apply analytical skills solve real-world issues data “i passion working data-driven, innovative companies. firm uses advanced technology address everyday problems consumers businesses alike, admire. enjoy solving issues analytical approach passionate incorporating technology work. believe skills passion match company’s drive capabilities.” 23. differences overfitting underfitting? statistics machine learning, common tasks fit model set training data, able reliable predictions general untrained data. overfitting, statistical model describes random error noise instead underlying relationship. overfitting occurs model excessively complex, having parameters relative number observations. model overfitting poor predictive performance, overreacts minor fluctuations training data. underfitting occurs statistical model machine learning algorithm capture underlying trend data. underfitting occur, example, fitting linear model non-linear data. model poor predictive performance. 24. machine learning? machine learning explores study construction algorithms learn predictions data. closely related computational statistics. devise complex models algorithms lend prediction commercial use known predictive analytics. 25. enumerate differences supervised unsupervised learning?\n",
      "\n",
      " supervised learning type machine learning function inferred labeled training data. training data contains set training examples. unsupervised learning, hand, type machine learning inferences drawn datasets containing input data labeled responses. following differences types machine learning algorithms – supervised learning makes use decision trees, k-nearest neighbor algorithm, neural networks, regression, support vector machines. unsupervised learning uses anomaly detection, clustering, latent variable models, neural networks. enables – supervised learning enables classification regression, unsupervised learning enables classification, dimension reduction, density estimation use – supervised learning prediction, unsupervised learning finds use analysis 26. underfitting? prediction rate provides low prediction training error test error leads high business problem, error rate training set high error rate test set high, conclude overfitting model. 27. understand problems faced data analysis? problem faced hands-on analysis data science poor understanding problem hand concentrating tools, end results aspects project. breaking problem granular level understanding takes lot time practice master. coming square data science projects seen lot companies project kaggle problems. 28. sas stand best data analytics tools? ease understand provisions included sas remarkably easy learn. further, offers suitable option aware sql. hand, r comes steep training cover supposed low-level programming style. data handling capacities par leading tool includes r& python. advances handling huge data, best platform engage graphical capacities comes functional graphical capacities limited knowledge field. useful customize plots better tool management benefits release updates regards controlled conditions. main reason tested. considered r&python, open contribution risk errors current development high.\n",
      "\n",
      "29. best programming language use data science? data science handled programming languages like python r programming language. popular languages data scientists data analysts. r python open source free use came existence 1990s. python r different advantages depending applications required business goal. python better cases repeated tasks jobs data manipulations r programming querying retrieving datasets customized data analysis. python preferred types data science applications time r programming preferred cases high complex data applications. python easier learn learning curve r deep learning curve. python preferred cases general-purpose programming language found applications data science too. r seen data science area data analysis standalone servers computing separately. 30. linear regression data science? frequently asked data science interview s interview. linear regression technique supervised machine learning algorithmic process area data science. method predictive analysis. predictive analytics area statistical sciences existing information extracted processed predict trends outcomes pattern. core subject lies analysis existing context predict unknown event. process linear regression method predict variable called target variable making best relationship dependent variable independent variable. dependent variable outcome variable response variable independent variable predictor variable explanatory variable. example real life, depending expenses occurred financial year monthly expenses, predictions happen calculating approximate upcoming months financial years expenses. method, implementation python programming technique important method machine learning technique area data science. linear regression called regression analysis comes area statistical sciences integrated data science. 31. recommender system? recommender system today widely deployed multiple fields like movie recommendations, music preferences, social tags, research articles, search queries on. recommender systems work collaborative content-based filtering deploying personality-based approach. type system works based person’s past behavior order build model future. predict future product buying, movie viewing book reading people. creates filtering\n",
      "\n",
      "approach discrete characteristics items recommending additional items. 32. data scientists use statistics? statistics help data scientists look data patterns, hidden insights convert big data big insights. helps better idea customers expecting. data scientists learn consumer behavior, interest, engagement, retention finally conversion power insightful statistics. helps build powerful data models order validate certain inferences predictions. converted powerful business proposition giving users want precisely want it. 33. understand term normal distribution? set continuous variable spread normal curve shape bell curve. considered continuous probability distribution useful statistics. common distribution curve useful analyze variables relationships normal distribution curve. normal distribution curve symmetrical. non-normal distribution approaches normal distribution size samples increases. easy deploy central limit theorem. method helps sense data random creating order interpreting results bell-shaped graph. 34. collaborative filtering? filtering process recommender systems find patterns information numerous data sources, agents, collaborating perspectives. words, collaborative method process making automatic predictions human preferences interests. 35. explain difference overfitting underfitting? machine learning statistics, common task undergo fit model set training data. helps making reliable predictions general untrained data. overfitting, statistical model help letting know random noise errors instead underlying relationship. overfitting comes light data associated complexity, means associated parameters relative number observations. model overfitted performed poor predictive performance acts overly minor fluctuations training data. unnderfittinng happens machine learning algorithm statistical model unable focus underlying insights data. case trying fix linear model nonlinear one. kind model result poor predictive performance. 36. systematic sampling? systematic sampling technique, resembles follows systematic way, samples selected ordered sampling frame. systematic sampling, list actually circular manner selection starts end reaches final, cycle goes on. equal probability method best example systematic sampling.\n",
      "\n",
      "37. recommender systems? recommender systems treated information filtering systems work predict likeness user product. recommender systems widely areas like news, movies, social tags, music, products, etc. movie recommenders netflix, imdb, & bookmyshow, product recommender e-commerce sites like ebay, amazon, flipcart, youtube video recommendations, game recommendations. 38. artificial neural networks? artificial neural networks main elements machine learning popular. neural networks developed based functionality human brain. artificial neural networks trained learn examples experiences programmed explicitly. artificial neural networks work based nodes called artificial neurons connected another. connection acts similar synapses human brain helps transmitting signals artificial neurons. 39. explain role activation function? activation function helps introducing nonlinearity neural network enables neural network learn complex functions. this, challenging linear function analyze complex data. activation function function artificial neuron delivers output based input given. 40. difference supervised learning unsupervised learning? algorithm learns training data knowledge applied test data, referred supervised learning. classification example supervised learning. algorithm learn response variable training data, referred unsupervised learning. clustering example unsupervised learning. 41. central limit theorem important? “suppose interested estimating average height people. collecting data person world impossible. can’t obtain height measurement population, sample people. becomes, average height entire population given single sample. 42. feature vectors? feature vector n-dimensional vector numerical features represent object. machine learning, feature vectors represent numeric symbolic characteristics, called features, object mathematical, easily analyzable way. 43. cluster sampling? cluster sampling technique difficult study target population spread wide area simple random sampling applied. cluster sample probability sample sampling unit collection cluster elements. eg., researcher wants survey academic performance high school students japan. divide entire population japan different clusters\n",
      "\n",
      "(cities). researcher selects number clusters depending research simple systematic random sampling. 44. steps involved analytics project? following steps involved analytics project understand business problem explore data familiar it. prepare data modeling detecting outliers, treating missing values, transforming variables, etc. data preparation, start running model, analyze result tweak approach. iterative step best possible outcome achieved. validate model new data set. start implementing model track result analyze performance model period time. 45. explain eigenvectors eigenvalues? eigenvectors help understanding linear transformations. calculated typically correlation covariance matrix data analysis. words, eigenvectors directions particular linear transformation acts compressing, flipping, stretching. eigenvalues understood strengths transformation direction eigenvectors factors compressions happens. 46. outlier values treat them? outlier values, simply outliers, data points statistics don’t belong certain population. outlier value abnormal observation different values belonging set. identification outlier values univariate graphical analysis method. outlier values assessed individually assessing large set outlier values require substitution 99th 1st percentile values. popular ways treating outlier values change value brought range simply remove value note extreme values outlier values. 47. choose right chart case creating viz? right chart represent data key aspects data visualization design principle. options choose deciding chart. fixing right chart comes experience, practice deep understanding end-user needs. dictates dashboard. 48. basic responsibility data scientist? data scientist, responsibility complex things simple context understand, trying convey. moment, start explaining simple things mission making complex simple goes away. happens lot data visualization. more. pushing information readers brain, need figure easily help consume dashboard chart.\n",
      "\n",
      "process simple difficult implement. bring complex business value self-explanatory chart. it’s skill data scientist strive good arsenal. 49. difference machine learning vs data mining? data mining working unlimited data extract level unusual unknown patterns identified. machine learning method study closely relates design, development concerning algorithms provide ability certain computers capacity learn. 50. types biases occur sampling? simple models selection bias described below. undercoverage occurs members population live badly represented inside sample. … survey relied service unit, drawn telephone directories car registration lists. • selection bias • coverage bias • survivorship bias 51. data cleaning plays vital role analysis? cleaning data multiple sources transform format data analysts data scientists work cumbersome process – number data sources increases, time clean data increases exponentially number sources volume data generated sources. 80% time cleaning data making critical analysis task. 52. eigenvalue eigenvector? eigenvectors understanding linear transformations. data analysis, usually calculate eigenvectors correlation covariance matrix. eigenvectors directions particular linear transformation acts flipping, compressing stretching. eigenvalue referred strength transformation direction eigenvector factor compression occurs. 53. define key performance indicators product playing product, think this key metrics product want optimize? data scientist’s role certain companies involves working closely product teams help define, measure, report metrics. exercise home, help interview process 54. data cleaning important analysis? knowledge-based relatively simple answer. data scientist’s time goes cleaning data – data gets bigger, time takes clean. cleaning right foundation analysis, time takes clean data, alone, makes important. 55. prefer python r text analytics? here, you’re asked insert opinion. however, data scientists agree right opinion python. python pandas\n",
      "\n",
      "library strong data analysis tools easy-to-use structure. what’s more, python typically faster text analytics. 56. explain star schema? traditional database schema central table. satellite tables map id’s physical description connected central fact table id fields; tables known lookup tables, principally useful real- time applications, save lot memory. star schemas involve layers summarization recover information faster. 57. mean word data science? data science extraction knowledge large volumes data structured unstructured, continuation field data mining predictive analytics, known knowledge discovery data mining. 58. understand term hash table collisions? hash table (hash map) kind data structure implement associative array, structure map keys values. ideally, hash function assign key unique bucket, possible keys generate identical hash causing keys point bucket. known hash collisions. 59. assess good logistic model? methods assess results logistic regression analysis- classification matrix look true negatives false positives. concordance helps identify ability logistic model differentiate event happening happening. lift helps assess logistic model comparing random selection. 60. want work data scientist?  plays definition data science. however, recruiters looking understand you’ll contribute you’ll gain field. focus makes path data scientist unique – mentor preferred method data extraction. 61. overcome barrier finding solution?  directly asks draw experiences ability problem-solve. data scientists are, all, numbers-based problem-solvers, so, it’s important determine example problem you’ve solved ahead time. it’s re-cleaning data different program, able explain process recruiter. 62. work random forest? underlying principle technique weak learners combined provide strong learner. steps involved build decision trees bootstrapped training samples data tree, time split considered, random sample mm predictors chosen split candidates, pp predictors\n",
      "\n",
      "rule thumb split m=p√m=p predictions majority rule. 63. explain cross-validation? model validation technique evaluating outcomes statistical analysis generalize independent data set. mainly backgrounds objective forecast wants estimate accurately model accomplish practice. goal cross-validation term data set test model training phase (i.e. validation data set) order limit problems like overfitting insight model generalize independent data set. 64. linear regression? linear regression statistical technique score variable y predicted score second variable x. x referred predictor variable y criterion variable. 65. explain difference test set validation set? validation set considered training set parameter selection avoid overfitting model built. hand, test set testing evaluating performance trained machine learning model. simple terms, differences summarized as- training set fit parameters i.e. weights. test set assess performance model i.e. evaluating predictive power generalization. 66. define data science?  allows interviewer are. example, what’s favorite process, what’s impactful project you’ve worked on? focus data science – means extracting insights numbers – explain makes personal. 67. devices tools help data scientist? asking , recruiters seeking learn qualifications. explain utilize coding language know, r sql, language helps complete certain tasks. opportunity explain education methods beyond. 68. algorithm updated? quasi-trick specific time-based answer. algorithm updated underlying data changing want model evolve time. understanding outcomes dynamic algorithms key answering confidence. 69. python r – prefer text analytics? best possible answer python pandas library provides easy use data structures high-performance data analysis tools.\n",
      "\n",
      "70. auto-encoder? auto-encoders learning networks work transforming inputs outputs errors minimized error. means output close input. add layers input output sizes layers smaller input layer. actually, auto-encoder provided unlabelled input transmitted reconstructing input. 71. propagation? backpropagation algorithm deep learning train multilayer neural network. method, error form end network inside it, brings efficient computation gradient. consists below-mentioned steps forward data propagation data training derivatives computed help output target. backpropagation computing derivative error. use output previously calculated output. update weights. 72. outlier values treated? identify outlier values graphical analysis method univariate method. easier assessed individually outlier values outlier values number values required substituted 1st 99th percentile values. common ways treat outlier values. bring change value remove value 73. explain difference univariate, bivariate multivariate analysis? univariate analysis descriptive analysis differentiate number variables involved given point time. instance, sales particular territory include variable, treated univariate analysis. bivariate analysis understand difference variables given time scatter pilot. best example bivariate analysis difference sale expenses happens particular product. multivariate analysis understand variables responses variables. 74. makes difference “long” “wide” format data? wide format method, subject, repeated responses recorded single row, recorded response separate column. comes long format data, row acts one-time point subject. wide format, columns generally divided groups long-form rows divided groups. 75. different selection biases, yes, they? sampling bias bias arises select particular people non-random selection samples happened. general terms, selection majority people belong group.\n",
      "\n",
      "time interval trial terminated earlier actual time (probably ethical reasons) extreme value finally taken consideration significant value variables similar mean. data data bias separate set data taken support conclusion eliminates terrible data based arbitrary grounds, instead generally relying generally stated criteria. attrition bias attrition bias defined error occurs unequal loss participants randomized controlled trial (rct). 76. meant supervised unsupervised learning data? supervised learning supervised learning process training machines labeled right kind data. supervised learning, machine uses labeled data base answer. unsupervised learning form training machines information unlabeled unstructured one. unlike supervised learning, special teacher predefined data machine quickly learn from. 77. data science? data science defined multidisciplinary subject extract meaningful insights different types data employing scientific methods scientific processes algorithms. data science helps solving analytically complex problems simplified way. acts stream utilize raw data generate business value. 78. cross-validation? model validation technique evaluate statistical analysis generalize independent dataset. helpful areas backgrounds objective exactly forecasted, people want estimate accurately model work real-time. main ambition cross-validation test model test model training phase limit problems like overfitting insights generalize independent data set. 79. outlier values treated? identify outlier values graphical analysis method univariate method. easier assessed individually outlier values outlier values number values required substituted 1st 99th percentile values. common ways treat outlier values. bring change value remove value 80. list variants backpropagation? mentioned different variants backpropagation stochastic gradient descent module, help single training example updating parameters calculation gradient. batch gradient descent backpropagation method, consider data calculating gradient executes update iteration. mini-batch gradient descent considered popular optimization algorithm\n",
      "\n",
      "deep learning. mini-batch gradient descent instead single training example, mini-batch samples used. 81. boltzmann machine? boltzmann developed simple learning algorithms allow find important information presented complex regularities data. machines generally optimize quantity weights given problem. learning program works slow networks layers feature detectors. consider restricted boltzmann machines, single algorithm feature detectors faster compared others. 82. gradient descent methods times converge similar point? no, cases reach local minimum local optima point. reach global optima point. governed data starting conditions. 83. eigenvalue eigenvector? eigenvectors understanding linear transformations. data analysis, usually calculate eigenvectors correlation covariance matrix. eigenvalues directions particular linear transformation acts flipping, compressing stretching. 84. selection bias? selection bias kind error occurs researcher decides going studied. usually associated research selection participants isn’t random. referred selection effect. distortion statistical analysis, resulting method collecting samples. selection bias taken account, conclusions study accurate. types selection bias include sampling bias systematic error non-random sample population causing members population likely included resulting biased sample. time interval trial terminated early extreme value (often ethical reasons), extreme value likely reached variable largest variance, variables similar mean. data specific subsets data chosen support conclusion rejection bad data arbitrary grounds, instead according previously stated generally agreed criteria. attrition attrition bias kind selection bias caused attrition (loss participants) discounting trial subjects/tests run completion. 85. data cleaning plays vital role analysis? data cleaning help analysis because cleaning data multiple sources helps transform format data analysts data scientists work with. data cleaning helps increase accuracy model machine learning. cumbersome process number data sources increases, time taken clean data increases exponentially number sources volume data generated sources.\n",
      "\n",
      "80% time cleaning data making critical analysis task. 86. explain difference validation set test set? validation set considered training set parameter selection avoid overfitting model built. hand, test set testing evaluating performance trained machine learning model. simple terms, differences summarized as; training set fit parameters i.e. weights test set assess performance model i.e. evaluating predictive power generalization. 87. mean deep learning popular now? deep learning paradigm machine learning shown incredible promise recent years. fact deep learning shows great analogy functioning human brain. deep learning years, major breakthroughs techniques came recent years. main reasons increase data generated sources growth hardware resources required run models gpus multiple times faster help build bigger deeper deep learning models comparatively time required previously. 88. variants propagation? stochastic gradient descent use single training example calculation gradient update parameters. batch gradient descent calculate gradient dataset perform update iteration. mini-batch gradient descent it’s popular optimization algorithms. it’s variant stochastic gradient descent instead single training example, mini-batch samples used. 89. explain role data cleaning data analysis. data cleaning daunting task fact increase number data sources, time required cleaning data increases exponential rate. vast volume data generated additional sources. also, data cleaning solely 80% total time required carrying data analysis task. nevertheless, reasons data cleaning data analysis. important ones are cleaning data different sources helps transforming data format easy work data cleaning increases accuracy machine learning model\n",
      "\n",
      "90. understand linear regression logistic regression? linear regression form statistical technique score variable y predicted basis score second variable x, referred predictor variable. y variable known criterion variable. known logit model, logistic regression statistical technique predicting binary outcome linear combination predictor variables. 91. understand deep learning? deep learning paradigm machine learning displays great degree analogy functioning human brain. neural network method based convolutional neural networks (cnn). deep learning wide array uses, ranging social network filtering medical image analysis speech recognition. deep learning present long time, it’s recently gained worldwide acclaim. mainly to increase data generation sources growth hardware resources required running deep learning models caffe, chainer, keras, microsoft cognitive toolkit, pytorch, tensorflow popular deep learning frameworks today. 92. overfitting? prediction rate high inconsistency training error test error leads ta high business problem, error rate training set low error rate ithe n test set high, conclude overfitting model. 93. advantages tableau prep? tableau prep reduce lot time like parent software (tableau) creating impressive visualizations. tool lot potentials taking professionals data cleaning, merging step creating final usable data linked tableau desktop getting visualization business insights. lot manual tasks reduced time better findings insights. 94. 3d plots/visualizations numpy/scipy? like 2d plotting, 3d graphics scope numpy scipy, 2d example, packages exist integrate numpy. matplotlib provides primary 3d plotting mplot3d subpackage, mayavi produces wide range high-quality 3d visualization features, utilizing powerful vtk engine. 95. compare sas, r, python programming? sas widely analytics tools biggest companies earth. best statistical functions, graphical user interface, come price tag readily adopted smaller enterprises r best r open source tool generously academia research community. robust tool statistical computation, graphical representation, reporting. open source nature, updated latest features readily available everybody.\n",
      "\n",
      "python python powerful open source programming language easy learn, works tools technologies. best python innumerable libraries community created modules making robust. functions statistical operation, 96. describe univariate, bivariate multivariate analysis? suggests analysis methodologies having single, double multiple variables. univariate analysis variable this, relationships, causes. major aspect univariate analysis summarize data find patterns actionable decisions. bivariate analysis deals relationship sets data. sets paired data come related sources, samples. tools analyze data including chi-squared tests t-tests data having correlation. data quantified analyzed graph plot scatterplot. strength correlation data sets tested bivariate analysis. 97. interpolation extrapolation? terms interpolation extrapolation extremely important statistical analysis. extrapolation determination estimation known set values facts extending taking area region unknown. technique inferring data available. interpolation, hand, method determining certain value falls certain set values sequence values. especially useful data extremities certain region don’t data points specific point. deploy interpolation determine value need. 98. data modeling different database design? data modeling considered step design database. data modeling creates conceptual model based relationship data models. process involves moving conceptual stage logical model physical schema. involves systematic method applying data modeling techniques. database design process designing database. database design creates output detailed data model database. strictly speaking, database design includes detailed logical model database include physical design choices storage parameters. 99. differentiate data modeling database design? data modeling – data modeling (or modeling) software engineering process creating data model information system applying formal data modeling techniques.\n",
      "\n",
      "database design– database design system producing detailed data model database. term database design describe different parts design overall database system. 100. selection bias matter? selection bias product inadequately improperly randomized data leading data sets representative whole. interview, express importance terms effect solution. data representative, solutions likely either. 101. differentiate univariate, bivariate multivariate analysis? univariate analyses descriptive statistical analysis techniques differentiated based number variables involved given point time. example, pie charts sales based territory involve variable analysis referred univariate analysis. bivariate analysis attempts understand difference variables time scatterplot. example, analyzing volume sale spending considered example bivariate analysis. multivariate analysis deals study variables understand effect variables responses. 102. cite examples false negative important false positive? 1 assume airport ‘a’ received high-security threats based certain characteristics identify particular passenger threat not. shortage staff, decide scan passengers predicted risk positives predictive model. happen true threat customer flagged non-threat airport model? 2 jury judge decides criminal free? 3 rejected marry good person based predictive model happen meet him/her years realize false negative? 103. describe structure artificial neural networks? artificial neural networks works principle biological neural network. consists inputs processed weighted sums bias, help activation functions. 104. understand selection bias? types? selection bias typically associated research doesn’t random selection participants. type error occurs researcher decides going studied. occasions, selection bias referred selection effect. words, selection bias distortion statistical analysis results sample collecting method. selection bias taken account, conclusions research study accurate. following types selection bias\n",
      "\n",
      "sampling bias systematic error resulting non-random sample populace causing certain members likely included results biased sample. time interval – trial ended extreme value, usually ethical reasons, extreme value likely reached variable variance, variables similar mean. data – results specific data subsets selected supporting conclusion rejection bad data arbitrarily. attrition – caused attrition, i.e. loss participants, discounting trial subjects tests didn’t run completion. 105. explain recommender systems application? recommender systems subclass information filtering systems, meant predicting preferences ratings awarded user product. application recommender system product recommendations section amazon. section contains items based user’s search history past orders. 106. explain define number clusters clustering algorithm? primary objective clustering group similar identities way entities group similar other, groups remain different another. generally, sum squares explaining homogeneity cluster. defining number clusters clustering algorithm, wss plotted range pertaining number clusters. resultant graph known elbow curve. elbow curve graph contains point represents point post aren’t decrements wss. known bending point represents k k–means. aforementioned widely-used approach, important approach hierarchical clustering. approach, dendrograms created distinct groups identified there. 106. types machine learning? • supervised learning • unsupervised learning • reinforcement learning 107. random forest? random forest versatile method machine learning performs classification regression tasks. helps areas like treats missing values, dimensionality reduction, outlier values. like gathering weak modules comes form robust model 108. reinforcement learning? reinforcement learning maps situations map actions. end result reinforcement learning maximize numerical reward signal. learner defined action instead\n",
      "\n",
      "discover actions maximum reward. reinforcement learning developed learning process human beings. works based reward/penalty mechanism. 109. p-value signify statistical data? p-value determine significance results hypothesis test statistics. p-value helps readers draw conclusions 0 1. p-value – 0.05 denotes weak evidence null hypothesis means null hypothesis rejected. p-value -0.05 denotes strong evidence null hypothesis means null hypothesis rejected. p-value -0.05is marginal value indicating possible way. hands-on experience interviews free access solved code example. 110. example data set non-gaussian distribution? gaussian distribution exponential family distributions, lot them, sort ease use, cases, person machine learning solid grounding statistics, utilized appropriate 111. regularly algorithm updated? want update algorithm when want model evolve data streams infrastructure underlying data source changing case non-stationarity planning data science certification r – programming? here’re 100 data science foundations s. free practice test know stand. 112. prior experience prepared role data science?  helps determine candidate’s experience holistic perspective reveals experience demonstrating interpersonal, communication technical skills. important understand data scientists able communicate findings, work team environment skills perform task. 113. unsupervised learning? unsupervised learning type machine learning algorithm draw inferences datasets consisting input data labeled responses. algorithms clustering, anomaly detection, neural networks, latent variable models data science mock interviews interviews industry expertspersonalized detailed interview feedback access exclusive curated content e.g. example, fruit clustering categorize “fruits soft skin lots dimples”, “fruits shiny hard skin” “elongated yellow fruits”.\n",
      "\n",
      "114. draw comparison overfitting underfitting? order reliable predictions general untrained data machine learning statistics, required fit model set training data. overfitting underfitting common modeling errors occur so. following differences overfitting underfitting definition – statistical model suffering overfitting describes random error noise place underlying relationship. underfitting occurs, statistical model machine learning algorithm fails capturing underlying trend data. occurrence – statistical model machine learning algorithm excessively complex, result overfitting. example complex model having parameters compared total number observations. underfitting occurs trying fit linear model non-linear data. poor predictive performance – overfitting underfitting yield poor predictive performance, way different. overfitted model overreacts minor fluctuations training data, underfit model under-reacts bigger fluctuations. 115. compare validation set test set? validation set training set parameter selection avoiding overfitting machine learning model developed. contrary, test set meant evaluating testing performance trained machine learning model. 116. explain concept boltzmann machine. boltzmann machine features simple learning algorithm enables discover fascinating features representing complex regularities present training data. basically optimizing quantity weight given problem. simple learning algorithm involved boltzmann machine slow networks layers feature detectors. 117. time series algorithms? time series algorithms like arima, arimax, sarima, holts winters interesting learn use solve lot complex problems businesses. data preparation time series analysis plays vital role. stationarity, seasonality, cycles, noises need time attention. time like data right. run model it. 118. companies heavily investing money time dashboards. why? stakeholders aware business data. working visualization projects helps develop key skills data scientist possess i.e. thinking shoes end-user. you’re learning visualization tool, download dataset kaggle. building charts graphs dashboard step. research domain think kpis like dashboard you’re going end-user. start building dashboard piece piece.\n",
      "\n",
      "119. explain benefits r language? r programming language includes set software suite graphical representation, statistical computing, data manipulation, calculation. highlights r programming environment include following extensive collection tools data analysis operators performing calculations matrix array data analysis technique graphical representation highly developed simple effective programming language extensively supports machine learning applications acts connecting link software, tools, datasets create high-quality reproducible analysis flexible powerful provides robust package ecosystem diverse needs useful solve data-oriented problem 120. data cleansing important data analysis? data coming multiple sources important ensure data good analysis. data cleansing extremely vital. data cleansing extensively deals process detecting correcting data records, ensuring data complete accurate components data irrelevant deleted modified needs. process deployed concurrence data wrangling batch processing. data cleaned confirms rules data sets system. data cleansing essential data science data prone error human negligence, corruption transmission storage things. data cleansing takes huge chunk time effort data scientist multiple sources data emanates speed comes.\n",
      "\n",
      "deep learning interview s know 1.3k views kurt updated 22,2019 deep learning hottest topics 2018-19 good reason. advancements industry time come machines computer programs actually replacing humans. arti+cial intelligence going create 2.3 million jobs 2020 crack job interview come set deep learning interview s. divided article sections basic deep learning interview s advance deep learning interview s basics deep learning interview s q1. differentiate ai, machine learning deep learning. artificial intelligence technique enables machines mimic human behavior. machine learning subset ai technique uses statistical methods enable machines improve experience. deep learning subset ml computation multi-layer neural network feasible. uses neural networks simulate human-like decision making. q2. think deep learning better machine learning? so, why? traditional ml algorithms solve lot cases, useful working high dimensional data, large number inputs outputs. example, case handwriting recognition, large input different type inputs associated different type handwriting. second major challenge tell computer features look play important role predicting outcome achieve better accuracy so. q3. perceptron? work? focus structure biological neuron, dendrites receive inputs. inputs summed cell body axon passed biological neuron shown below. dendrite receives signals neurons cell body sums inputs axon transmit signals cells similarly, perceptron receives multiple inputs, applies transformations functions provides output. perceptron linear model binary classi+cation. models neuron set inputs, given specific weight. neuron computes function weighted inputs gives output.  subscribe \n",
      "\n",
      "q4. role weights bias? perceptron, input called bias. weights determine slope classifier line, bias allows shift line left right. normally bias treated weighted input input value x q5. activation functions? activation function translates inputs outputs. activation function decides neuron activated calculating weighted sum adding bias it. purpose activation function introduce non-linearity output neuron. activation functions like linear identity unit binary step sigmoid logistic tanh relu softmax q6. explain learning perceptron. 1. initializing weights threshold. 2. provide input calculate output. 3. update weights. 4. repeat steps 2 3 wj (t+1) – updated weight wj (t) – old weight d – desired output y – actual output x – input q7. significance cost/loss function? cost function measure accuracy neural network respect given training sample expected output. provides performance neural network whole. deep learning, goal minimize cost function. that, use concept gradient descent. q8. gradient descent? gradient descent optimization algorithm minimize function iteratively moving direction steepest descent defined negative gradient. stochastic gradient descent uses single training example calculate gradient update parameters. batch gradient descent calculate gradients dataset perform update iteration. mini-batch gradient descent mini-batch gradient variation stochastic gradient descent instead single training example, mini-batch samples used. it’s popular optimization algorithms. q9. benefits mini-batch gradient descent? efficient compared stochastic gradient descent. generalization finding flat minima. mini-batches allows help approximate gradient entire training set helps avoid local minima. 0.\n",
      "\n",
      "q10.what steps gradient descent algorithm? initialize random weight bias. pass input network values output layer. calculate error actual value predicted value. neuron contributes error change respective values reduce error. reiterate find best weights network. q11. create gradient descent python. q12. shortcomings single layer perceptron? well, major problems single-layer perceptrons classify non-linearly separable data points. complex problems, involve lot parameters solved single-layer perceptrons q13. multi-layer-perceptron multilayer perceptron (mlp) deep, arti+cial neural network. composed perceptron. composed input layer receive signal, output layer makes decision prediction input, two, arbitrary number hidden layers true computational engine mlp. q14. different parts multi-layer perceptron? input nodes input nodes provide information outside world network referred “input layer”. computation performed input nodes – pass information hidden nodes. hidden nodes hidden nodes perform computations transfer information input nodes output nodes. collection hidden nodes forms “hidden layer”. network single input layer single output layer, zero multiple hidden layers. output nodes output nodes collectively referred “output layer” responsible computations transferring information network outside world. q15. data normalization need it? data normalization important preprocessing step, rescale values +t speci+c range assure better convergence backpropagation. general, boils subtracting mean data point dividing standard deviation. basic deep learning interview s. now, let’s advanced ones. advance interview s q16. better deep networks shallow ones? why? networks, shallow deep capable approximating function. matters precise network terms getting results. shallow network works features, can’t extract more. deep network goes deep computing efficiently working features/parameters. q17. weight initialization important neural networks? weight initialization important steps. bad weight initialization prevent network learning good weight initialization helps giving quicker convergence better overall error. 1 2 3 4 5 6 7 8 9 10 11 12 13 params = [weights_hidden, weights_output, bias_hidden, bias_output] def sgd(cost, params, lr=0.05) grads = t.grad(cost=cost, wrt=params) updates = [] p, g zip(params, grads) updates.append([p, p - g * lr]) return updates updates = sgd(cost, params)\n",
      "\n",
      "biases generally initialized zero. rule setting weights close zero small. q18. what’s difference feed-forward backpropagation neural network? feed-forward neural network type neural network architecture connections “fed forward”, i.e. form cycles. term “feed-forward” input input layer travels input hidden hidden output layer. backpropagation training algorithm consisting 2 steps feed-forward values. calculate error propagate earlier layers. precise, forward-propagation backpropagation algorithm comes back-propagating. q19. hperparameteres? neural network. hyperparameters variables determine network structure(eg number hidden units) variables determine network trained(eg learning rate). hyperparameters set training. number hidden layers network weight initialization activation function learning rate momentum number epochs batch size q20. explain different hyperparameters related network training. network hyperparameters number hidden layers hidden units layer regularization techniques increase accuracy. smaller number units cause underfitting. network weight initialization ideally, better use different weight initialization schemes according activation function layer. uniform distribution used. activation function activation functions introduce nonlinearity models, allows deep learning models learn nonlinear prediction boundaries. training hyperparameters learning rate learning rate de+nes quickly network updates parameters. low learning rate slows learning process converges smoothly. larger learning rate speeds learning converge. momentum momentum helps know direction step knowledge previous steps. helps prevent oscillations. typical choice momentum 0.5 0.9. number epochs number epochs number times training data shown network training. increase number epochs validation accuracy starts decreasing training accuracy increasing(overfitting). batch size mini batch size number sub-samples given network parameter update happens. good default batch size 32. try 32, 64, 128, 256, on. q21. dropout? dropout regularization technique avoid over+tting increasing generalizing power. generally, use small dropout value 20%-50% neurons 20% providing good starting point. probability low minimal effect value high results under-learning network. use larger network. likely better performance dropout larger network, giving model opportunity learn independent representations. q22. training neural network, notice loss decrease starting epochs. reason? reasons be learning rate low regularization parameter high stuck local minima\n",
      "\n",
      "q23. deep learning frameworks tensorflow caffe microsoft cognitive toolkit/cntk torch/pytorch mxnet chainer keras q24. tensors? tensors de facto representing data deep learning. multidimensional arrays, allows represent data having higher dimensions. general, deep learning deal high dimensional data sets dimensions refer different features present data set. q25. list advantages tensorflow? platform flexibility easily trainable cpu gpu distributed computing. tensorflow auto differentiation capabilities advanced support threads, asynchronous computation, queue es. customizable open source. q26. computational graph? computational graph series tensorflow operations arranged nodes graph. node takes zero tensors input produces tensor output. basically, think computational graph alternative way conceptualizing mathematical calculations takes place tensorflow program. operations assigned different nodes computational graph performed parallel, thus, providing better performance terms computations. q27. cnn? convolutional neural network (cnn, convnet) class deep neural networks, commonly applied analyzing visual imagery. unlike neural networks, input vector, input multi-channeled image. cnns use variation multilayer perceptrons designed require minimal preprocessing. q28. explain different layers cnn. layered concepts understand convolutional neural networks convolution convolution layer comprises set independent +lters. +lters initialized randomly parameters learned network subsequently. relu layer convolutional layer.\n",
      "\n",
      "pooling function progressively reduce spatial size representation reduce number parameters computation network. pooling layer operates feature map independently. connectedness neurons fully connected layer connections activations previous layer, seen regular neural networks. activations computed matrix multiplication followed bias offset. q29. rnn? recurrent networks type arti+cial neural network designed recognize patterns sequences data, text, genomes, handwriting, spoken word, numerical times series data. recurrent neural networks use backpropagation algorithm training internal memory, rnn’s able remember important things input received, enables precise predicting what’s coming next. q30. issues faced training rnn? recurrent neural networks use backpropagation algorithm training, applied timestamp. commonly known back-propagation time (btt). issues back-propagation as vanishing gradient exploding gradient q31. vanishing gradient? harmful? back-propagation, gradients tend smaller smaller moving backward network. means neurons earlier layers learn slowly compared neurons later layers hierarchy. earlier layers network important responsible learn detecting simple patterns actually building blocks network. obviously, improper inaccurate results, expect layers complete network perform nicely produce accurate results. training process takes long prediction accuracy model decrease. q32. exploding gradient descent? exploding gradients problem large error gradients accumulate result large updates neural network model weights training. gradient descent process works best updates small controlled. magnitudes gradients accumulate, unstable network likely occur, cause poor prediction results model reports useful ever. q33. explain importance lstm. long short-term memory(lstm) arti+cial recurrent neural network architecture +eld deep learning. unlike standard feedforward neural networks, lstm feedback connections “general purpose computer”. process single data points, entire sequences data. special kind recurrent neural networks capable learning long-term dependencies. q34. capsules capsule neural network? capsules vector specifying features object likelihood. features instantiation parameters like position, size, orientation, deformation, velocity, hue, texture more.\n",
      "\n",
      "capsule specify attributes like angle size represent generic information. now, like neural network layers neurons, capsule network layers capsules. now, let’s continue deep learning interview s section autoencoders rbms. q35. explain autoencoders it’s uses. autoencoder neural network unsupervised machine learning algorithm applies backpropagation, setting target values equal inputs. autoencoders reduce size inputs smaller representation. needs original data, reconstruct compressed data. q36. terms dimensionality reduction, autoencoder differ pcas? autoencoder learn non-linear transformations non-linear activation function multiple layers. doesn’t learn dense layers. use convolutional layers learn better video, image series data. efficient learn layers autoencoder learn huge transformation pca. autoencoder provides representation layer output. use pre-trained layers model apply transfer learning enhance encoder/decoder. q37. real-life examples autoencoders applied. image coloring autoencoders converting black white picture colored image. depending picture, possible tell color be. feature variation extracts required features image generates output removing noise unnecessary interruption. dimensionality reduction reconstructed image input reduced dimensions. helps providing similar image reduced pixel value. denoising image input seen autoencoder raw input stochastically corrupted version. denoising autoencoder trained reconstruct original input noisy version. q38. different layers autoencoders? autoencoder consist layers encoder code decoder q39. explain architecture autoencoder. encoder network compresses input latent space representation. encoder layer encodes input image compressed representation reduced dimension. compressed image distorted version original image.\n",
      "\n",
      "code network represents compressed input fed decoder. decoder layer decodes encoded image original dimension. decoded image lossy reconstruction original image reconstructed latent space representation. q40. bottleneck autoencoder used? layer encoder decoder, ie. code known bottleneck. well-designed approach decide aspects observed data relevant information aspects discarded. balancing criteria compactness representation, measured compressibility. retains behaviourally relevant variables input. q41. variation autoencoders? convolution autoencoders sparse autoencoders deep autoencoders contractive autoencoders q42. deep autoencoders? extension simple autoencoder deep autoencoder. +rst layer deep autoencoder +rst-order features raw input. second layer second- order features corresponding patterns appearance first-order features. deeper layers deep autoencoder tend learn higher-order features. deep autoencoder composed two, symmetrical deep-belief networks shallow layers representing encoding half net. second set layers decoding half. q43. restricted boltzmann machine? restricted boltzmann machine undirected graphical model plays major role deep learning framework recent times. algorithm useful dimensionality reduction, classification, regression, collaborative filtering, feature learning, topic modeling. q44. rbm differ autoencoders? autoencoder simple 3-layer neural network output units directly connected input units. typically, number hidden units number visible ones. task training minimize error reconstruction, i.e. find efficient compact representation input data. rbm shares similar idea, uses stochastic units particular distribution instead deterministic distribution. task training +nd sets variables actually\n",
      "\n",
      "o interview prep 40 artificial intelligence s ver decade, artificial intelligence (ai) grown pipe dream driving force fourth industrial revolution. browse world’s leading job boards, you’ll find it’s heart in-demand tech careers today. “everyone’s trying figure ways optimize businesses practices, automate day-to-day lives little bit easier, little bit productive functional,” www.springboard.com 20 mins read\n",
      "\n",
      "notes stephen zafarino, vice president national recruiting mondo. so, job interview related data science, machine learning (ml), deep learning (dl), bet artifi- cial intelligence s come up. artificial intelligence s categories it’s broad area computer science, ai s popping job interview scenarios. easier navigate space, curated list s artificial intelligence divided multiple categories. you’re hoping data science career ladder looking start machine learning internship, sure brush ai interview s answers walk interview oozing confidence. artificial intelligence s introduction ai ai interview internship, there’s good chance interviewer try break ice feel com- fortable asking “simple” general interest s. types s usually cover basics, sound straightforward, sure don’t stumped (seemingly simple s require answer\n",
      "\n",
      "delivered easily flawlessly). however, quickly involved, ready throw you. related common machine learning terms, explained 1. artificial intelligence? ai described area computer science simulates human intelligence machines. it’s smart algorithms making decisions based available data. it’s amazon’s alexa self-driving car, goal mimic human intelligence lightning speed (and reduced rate error). reading ai? need know artifi- cial intelligence 2. intelligent agents? intelligent agent autonomous entity leverages sensors understand situation decisions. use actuators perform simple complex tasks. beginning, great performing task, improve time. roomba vacuum cleaner excellent example this. reading intelligent agents defending iot world 3. what’s popular programming language ai? open-source modular programming language python leads ai industry simplicity predictable coding behavior.\n",
      "\n",
      "popularity attributed open-source libraries like mat- plotlib numpy, efficient frameworks scikit-learn, practical version libraries like tensorflow vtk. there’s chance interviewer conversation going ask examples. happens, men- tion following • java • julia • haskell • lisp reading 5 best programming languages ai 4. ai neural networks? neural networks ai mathematically model human brain works. approach enables machine think learn humans do. smart technology today recognizes speech, objects, more. reading exploring neural networks activation atlases 5. what’s difference strong ai weak ai? difference like terms sound. strong ai successfully imitate human intelligence core advanced robotics. weak ai predict specific characteristics resemble human intelligence. alexa siri excellent examples weak ai. strong ai • applied widely • extensive scope\n",
      "\n",
      "• human-level intelligence • processes data clustering association weak ai • great performing simple tasks • uses supervised unsupervised learning • scope minimal reading what’s difference weak strong ai? 6. what’s difference ai ml? (source) ai ml closely related, terms aren’t interchangeable. ml actually falls umbrella ai. demands machines carry tasks way humans do. current application ml ai based idea enable access data machines observe learn themselves. reading what’s difference machine learning ai?\n",
      "\n",
      "7. describe ml non-technical person? ml geared pattern recognition. great example facebook newsfeed netflix’s recommendation engine. scenario, ml algorithms observe patterns learn them. deploy ml program, learning improving attempt. interviewer prods provide real-world examples, list following • amazon product recommendations • fraud detection • search ranking • spam detection • spell correction reading explain machine learning data mining non-computer science people? 8. examples ai use? compelling examples ai applications are • chatbots • facial recognition • image tagging • natural language processing • sales prediction • self-driving cars • sentiment analysis reading ask ai experts applications ai?\n",
      "\n",
      "9. what’s turing test? turing test, named alan turing, method testing machine’s human-level intelligence. example, human-versus- machine scenario, judge tasked identifying ter- minal occupied human occupied computer based individual performance. computer pass human, it’s deemed intelli- gent. game evolved, premise remains same. reading turing test 10. what’s tensorflow? tensorflow open-source framework dedicated ml. it’s com- prehensive highly adaptable ecosystem libraries, tools, community resources help developers build deploy ml-pow- ered applications. alphago google cloud vision built tensorflow platform. reading tensorflow tutorial scratch building deep learning model fashion mnist dataset (part 1) 11. game theory important ai? game theory, developed american mathematician josh nash, essential ai plays underlying role smart algorithms improve time. basic, ai algorithms deployed find solutions problems. game theory players opposition try- ing achieve specific goals. aspects life compe- tition, game theory meaningful real-world applications. problems tend dynamic. game theory problems natural candidates ai algorithms. so, game theory\n",
      "\n",
      "applied, multiple ai agents interact care utility itself. data scientists space aware following games • symmetric vs. asymmetric • perfect vs. imperfect information • cooperative vs. non-cooperative • simultaneous vs. sequential • zero-sum vs. non-zero-sum reading connection game theory ai? 12. opinion, ai impact application development? types s help interviewer ascertain level interest field. you’re naturally passionate ai every- thing related it, knowledge current industry trends. so, actively following space, you’ll know aiops. coming months, expect ai involved build applications. potential trans- form use manage infrastructure micro macro level. devops replaced calling aiops allows developers engage accurate root cause analysis combining big data, ml, visualization. aiops described multilayered platform automate improve operations. scenario, developers leverage analytics ml collect process data variety sources. information analyzed real time identify rectify problems.\n",
      "\n",
      "(source) reading aiops devops ready infusion artificial intelligence? 13. common misconceptions ai? ai-related misconceptions making rounds age “fake news.” common ones are • ai replace humans • ai systems aren’t safe • ai lead significant unemployment\n",
      "\n",
      "types stories common, they’re far truth. ai-based technology able complete tasks—for example, analyzing zettabytes data second—it needs humans gather data define pat- terns identification. aren’t near reality technology potential replace jobs. reading what’s hype artificial intelligence? 14. properties good knowledge representation system? perspective systems theory, good knowledge represen- tation system following • acquisition efficiency acquire incorporate new data • inferential adequacy derive knowledge representation structures like symbols new knowledge learned old knowledge • inferential efficiency enable addition data existing knowledge structures help inference process • representation adequacy represent knowledge required specific domain reading knowledge representation ai 15. different types keys relational database? variety keys relational database, including • alternate keys candidate keys exclude primary keys.\n",
      "\n",
      "• artificial keys created assigning unique number occurrence record aren’t compound standalone keys. • compound keys combining multiple elements develop unique identifier construct isn’t single data element uniquely identifies occurrences construct. known composite key concatenated key, compound keys consist attributes. • foreign keys groups fields database record point key field group fields create key database record that’s usually different table. often, foreign keys table refer primary keys another. referenced data linked quickly, critical database normalization. • natural keys data elements stored constructs utilized primary keys. • primary keys values identify unique rows table attributes associated them. example, form social security number that’s related specific person. relational model data, primary key candidate key. it’s primary method identify tuple possible relation. • super keys defined relational model set attributes relation variable. holds relations assigned variable don’t distinct tuples. don’t values attributes set. super keys defined set attributes relational variable functionality depends. reading different types keys rdbms?\n",
      "\n",
      "artificial intelligence s statistics ai, ml, data science great deal overlap, it’s crucial cover bases ai interview. however, it’s important note fields aren’t interchangeable. relative, ai produces actions, ml produces predictions, data sci- ence produces insights. kind potential data science-related ai s prepared for? let’s look. 16. python’s standard library, packages useful data scientists? python wasn’t built data science. however, recent years grown go-to programming language following • machine learning • predictive analytics • simple data analytics • statistics data science projects, following packages python stand- ard library life easier accelerate deliveries • numpy (to process large multidimensional arrays, extensive collections high-level mathematical functions, matrices) • pandas (to leverage built-in methods rapidly combining, filtering, grouping data) • scipy (to extend numpy’s capabilities solve tasks related integral calculus, linear algebra, probability theory)\n",
      "\n",
      "reading 20 python interview s answers—start pre- paring ideal job 17. collaborative filtering? collaborative filtering described process finding pat- terns available information build personalized recommenda- tions. find collaborative filtering action visit websites like amazon imdb. known social filtering, approach essentially makes sug- gestions based recommendations preferences peo- ple share similar interests. reading collaborative filtering 18. list disadvantages related linear models? disadvantages linear models, main ones are • errors linearity assumptions • lacks autocorrelation • can’t solve overfitting problems • can’t use calculate outcomes binary outcomes reading limitations linear regression modeling data analysis? 19. what’s feature vector? feature vector n-dimensional vector contains essential information describes characteristics object. exam- ple, object’s numerical features list numbers taken output neural network layer.\n",
      "\n",
      "ai data science, feature vectors represent numeric symbolic characteristics object mathematical terms seamless analysis. let’s break down. data set usually organized multiple examples example features. however, feature vector won’t feature numerous examples. instead, example correspond feature vector contain numerical values example object. feature vectors stacked design matrix. sce- nario, row feature vector example. column feature examples correspond particular fea- ture. means like matrix, row multiple columns (or single column multiple rows) like [1,2,3,5,6,3,2,0]. reading extract feature vector image pytorch 20. typical characteristics elements list dictionary? lists, elements maintain order explicitly com- manded re-order. data type mixed. however, elements lists accessed numeric, zero-based indices. dictionary, order isn’t guaranteed. however, entry assigned key value. result, elements diction- ary accessed individual key. set unique keys, use diction- ary. collection items order, use list. it’s difficult predict ai interview unfold, fol- low asking list keys dictionary, respond following\n",
      "\n",
      "obtain list keys dictionary, you’ll use following function keys() mydict={‘a’1,’b’2,’c’3,’e’5} mydict.keys() dict_keys([‘a’, ‘b’, ‘c’, ‘e’]) reading sort python dictionaries key value 21. what’s selection bias? types biases encounter sampling? you’re dealing non-random sample, selection bias occur flaws selection process. happens sub- set data consistently excluded particular attrib- ute. exclusion distort results influence statistical significance test. types biases include survivorship bias undercoverage bias. it’s important consider reduce biases you’ll want smart algorithms accurate predic- tions based data. reading mitigating bias ai models 22. what’s random forest? explain role ai? random forest data construct that’s applied ml projects develop large number random decision trees analyzing var- iables.\n",
      "\n",
      "(source) algorithms leveraged improve way technologies analyze complex data sets. basic premise multiple weak learners combined build strong learner. excellent tool ai ml projects work large labeled unlabeled data sets large number attributes. maintain accuracy data missing. model importance attributes, dimen- sionality reduction. reading machine learning random forest 23. what’s eigenvalue? eigenvector? directions particular linear transformation com- presses, flips, stretches called eigenvalue. eigenvectors understand linear transformations.\n",
      "\n",
      "example, better sense covariance covariance matrix, eigenvector help identify direction covariances going. eigenvalues express importance feature. eigenvalues eigenvectors critical computer vision ml applications. popular known principal component analysis dimensionality reduction (e.g., eigenfaces face recognition). reading eigenvectors eigenvalues? 24. use batch normalization? so, explain why? idea standardize data sending layer. approach helps reduce impact previous layers keeping mean variance constant. makes layers independent achieve rapid convergence. example, normalize features 0 1 1 100, helps accelerate learning cycle. check video batch normalization work? artificial intelligence s programming ai interview s bound enter sphere programming sooner later. let’s dive right following ai s answers.\n",
      "\n",
      "25. what’s hash table? parts hash table. array, actual table data stored, mapping function that’s known hash function. it’s data structure implements associative array abstract data type map key values. compute index array slots buckets desired value found. (source) reading basics hash tables 26. different algorithm techniques use ai ml? algorithm techniques leveraged are • learning learn • reinforcement learning (deep adversarial networks, q-learning, temporal difference) • semi-supervised learning\n",
      "\n",
      "• supervised learning (decision trees, linear regression, naive bayes, nearest neighbor, neural networks, support vector machines) • transduction • unsupervised learning (association rules k-means clustering) reading types machine learning algorithms know 27. choosing algorithm solve business problem? first, develop “problem statement” that’s based problem provided business. step essential it’ll help ensure fully understand type problem input output problem want solve. problem statement simple single sentence. example, let’s consider enterprise spam requires algorithm identify it. problem statement be “is email fake/spam not?” scenario, identification it’s fake/spam output. defined problem statement, identify appropriate algorithm following • classification algorithm • clustering algorithm • regression algorithm • recommendation algorithm algorithm use depend specific problem you’re trying solve. scenario, forward cluster-\n",
      "\n",
      "ing algorithm choose k-means algorithm achieve goal filtering spam email system. examples aren’t necessary answering s artificial intelligence, help easier point across. reading choose ml algorithm machine learning ques- tions & answers – iii 28. necessary update algorithm? update algorithm underlying data source changed there’s case non-stationarity. algorithm updated want model evolve data streams infrastructure. reading “the data changed” error stepping main form subform 29. what’s regularization? underfitting overfitting issues statistical model, use regularization technique resolve it. regular- ization techniques like lasso help penalize model parameters likely lead overfitting. interviewer follows methods avoid overfitting, mention cross-valida- tion techniques k-folds cross-validation. approach model simple taking account fewer variables parameters. helps remove noise training data. reading machine learning explained regularization\n",
      "\n",
      "30. what’s difference inductive, deductive, abductive learning? inductive learning describes smart algorithms learn set instances draw conclusions. statistical ml, k-nearest neighbor support vector machine good examples inductive learning. literals (top-down) inductive learning • arithmetic literals • equality inequality • predicates deductive learning, smart algorithms draw conclusions fol- lowing truth-generating structure (major premise, minor premise, conclusion) improve based previous decisions. scenario, ml algorithm engages deductive reasoning decision tree. abductive learning dl technique conclusions based instances. approach, inductive reasoning applied causal relationships deep neural networks. reading what’s difference “inductive, “deductive” “abductive” reasoning? 31. steps evaluate effectiveness ml model? split data set training test sets. option cross-validation technique seg- ment data set composite training test sets data. implement choice selection performance metrics like following\n",
      "\n",
      "• confusion matrix • accuracy • precision • recall sensitivity • specificity • f1 score part, use measures accuracy, confusion matrix, f1 score. however, it’ll critical demonstrate understand nuances model measured choosing right performance measure match problem. reading performance metrics classification problems machine learning 32. data data set missing corrupted? data missing corrupted, replace value drop rows columns altogether. pandas, isnull() dropna() handy tools find missing cor- rupted data drop values. use fillna() method fill invalid values placeholder—for example, “0.” reading 5 ways handle missing values machine learning datasets 33. know build simple neural network? demonstrate python code? types s designed ascertain programming skills. you’re coding ninja, able achieve lines python code.\n",
      "\n",
      "(source) reading build simple neural network 9 lines python code 34. write python program draw flower shown below? (source)\n",
      "\n",
      "again, interviewer trying test coding skills. it’s good perceive types s opportunity potential employer do. demonstrate program- ming skills hesitation confidence. (source) reading drawing flower python turtle artificial intelligence s general ai interest tech talent shortage created fierce demand skills, land “dream job” it’ll help demonstrate pas- sion field. scheduled ai interview startup established tech giant, ready wide-ranging ques- tions like ones listed below.\n",
      "\n",
      "35. research experience ai? present, lot work ai space research-based. result, organizations digging background ascertain kind experience area. authored co-authored research papers supervised industry leaders, sure share information. fact, step summary research experience research papers ready share interviewing panel. however, don’t formal research experience, explanation ready. example, talk ai jour- ney started weekend hobby grew space years. 36. what’s ai-related research paper read? conclusions? you’re passionate ai, scientific research field. excellent place start following sciencedirect track published research papers what’s pipeline.\n",
      "\n",
      "(source) 37. what’s favorite use case? like research, date what’s going industry. such, you’re asked use cases, sure examples mind share. possible, bring personal experiences. share what’s happening industry. example, you’re interested use ai medical images, health analyt- ics interesting use cases • detecting fractures musculoskeletal injuries • aiding diagnosis neurological diseases • flagging thoracic complications conditions • screening common cancers 38. conferences hoping attend year? keynote speeches you’re hoping catch? conferences great places network, attend workshops, learn, grow. you’re planning stick career artificial intelli- gence, going these. example, deep learning world great summer. year’s event las vegas feature keynote speakers like dr. dyann daley (founder ceo predict align prevent), siddha ganju (solutions architect nvidia), dr. alex glushkovsky (prin- cipal data scientist bmo financial group, others). 39. google training data self-driving cars? you’re interested heavily involved space, ques- tion no-brainer. know answer, it’ll demonstrate knowledge variety ml methods ml applied\n",
      "\n",
      "autonomous vehicles. don’t know answer, stab creativity inventive nature. google recaptcha source labeled data store- fronts traffic signs years now. company training data collected sebastian thrun, ceo kitty hawk corporation co-founder (and ceo) udacity. information, significant, potential employer you’re interested excited field. reading google x leveraging data algorithms self-driv- ing cars 40. usually source data sets? talk ai projects you’ve worked free time, interviewer probably ask sourced data sets. you’re genuinely passionate field, worked projects know find free data sets. example, freely available public data sets know (without conducting google search) • celebfaces (with 200,000 celebrity images 40 attribute annotations) • cifar (with 60,000 images map 10 different classes) • youtube-8m (with 4,000 annotated entities taken enormous data set youtube videos) researchers released hundreds free resources like actual network architecture weights exam- ples. serve explore data sets run experiments heading ai interview. reading find datasets artificial intelligence training\n",
      "\n",
      "41 essential machine learning interview s www.springboard.com 18 mins read\n",
      "\n",
      "m achine learning interview s integral data science interview path data scientist, machine learning engineer, data engi- neer. springboard created free guide data science interviews, know exactly trip candidates! order help resolve that, curated created list key s machine learning interview. answers don’t stumped. you’ll able job interview (even machine learning internship) reading piece. machine learning interview s categories we’ve traditionally seen machine learning interview s pop categories. algorithms theory machine learning. you’ll under- standing algorithms compare measure efficacy accuracy right way. second cat- egory programming skills ability exe- cute algorithms theory. general interest machine learning you’ll asked what’s going industry latest machine learning trends. finally, company industry-spe- cific s test ability general machine\n",
      "\n",
      "learning knowledge turn actionable points drive bot- tom line forward. we’ve divided guide machine learning interview s categories mentioned easily information need comes machine learning inter- view s. machine learning interview s algorithms/theory algorithms s test grasp theory machine learning. q1- what’s trade-off bias variance? reading bias-variance tradeoff (wikipedia) bias error erroneous overly simplistic assumptions learning algorithm you’re using. lead model underfit- ting data, making hard high predictive accuracy generalize knowledge training set test set. variance error complexity learning algo- rithm you’re using. leads algorithm highly sensitive high degrees variation training data, lead model overfit data. you’ll carrying noise training data model useful test data. bias-variance decomposition essentially decomposes learning error algorithm adding bias, variance bit irreducible error noise underlying dataset. essentially, model complex add variables, you’ll lose bias gain variance — order optimally reduced error, you’ll tradeoff bias variance. don’t want high bias high variance model.\n",
      "\n",
      "q2- difference supervised unsu- pervised machine learning? reading difference supervised unsuper- vised machine learning? (quora) supervised learning requires training labeled data. example, order classification (a supervised learning task), you’ll need label data you’ll use train model classify data labeled groups. unsupervised learning, contrast, require labeling data explicitly. q3- knn different k-means clustering? reading k-nearest neighbor algorithm different k-means clustering? (quora) k-nearest neighbors supervised classification algorithm, k-means clustering unsupervised clustering algorithm. mechanisms similar first, means order k-nearest neighbors work, need labeled data want classify unlabeled point (thus nearest neighbor part). k-means clustering requires set unlabeled points threshold algorithm unlabeled points gradually learn cluster groups computing mean distance different points. critical difference knn needs labeled points supervised learning, k-means doesn’t — unsu- pervised learning. q4- explain roc curve works. reading receiver operating characteristic (wikipedia) roc curve graphical representation contrast true positive rates false positive rate thresholds. it’s proxy trade-off sensitivity\n",
      "\n",
      "model (true positives) vs fall-out probability trig- ger false alarm (false positives). q5- define precision recall. reading precision recall (wikipedia) recall known true positive rate positives model claims compared actual number positives data. precision known positive pre- dictive value, measure accurate positives model claims compared number positives actually claims. easier think recall precision context case you’ve predicted 10 apples 5 oranges case 10 apples. you’d perfect recall (there actually 10 apples, predicted 10) 66.7% precision 15 events predicted, 10 (the apples) correct.\n",
      "\n",
      "q6- bayes’ theorem? useful machine learning context? reading intuitive (and short) explanation bayes’ theorem (betterexplained) bayes’ theorem gives posterior probability event given known prior knowledge. mathematically, it’s expressed true positive rate condition sample divided sum false positive rate population true positive rate condition. 60% chance actually having flu flu test, people flu, test false 50% time, overall population 5% chance having flu. actually 60% chance having flu having positive test? bayes’ theorem says no. says (.6 * 0.05) (true pos- itive rate condition sample) / (.6*0.05)(true positive rate condition sample) + (.5*0.95) (false positive rate population) = 0.0594 5.94% chance getting flu. bayes’ theorem basis branch machine learning notably includes naive bayes classifier. that’s\n",
      "\n",
      "important consider you’re faced machine learning inter- view s. q7- “naive” bayes naive? reading “naive bayes” naive? (quora) despite practical applications, especially text mining, naive bayes considered “naive” makes assumption virtually impossible real-life data conditional probabil- ity calculated pure product individual probabilities components. implies absolute independence features — condition probably met real life. quora commenter whimsically, naive bayes classifier figured liked pickles ice cream probably naively recommend pickle ice cream. q8- explain difference l1 l2 regulariza- tion. reading difference l1 l2 regulariza- tion? (quora) l2 regularization tends spread error terms, l1 binary/sparse, variables assigned 1 0 weighting. l1 corresponds setting laplacean prior terms, l2 corresponds gaussian prior.\n",
      "\n",
      "q9- what’s favorite algorithm, explain minute? type tests understanding communi- cate complex technical nuances poise ability sum- marize quickly efficiently. sure choice sure explain different algorithms simply effectively five-year-old grasp basics! q10- what’s difference type type ii error? reading type type ii errors (wikipedia) don’t think trick ! machine learning inter- view s attempt lob basic s sure you’re game you’ve prepared bases. type error false positive, type ii error false negative. briefly stated, type error means claiming happened hasn’t, type ii error means claim happening fact is. clever way think think type error telling man pregnant, type ii error means tell pregnant woman isn’t carrying baby. q11- what’s fourier transform? reading fourier transform (wikipedia)\n",
      "\n",
      "fourier transform generic method decompose generic func- tions superposition symmetric functions. intuitive tutorial puts it, given smoothie, it’s find rec- ipe. fourier transform finds set cycle speeds, amplitudes phases match time signal. fourier transform converts signal time frequency domain — it’s common way extract features audio signals time series sen- sor data. q12- what’s difference probability likeli- hood? reading difference “likelihood” “proba- bility”? (cross validated) q13- deep learning, contrast machine learning algorithms? reading deep learning (wikipedia) deep learning subset machine learning concerned neural networks use backpropagation certain principles\n",
      "\n",
      "neuroscience accurately model large sets unlabelled semi-structured data. sense, deep learning represents unsupervised learning algorithm learns representations data use neural nets. q14- what’s difference generative dis- criminative model? reading difference generative dis- criminative algorithm? (stack overflow) generative model learn categories data discrimina- tive model simply learn distinction different catego- ries data. discriminative models generally outperform generative models classification tasks. q15- cross-validation technique use time series dataset? reading k-fold cross-validation time-series model selection (crossvalidated) instead standard k-folds cross-validation, pay attention fact time series randomly distributed data — inherently ordered chronological order. pattern emerges later time periods example, model pick effect doesn’t hold earlier years! you’ll want like forward chaining you’ll able model past data look forward-facing data. • fold 1 training [1], test [2] • fold 2 training [1 2], test [3] • fold 3 training [1 2 3], test [4] • fold 4 training [1 2 3 4], test [5] • fold 5 training [1 2 3 4 5], test [6] q16- decision tree pruned?\n",
      "\n",
      "reading pruning (decision trees) pruning happens decision trees branches weak predictive power removed order reduce complexity model increase predictive accuracy decision tree model. pruning happen bottom-up top-down, approaches reduced error pruning cost complexity prun- ing. reduced error pruning simplest version replace node. doesn’t decrease predictive accuracy, pruned. simple, heuristic actually comes pretty close approach optimize maximum accuracy. q17- important you– model accuracy, model performance? reading accuracy paradox (wikipedia) tests grasp nuances machine learning model performance! machine learning interview s look details. models higher accuracy perform worse predictive power — sense? well, model accuracy sub- set model performance, that, misleading one. example, wanted detect fraud massive dataset sample millions, accurate model likely predict fraud vast minority cases fraud. however, useless predictive model — model designed find fraud asserted fraud all! s like help demonstrate understand model accuracy isn’t be-all end-all model performance. q18- what’s f1 score? use it? reading f1 score (wikipedia)\n",
      "\n",
      "f1 score measure model’s performance. weighted average precision recall model, results tending 1 best, tending 0 worst. use classification tests true negatives don’t matter much. q19- handle imbalanced dataset? reading 8 tactics combat imbalanced classes machine learning dataset (machine learning mastery) imbalanced dataset have, example, classification test 90% data class. leads problems accuracy 90% skewed predictive power category data! tactics hump 1- collect data imbalances dataset. 2- resample dataset correct imbalances. 3- try different algorithm altogether dataset. what’s important keen sense damage unbalanced dataset cause, balance that. q20- use classification regression? reading regression vs classification (math stackexchange) classification produces discrete values dataset strict catego- ries, regression gives continuous results allow better distinguish differences individual points. use classification regression wanted results reflect belongingness data points dataset certain explicit categories (ex wanted know male female correlated male female names.)\n",
      "\n",
      "q21- example ensemble techniques useful. reading ensemble learning (wikipedia) ensemble techniques use combination learning algorithms optimize better predictive performance. typically reduce overfit- ting models model robust (unlikely influ- enced small changes training data). list examples ensemble methods, bagging boosting “bucket models” method demonstrate increase predictive power. q22- ensure you’re overfitting model? reading avoid overfitting? (quora) simple restatement fundamental problem machine learning possibility overfitting training data carrying noise data test set, providing inaccu- rate generalizations. main methods avoid overfitting 1- model simpler reduce variance taking account fewer variables parameters, removing noise training data. 2- use cross-validation techniques k-folds cross-validation. 3- use regularization techniques lasso penalize certain model parameters they’re likely cause overfitting. q23- evaluation approaches work gauge effectiveness machine learning model? reading evaluate machine learning algorithms (machine learning mastery)\n",
      "\n",
      "split dataset training test sets, per- haps use cross-validation techniques segment dataset composite sets training test sets data. implement choice selection performance metrics fairly comprehensive list. use measures f1 score, accuracy, confusion matrix. what’s important demonstrate understand nuances model measured choose right performance measures right situations. q24- evaluate logistic regression model? reading evaluating logistic regression (crossvalidated), lo- gistic regression plain english subsection above. demonstrate understanding typical goals logistic regression (classification, prediction, etc.) bring examples use cases. q25- what’s “kernel trick” useful? reading kernel method (wikipedia) kernel trick involves kernel functions enable higher- dimension spaces explicitly calculating coordinates points dimension instead, kernel functions compute inner products images pairs data feature space. allows useful attribute calculating coordinates higher dimensions computationally cheaper explicit calculation said coordinates. algo- rithms expressed terms inner products. kernel trick enables effectively run algorithms high-dimensional space lower-dimensional data. (learn springboard’s ai / machine learning bootcamp, kind come job guaran- tee.)\n",
      "\n",
      "machine learning interview s programming machine learning interview s test knowledge programming principles need implement machine learning principles practice. machine learning interview s tend technical s test logic programming skills section focuses latter. q26- handle missing corrupted data dataset? reading handling missing data (o’reilly) find missing/corrupted data dataset drop rows columns, decide replace value. pandas, useful methods isnull() dropna() help find columns data missing corrupted data drop values. want fill invalid values placeholder value (for example, 0), use fillna() method. q27- experience spark big data tools machine learning? reading 50 open source tools big data (datamation) you’ll want familiar meaning big data different companies different tools they’ll want. spark big data tool demand now, able handle immense datasets speed. honest don’t experience tools\n",
      "\n",
      "demanded, look job descriptions tools pop up you’ll want invest familiarizing them. q28- pick algorithm. write psuedo-code paral- lel implementation. reading writing pseudocode parallel programming (stack overflow) kind demonstrates ability think parallel- ism handle concurrency programming imple- mentations dealing big data. look pseudocode frameworks peril-l visualization tools web sequence diagrams help demonstrate ability write code reflects parallelism. q29- differences linked list array? reading array versus linked list (stack overflow) array ordered collection objects. linked list series objects pointers direct process sequentially. array assumes element size, unlike linked list. linked list easily grow organically array pre-defined re-defined organic growth. shuffling linked list involves changing points direct — meanwhile, shuffling array complex takes memory. q30- describe hash table. reading hash table (wikipedia) hash table data structure produces associative array. key mapped certain values use hash function. tasks database indexing.\n",
      "\n",
      "q31- data visualization libraries use? thoughts best data visualization tools? reading 31 free data visualization tools (springboard) what’s important define views properly vis- ualize data personal preferences comes tools. popular tools include r’s ggplot, python’s seaborn matplotlib, tools plot.ly tableau. related 20 python interview s machine learning interview s company/industry specific machine learning interview s deal imple- ment general machine learning knowledge specific compa- ny’s requirements. you’ll asked create case studies extend knowledge company industry you’re applying machine learning skills. q32- implement recommendation system company’s users? reading implement recommendation system? (stack overflow) lot machine learning interview s type involve implementation machine learning models company’s problems. you’ll research company industry in-depth, espe-\n",
      "\n",
      "cially revenue drivers company has, types users company takes context industry it’s in. q33- use machine learning skills gen- erate revenue? reading startup metrics startups (500 startups) tricky . ideal answer demonstrate knowledge drives business skills relate. example, interviewing music-streaming startup spotify, remark skills developing bet- ter recommendation model increase user retention, increase revenue long run. startup metrics slideshare linked help understand exactly performance indicators important startups tech companies think revenue growth. q34- think current data process? reading data science process email course – springboard\n",
      "\n",
      "kind requires listen carefully impart feed- manner constructive insightful. interviewer trying gauge you’d valuable member team grasp nuances certain things set way company’s data process based company- indus- try-specific conditions. they’re trying intellec- tual peer. act accordingly. machine learning interview s general machine learning interest series machine learning interview s attempts gauge passion interest machine learning. right answers serve testament commitment lifelong learner machine learning. q35- machine learning papers you’ve read? reading best research papers/books machine learning? keeping latest scientific literature machine learning want demonstrate interest machine learning position. overview deep learning nature scions deep learning (from hinton bengio lecun) good reference paper overview what’s happening deep learning — kind paper want cite. q36- research experience machine learn- ing?\n",
      "\n",
      "related point, organizations hiring machine learn- ing positions look formal experience field. research papers, co-authored supervised leaders field, difference hired not. sure summary research experience papers ready — explanation background lack formal research experience don’t. q37- favorite use cases machine learning models? reading typical use cases different machine learning algorithms? (quora) quora thread contains examples, decision trees categorize people different tiers intelligence based iq scores. sure examples mind describe resonated you. it’s important demon- strate interest machine learning implemented. q38- approach “netflix prize” competi- tion? reading netflix prize (wikipedia) netflix prize famed competition netflix offered $1,000,000 better collaborative filtering algorithm. team won called bellkor 10% improvement ensem- ble different methods win. familiarity case solution help demonstrate you’ve paid attention machine learning while. q39- usually source datasets? reading 19 free public data sets data science project (springboard) machine learning interview s like try heart machine learning interest. somebody truly passionate\n",
      "\n",
      "machine learning gone projects own, good idea great datasets there. you’re missing any, check quandl economic financial data, kaggle’s datasets collection great list. q40- think google training data self- driving cars? reading waymo tech machine learning interview s like test knowledge different machine learning methods, inven- tiveness don’t know answer. google currently recaptcha source labeled data storefronts traffic signs. building training data collected sebastian thrun googlex — obtained grad students driving buggies desert dunes! q41- simulate approach alphago took beat lee sidol go? reading mastering game deep neural networks tree search (nature) alphago beating lee sidol, best human player go, best-of- series truly seminal event history machine learn- ing deep learning. nature paper describes accomplished “monte-carlo tree search deep neural networks trained supervised learning, human expert games, reinforcement learning games self- play.” related 40 artificial intelligence interview s looking land role machine learning engineer? find springboard’s machine learning engineer- ing career track, kind come job guarantee.\n",
      "\n",
      "o interview prep 40 artificial intelligence s ver decade, artificial intelligence (ai) grown pipe dream driving force fourth industrial revolution. browse world’s leading job boards, you’ll find it’s heart in-demand tech careers today. “everyone’s trying figure ways optimize businesses practices, automate day-to-day lives little bit easier, little bit productive functional,” www.springboard.com 20 mins read\n",
      "\n",
      "notes stephen zafarino, vice president national recruiting mondo. so, job interview related data science, machine learning (ml), deep learning (dl), bet artifi- cial intelligence s come up. artificial intelligence s categories it’s broad area computer science, ai s popping job interview scenarios. easier navigate space, curated list s artificial intelligence divided multiple categories. you’re hoping data science career ladder looking start machine learning internship, sure brush ai interview s answers walk interview oozing confidence. artificial intelligence s introduction ai ai interview internship, there’s good chance interviewer try break ice feel com- fortable asking “simple” general interest s. types s usually cover basics, sound straightforward, sure don’t stumped (seemingly simple s require answer\n",
      "\n",
      "delivered easily flawlessly). however, quickly involved, ready throw you. related common machine learning terms, explained 1. artificial intelligence? ai described area computer science simulates human intelligence machines. it’s smart algorithms making decisions based available data. it’s amazon’s alexa self-driving car, goal mimic human intelligence lightning speed (and reduced rate error). reading ai? need know artifi- cial intelligence 2. intelligent agents? intelligent agent autonomous entity leverages sensors understand situation decisions. use actuators perform simple complex tasks. beginning, great performing task, improve time. roomba vacuum cleaner excellent example this. reading intelligent agents defending iot world 3. what’s popular programming language ai? open-source modular programming language python leads ai industry simplicity predictable coding behavior.\n",
      "\n",
      "popularity attributed open-source libraries like mat- plotlib numpy, efficient frameworks scikit-learn, practical version libraries like tensorflow vtk. there’s chance interviewer conversation going ask examples. happens, men- tion following • java • julia • haskell • lisp reading 5 best programming languages ai 4. ai neural networks? neural networks ai mathematically model human brain works. approach enables machine think learn humans do. smart technology today recognizes speech, objects, more. reading exploring neural networks activation atlases 5. what’s difference strong ai weak ai? difference like terms sound. strong ai successfully imitate human intelligence core advanced robotics. weak ai predict specific characteristics resemble human intelligence. alexa siri excellent examples weak ai. strong ai • applied widely • extensive scope\n",
      "\n",
      "• human-level intelligence • processes data clustering association weak ai • great performing simple tasks • uses supervised unsupervised learning • scope minimal reading what’s difference weak strong ai? 6. what’s difference ai ml? (source) ai ml closely related, terms aren’t interchangeable. ml actually falls umbrella ai. demands machines carry tasks way humans do. current application ml ai based idea enable access data machines observe learn themselves. reading what’s difference machine learning ai?\n",
      "\n",
      "7. describe ml non-technical person? ml geared pattern recognition. great example facebook newsfeed netflix’s recommendation engine. scenario, ml algorithms observe patterns learn them. deploy ml program, learning improving attempt. interviewer prods provide real-world examples, list following • amazon product recommendations • fraud detection • search ranking • spam detection • spell correction reading explain machine learning data mining non-computer science people? 8. examples ai use? compelling examples ai applications are • chatbots • facial recognition • image tagging • natural language processing • sales prediction • self-driving cars • sentiment analysis reading ask ai experts applications ai?\n",
      "\n",
      "9. what’s turing test? turing test, named alan turing, method testing machine’s human-level intelligence. example, human-versus- machine scenario, judge tasked identifying ter- minal occupied human occupied computer based individual performance. computer pass human, it’s deemed intelli- gent. game evolved, premise remains same. reading turing test 10. what’s tensorflow? tensorflow open-source framework dedicated ml. it’s com- prehensive highly adaptable ecosystem libraries, tools, community resources help developers build deploy ml-pow- ered applications. alphago google cloud vision built tensorflow platform. reading tensorflow tutorial scratch building deep learning model fashion mnist dataset (part 1) 11. game theory important ai? game theory, developed american mathematician josh nash, essential ai plays underlying role smart algorithms improve time. basic, ai algorithms deployed find solutions problems. game theory players opposition try- ing achieve specific goals. aspects life compe- tition, game theory meaningful real-world applications. problems tend dynamic. game theory problems natural candidates ai algorithms. so, game theory\n",
      "\n",
      "applied, multiple ai agents interact care utility itself. data scientists space aware following games • symmetric vs. asymmetric • perfect vs. imperfect information • cooperative vs. non-cooperative • simultaneous vs. sequential • zero-sum vs. non-zero-sum reading connection game theory ai? 12. opinion, ai impact application development? types s help interviewer ascertain level interest field. you’re naturally passionate ai every- thing related it, knowledge current industry trends. so, actively following space, you’ll know aiops. coming months, expect ai involved build applications. potential trans- form use manage infrastructure micro macro level. devops replaced calling aiops allows developers engage accurate root cause analysis combining big data, ml, visualization. aiops described multilayered platform automate improve operations. scenario, developers leverage analytics ml collect process data variety sources. information analyzed real time identify rectify problems.\n",
      "\n",
      "(source) reading aiops devops ready infusion artificial intelligence? 13. common misconceptions ai? ai-related misconceptions making rounds age “fake news.” common ones are • ai replace humans • ai systems aren’t safe • ai lead significant unemployment\n",
      "\n",
      "types stories common, they’re far truth. ai-based technology able complete tasks—for example, analyzing zettabytes data second—it needs humans gather data define pat- terns identification. aren’t near reality technology potential replace jobs. reading what’s hype artificial intelligence? 14. properties good knowledge representation system? perspective systems theory, good knowledge represen- tation system following • acquisition efficiency acquire incorporate new data • inferential adequacy derive knowledge representation structures like symbols new knowledge learned old knowledge • inferential efficiency enable addition data existing knowledge structures help inference process • representation adequacy represent knowledge required specific domain reading knowledge representation ai 15. different types keys relational database? variety keys relational database, including • alternate keys candidate keys exclude primary keys.\n",
      "\n",
      "• artificial keys created assigning unique number occurrence record aren’t compound standalone keys. • compound keys combining multiple elements develop unique identifier construct isn’t single data element uniquely identifies occurrences construct. known composite key concatenated key, compound keys consist attributes. • foreign keys groups fields database record point key field group fields create key database record that’s usually different table. often, foreign keys table refer primary keys another. referenced data linked quickly, critical database normalization. • natural keys data elements stored constructs utilized primary keys. • primary keys values identify unique rows table attributes associated them. example, form social security number that’s related specific person. relational model data, primary key candidate key. it’s primary method identify tuple possible relation. • super keys defined relational model set attributes relation variable. holds relations assigned variable don’t distinct tuples. don’t values attributes set. super keys defined set attributes relational variable functionality depends. reading different types keys rdbms?\n",
      "\n",
      "artificial intelligence s statistics ai, ml, data science great deal overlap, it’s crucial cover bases ai interview. however, it’s important note fields aren’t interchangeable. relative, ai produces actions, ml produces predictions, data sci- ence produces insights. kind potential data science-related ai s prepared for? let’s look. 16. python’s standard library, packages useful data scientists? python wasn’t built data science. however, recent years grown go-to programming language following • machine learning • predictive analytics • simple data analytics • statistics data science projects, following packages python stand- ard library life easier accelerate deliveries • numpy (to process large multidimensional arrays, extensive collections high-level mathematical functions, matrices) • pandas (to leverage built-in methods rapidly combining, filtering, grouping data) • scipy (to extend numpy’s capabilities solve tasks related integral calculus, linear algebra, probability theory)\n",
      "\n",
      "reading 20 python interview s answers—start pre- paring ideal job 17. collaborative filtering? collaborative filtering described process finding pat- terns available information build personalized recommenda- tions. find collaborative filtering action visit websites like amazon imdb. known social filtering, approach essentially makes sug- gestions based recommendations preferences peo- ple share similar interests. reading collaborative filtering 18. list disadvantages related linear models? disadvantages linear models, main ones are • errors linearity assumptions • lacks autocorrelation • can’t solve overfitting problems • can’t use calculate outcomes binary outcomes reading limitations linear regression modeling data analysis? 19. what’s feature vector? feature vector n-dimensional vector contains essential information describes characteristics object. exam- ple, object’s numerical features list numbers taken output neural network layer.\n",
      "\n",
      "ai data science, feature vectors represent numeric symbolic characteristics object mathematical terms seamless analysis. let’s break down. data set usually organized multiple examples example features. however, feature vector won’t feature numerous examples. instead, example correspond feature vector contain numerical values example object. feature vectors stacked design matrix. sce- nario, row feature vector example. column feature examples correspond particular fea- ture. means like matrix, row multiple columns (or single column multiple rows) like [1,2,3,5,6,3,2,0]. reading extract feature vector image pytorch 20. typical characteristics elements list dictionary? lists, elements maintain order explicitly com- manded re-order. data type mixed. however, elements lists accessed numeric, zero-based indices. dictionary, order isn’t guaranteed. however, entry assigned key value. result, elements diction- ary accessed individual key. set unique keys, use diction- ary. collection items order, use list. it’s difficult predict ai interview unfold, fol- low asking list keys dictionary, respond following\n",
      "\n",
      "obtain list keys dictionary, you’ll use following function keys() mydict={‘a’1,’b’2,’c’3,’e’5} mydict.keys() dict_keys([‘a’, ‘b’, ‘c’, ‘e’]) reading sort python dictionaries key value 21. what’s selection bias? types biases encounter sampling? you’re dealing non-random sample, selection bias occur flaws selection process. happens sub- set data consistently excluded particular attrib- ute. exclusion distort results influence statistical significance test. types biases include survivorship bias undercoverage bias. it’s important consider reduce biases you’ll want smart algorithms accurate predic- tions based data. reading mitigating bias ai models 22. what’s random forest? explain role ai? random forest data construct that’s applied ml projects develop large number random decision trees analyzing var- iables.\n",
      "\n",
      "(source) algorithms leveraged improve way technologies analyze complex data sets. basic premise multiple weak learners combined build strong learner. excellent tool ai ml projects work large labeled unlabeled data sets large number attributes. maintain accuracy data missing. model importance attributes, dimen- sionality reduction. reading machine learning random forest 23. what’s eigenvalue? eigenvector? directions particular linear transformation com- presses, flips, stretches called eigenvalue. eigenvectors understand linear transformations.\n",
      "\n",
      "example, better sense covariance covariance matrix, eigenvector help identify direction covariances going. eigenvalues express importance feature. eigenvalues eigenvectors critical computer vision ml applications. popular known principal component analysis dimensionality reduction (e.g., eigenfaces face recognition). reading eigenvectors eigenvalues? 24. use batch normalization? so, explain why? idea standardize data sending layer. approach helps reduce impact previous layers keeping mean variance constant. makes layers independent achieve rapid convergence. example, normalize features 0 1 1 100, helps accelerate learning cycle. check video batch normalization work? artificial intelligence s programming ai interview s bound enter sphere programming sooner later. let’s dive right following ai s answers.\n",
      "\n",
      "25. what’s hash table? parts hash table. array, actual table data stored, mapping function that’s known hash function. it’s data structure implements associative array abstract data type map key values. compute index array slots buckets desired value found. (source) reading basics hash tables 26. different algorithm techniques use ai ml? algorithm techniques leveraged are • learning learn • reinforcement learning (deep adversarial networks, q-learning, temporal difference) • semi-supervised learning\n",
      "\n",
      "• supervised learning (decision trees, linear regression, naive bayes, nearest neighbor, neural networks, support vector machines) • transduction • unsupervised learning (association rules k-means clustering) reading types machine learning algorithms know 27. choosing algorithm solve business problem? first, develop “problem statement” that’s based problem provided business. step essential it’ll help ensure fully understand type problem input output problem want solve. problem statement simple single sentence. example, let’s consider enterprise spam requires algorithm identify it. problem statement be “is email fake/spam not?” scenario, identification it’s fake/spam output. defined problem statement, identify appropriate algorithm following • classification algorithm • clustering algorithm • regression algorithm • recommendation algorithm algorithm use depend specific problem you’re trying solve. scenario, forward cluster-\n",
      "\n",
      "ing algorithm choose k-means algorithm achieve goal filtering spam email system. examples aren’t necessary answering s artificial intelligence, help easier point across. reading choose ml algorithm machine learning ques- tions & answers – iii 28. necessary update algorithm? update algorithm underlying data source changed there’s case non-stationarity. algorithm updated want model evolve data streams infrastructure. reading “the data changed” error stepping main form subform 29. what’s regularization? underfitting overfitting issues statistical model, use regularization technique resolve it. regular- ization techniques like lasso help penalize model parameters likely lead overfitting. interviewer follows methods avoid overfitting, mention cross-valida- tion techniques k-folds cross-validation. approach model simple taking account fewer variables parameters. helps remove noise training data. reading machine learning explained regularization\n",
      "\n",
      "30. what’s difference inductive, deductive, abductive learning? inductive learning describes smart algorithms learn set instances draw conclusions. statistical ml, k-nearest neighbor support vector machine good examples inductive learning. literals (top-down) inductive learning • arithmetic literals • equality inequality • predicates deductive learning, smart algorithms draw conclusions fol- lowing truth-generating structure (major premise, minor premise, conclusion) improve based previous decisions. scenario, ml algorithm engages deductive reasoning decision tree. abductive learning dl technique conclusions based instances. approach, inductive reasoning applied causal relationships deep neural networks. reading what’s difference “inductive, “deductive” “abductive” reasoning? 31. steps evaluate effectiveness ml model? split data set training test sets. option cross-validation technique seg- ment data set composite training test sets data. implement choice selection performance metrics like following\n",
      "\n",
      "• confusion matrix • accuracy • precision • recall sensitivity • specificity • f1 score part, use measures accuracy, confusion matrix, f1 score. however, it’ll critical demonstrate understand nuances model measured choosing right performance measure match problem. reading performance metrics classification problems machine learning 32. data data set missing corrupted? data missing corrupted, replace value drop rows columns altogether. pandas, isnull() dropna() handy tools find missing cor- rupted data drop values. use fillna() method fill invalid values placeholder—for example, “0.” reading 5 ways handle missing values machine learning datasets 33. know build simple neural network? demonstrate python code? types s designed ascertain programming skills. you’re coding ninja, able achieve lines python code.\n",
      "\n",
      "(source) reading build simple neural network 9 lines python code 34. write python program draw flower shown below? (source)\n",
      "\n",
      "again, interviewer trying test coding skills. it’s good perceive types s opportunity potential employer do. demonstrate program- ming skills hesitation confidence. (source) reading drawing flower python turtle artificial intelligence s general ai interest tech talent shortage created fierce demand skills, land “dream job” it’ll help demonstrate pas- sion field. scheduled ai interview startup established tech giant, ready wide-ranging ques- tions like ones listed below.\n",
      "\n",
      "35. research experience ai? present, lot work ai space research-based. result, organizations digging background ascertain kind experience area. authored co-authored research papers supervised industry leaders, sure share information. fact, step summary research experience research papers ready share interviewing panel. however, don’t formal research experience, explanation ready. example, talk ai jour- ney started weekend hobby grew space years. 36. what’s ai-related research paper read? conclusions? you’re passionate ai, scientific research field. excellent place start following sciencedirect track published research papers what’s pipeline.\n",
      "\n",
      "(source) 37. what’s favorite use case? like research, date what’s going industry. such, you’re asked use cases, sure examples mind share. possible, bring personal experiences. share what’s happening industry. example, you’re interested use ai medical images, health analyt- ics interesting use cases • detecting fractures musculoskeletal injuries • aiding diagnosis neurological diseases • flagging thoracic complications conditions • screening common cancers 38. conferences hoping attend year? keynote speeches you’re hoping catch? conferences great places network, attend workshops, learn, grow. you’re planning stick career artificial intelli- gence, going these. example, deep learning world great summer. year’s event las vegas feature keynote speakers like dr. dyann daley (founder ceo predict align prevent), siddha ganju (solutions architect nvidia), dr. alex glushkovsky (prin- cipal data scientist bmo financial group, others). 39. google training data self-driving cars? you’re interested heavily involved space, ques- tion no-brainer. know answer, it’ll demonstrate knowledge variety ml methods ml applied\n",
      "\n",
      "autonomous vehicles. don’t know answer, stab creativity inventive nature. google recaptcha source labeled data store- fronts traffic signs years now. company training data collected sebastian thrun, ceo kitty hawk corporation co-founder (and ceo) udacity. information, significant, potential employer you’re interested excited field. reading google x leveraging data algorithms self-driv- ing cars 40. usually source data sets? talk ai projects you’ve worked free time, interviewer probably ask sourced data sets. you’re genuinely passionate field, worked projects know find free data sets. example, freely available public data sets know (without conducting google search) • celebfaces (with 200,000 celebrity images 40 attribute annotations) • cifar (with 60,000 images map 10 different classes) • youtube-8m (with 4,000 annotated entities taken enormous data set youtube videos) researchers released hundreds free resources like actual network architecture weights exam- ples. serve explore data sets run experiments heading ai interview. reading find datasets artificial intelligence training\n",
      "\n",
      "41 essential machine learning interview s www.springboard.com 18 mins read\n",
      "\n",
      "m achine learning interview s integral data science interview path data scientist, machine learning engineer, data engi- neer. springboard created free guide data science interviews, know exactly trip candidates! order help resolve that, curated created list key s machine learning interview. answers don’t stumped. you’ll able job interview (even machine learning internship) reading piece. machine learning interview s categories we’ve traditionally seen machine learning interview s pop categories. algorithms theory machine learning. you’ll under- standing algorithms compare measure efficacy accuracy right way. second cat- egory programming skills ability exe- cute algorithms theory. general interest machine learning you’ll asked what’s going industry latest machine learning trends. finally, company industry-spe- cific s test ability general machine\n",
      "\n",
      "learning knowledge turn actionable points drive bot- tom line forward. we’ve divided guide machine learning interview s categories mentioned easily information need comes machine learning inter- view s. machine learning interview s algorithms/theory algorithms s test grasp theory machine learning. q1- what’s trade-off bias variance? reading bias-variance tradeoff (wikipedia) bias error erroneous overly simplistic assumptions learning algorithm you’re using. lead model underfit- ting data, making hard high predictive accuracy generalize knowledge training set test set. variance error complexity learning algo- rithm you’re using. leads algorithm highly sensitive high degrees variation training data, lead model overfit data. you’ll carrying noise training data model useful test data. bias-variance decomposition essentially decomposes learning error algorithm adding bias, variance bit irreducible error noise underlying dataset. essentially, model complex add variables, you’ll lose bias gain variance — order optimally reduced error, you’ll tradeoff bias variance. don’t want high bias high variance model.\n",
      "\n",
      "q2- difference supervised unsu- pervised machine learning? reading difference supervised unsuper- vised machine learning? (quora) supervised learning requires training labeled data. example, order classification (a supervised learning task), you’ll need label data you’ll use train model classify data labeled groups. unsupervised learning, contrast, require labeling data explicitly. q3- knn different k-means clustering? reading k-nearest neighbor algorithm different k-means clustering? (quora) k-nearest neighbors supervised classification algorithm, k-means clustering unsupervised clustering algorithm. mechanisms similar first, means order k-nearest neighbors work, need labeled data want classify unlabeled point (thus nearest neighbor part). k-means clustering requires set unlabeled points threshold algorithm unlabeled points gradually learn cluster groups computing mean distance different points. critical difference knn needs labeled points supervised learning, k-means doesn’t — unsu- pervised learning. q4- explain roc curve works. reading receiver operating characteristic (wikipedia) roc curve graphical representation contrast true positive rates false positive rate thresholds. it’s proxy trade-off sensitivity\n",
      "\n",
      "model (true positives) vs fall-out probability trig- ger false alarm (false positives). q5- define precision recall. reading precision recall (wikipedia) recall known true positive rate positives model claims compared actual number positives data. precision known positive pre- dictive value, measure accurate positives model claims compared number positives actually claims. easier think recall precision context case you’ve predicted 10 apples 5 oranges case 10 apples. you’d perfect recall (there actually 10 apples, predicted 10) 66.7% precision 15 events predicted, 10 (the apples) correct.\n",
      "\n",
      "q6- bayes’ theorem? useful machine learning context? reading intuitive (and short) explanation bayes’ theorem (betterexplained) bayes’ theorem gives posterior probability event given known prior knowledge. mathematically, it’s expressed true positive rate condition sample divided sum false positive rate population true positive rate condition. 60% chance actually having flu flu test, people flu, test false 50% time, overall population 5% chance having flu. actually 60% chance having flu having positive test? bayes’ theorem says no. says (.6 * 0.05) (true pos- itive rate condition sample) / (.6*0.05)(true positive rate condition sample) + (.5*0.95) (false positive rate population) = 0.0594 5.94% chance getting flu. bayes’ theorem basis branch machine learning notably includes naive bayes classifier. that’s\n",
      "\n",
      "important consider you’re faced machine learning inter- view s. q7- “naive” bayes naive? reading “naive bayes” naive? (quora) despite practical applications, especially text mining, naive bayes considered “naive” makes assumption virtually impossible real-life data conditional probabil- ity calculated pure product individual probabilities components. implies absolute independence features — condition probably met real life. quora commenter whimsically, naive bayes classifier figured liked pickles ice cream probably naively recommend pickle ice cream. q8- explain difference l1 l2 regulariza- tion. reading difference l1 l2 regulariza- tion? (quora) l2 regularization tends spread error terms, l1 binary/sparse, variables assigned 1 0 weighting. l1 corresponds setting laplacean prior terms, l2 corresponds gaussian prior.\n",
      "\n",
      "q9- what’s favorite algorithm, explain minute? type tests understanding communi- cate complex technical nuances poise ability sum- marize quickly efficiently. sure choice sure explain different algorithms simply effectively five-year-old grasp basics! q10- what’s difference type type ii error? reading type type ii errors (wikipedia) don’t think trick ! machine learning inter- view s attempt lob basic s sure you’re game you’ve prepared bases. type error false positive, type ii error false negative. briefly stated, type error means claiming happened hasn’t, type ii error means claim happening fact is. clever way think think type error telling man pregnant, type ii error means tell pregnant woman isn’t carrying baby. q11- what’s fourier transform? reading fourier transform (wikipedia)\n",
      "\n",
      "fourier transform generic method decompose generic func- tions superposition symmetric functions. intuitive tutorial puts it, given smoothie, it’s find rec- ipe. fourier transform finds set cycle speeds, amplitudes phases match time signal. fourier transform converts signal time frequency domain — it’s common way extract features audio signals time series sen- sor data. q12- what’s difference probability likeli- hood? reading difference “likelihood” “proba- bility”? (cross validated) q13- deep learning, contrast machine learning algorithms? reading deep learning (wikipedia) deep learning subset machine learning concerned neural networks use backpropagation certain principles\n",
      "\n",
      "neuroscience accurately model large sets unlabelled semi-structured data. sense, deep learning represents unsupervised learning algorithm learns representations data use neural nets. q14- what’s difference generative dis- criminative model? reading difference generative dis- criminative algorithm? (stack overflow) generative model learn categories data discrimina- tive model simply learn distinction different catego- ries data. discriminative models generally outperform generative models classification tasks. q15- cross-validation technique use time series dataset? reading k-fold cross-validation time-series model selection (crossvalidated) instead standard k-folds cross-validation, pay attention fact time series randomly distributed data — inherently ordered chronological order. pattern emerges later time periods example, model pick effect doesn’t hold earlier years! you’ll want like forward chaining you’ll able model past data look forward-facing data. • fold 1 training [1], test [2] • fold 2 training [1 2], test [3] • fold 3 training [1 2 3], test [4] • fold 4 training [1 2 3 4], test [5] • fold 5 training [1 2 3 4 5], test [6] q16- decision tree pruned?\n",
      "\n",
      "reading pruning (decision trees) pruning happens decision trees branches weak predictive power removed order reduce complexity model increase predictive accuracy decision tree model. pruning happen bottom-up top-down, approaches reduced error pruning cost complexity prun- ing. reduced error pruning simplest version replace node. doesn’t decrease predictive accuracy, pruned. simple, heuristic actually comes pretty close approach optimize maximum accuracy. q17- important you– model accuracy, model performance? reading accuracy paradox (wikipedia) tests grasp nuances machine learning model performance! machine learning interview s look details. models higher accuracy perform worse predictive power — sense? well, model accuracy sub- set model performance, that, misleading one. example, wanted detect fraud massive dataset sample millions, accurate model likely predict fraud vast minority cases fraud. however, useless predictive model — model designed find fraud asserted fraud all! s like help demonstrate understand model accuracy isn’t be-all end-all model performance. q18- what’s f1 score? use it? reading f1 score (wikipedia)\n",
      "\n",
      "f1 score measure model’s performance. weighted average precision recall model, results tending 1 best, tending 0 worst. use classification tests true negatives don’t matter much. q19- handle imbalanced dataset? reading 8 tactics combat imbalanced classes machine learning dataset (machine learning mastery) imbalanced dataset have, example, classification test 90% data class. leads problems accuracy 90% skewed predictive power category data! tactics hump 1- collect data imbalances dataset. 2- resample dataset correct imbalances. 3- try different algorithm altogether dataset. what’s important keen sense damage unbalanced dataset cause, balance that. q20- use classification regression? reading regression vs classification (math stackexchange) classification produces discrete values dataset strict catego- ries, regression gives continuous results allow better distinguish differences individual points. use classification regression wanted results reflect belongingness data points dataset certain explicit categories (ex wanted know male female correlated male female names.)\n",
      "\n",
      "q21- example ensemble techniques useful. reading ensemble learning (wikipedia) ensemble techniques use combination learning algorithms optimize better predictive performance. typically reduce overfit- ting models model robust (unlikely influ- enced small changes training data). list examples ensemble methods, bagging boosting “bucket models” method demonstrate increase predictive power. q22- ensure you’re overfitting model? reading avoid overfitting? (quora) simple restatement fundamental problem machine learning possibility overfitting training data carrying noise data test set, providing inaccu- rate generalizations. main methods avoid overfitting 1- model simpler reduce variance taking account fewer variables parameters, removing noise training data. 2- use cross-validation techniques k-folds cross-validation. 3- use regularization techniques lasso penalize certain model parameters they’re likely cause overfitting. q23- evaluation approaches work gauge effectiveness machine learning model? reading evaluate machine learning algorithms (machine learning mastery)\n",
      "\n",
      "split dataset training test sets, per- haps use cross-validation techniques segment dataset composite sets training test sets data. implement choice selection performance metrics fairly comprehensive list. use measures f1 score, accuracy, confusion matrix. what’s important demonstrate understand nuances model measured choose right performance measures right situations. q24- evaluate logistic regression model? reading evaluating logistic regression (crossvalidated), lo- gistic regression plain english subsection above. demonstrate understanding typical goals logistic regression (classification, prediction, etc.) bring examples use cases. q25- what’s “kernel trick” useful? reading kernel method (wikipedia) kernel trick involves kernel functions enable higher- dimension spaces explicitly calculating coordinates points dimension instead, kernel functions compute inner products images pairs data feature space. allows useful attribute calculating coordinates higher dimensions computationally cheaper explicit calculation said coordinates. algo- rithms expressed terms inner products. kernel trick enables effectively run algorithms high-dimensional space lower-dimensional data. (learn springboard’s ai / machine learning bootcamp, kind come job guaran- tee.)\n",
      "\n",
      "machine learning interview s programming machine learning interview s test knowledge programming principles need implement machine learning principles practice. machine learning interview s tend technical s test logic programming skills section focuses latter. q26- handle missing corrupted data dataset? reading handling missing data (o’reilly) find missing/corrupted data dataset drop rows columns, decide replace value. pandas, useful methods isnull() dropna() help find columns data missing corrupted data drop values. want fill invalid values placeholder value (for example, 0), use fillna() method. q27- experience spark big data tools machine learning? reading 50 open source tools big data (datamation) you’ll want familiar meaning big data different companies different tools they’ll want. spark big data tool demand now, able handle immense datasets speed. honest don’t experience tools\n",
      "\n",
      "demanded, look job descriptions tools pop up you’ll want invest familiarizing them. q28- pick algorithm. write psuedo-code paral- lel implementation. reading writing pseudocode parallel programming (stack overflow) kind demonstrates ability think parallel- ism handle concurrency programming imple- mentations dealing big data. look pseudocode frameworks peril-l visualization tools web sequence diagrams help demonstrate ability write code reflects parallelism. q29- differences linked list array? reading array versus linked list (stack overflow) array ordered collection objects. linked list series objects pointers direct process sequentially. array assumes element size, unlike linked list. linked list easily grow organically array pre-defined re-defined organic growth. shuffling linked list involves changing points direct — meanwhile, shuffling array complex takes memory. q30- describe hash table. reading hash table (wikipedia) hash table data structure produces associative array. key mapped certain values use hash function. tasks database indexing.\n",
      "\n",
      "q31- data visualization libraries use? thoughts best data visualization tools? reading 31 free data visualization tools (springboard) what’s important define views properly vis- ualize data personal preferences comes tools. popular tools include r’s ggplot, python’s seaborn matplotlib, tools plot.ly tableau. related 20 python interview s machine learning interview s company/industry specific machine learning interview s deal imple- ment general machine learning knowledge specific compa- ny’s requirements. you’ll asked create case studies extend knowledge company industry you’re applying machine learning skills. q32- implement recommendation system company’s users? reading implement recommendation system? (stack overflow) lot machine learning interview s type involve implementation machine learning models company’s problems. you’ll research company industry in-depth, espe-\n",
      "\n",
      "cially revenue drivers company has, types users company takes context industry it’s in. q33- use machine learning skills gen- erate revenue? reading startup metrics startups (500 startups) tricky . ideal answer demonstrate knowledge drives business skills relate. example, interviewing music-streaming startup spotify, remark skills developing bet- ter recommendation model increase user retention, increase revenue long run. startup metrics slideshare linked help understand exactly performance indicators important startups tech companies think revenue growth. q34- think current data process? reading data science process email course – springboard\n",
      "\n",
      "kind requires listen carefully impart feed- manner constructive insightful. interviewer trying gauge you’d valuable member team grasp nuances certain things set way company’s data process based company- indus- try-specific conditions. they’re trying intellec- tual peer. act accordingly. machine learning interview s general machine learning interest series machine learning interview s attempts gauge passion interest machine learning. right answers serve testament commitment lifelong learner machine learning. q35- machine learning papers you’ve read? reading best research papers/books machine learning? keeping latest scientific literature machine learning want demonstrate interest machine learning position. overview deep learning nature scions deep learning (from hinton bengio lecun) good reference paper overview what’s happening deep learning — kind paper want cite. q36- research experience machine learn- ing?\n",
      "\n",
      "related point, organizations hiring machine learn- ing positions look formal experience field. research papers, co-authored supervised leaders field, difference hired not. sure summary research experience papers ready — explanation background lack formal research experience don’t. q37- favorite use cases machine learning models? reading typical use cases different machine learning algorithms? (quora) quora thread contains examples, decision trees categorize people different tiers intelligence based iq scores. sure examples mind describe resonated you. it’s important demon- strate interest machine learning implemented. q38- approach “netflix prize” competi- tion? reading netflix prize (wikipedia) netflix prize famed competition netflix offered $1,000,000 better collaborative filtering algorithm. team won called bellkor 10% improvement ensem- ble different methods win. familiarity case solution help demonstrate you’ve paid attention machine learning while. q39- usually source datasets? reading 19 free public data sets data science project (springboard) machine learning interview s like try heart machine learning interest. somebody truly passionate\n",
      "\n",
      "machine learning gone projects own, good idea great datasets there. you’re missing any, check quandl economic financial data, kaggle’s datasets collection great list. q40- think google training data self- driving cars? reading waymo tech machine learning interview s like test knowledge different machine learning methods, inven- tiveness don’t know answer. google currently recaptcha source labeled data storefronts traffic signs. building training data collected sebastian thrun googlex — obtained grad students driving buggies desert dunes! q41- simulate approach alphago took beat lee sidol go? reading mastering game deep neural networks tree search (nature) alphago beating lee sidol, best human player go, best-of- series truly seminal event history machine learn- ing deep learning. nature paper describes accomplished “monte-carlo tree search deep neural networks trained supervised learning, human expert games, reinforcement learning games self- play.” related 40 artificial intelligence interview s looking land role machine learning engineer? find springboard’s machine learning engineer- ing career track, kind come job guarantee.\n",
      "\n",
      "a.i. wiki interested ai? tips & stories inbox  directory beginner's guide neural networks deep learning contents neural network definition concrete examples neural network elements key concepts deep neural networks example feedforward networks & backprop multiple linear regression updaters custom layers, activation functions loss functions logistic regression & classifiers loss functions deeplearning4j neural networks & artificial intelligence neural network definition neural networks set algorithms, modeled loosely human brain, designed recognize patterns. interpret sensory data kind machine perception, labeling clustering raw input. patterns recognize numerical, contained vectors, real-world data, images, sound, text time series, translated. neural networks help cluster classify. think clustering classification layer data store manage. help group unlabeled data according similarities example inputs, classify data labeled dataset train on. (neural networks extract features fed algorithms clustering classification; think deep neural networks components larger machine-learning applications involving algorithms reinforcement learning, classification regression.) \n",
      "\n",
      "kind problems deep learning solve, importantly, solve yours? know answer, need ask s outcomes care about? outcomes labels applied data example, spam not_spam email filter, good_guy bad_guy fraud detection, angry_customer happy_customer customer relationship management. data accompany labels? is, find labeled data, create labeled dataset (with service like aws mechanical turk figure mighty.ai) spam labeled spam, order teach algorithm correlation labels inputs? concrete examples deep learning maps inputs outputs. finds correlations. known “universal approximator”, learn approximate unknown function f(x) = y input x output y, assuming related (by correlation causation, example). process learning, neural network finds right f, correct manner transforming x y, f(x) = 3x + 12 f(x) = 9x - 0.1. examples deep learning do. classification classification tasks depend labeled datasets; is, humans transfer knowledge dataset order neural network learn correlation labels data. known supervised learning. detect faces, identify people images, recognize facial expressions (angry, joyful) identify objects images (stop signs, pedestrians, lane markers…) recognize gestures video detect voices, identify speakers, transcribe speech text, recognize sentiment voices classify text spam (in emails), fraudulent (in insurance claims); recognize sentiment text (customer feedback) labels humans generate, outcomes care correlate data, train neural network. clustering clustering grouping detection similarities. deep learning require labels detect similarities. learning labels called unsupervised learning. unlabeled data majority data world. law machine learning is data algorithm train on, accurate be. learn build ai apps »\n",
      "\n",
      "therefore, unsupervised learning potential produce highly accurate models. search comparing documents, images sounds surface similar items. anomaly detection flipside detecting similarities detecting anomalies, unusual behavior. cases, unusual behavior correlates highly things want detect prevent, fraud. predictive analytics regressions classification, deep learning able establish correlations between, say, pixels image person. static prediction. token, exposed right data, deep learning able establish correlations present events future events. run regression past future. future event like label sense. deep learning doesn’t necessarily care time, fact hasn’t happened yet. given time series, deep learning read string number predict number likely occur next. hardware breakdowns (data centers, manufacturing, transport) health breakdowns (strokes, heart attacks based vital stats data wearables) customer churn (predicting likelihood customer leave, based web activity metadata) employee turnover (ditto, employees) better predict, better prevent pre-empt. see, neural networks, we’re moving world fewer surprises. zero surprises, marginally fewer. we’re moving world smarter agents combine neural networks algorithms like reinforcement learning attain goals. brief overview deep learning use cases, let’s look neural nets of. neural network elements deep learning use “stacked neural networks”; is, networks composed layers. machine learning enterprise applications? skymind platform help ship faster. read platform overview request demo. layers nodes. node place computation happens, loosely patterned neuron human brain, fires encounters sufficient stimuli. node combines input data set coefficients, weights, amplify dampen input, assigning significance inputs regard task algorithm trying learn; e.g. input helpful classifying data error? input-weight\n",
      "\n",
      "products summed sum passed node’s so-called activation function, determine extent signal progress network affect ultimate outcome, say, act classification. signals passes through, neuron “activated.” here’s diagram node look like. node layer row neuron-like switches turn input fed net. layer’s output simultaneously subsequent layer’s input, starting initial input layer receiving data. pairing model’s adjustable weights input features assign significance features regard neural network classifies clusters input. key concepts deep neural networks deep-learning networks distinguished commonplace single- hidden-layer neural networks depth; is, number node layers data pass multistep process pattern recognition. earlier versions neural networks perceptrons shallow, composed input output layer, hidden layer between. layers (including input output) qualifies “deep” learning. deep buzzword algorithms like read sartre listen bands haven’t heard yet. strictly defined term means hidden layer. deep-learning networks, layer nodes trains distinct set features based previous layer’s output. advance neural net, complex features nodes recognize, aggregate recombine features previous layer.\n",
      "\n",
      "known feature hierarchy, hierarchy increasing complexity abstraction. makes deep-learning networks capable handling large, high-dimensional data sets billions parameters pass nonlinear functions. all, neural nets capable discovering latent structures unlabeled, unstructured data, vast majority data world. word unstructured data raw media; i.e. pictures, texts, video audio recordings. therefore, problems deep learning solves best processing clustering world’s raw, unlabeled media, discerning similarities anomalies data human organized relational database to. example, deep learning million images, cluster according similarities cats corner, ice breakers another, photos grandmother. basis so-called smart photo albums. apply idea data types deep learning cluster raw text emails news articles. emails angry complaints cluster corner vector space, satisfied customers, spambot messages, cluster others. basis messaging filters, customer-relationship management (crm). applies voice messages. time series, data cluster normal/healthy behavior anomalous/dangerous behavior. time series data generated smart phone, provide insight users’ health habits; generated autopart, prevent catastrophic breakdowns. deep-learning networks perform automatic feature extraction human intervention, unlike traditional machine-learning algorithms. given feature extraction task teams data scientists years accomplish, deep learning way circumvent chokepoint limited experts. augments powers small data science teams, nature scale.\n",
      "\n",
      "training unlabeled data, node layer deep network learns features automatically repeatedly trying reconstruct input draws samples, attempting minimize difference network’s guesses probability distribution input data itself. restricted boltzmann machines, examples, create so-called reconstructions manner. process, neural networks learn recognize correlations certain relevant features optimal results – draw connections feature signals features represent, reconstruction, labeled data. deep-learning network trained labeled data applied unstructured data, giving access input machine-learning nets. recipe higher performance data net train on, accurate likely be. (bad algorithms trained lots data outperform good algorithms trained little.) deep learning’s ability process learn huge quantities unlabeled data distinct advantage previous algorithms. deep-learning networks end output layer logistic, softmax, classifier assigns likelihood particular outcome label. predictive, predictive broad sense. given raw data form image, deep-learning network decide, example, input data 90 percent likely represent person. example feedforward networks goal neural net arrive point error fast possible. running race, race track, pass points repeatedly loop. starting line race state weights initialized, finish line state parameters capable producing sufficiently accurate classifications predictions. race involves steps, steps resembles steps after. like runner, engage repetitive act arrive finish. step neural network involves guess, error measurement slight update weights, incremental adjustment coefficients, slowly learns pay attention important features. collection weights, start end state, called model, attempt model data’s relationship ground-truth labels, grasp data’s structure. models normally start bad end bad, changing time neural network updates parameters. neural network born ignorance. know weights biases translate input best correct guesses. start guess, try better guesses sequentially\n",
      "\n",
      "learns mistakes. (you think neural network miniature enactment scientific method, testing hypotheses trying – scientific method blindfold on. like child born knowing much, exposure life experience, slowly learn solve problems world. neural networks, data experience.) simple explanation happens learning feedforward neural network, simplest architecture explain. input enters network. coefficients, weights, map input set guesses network makes end. input * weight = guess weighted input results guess input is. neural takes guess compares ground-truth data, effectively asking expert “did right?” ground truth - guess = error difference network’s guess ground truth error. network measures error, walks error model, adjusting weights extent contributed error. error * weight's contribution error = adjustment pseudo-mathematical formulas account key functions neural networks scoring input, calculating loss applying update model – begin three-step process again. neural network corrective feedback loop, rewarding weights support correct guesses, punishing weights lead err. let’s linger step above. multiple linear regression despite biologically inspired name, artificial neural networks math code, like machine-learning algorithm. fact, understands linear regression, methods learn statistics, understand neural net works. simplest form, linear regression expressed y_hat = bx + y_hat estimated output, x input, b slope intercept line vertical axis two-dimensional graph. (to concrete x radiation exposure y cancer risk; x daily pushups y_hat total weight benchpress; x fertilizer y_hat size crop.) imagine time add unit x, dependent variable y_hat increases proportionally, matter far x axis. simple relation variables moving starting point.\n",
      "\n",
      "step imagine multiple linear regression, input variables producing output variable. it’s typically expressed like this y_hat = b_1*x_1 + b_2*x_2 + b_3*x_3 + (to extend crop example above, add sunlight rainfall growing season fertilizer variable, affecting y_hat.) now, form multiple linear regression happening node neural network. node single layer, input node previous layer recombined input node. is, inputs mixed different proportions, according coefficients, different leading node subsequent layer. way, net tests combination input significant tries reduce error. sum node inputs arrive y_hat, it’s passed non-linear function. here’s why node merely performed multiple linear regression, y_hat increase linearly limit x’s increase, doesn’t suit purposes. trying build node switch (like neuron…) turns off, depending let signal input pass affect ultimate decisions network. switch, classification problem. input’s signal indicate node classify enough, not_enough, off? binary decision expressed 1 0, logistic regression non-linear function squashes input translate space 0 1. nonlinear transforms node usually s-shaped functions similar logistic regression. names sigmoid (the greek word “s”), tanh, hard tanh, etc., shaping output node. output nodes, squashed s-shaped space 0 1, passed input layer feed forward neural network, signal reaches final layer net, decisions made. gradient descent commonly optimization function adjusts weights according error caused called “gradient descent.” gradient word slope, slope, typical form x-y graph, represents variables relate other rise run, change money change time, etc. particular case, slope care describes relationship network’s error single weight; i.e. is, error vary weight adjusted. finer point it, weight produce error? correctly represents signals contained input data, translates correct classification? hear “nose” input image, know labeled face frying pan?\n",
      "\n",
      "neural network learns, slowly adjusts weights map signal meaning correctly. relationship network error weights derivative, de/dw, measures degree slight change weight causes slight change error. weight factor deep network involves transforms; signal weight passes activations sums layers, use chain rule calculus march networks activations outputs finally arrive weight , relationship overall error. chain rule calculus states feedforward network, relationship net’s error single weight look like this is, given variables, error weight, mediated variable, activation, weight passed, calculate change weight affects change error calculating change activation affects change error, change weight affects change activation. essence learning deep learning that adjusting model’s weights response error produces, can’t reduce error more. optimization algorithms examples optimization algorithms include adadelta adagrad adam nesterovs rmsprop sgd conjugate gradient hessian free lbfgs line gradient descent activation functions activation function determines output node generate, based input. deeplearning4j, activation function set layer level\n",
      "\n",
      "applies neurons layer. examples include cube elu hardsigmoid hardtanh identity leakyrelu rationaltanh relu rrelu sigmoid softmax softplus softsign tanh custom layers, activation functions loss functions deeplearning4j, major ai frameworks skymind supports alongside keras, includes custom layers, activations loss functions. logistic regression deep neural network layers, final layer particular role. dealing labeled input, output layer classifies example, applying likely label. node output layer represents label, node turns according strength signal receives previous layer’s input parameters. output node produces possible outcomes, binary output values 0 1, input variable deserves label . all, thing little pregnant. neural networks working labeled data produce binary output, input receive continuous. is, signals network receives input span range values include number metrics, depending problem seeks solve. example, recommendation engine binary decision serve ad not. input bases decision include customer spent amazon week, customer visits site. output layer condense signals $67.59 spent diapers, 15 visits website, range 0 1; i.e. probability given input labeled not.\n",
      "\n",
      "mechanism use convert continuous signals binary output called logistic regression. unfortunate, logistic regression classification regression linear sense people familiar with. calculates probability set inputs match label. let’s examine little formula. continuous inputs expressed probabilities, output positive results, thing negative probability. that’s input exponent e denominator – exponents force results greater zero. consider relationship e’s exponent fraction 1/1. one, know, ceiling probability, results can’t absurd. (we’re 120% sure that.) input x triggers label grows, expression e x shrinks zero, leaving fraction 1/1, 100%, means approach (without reaching) absolute certainty label applies. input correlates negatively output value flipped negative sign e’s exponent, negative signal grows, quantity e x larger, pushing entire fraction closer zero. imagine that, having x exponent, sum products weights corresponding inputs – total signal passing net. that’s you’re feeding logistic regression layer output layer neural network classifier. layer, set decision threshold example labeled 1, not. set different thresholds prefer – low threshold increase number false positives, higher increase number false negatives – depending like err. loss functions deeplearning4j deeplearning4j supports following loss functions. mse mean squared error linear regression expll exponential log likelihood poisson regression xent cross entropy binary classification mcxent multiclass cross entropy rmse_xent rmse cross entropy squared_loss squared loss negativeloglikelihood negative log likelihood neural networks & artificial intelligence circles, neural networks thought “brute force” ai,\n",
      "\n",
      "start blank slate hammer way accurate model. effective, eyes inefficient approach modeling, can’t assumptions functional dependencies output input. said, gradient descent recombining weight find best match – method pathfinding shrinks relevant weight space, number updates required computation, orders magnitude. moreover, algorithms hinton’s capsule networks require far fewer instances data converge accurate model; is, present research potential resolve brute force nature deep learning. reading recipe training neural networks, andrej karpathy interactive demo learn build ai applications interactive learning portal. try company press kit contact press privacy platform skil subscriptions documentation community support international english japanese follow facebook twitter linkedin\n",
      "\n",
      "gitter subscribe integrateai, bi-weekly newsletter ai applications real world subscribe\n",
      "\n",
      "data science interview s\n",
      "\n",
      "faq interviews calm nerves know job seeker struggles. that’s data science interview s cover bunch different topics (data science interdisciplinary field, all) cheeky interviewers love throw odd curveball. faq interviews… problem\n",
      "\n",
      "1. data science mean? 2. assumptions linear regres- sion? 3. difference factor analysis cluster analysis? 4. iterator generator? 5. write sql script return data tables. 6. draw graphs relevant pay-per-click ad- verts ticket purchases. 7. explain random forest non-technical person? 8. prove improvement introduced model actually working? 9. root cause analysis? faq interviews\n",
      "\n",
      "10. explain k-means. 11. kind rdbms software experience with? non-relational databases? 12. supervised learning vs unsupervised learning. 13. overfitting fix it? 14. difference sql, mysql sql server? 15. start cleaning big dataset? 16. examples false negative important false positive, vice versa. 17. state biases likely en- counter cleaning database. 18. logistic regression? faq interviews\n",
      "\n",
      "calm nerves know job seeker struggles. that’s data science interview s cover bunch different topics (data science interdisciplinary field, all) cheeky interviewers love throw odd curveball. step hitting curveballs park coming, coming you’ve got confident rest game. so, homework! interviewer spot hasn’t mile away, wouldn’t didn’t know though, you? faq interviews\n",
      "\n",
      "plenty articles example answers hope yes, technical s come up. remember hundred-odd different examples serve confuse more, plus comes didn’t study for? want interview typology. data science interview s interviewers looking for. answering data science s\n",
      "\n",
      "1. technical s 1.1 mathematics 1.2 statistics 1.3 coding 1.4 machine learning 2. practical experience s 3. behavioral s 4. scenarios (case study s) contents let’s break things\n",
      "\n",
      "technical s strong grasp mathematics, statistics, coding, machine learning data scientist. likely asked demonstrate hands-on technical skills prepare theoretical techniques, too!\n",
      "\n",
      "mathematics\n",
      "\n",
      "mathematics underpins study machine learning, statistics, algorithms, computer architecture, others. so, applied maths heart matter. showing good grasp mathematics signals interviewer quickly adapt fields. s like check basic maths skills shouldn’t tricky you. mathematics\n",
      "\n",
      "prepared answer quick (mental) maths s, as • sum numbers 1 100? • snail falls 50ft deep. day climbs 3ft, night slides 1ft. days out? • 10x10x10 cube, thousand 1x1x1 cubes. remove outer layer structure, cubes left? mathematics\n",
      "\n",
      "real-life data science interview s • race track 5 lanes. 25 horses like find 3 fastest horses 25. minimum number races need conduct determine 3 fastest horses? things little interesting encountering puzzle s test lateral thinking. mathematics\n",
      "\n",
      "• people need cross rickety bridge night. unfortunately, single torch bridge dangerous cross one. bridge strong support people time. people time cross bridge. times person 1 min, 2 mins, 7 mins 10 mins. shortest time needed cross bridge? mathematics\n",
      "\n",
      "finally, hard maths problems. unlikely you’ll given equation solve, you’ll asked simply worded requires conceptual preparation answer. furthermore, intertwine probability theory, doesn’t. mathematics\n",
      "\n",
      "examples are • consider extension rock, paper, scissors n options instead 3 options. values n possible construct fair game, ‘fair’ mean player plays equal number moves beat lose it? • country people want boys, family continues children boy. girl, child. boy, stop. proportion boys girls country? mathematics\n",
      "\n",
      "statistics\n",
      "\n",
      "know, data scientists called statisticians? professions aren’t same, data scientists finished statistics degree. that’s wonder! statistics ‘founding fathers’ data science. logically, tested ability reason statistically. theoretical knowledge isn’t strongest suit, need use precise technical language. statistics\n",
      "\n",
      "consider following difference false positive false negative? need provide textbook definitions…got you! wants hear generic theory; it’s boring blend crowd. employers want identify situations implement theory. statistics\n",
      "\n",
      "talking statistics, s pop up? • null hypothesis state it? • explain linear regression business executive? • tell heteroskedasticity solve it. statistics\n",
      "\n",
      "• what’s central limit theorem practical implications? • find correlation categorical variable continuous variable? • explain p-value. present talking client. • understand statistical power calculate it? • explain differences overfitting underfitting. • explain cross-validation is. used? statistics\n",
      "\n",
      "think machine learning s? spotted, ml overlaps statistical concepts! • examples data gaussian distribution, log-normal? • explain bootstrapping you’re talking non-technical person. • state biases likely encounter cleaning database. statistics\n",
      "\n",
      "coding\n",
      "\n",
      "data scientist needs certain programming knowledge. don’t pro, employers want decent grip potential rapid improvement. python, r, sql bread-and-butter programming languages data science. s staples come surprise. coding\n",
      "\n",
      "r • missing values impossible values represented r? • difference lapply sapply? • merge data frames r? • command store r objects file? • split continuous variable different groups/ranks r? • explain key differences python r. coding\n",
      "\n",
      "python • python library prefer use data wrangling? • build simple logistic regression python? • what’s shortest way open text file python? • web scraping python? that? • explain ‘pass’ python. • explain perform pattern matching python. • tool use find bugs? • what’s preferred library plotting python seaborn matplotlib? coding\n",
      "\n",
      "sql • table called cust_id, order_date, order_id, tran_amt. select 100 customers highest spend year-long period? • describe different parts sql query. • difference union union all? • write sql script return data tables. • tell difference primary key unique key. • difference sql, mysql sql server? coding\n",
      "\n",
      "machine learning\n",
      "\n",
      "machine learning familiarity machine learning methodologies essential aspiring data scientist. prepared explain key concepts nutshell. it’s possible interviewer outline prediction problem ask come algorithms. algorithms, expect touch commonly observed problems fixes.\n",
      "\n",
      "check following machine learning s we’ve picked you • difference supervised unsupervised machine learning? • deal imbalanced dataset? • ensure overfitting model? • approaches use evaluate prediction accuracy logistics regression model? • deal sparse data? • explain bias- variance trade-off? machine learning\n",
      "\n",
      "additionally, stumble way specific way vague s as • explain difference gaussian mixture model k- means. • tell machine learning project admire. machine learning\n",
      "\n",
      "practical experience s\n",
      "\n",
      "technical s important, data scientist needs know answers practice. countless data science s interviewer going waste time asking dozens s gauge candidate them. instead, ask experience. practical experience qs\n",
      "\n",
      "practical experience qs practical experience s, designed shed light pace work, experiences, habits. avoid having sift catalogue experiences spot, mind experiences versatile – ones exemplify different skills based .\n",
      "\n",
      "practical experience qs let’s taste those • summarize experience. • tell data science pet project. • news politics, economics, business? data science? • so, python preferred programming language. experience r? tell that.\n",
      "\n",
      "practical experience qs course, vice-versa • so, r preferred programming language. experience python? tell that. • experience tableau? • kind rdbms software experience with?\n",
      "\n",
      "behavioral s\n",
      "\n",
      "like job interview, employers interested handle workplace situations, work team good fit company. behavioural s asked indirectly, example, interviewer pose broad s motivation tasks enjoy. certainly, right answer here. intent judge past responses accurately predict future behavior. let’s example describe situation faced conflict working team project. behavioral qs\n",
      "\n",
      "instead asking hypothetical s (“how deal with…”), interviewer hoping elicit meaningful response pushing chat real-life past event. interviewer looking things story situation context? (devote 10% answer time) task needed done? (devote 10% answer time) action do? (devote 70% answer time) results accomplishments? (devote 10% answer time) known star technique, steps help present answers clear succinct fashion. behavioral qs\n",
      "\n",
      "dying examples? go • describe data science project worked (yes! overlaps ‘practical experience category!) • tell situation balance competing priorities. • describe time managed persuade things way. • describe time bored work. motivate yourself? • failed meet deadline. • team brand new under- financed. standard procedures training, ad-hoc. situation? behavioral qs\n",
      "\n",
      "case study s\n",
      "\n",
      "purpose scenarios (case study s) test experience data science fields. case study s likely look skills outside technical toolkit. instance, looking logical reasoning business understanding. it’s important demonstrate structured thinking, reasoning, problem-solving skills. all, can’t good data scientist identify underlying problems. case study qs\n",
      "\n",
      "let’s works • sales department increased selling price items 5%. 10 items, different price tags. price increase, gross revenue $500,000 average selling price $1. price increase, gross revenue $505,000, average selling price $0.95. hasn’t price increase desired impact increasing revenue average selling price? case study qs\n",
      "\n",
      "given market sizing s, called guestimates some, term sounds like need stab dark, case. reaching conclusion require degree guesswork estimation, process use difficult requires rigid logic. case study qs\n",
      "\n",
      "single correct answer s like chances interviewer doesn’t know exact answer, either. example • suv’s parking lot downstairs? ping-pong balls fit room? case study qs\n",
      "\n",
      "interview dialogue, written test! excellent, consider typology starting point interview prep. however, scratched surface comes examples data science interview s encounter. industry booming such, companies constantly adapting interview sessions (what common today hardly asked 2 years). data science interview s vary peculiarities, types s remain same, having base knowledge types good preparation allow logically tackle interviewer sleeve.\n",
      "\n",
      "authors 365 data science online educational career website offers incredible opportunity find way data science world matter previous knowledge experience. comprehensive programs suit needs aspiring bi analysts, data analysts, data scientists. we, authors, committed educators believe curio- sity hindered inability access good learning resources. focus efforts creating high-quality educational content access online. courses cover necessary topics build data science skills ground up, including mathematics statistics, python, r, sql, data visualization, machine deep learning.\n",
      "\n",
      "comprehensive data science curriculum grow data science skillset training 365 data science program comprehensive set courses, work help student learn need expert data scientist months. training includes sought-after skills, including • fundamentals mathematics • probability • intro data & data science • tableau • sql • r • python • machine learning program consists 45 hours on-demand video, split 12 courses, real-life business examples, 300 exercises.\n",
      "\n",
      "good luck!\n",
      "\n",
      "https//career.guru99.com/ 50 machine learning interview s & answers 1) machine learning? machine learning branch computer science deals system programming order automatically learn improve experience. example robots programed perform task based data gather sensors. automatically learns programs data. 2) mention diﬀerence data mining machine learning? machine learning relates study, design development algorithms computers capability learn explicitly programmed. while, data mining deﬁned process unstructured data tries extract knowledge unknown interesting patterns. process machine, learning algorithms used. 3) ‘overﬁtting’ machine learning? machine learning, statistical model describes random error noise instead underlying relationship ‘overﬁtting’ occurs. model excessively complex, overﬁtting normally observed, having parameters respect number training data types. model exhibits poor performance overﬁt. 4) overﬁtting happens? possibility overﬁtting exists criteria training model criteria judge eﬃcacy model. 5) avoid overﬁtting ? lot data overﬁtting avoided, overﬁtting happens relatively small dataset, try learn it. small database forced come model based that. situation, use technique known cross validation. method dataset splits section, testing training datasets, testing dataset test model while, training dataset, datapoints come model. technique, model usually given dataset known data training (training data set) run dataset unknown data model tested. idea cross validation deﬁne dataset “test” model training phase. 6) inductive machine learning? inductive machine learning involves process learning examples, system, set observed instances tries induce general rule.\n",
      "\n",
      "7) ﬁve popular algorithms machine learning? a) decision trees b) neural networks (back propagation) c) probabilistic networks d) nearest neighbor e) support vector machines 8) diﬀerent algorithm techniques machine learning? diﬀerent types techniques machine learning a) supervised learning b) unsupervised learning c) semi-supervised learning d) reinforcement learning e) transduction f) learning learn 9) stages build hypotheses model machine learning? a) model building b) model testing c) applying model 10) standard approach supervised learning? standard approach supervised learning split set example training set test. 11) ‘training set’ ‘test set’? areas information science like machine learning, set data discover potentially predictive relationship known ‘training set’. training set examples given learner, test set test accuracy hypotheses generated learner, set example held learner. training set distinct test set. 12) list approaches machine learning? diﬀerent approaches machine learning a) concept vs classiﬁcation learning b) symbolic vs statistical learning\n",
      "\n",
      "c) inductive vs analytical learning 13) machine learning? a) artiﬁcial intelligence b) rule based inference 14) explain function ‘unsupervised learning’? a) find clusters data b) find low-dimensional representations data c) find interesting directions data d) interesting coordinates correlations e) find novel observations/ database cleaning 15) explain function ‘supervised learning’? a) classiﬁcations b) speech recognition c) regression d) predict time series e) annotate strings 16) algorithm independent machine learning? machine learning mathematical foundations independent particular classiﬁer learning algorithm referred algorithm independent machine learning? 17) diﬀerence artiﬁcial learning machine learning? designing developing algorithms according behaviours based empirical data known machine learning. artiﬁcial intelligence addition machine learning, covers aspects like knowledge representation, natural language processing, planning, robotics etc. 18) classiﬁer machine learning? classiﬁer machine learning system inputs vector discrete continuous feature values outputs single discrete value, class. 19) advantages naive bayes? naïve bayes classiﬁer converge quicker discriminative models like logistic regression, need training data. main advantage can’t learn interactions features. 20) areas pattern recognition used? pattern recognition\n",
      "\n",
      "a) computer vision b) speech recognition c) data mining d) statistics e) informal retrieval f) bio-informatics 21) genetic programming? genetic programming techniques machine learning. model based testing selecting best choice set results. 22) inductive logic programming machine learning? inductive logic programming (ilp) subﬁeld machine learning uses logical programming representing background knowledge examples. 23) model selection machine learning? process selecting models diﬀerent mathematical models, describe data set known model selection. model selection applied ﬁelds statistics, machine learning data mining. 24) methods calibration supervised learning? methods predicting good probabilities supervised learning a) platt calibration b) isotonic regression methods designed binary classiﬁcation, trivial. 25) method frequently prevent overﬁtting? suﬃcient data ‘isotonic regression’ prevent overﬁtting issue. 26) diﬀerence heuristic rule learning heuristics decision trees? diﬀerence heuristics decision trees evaluate average quality number disjointed sets rule learners evaluate quality set instances covered candidate rule. 27) perceptron machine learning? machine learning, perceptron algorithm supervised classiﬁcation input possible non-binary outputs. 28) explain components bayesian logic program?\n",
      "\n",
      "bayesian logic program consists components. ﬁrst component logical ; consists set bayesian clauses, captures qualitative structure domain. second component quantitative one, encodes quantitative information domain. 29) bayesian networks (bn) ? bayesian network represent graphical model probability relationship set variables . 30) instance based learning algorithm referred lazy learning algorithm? instance based learning algorithm referred lazy learning algorithm delay induction generalization process classiﬁcation performed. 31) classiﬁcation methods svm ( support vector machine) handle? a) combining binary classiﬁers b) modifying binary incorporate multiclass learning 32) ensemble learning? solve particular computational program, multiple models classiﬁers experts strategically generated combined. process known ensemble learning. 33) ensemble learning used? ensemble learning improve classiﬁcation, prediction, function approximation etc model. 34) use ensemble learning? ensemble learning build component classiﬁers accurate independent other. 35) paradigms ensemble methods? paradigms ensemble methods a) sequential ensemble methods b) parallel ensemble methods 36) general principle ensemble method bagging boosting ensemble method? general principle ensemble method combine predictions models built given learning algorithm order improve robustness single model. bagging method ensemble improving unstable estimation classiﬁcation schemes. boosting method sequentially reduce bias combined model. boosting bagging reduce errors reducing variance term. 37) bias-variance decomposition classiﬁcation error ensemble method?\n",
      "\n",
      "expected error learning algorithm decomposed bias variance. bias term measures closely average classiﬁer produced learning algorithm matches target function. variance term measures learning algorithm’s prediction ﬂuctuates diﬀerent training sets. 38) incremental learning algorithm ensemble? incremental learning method ability algorithm learn new data available classiﬁer generated available dataset. 39) pca, kpca ica for? pca (principal components analysis), kpca ( kernel based principal component analysis) ica ( independent component analysis) important feature extraction techniques dimensionality reduction. 40) dimension reduction machine learning? machine learning statistics, dimension reduction process reducing number random variables considerations divided feature selection feature extraction 41) support vector machines? support vector machines supervised learning algorithms classiﬁcation regression analysis. 42) components relational evaluation techniques? important components relational evaluation techniques a) data acquisition b) ground truth acquisition c) cross validation technique d) query type e) scoring metric f) signiﬁcance test 43) diﬀerent methods sequential supervised learning? diﬀerent methods solve sequential supervised learning problems a) sliding-window methods b) recurrent sliding windows c) hidden markow models d) maximum entropy markow models e) conditional random ﬁelds\n",
      "\n",
      "f) graph transformer networks 44) areas robotics information processing sequential prediction problem arises? areas robotics information processing sequential prediction problem arises a) imitation learning b) structured prediction c) model based reinforcement learning 45) batch statistical learning? statistical learning techniques allow learning function predictor set observed data predictions unseen future data. techniques provide guarantees performance learned predictor future unseen data based statistical assumption data generating process. 46) pac learning? pac (probably approximately correct) learning learning framework introduced analyze learning algorithms statistical eﬃciency. 47) diﬀerent categories categorized sequence learning process? a) sequence prediction b) sequence generation c) sequence recognition d) sequential decision 48) sequence learning? sequence learning method teaching learning logical manner. 49) techniques machine learning ? techniques machine learning a) genetic programming b) inductive learning 50) popular application machine learning day day basis? recommendation engine implemented major ecommerce websites uses machine learning guru99 provides free online tutorial courses like\n",
      "\n",
      "java mis mongodb bigdata cassandra web services sqlite jsp informatica accounting sap training python excel asp net hbase project management test management business analyst ethical hacking pmp live project soapui photoshop manual testing mobile testing data warehouse r tutorial tableau devops aws jenkins agile testing rpa junit software engineering selenium ccna angularjs nodejs plsql\n",
      "\n",
      "bagging ensemble learning general, machine learning problems try find best possible optimal model given problem. means finding best possible model given model family, example, finding best possible decision tree finding best possible knn model. time try model families available, come best possible regression model, best possible knn model, best possible svm model etc. select best possible model, knn, svm other. ensemble learning says, build multiple models select best 2, 3 10. find 10 deploy 10 models. new data comes, prediction 10 models combine predictions finally joint prediction. key idea ensemble learning. s come mind. 1. meant building/training different models? 2. combine predictions? building/training different models means below • select different model families knn, decision trees, linear regression etc. • select model family train different training samples resulting different models ml family. • consider different feature spaces training result different models. • talk impact selecting different features coming paragraphs.\n",
      "\n",
      "combine predictions models means • regression (weighted) mean, median, max, min o regression problem mean, median mode models outcomes final outcome. o weighted mean build models different set features different samples features high importance generate optimal model need assign weight models combining. o also, assign weight based model's performance test data. model performing better weight. • classification (weighted) majority voting. o normal majority counting weighted majority counting. (the weighted reason described point.) hope basic idea ensemble learning clear. let’s discuss important widely techniques ensemble learning bagging. technique boosting discuss post. focus complete explanation bagging. bagging bagging bootstrap aggregation – basic idea 1. create sample data set big data set train model. 2. select model family decision trees, knn etc. high variance models. 3. train model sample data. called model 1.\n",
      "\n",
      "4. create new sample replacement strategy means mix original dataset sample created step 1 create new fresh sample. 5. train model family new sample data. called model 2. 6. way create multiple models training different samples data. important reasons use bagging reduces model variance. studied, best model low bias low variance. complex machine learning algorithms bias reduces variance increases. optimal performance need reduce variance bagging technique helpful. let’s understand bagging reduces variance model. suppose build 5 models outcome y1, y2, y3, y4 y5 respectively. mean (y1+y2+y3+y4+y5)/5 outcome final prediction. note taking mean explain concept, median, mode etc based problem set. remember outcomes y1, y2, y3, y4, y5 predicted models trained different samples big dataset. recall central limit theorem, says large population size good number (greater 30 optimal) samples means samples mean follow normal distribution population mean. important standard deviation samples reduced square root n n sample size. standard deviation square root variance hance variance reduced factor 1/n. combiner bagging reduces model variance. hance bagging high variance machine learning algorithms like decision trees, knn neural networks. important points bagging • algorithm independent general-purpose technique, work machine learning algorithms. (however preferable use high variance algorithms) • suited high variance algorithms. • variance reduction averaging set observations reduces variance – central limit theorem. (explained above) • choose # models build. (this hyperparameter, mathematical formula determine this) important thought, different k-fold cross-validation technique.\n",
      "\n",
      "k-fold cross-validation, use training data different iteration choosing different hyperparameter values. basically find best possible hyperparameter. bagging use different training data set. • easy parallelize. • limitation loss interpretability, example, decision tree single model, easy interpret build multiple decision trees different sample forest decide outcome combining outputs models lose interpretability. • limitation features dominates? feature dominates build multiple decision trees exactly (think if-else rule decision trees) bagging won't work properly. different feature selection different sampling assign different weight models depending dominating features. involve different feature subspace different sampling method called random forest. bagging described infographics bagging technique. let’s understand random feature subspace detail. random feature subspaces build different models using • different subset training data (create samples replacement) • random subset features! (in bagging time features) • ml algorithm training things gets change.\n",
      "\n",
      "need random feature subspaces? • think “regularization” • strong predictor & moderately strong predictors. • models high importance strong predictor means models ensemble similar. • reason feature subspace different samples. • choose # models build # features (m) sample p available features. • recommended heuristics select m p m = sqrt(p) • m = p approach reduces bagged trees. (means include features sample training data set) advantages • de-correlates models ensemble • improve accuracy prediction • of-course reduces model variance. comes bagged trees. random feature spaces decision trees known random forests algorithm • decision trees high variance. • resulting tree (model) depends underlying training data. • bagged trees help reduce variance; random forests so… random forests • sample replacement (shift 1 training set multiple training sets) • train model training set • tree uses random subset feature random forest • dt predicts • mean / majority vote prediction final prediction • faster bagging (fewer splits evaluate tree)\n",
      "\n",
      "thank like posts machine learning, connect follow blog https//ashutoshtripathi.com/ linkedin https//www.linkedin.com/in/ashutoshtripathi1/ instagram https//www.instagram.com/ashutosh_ai/ medium articles https//medium.com/@ashutosh.optimistic\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x data science interview s statistics 1. central limit theorem important? “suppose interested estimating average height people. collecting data person world impossible. can’t obtain height measurement population, sample people. becomes, average height entire population given single sample. central limit theorem addresses exactly.” read here. 2. sampling? sampling methods know? “data sampling statistical analysis technique select, manipulate analyze representative subset data points identify patterns trends larger data set examined.” read answer here. 3. difference type vs type ii error? “a type error occurs null hypothesis true, rejected. type ii error occurs null hypothesis false, erroneously fails rejected.” read answer here. 4. linear regression? terms p-value, coefficient, r-squared value mean? significance components? linear regression good tool quick predictive analysis example, price house depends myriad factors, size location. order relationship variables, need build linear regression, predicts line best fit help conclude factors positive negative relationship. read here. 5. assumptions required linear regression? major assumptions 1. linear relationship dependent variables regressors, meaning model creating actually fits data, 2. errors residuals data normally distributed independent other, 3. minimal multicollinearity explanatory variables, 4. homoscedasticity. means variance regression line values predictor variable. 6. statistical interaction? ”basically, interaction effect factor (input variable) dependent variable (output variable) differs levels factor.” read here. 7. selection bias? “selection (or ‘sampling’) bias occurs ‘active,’ sense sample data gathered prepared modeling characteristics representative true, future population cases\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x model see. is, active selection bias occurs subset data systematically (i.e., non-randomly) excluded analysis.” read here. 8. example data set non-gaussian distribution? “the gaussian distribution exponential family distributions, lot them, sort ease use, cases, person machine learning solid grounding statistics, utilized appropriate.” read here. 9. binomial probability formula? “the binomial distribution consists probabilities possible numbers successes n trials independent events probability π (the greek letter pi) occurring.” read data science q1. data science? list differences supervised unsupervised learning. data science blend tools, algorithms, machine learning principles goal discover hidden patterns raw data. different statisticians years? answer lies difference explaining predicting. differences supervised unsupervised learning follows; supervised learning unsupervised learning input data labelled. input data unlabelled. uses training data set. uses input data set. prediction. analysis. enables classification regression. enables classification, density estimation, & dimension reduction q2. selection bias? selection bias kind error occurs researcher decides going studied. usually associated research selection participants isn’t random. referred\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x selection effect. distortion statistical analysis, resulting method collecting samples. selection bias taken account, conclusions study accurate. types selection bias include 1. sampling bias systematic error non-random sample population causing members population likely included resulting biased sample. 2. time interval trial terminated early extreme value (often ethical reasons), extreme value likely reached variable largest variance, variables similar mean. 3. data specific subsets data chosen support conclusion rejection bad data arbitrary grounds, instead according previously stated generally agreed criteria. 4. attrition attrition bias kind selection bias caused attrition (loss participants) discounting trial subjects/tests run completion. q3. bias-variance trade-off? bias bias error introduced model oversimplification machine learning algorithm. lead underfitting. train model time model makes simplified assumptions target function easier understand. low bias machine learning algorithms — decision trees, k-nn svm high bias machine learning algorithms — linear regression, logistic regression variance variance error introduced model complex machine learning algorithm, model learns noise training data set performs badly test data set. lead high sensitivity overfitting. normally, increase complexity model, reduction error lower bias model. however, happens particular point. continue model complex, end over-fitting model model start suffering high variance. bias-variance trade-off goal supervised machine learning algorithm low bias low variance achieve good prediction performance.\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x 1. k-nearest neighbour algorithm low bias high variance, trade-off changed increasing value k increases number neighbours contribute prediction turn increases bias model. 2. support vector machine algorithm low bias high variance, trade-off changed increasing c parameter influences number violations margin allowed training data increases bias decreases variance. escaping relationship bias variance machine learning. increasing bias decrease variance. increasing variance decrease bias. q4. confusion matrix? confusion matrix 2x2 table contains 4 outputs provided binary classifier. measures, error-rate, accuracy, specificity, sensitivity, precision recall derived it. confusion matrix data set performance evaluation called test data set. contain correct labels predicted labels. predicted labels exactly performance binary classifier perfect.\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x predicted labels usually match observed labels real-world scenarios. binary classifier predicts data instances test data set positive negative. produces outcomes- 1. true-positive(tp) — correct positive prediction 2. false-positive(fp) — incorrect positive prediction 3. true-negative(tn) — correct negative prediction 4. false-negative(fn) — incorrect negative prediction basic measures derived confusion matrix 1. error rate = (fp+fn)/(p+n) 2. accuracy = (tp+tn)/(p+n) 3. sensitivity(recall true positive rate) = tp/p 4. specificity(true negative rate) = tn/n 5. precision(positive predicted value) = tp/(tp+fp) 6. f-score(harmonic mean precision recall) = (1+b)(prec.rec)/(b²prec+rec) b commonly 0.5, 1, 2.\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x statistics interview s q5. difference “long” “wide” format data? wide-format, subject’s repeated responses single row, response separate column. long-format, row one-time point subject. recognize data wide format fact columns generally represent groups. q6. understand term normal distribution? data usually distributed different ways bias left right jumbled up. however, chances data distributed central value bias left right reaches normal distribution form bell-shaped curve. figure normal distribution bell curve random variables distributed form symmetrical, bell-shaped curve. properties normal distribution follows; 1. unimodal -one mode 2. symmetrical -left right halves mirror images 3. bell-shaped -maximum height (mode) mean 4. mean, mode, median located center 5. asymptotic q7. correlation covariance statistics?\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x covariance correlation mathematical concepts; approaches widely statistics. correlation covariance establish relationship measure dependency random variables. work similar mathematical terms, different other. correlation correlation considered described best technique measuring estimating quantitative relationship variables. correlation measures strongly variables related. covariance covariance items vary it’s measure indicates extent random variables change cycle. statistical term; explains systematic relation pair random variables, changes variable reciprocal corresponding change variable. q8. difference point estimates confidence interval? point estimation gives particular value estimate population parameter. method moments maximum likelihood estimator methods derive point estimators population parameters. confidence interval gives range values likely contain population parameter. confidence interval generally preferred, tells likely interval contain population parameter. likeliness probability called confidence level confidence coefficient represented 1 — alpha, alpha level significance. q9. goal a/b testing? hypothesis testing randomized experiment variables b. goal a/b testing identify changes web page maximize increase outcome interest. a/b testing fantastic method figuring best online promotional marketing strategies business. test website copy sales emails search ads\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x example identifying click-through rate banner ad. q10. p-value? perform hypothesis test statistics, p-value help determine strength results. p-value number 0 1. based value denote strength results. claim trial called null hypothesis. low p-value (≤ 0.05) indicates strength null hypothesis means reject null hypothesis. high p-value (≥ 0.05) indicates strength null hypothesis means accept null hypothesis p-value 0.05 indicates hypothesis way. way, high p values data likely true null. low p values data unlikely true null. q11. 15-minute interval, 20% probability shooting star. probability shooting star period hour? probability seeing shooting star 15 minutes = 1 – p( seeing shooting star ) = 1 – 0.2 = 0.8 probability seeing shooting star period hour = (0.8) ^ 4 = 0.4096 probability seeing shooting star hour = 1 – p( seeing star ) = 1 – 0.4096 = 0.5904 q12. generate random number 1 – 7 die? • die sides 1-6. way seven equal outcomes single rolling die. roll die twice consider event rolls, 36 different outcomes. • 7 equal outcomes reduce 36 number divisible 7. consider 35 outcomes exclude one. • simple scenario exclude combination (6,6), i.e., roll die 6 appears twice. • remaining combinations (1,1) till (6,5) divided 7 parts 5 each. way seven sets outcomes equally likely. q13. certain couple tells children, girl. probability girls? case children, 4 equally likely possibilities bb, bg, gb gg; b = boy g = girl letter denotes child. , exclude case bb. remaining 3 possibilities bg, gb & bb, find probability case girls. thus, p(having girls given girl) = 1 / 3\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x q14. jar 1000 coins, 999 fair 1 double headed. pick coin random, toss 10 times. given 10 heads, probability toss coin head? ways choosing coin. pick fair coin pick heads. probability selecting fair coin = 999/1000 = 0.999 probability selecting unfair coin = 1/1000 = 0.001 selecting 10 heads row = selecting fair coin * getting 10 heads + selecting unfair coin p (a) = 0.999 * (1/2)^5 = 0.999 * (1/1024) = 0.000976 p (b) = 0.001 * 1 = 0.001 p( / + b ) = 0.000976 / (0.000976 + 0.001) = 0.4939 p( b / + b ) = 0.001 / 0.001976 = 0.5061 probability selecting head = p(a/a+b) * 0.5 + p(b/a+b) * 1 = 0.4939 * 0.5 + 0.5061 = 0.7531 q15. understand statistical power sensitivity calculate it? sensitivity commonly validate accuracy classifier (logistic, svm, random forest etc.). sensitivity “predicted true events/ total events”. true events events true model predicted true. calculation seasonality pretty straightforward. seasonality = ( true positives ) / ( positives actual dependent variable ) q16. re-sampling done? resampling cases • estimating accuracy sample statistics subsets accessible data drawing randomly replacement set data points • substituting labels data points performing significance tests • validating models random subsets (bootstrapping, cross-validation) q17. differences over-fitting under-fitting? statistics machine learning, common tasks fit model set training data, able reliable predictions general untrained data. follow steve nouri ai data science posts https//lnkd.in/gzu463x\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x overfitting, statistical model describes random error noise instead underlying relationship. overfitting occurs model excessively complex, having parameters relative number observations. model overfitted, poor predictive performance, overreacts minor fluctuations training data. underfitting occurs statistical model machine learning algorithm capture underlying trend data. underfitting occur, example, fitting linear model non-linear data. model poor predictive performance. q18. combat overfitting underfitting? combat overfitting underfitting, resample data estimate model accuracy (k-fold cross-validation) having validation dataset evaluate model. q19. regularisation? useful? data scientist masters program explore curriculum regularisation process adding tuning parameter model induce smoothness order prevent overfitting. adding constant multiple existing weight vector. constant l1(lasso) l2(ridge). model predictions minimize loss function calculated regularized training set. q20. law large numbers?\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x theorem describes result performing experiment large number times. theorem forms basis frequency-style thinking. says sample means, sample variance sample standard deviation converge trying estimate. q21. confounding variables? statistics, confounder variable influences dependent variable independent variable. example, researching lack exercise leads weight gain, lack exercise = independent variable weight gain = dependent variable. confounding variable variable affects variables, age subject. q22. types biases occur sampling? • selection bias • coverage bias • survivorship bias q23. survivorship bias? logical error focusing aspects support surviving process casually overlooking work lack prominence. lead wrong conclusions numerous different means. q24. selection bias? selection bias occurs sample obtained representative population intended analysed. q25. explain roc curve works? roc curve graphical representation contrast true positive rates false-positive rates thresholds. proxy trade-off sensitivity(true positive rate) false-positive rate.\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x q26. tf/idf vectorization? tf–idf short term frequency-inverse document frequency, numerical statistic intended reflect important word document collection corpus. weighting factor information retrieval text mining. tf–idf value increases proportionally number times word appears document offset frequency word corpus, helps adjust fact words appear frequently general. q27. generally use softmax non-linearity function operation in-network? takes vector real numbers returns probability distribution. definition follows. let x vector real numbers (positive, negative, whatever, constraints). i’th component softmax(x) —\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x clear output probability distribution element non-negative sum components 1. data analysis interview s q28. python r – prefer text analytics? prefer python following reasons • python best option pandas library provides easy use data structures high-performance data analysis tools. • r suitable machine learning text analysis. • python performs faster types text analytics. q29. data cleaning plays vital role analysis? data cleaning help analysis because • cleaning data multiple sources helps transform format data analysts data scientists work with. • data cleaning helps increase accuracy model machine learning. • cumbersome process number data sources increases, time taken clean data increases exponentially number sources volume data generated sources. • 80% time cleaning data making critical analysis task. q30. differentiate univariate, bivariate multivariate analysis. univariate analyses descriptive statistical analysis techniques differentiated based number variables involved given point time. example, pie charts sales based territory involve variable analysis referred univariate analysis. bivariate analysis attempts understand difference variables time scatterplot. example, analyzing volume sale spending considered example bivariate analysis. multivariate analysis deals study variables understand effect variables responses.\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x q31. explain star schema. traditional database schema central table. satellite tables map ids physical names descriptions connected central fact table id fields; tables known lookup tables principally useful real-time applications, save lot memory. star schemas involve layers summarization recover information faster. q32. cluster sampling? cluster sampling technique difficult study target population spread wide area simple random sampling applied. cluster sample probability sample sampling unit collection cluster elements. eg., researcher wants survey academic performance high school students japan. divide entire population japan different clusters (cities). researcher selects number clusters depending research simple systematic random sampling. let’s continue data science interview s blog statistics s. q33. systematic sampling? systematic sampling statistical technique elements selected ordered sampling frame. systematic sampling, list progressed circular manner reach end list, progressed again. best example systematic sampling equal probability method. q34. eigenvectors eigenvalues? eigenvectors understanding linear transformations. data analysis, usually calculate eigenvectors correlation covariance matrix. eigenvectors directions particular linear transformation acts flipping, compressing stretching. eigenvalue referred strength transformation direction eigenvector factor compression occurs. q35. cite examples false positive important false negative? let understand false positives false negatives are. • false positives cases wrongly classified non-event event a.k.a type error. • false negatives cases wrongly classify events non-events, a.k.a type ii error. example 1 medical field, assume chemotherapy patients. assume patient comes hospital tested positive cancer, based lab prediction actually doesn’t cancer. case false positive. utmost danger start chemotherapy patient actually cancer. absence cancerous cell, chemotherapy certain damage normal healthy cells lead severe diseases, cancer. example 2 let’s e-commerce company decided $1000 gift voucher customers assume purchase $10,000 worth items. send free voucher mail directly 100 customers minimum purchase condition assume 20% profit sold items $10,000. issue send $1000 gift vouchers customers actually purchased marked having $10,000 worth purchase.\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x q36. cite examples false negative important false positive? example 1 assume airport ‘a’ received high-security threats based certain characteristics identify particular passenger threat not. shortage staff, decide scan passengers predicted risk positives predictive model. happen true threat customer flagged non-threat airport model? example 2 jury judge decides criminal free? example 3 rejected marry good person based predictive model happen meet him/her years realize false negative? q37. cite examples false positive false negatives equally important? banking industry giving loans primary source making money time repayment rate good profit, risk huge losses. banks don’t want lose good customers point time, don’t want acquire bad customers. scenario, false positives false negatives important measure. q38. explain difference validation set test set? validation set considered training set parameter selection avoid overfitting model built. hand, test set testing evaluating performance trained machine learning model. simple terms, differences summarized as; training set fit parameters i.e. weights test set assess performance model i.e. evaluating predictive power generalization. q39. explain cross-validation. cross-validation model validation technique evaluating outcomes statistical analysis generalize independent dataset. mainly backgrounds objective forecast wants estimate accurately model accomplish practice. goal cross-validation term data set test model training phase (i.e. validation data set) order limit problems like overfitting insight model generalize independent data set. machine learning interview s q40. machine learning? machine learning explores study construction algorithms learn predictions data. closely related computational statistics. devise complex models algorithms lend prediction commercial use known predictive analytics. given below, image representing domains machine learning lends to.\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x q41. supervised learning? supervised learning machine learning task inferring function labeled training data. training data consist set training examples. algorithms support vector machines, regression, naive bayes, decision trees, k-nearest neighbor algorithm neural networks e.g. built fruit classifier, labels “this orange, apple banana”, based showing classifier examples apples, oranges bananas. q42. unsupervised learning? unsupervised learning type machine learning algorithm draw inferences datasets consisting input data labelled responses. algorithms clustering, anomaly detection, neural networks latent variable models e.g. example, fruit clustering categorize “fruits soft skin lots dimples”, “fruits shiny hard skin” “elongated yellow fruits”. q43. classification algorithms? diagram lists important classification algorithms. q44. ‘naive’ naive bayes? naive bayes algorithm based bayes theorem. bayes’ theorem describes probability event, based prior knowledge conditions related event. algorithm ‘naive’ makes assumptions turn correct. q45. explain svm algorithm detail. svm stands support vector machine, supervised machine learning algorithm regression classification. n features training data set, svm tries plot\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x n-dimensional space value feature value particular coordinate. svm uses hyperplanes separate different classes based provided kernel function. q46. support vectors svm? diagram, thinner lines mark distance classifier closest data points called support vectors (darkened data points). distance thin lines called margin. q47. different kernels svm? types kernels svm. 1. linear kernel 2. polynomial kernel 3. radial basis kernel 4. sigmoid kernel q48. explain decision tree algorithm detail.\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x decision tree supervised machine learning algorithm mainly regression classification. breaks data set smaller smaller subsets time associated decision tree incrementally developed. final result tree decision nodes leaf nodes. decision tree handle categorical numerical data. q49. entropy information gain decision tree algorithm? core algorithm building decision tree called id3. id3 uses entropy information gain entropy decision tree built top-down root node involve partitioning data homogenious subsets. id3 uses enteropy check homogeneity sample. sample completely homogenious entropy zero sample equally divided entropy one. information gain information gain based decrease entropy dataset split attribute. constructing decision tree finding attributes return highest information gain.\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x q50. pruning decision tree? pruning technique machine learning search algorithms reduces size decision trees removing sections tree provide little power classify instances. so, remove sub-nodes decision node, process called pruning opposite process splitting. q51. logistic regression? state example logistic regression recently. logistic regression referred logit model technique predict binary outcome linear combination predictor variables. example, want predict particular political leader win election not. case, outcome prediction binary i.e. 0 1 (win/lose). predictor variables money spent election campaigning particular candidate, time spent campaigning, etc. q52. linear regression? linear regression statistical technique score variable y predicted score second variable x. x referred predictor variable y criterion variable.\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x follow ai data science posts https//lnkd.in/gzu463x q53. drawbacks linear model? drawbacks linear model are • assumption linearity errors. • can’t count outcomes binary outcomes • overfitting problems can’t solve q54. difference regression classification ml techniques? regression classification machine learning techniques come supervised machine learning algorithms. supervised machine learning algorithm, train model labelled data set, training explicitly provide correct labels algorithm tries learn pattern input output. labels discrete values classification problem, e.g a,b etc. labels continuous values regression problem, e.g 1.23, 1.333 etc. q55. recommender systems? recommender systems subclass information filtering systems meant predict preferences ratings user product. recommender systems widely movies, news, research articles, products, social tags, music, etc. examples include movie recommenders imdb, netflix & bookmyshow, product recommenders e- commerce sites like amazon, ebay & flipkart, youtube video recommendations game recommendations xbox. q56. collaborative filtering? process filtering recommender systems find patterns information collaborating viewpoints, data sources multiple agents.\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x example collaborative filtering predict rating particular user based his/her ratings movies others’ ratings movies. concept widely recommending movies imdb, netflix & bookmyshow, product recommenders e-commerce sites like amazon, ebay & flipkart, youtube video recommendations game recommendations xbox. q57. outlier values treated? outlier values identified univariate graphical analysis method. number outlier values assessed individually large number outliers, values substituted 99th 1st percentile values. extreme values outlier values. common ways treat outlier values 1. change value bring range. 2. remove value. q58. steps involved analytics project? following steps involved analytics project 1. understand business problem 2. explore data familiar it. 3. prepare data modelling detecting outliers, treating missing values, transforming variables, etc. 4. data preparation, start running model, analyze result tweak approach. iterative step best possible outcome achieved. 5. validate model new data set. 6. start implementing model track result analyze performance model period time. q59. analysis, treat missing values? extent missing values identified identifying variables missing values. patterns identified analyst concentrate lead interesting meaningful business insights. patterns identified, missing values substituted mean median values (imputation) simply ignored. assigning default value mean, minimum maximum value. getting data important. categorical variable, default value assigned. missing value assigned default value. distribution data coming, normal distribution mean value. 80% values variable missing answer dropping variable instead treating missing values.\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x q60. define number clusters clustering algorithm? clustering algorithm specified, reference k-means clustering “k” defines number clusters. objective clustering group similar entities way entities group similar groups different other. example, following image shows different groups. sum squares generally explain homogeneity cluster. plot wss range number clusters, plot shown below. • graph generally known elbow curve. • red circled point graph i.e. number cluster =6 point don’t decrement wss. • point known bending point taken k k – means. widely approach data scientists use hierarchical clustering create dendrograms identify distinct groups there. q61. ensemble learning? ensemble learning basically combining diverse set learners(individual models) improvise stability predictive power model. q62. describe brief type ensemble learning? ensemble learning types popular ensemble learning techniques mentioned below. bagging bagging tries implement similar learners small sample populations takes mean predictions. generalised bagging, use different learners different population. expect helps reduce variance error.\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x boosting boosting iterative technique adjusts weight observation based classification. observation classified incorrectly, tries increase weight observation vice versa. boosting general decreases bias error builds strong predictive models. however, fit training data. q63. random forest? work? random forest versatile machine learning method capable performing regression classification tasks. dimensionality reduction, treats missing values, outlier values. type ensemble learning method, group weak models combine form powerful model.\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x random forest, grow multiple trees opposed single tree. classify new object based attributes, tree gives classification. forest chooses classification having votes(overall trees forest) case regression, takes average outputs different trees. q64. work random forest? underlying principle technique weak learners combined provide keen learner. steps involved • build decision trees bootstrapped training samples data • tree, time split considered, random sample mm predictors chosen split candidates, pp predictors • rule thumb split m=p√m=p • predictions majority rule q65. cross-validation technique use time series data set? instead k-fold cross-validation, aware fact time series randomly distributed data — inherently ordered chronological order. case time series data, use techniques like forward=chaining — model past data look forward-facing data. fold 1 training[1], test[2] fold 1 training[1 2], test[3] fold 1 training[1 2 3], test[4] fold 1 training[1 2 3 4], test[5] q66. box-cox transformation? dependent variable regression analysis satisfy assumptions ordinary squares regression. residuals curve prediction increases follow skewed distribution. scenarios, necessary transform response variable data meets required assumptions. box cox transformation statistical technique transform non-normal dependent variables normal shape. given data normal statistical\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x techniques assume normality. applying box cox transformation means run broader number tests. box-cox transformation way transform non-normal dependent variables normal shape. normality important assumption statistical techniques, data isn’t normal, applying box-cox means able run broader number tests. box-cox transformation named statisticians george box sir david roxbee cox collaborated 1964 paper developed technique. q67. regularly algorithm updated? want update algorithm when • want model evolve data streams infrastructure • underlying data source changing • case non-stationarity • algorithm underperforms/ results lack accuracy q68. having 4gb ram machine want train model 10gb data set. problem? faced kind problem machine learning/data science experience far? all, ask ml model want train. neural networks batch size numpy array work. steps 1. load data numpy array. numpy array property create mapping complete data set, doesn’t load complete data set memory. 2. pass index numpy array required data. 3. use data pass neural network.\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x 4. small batch size. svm partial fit work steps 1. divide big data set small size data sets. 2. use partial fit method svm, requires subset complete data set. 3. repeat step 2 subsets. however, actually face issue reality. so, check best laptop machine learning prevent that. having said that, let’s s deep learning. deep learning interview s q69. mean deep learning? deep learning paradigm machine learning shown incredible promise recent years. fact deep learning shows great analogy functioning human brain. q70. difference machine learning deep learning? machine learning field computer science gives computers ability learn explicitly programmed. machine learning categorised following categories. 1. supervised machine learning, 2. unsupervised machine learning, 3. reinforcement learning deep learning subfield machine learning concerned algorithms inspired structure function brain called artificial neural networks. q71. what, opinion, reason popularity deep learning recent times?\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x deep learning years, major breakthroughs techniques came recent years. main reasons • increase data generated sources • growth hardware resources required run models gpus multiple times faster help build bigger deeper deep learning models comparatively time required previously. q72. reinforcement learning? reinforcement learning learning map situations actions. end result maximise numerical reward signal. learner told action instead discover action yield maximum reward. reinforcement learning inspired learning human beings, based reward/penalty mechanism. q73. artificial neural networks? artificial neural networks specific set algorithms revolutionized machine learning. inspired biological neural networks. neural networks adapt changing input network generates best possible result needing redesign output criteria. q74. describe structure artificial neural networks? artificial neural networks works principle biological neural network. consists inputs processed weighted sums bias, help activation functions.\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x q75. weights initialized network? methods here initialize weights zero assign randomly. initializing weights 0 makes model similar linear model. neurons layer perform operation, giving output making deep net useless. initializing weights randomly here, weights assigned randomly initializing close 0. gives better accuracy model neuron performs different computations. commonly method. q76. cost function? referred “loss” “error,” cost function measure evaluate good model’s performance is. it’s compute error output layer backpropagation. push error backwards neural network use different training functions. q77. hyperparameters? neural networks, you’re usually working hyperparameters data formatted correctly. hyperparameter parameter value set learning process begins. determines network trained structure network (such number hidden units, learning rate, epochs, etc.). q78. happen learning rate set inaccurately (too low high)? learning rate low, training model progress slowly making minimal updates weights. updates reaching minimum point. learning rate set high, causes undesirable divergent behaviour loss function drastic updates weights. fail converge (model good output) diverge (data chaotic network train). q79. difference epoch, batch, iteration deep learning? • epoch – represents iteration entire dataset (everything training model). • batch – refers pass entire dataset neural network once, divide dataset batches.\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x • iteration – 10,000 images data batch size 200. epoch run 50 iterations (10,000 divided 50). q80. different layers cnn? layers cnn 1. convolutional layer – layer performs convolutional operation, creating smaller picture windows data. 2. relu layer – brings non-linearity network converts negative pixels zero. output rectified feature map. 3. pooling layer – pooling down-sampling operation reduces dimensionality feature map. 4. fully connected layer – layer recognizes classifies objects image. q81. pooling cnn, work? pooling reduce spatial dimensions cnn. performs down-sampling operations reduce dimensionality creates pooled feature map sliding filter matrix input matrix. q82. recurrent neural networks(rnns)? rnns type artificial neural networks designed recognise pattern sequence data time series, stock market government agencies etc. understand recurrent nets, first, understand basics feedforward nets. networks rnn feed-forward named way channel information series mathematical orations performed nodes network. feeds information straight(never touching node twice), cycles loop, called recurrent.\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x recurrent networks, hand, input, current input example see, perceived previously time. decision recurrent neural network reached time t-1 affects decision reach moment later time t. recurrent networks sources input, present recent past, combine determine respond new data, life. error generate return backpropagation adjust weights error can’t lower. remember, purpose recurrent nets accurately classify sequential input. rely backpropagation error gradient descent so. q83. lstm network work? long-short-term memory (lstm) special kind recurrent neural network capable learning long- term dependencies, remembering information long periods default behaviour. steps lstm network • step 1 network decides forget remember. • step 2 selectively updates cell state values. • step 3 network decides current state makes output. q84. multi-layer perceptron(mlp)?\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x neural networks, mlps input layer, hidden layer, output layer. structure single layer perceptron hidden layers. single layer perceptron classify linear separable classes binary output (0,1), mlp classify nonlinear classes. input layer, node layers uses nonlinear activation function. means input layers, data coming in, activation function based nodes weights added together, producing output. mlp uses supervised learning method called “backpropagation.” backpropagation, neural network calculates error help cost function. propagates error backward came (adjusts weights train model accurately). q85. explain gradient descent. understand gradient descent, let’s understand gradient first. gradient measures output function changes change inputs little bit. simply measures change weights regard change error. think gradient slope function. gradient descent thought climbing valley, instead climbing hill. minimization algorithm minimizes given function (activation function). q86. exploding gradients?\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x training rnn, exponentially growing (very large) error gradients accumulate result large updates neural network model weights training, they’re known exploding gradients. extreme, values weights large overflow result nan values. effect model unstable unable learn training data. q87. vanishing gradients? training rnn, slope small; makes training difficult. slope small, problem known vanishing gradient. leads long training times, poor performance, low accuracy. q89. propagation explain it’s working. backpropagation training algorithm multilayer neural network. method, error end network weights inside network allowing efficient computation gradient. following steps data scientist masters program weekday / weekend batchessee batch details • forward propagation training data • derivatives computed output target • propagate computing derivative error wrt output activation • previously calculated derivatives output • update weights q90. variants propagation? • stochastic gradient descent use single training example calculation gradient update parameters. • batch gradient descent calculate gradient dataset perform update iteration. • mini-batch gradient descent it’s popular optimization algorithms. it’s variant stochastic gradient descent instead single training example, mini-batch samples used. q91. different deep learning frameworks? • pytorch • tensorflow • microsoft cognitive toolkit\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x • keras • caffe • chainer q92. role activation function? activation function introduce non-linearity neural network helping learn complex function. neural network able learn linear function linear combination input data. activation function function artificial neuron delivers output based inputs. q93. machine learning libraries purposes. purpose libraries scientific computation numpy tabular data pandas data modelling & preprocessing scikit learn time-series analysis statsmodels text processing regular expressions, nltk deep learning tensorflow, pytorch q94. auto-encoder? auto-encoders simple learning networks aim transform inputs outputs minimum possible error. means want output close input possible. add couple layers input output, sizes layers smaller input layer. auto-encoder receives unlabelled input encoded reconstruct input. q95. boltzmann machine? boltzmann machines simple learning algorithm allows discover interesting features represent complex regularities training data. boltzmann machine basically optimise weights quantity given problem. learning algorithm slow networks layers feature detectors. “restricted boltzmann machines” algorithm single layer feature detectors makes faster rest.\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x q96. dropout batch normalization? dropout technique dropping hidden visible units network randomly prevent overfitting data (typically dropping 20 cent nodes). doubles number iterations needed converge network. batch normalization technique improve performance stability neural networks normalizing inputs layer mean output activation zero standard deviation one. q97. difference batch gradient descent stochastic gradient descent? batch gradient descent stochastic gradient descent batch gradient computes gradient entire dataset. stochastic gradient computes gradient single sample. takes time converge volume data huge, weights update slowly. converges faster batch gradient updates weight frequently. q98. tensorflow preferred library deep learning? tensorflow provides c++ python apis, making easier work faster compilation time compared deep learning libraries like keras torch. tensorflow supports cpu gpu computing devices. q99. mean tensor tensorflow? tensor mathematical object represented arrays higher dimensions. arrays data different dimensions ranks fed input neural network called “tensors.”\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x q100. computational graph? tensorflow based creating computational graph. network nodes node operates, nodes represent mathematical operations, edges represent tensors. data flows form graph, called “dataflow graph.” q101. differences supervised unsupervised learning? supervised learning unsupervised learning • uses known labeled data input • supervised learning feedback mechanism • commonly supervised learning algorithms decision trees, logistic regression, support vector machine • uses unlabeled data input • unsupervised learning feedback mechanism • commonly unsupervised learning algorithms k-means clustering, hierarchical clustering, apriori algorithm 102. logistic regression done? logistic regression measures relationship dependent variable (our label want predict) independent variables (our features) estimating probability underlying logistic function (sigmoid). image shown depicts logistic regression works\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x formula graph sigmoid function shown 103. explain steps making decision tree. 1. entire data set input 2. calculate entropy target variable, predictor attributes 3. calculate information gain attributes (we gain information sorting different objects other) 4. choose attribute highest information gain root node 5. repeat procedure branch decision node branch finalized example, let's want build decision tree decide accept decline job offer. decision tree case shown\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x clear decision tree offer accepted if • salary greater $50,000 • commute hour • incentives offered 104. build random forest model? random forest built number decision trees. split data different packages decision tree different groups data, random forest brings trees together. steps build random forest model 1. randomly select 'k' features total of'm' features k << m 2. 'k' features, calculate node d best split point 3. split node daughter nodes best split 4. repeat steps leaf nodes finalized 5. build forest repeating steps 'n' times create 'n' number trees 105. avoid overfitting model? overfitting refers model set small data ignores bigger picture. main methods avoid overfitting 1. model simple—take fewer variables account, removing noise training data 2. use cross-validation techniques, k folds cross-validation 3. use regularization techniques, lasso, penalize certain model parameters they're likely cause overfitting 106. differentiate univariate, bivariate, multivariate analysis.\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x univariate univariate data contains variable. purpose univariate analysis describe data find patterns exist it. example height students height (in cm) 164 167.3 170 174.2 178 180 patterns studied drawing conclusions mean, median, mode, dispersion range, minimum, maximum, etc. bivariate bivariate data involves different variables. analysis type data deals causes relationships analysis determine relationship variables. example temperature ice cream sales summer season\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x temperature (in celcius) sales 20 2,000 25 2,100 26 2,300 28 2,400 30 2,600 36 3,100 here, relationship visible table temperature sales directly proportional other. hotter temperature, better sales. multivariate multivariate data involves variables, categorized multivariate. similar bivariate, contains dependent variable. example data house price prediction no. rooms floors area (sq ft) price 2 0 900 $4000,00\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x 3 2 1,100 $600,000 3.5 5 1,500 $900,000 4 3 2,100 $1,200,000 patterns studied drawing conclusions mean, median, mode, dispersion range, minimum, maximum, etc. start describing data guess price house be. 107. feature selection methods select right variables? main methods feature selection filter methods involves • linear discrimination analysis • anova • chi-square best analogy selecting features \"bad data in, bad answer out.\" we're limiting selecting features, it's cleaning data coming in. wrapper methods involves • forward selection test feature time adding good fit • backward selection test features start removing works better • recursive feature elimination recursively looks different features pair wrapper methods labor-intensive, high-end computers needed lot data analysis performed wrapper method. 108. choice language, write program prints numbers ranging 50.\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x multiples three, print \"fizz\" instead number multiples five, print \"buzz.\" numbers multiples five, print \"fizzbuzz\" code shown below note range mentioned 51, means zero 50. however, range asked 50. therefore, code, include range (1,51). output code shown 109. given data set consisting variables 30 percent missing values. deal them? following ways handle missing data values data set large, simply remove rows missing data values. quickest way; use rest data predict values. smaller data sets, substitute missing values mean average rest data pandas data frame python. different ways so, df.mean(), df.fillna(mean).\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x 110. given points, calculate euclidean distance python? plot1 = [1,3] plot2 = [2,5] euclidean distance calculated follows euclidean_distance = sqrt( (plot1[0]-plot2[0])**2 + (plot1[1]-plot2[1])**2 ) 111. dimensionality reduction benefits? dimensionality reduction refers process converting data set vast dimensions data fewer dimensions (fields) convey similar information concisely. reduction helps compressing data reducing storage space. reduces computation time fewer dimensions lead computing. removes redundant features; example, there's point storing value different units (meters inches). 112. calculate eigenvalues eigenvectors following 3x3 matrix? -2 -4 2 -2 1 2 4 2 5 characteristic equation shown expanding determinant (-2 – λ) [(1-λ) (5-λ)-2x2] + 4[(-2) x (5-λ) -4x2] + 2[(-2) x 2-4(1-λ)] =0 - λ3 + 4λ2 + 27λ – 90 = 0, λ3 - 4 λ2 -27 λ + 90 = 0 algebraic equation built eigenvectors. hit trial\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x 33 – 4 x 32 - 27 x 3 +90 = 0 hence, (λ - 3) factor λ3 - 4 λ2 - 27 λ +90 = (λ – 3) (λ2 – λ – 30) eigenvalues 3,-5,6 (λ – 3) (λ2 – λ – 30) = (λ – 3) (λ+5) (λ-6), calculate eigenvector λ = 3 x = 1, -5 - 4y + 2z =0, -2 - 2y + 2z =0 subtracting equations 3 + 2y = 0, subtracting second equation y = -(3/2) z = -(1/2) similarly, calculate eigenvectors -5 6. 113. maintain deployed model? steps maintain deployed model are monitor constant monitoring models needed determine performance accuracy. change something, want figure changes going affect things. needs monitored ensure it's it's supposed do. evaluate evaluation metrics current model calculated determine new algorithm needed. compare\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x new models compared determine model performs best. rebuild best performing model re-built current state data. 114. recommender systems? recommender system predicts user rate specific product based preferences. split different areas collaborative filtering example, last.fm recommends tracks users similar interests play often. commonly seen amazon making purchase; customers notice following message accompanied product recommendations \"users bought bought…\" content-based filtering example pandora uses properties song recommend music similar properties. here, look content, instead looking listening music. 115. find rmse mse linear regression model? rmse mse common measures accuracy linear regression model. rmse indicates root mean square error. mse indicates mean square error. 116. select k k-means? use elbow method select k k-means clustering. idea elbow method run k- means clustering data set 'k' number clusters.\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x sum squares (wss), defined sum squared distance member cluster centroid. 117. significance p-value? p-value typically ≤ 0.05 indicates strong evidence null hypothesis; reject null hypothesis. p-value typically > 0.05 indicates weak evidence null hypothesis, accept null hypothesis. p-value cutoff 0.05 considered marginal, meaning way. 118. outlier values treated? drop outliers garbage value. example height adult = abc ft. true, height string value. case, outliers removed. outliers extreme values, removed. example, data points clustered zero 10, point lies 100, remove point. drop outliers, try following • try different model. data detected outliers linear models fit nonlinear models. therefore, sure choosing correct model. • try normalizing data. way, extreme data points pulled similar range. • use algorithms affected outliers; example random forests. 119. time-series data declared stationery? stationary variance mean series constant time. visual example\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x graph, variance constant time. here, x time factor y variable. value y goes points time; words, stationary. second graph, waves bigger, means non-stationary variance changing time. 120. calculate accuracy confusion matrix? consider confusion matrix values total data, actual values, predicted values. formula accuracy is accuracy = (true positive + true negative) / total observations = (262 + 347) / 650 = 609 / 650 = 0.93 result, accuracy 93 percent. 121. write equation calculate precision recall rate.\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x consider confusion matrix previous . precision = (true positive) / (true positive + false positive) = 262 / 277 = 0.94 recall rate = (true positive) / (total positive + false negative) = 262 / 288 = 0.90 122. 'people bought bought…' recommendations seen amazon result algorithm? recommendation engine accomplished collaborative filtering. collaborative filtering explains behavior users purchase history terms ratings, selection, etc. engine makes predictions interest person based preferences users. algorithm, item features unknown. example, sales page shows certain number people buy new phone buy tempered glass time. time, person buys phone, recommendation buy tempered glass well. 123. generative adversarial network?\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x suppose wine shop purchasing wine dealers, resell later. dealers sell fake wine. case, shop owner able distinguish fake authentic wine. forger try different techniques sell fake wine sure specific techniques past shop owner’s check. shop owner probably feedback wine experts wine original. owner improve determines wine fake authentic. forger’s goal create wines indistinguishable authentic ones shop owner intends tell wine real accurately let understand example help image. noise vector coming forger generating fake wine. forger acts generator. shop owner acts discriminator. discriminator gets inputs; fake wine, real authentic wine. shop owner figure real fake. so, primary components generative adversarial network (gan) named 1. generator 2. discriminator generator cnn keeps keys producing images closer appearance real images discriminator tries determine difference real fake images ultimate aim discriminator learn identify real fake images. apart technical s, interviewer hit simple ones check overall confidence, likes following. 124. given dataset cancer detection. built classification model achieved accuracy 96 percent. shouldn't happy model performance? it? cancer detection results imbalanced data. imbalanced dataset, accuracy based measure performance. important focus remaining percent, represents patients wrongly diagnosed. early diagnosis crucial comes cancer detection, greatly improve patient's prognosis.\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x hence, evaluate model performance, use sensitivity (true positive rate), specificity (true negative rate), f measure determine class wise performance classifier. 125. following machine learning algorithms inputting missing values categorical continuous variables? • k-means clustering • linear regression • k-nn (k-nearest neighbor) • decision trees k nearest neighbor algorithm compute nearest neighbor doesn't value, computes nearest neighbor based features. you're dealing k-means clustering linear regression, need pre- processing, otherwise, they'll crash. decision trees problem, variance. 126. actual values target variable train file. entropy target variable? [0, 0, 0, 1, 1, 1, 1, 1] choose correct answer. 1. -(5/8 log(5/8) + 3/8 log(3/8)) 2. 5/8 log(5/8) + 3/8 log(3/8) 3. 3/8 log(5/8) + 5/8 log(3/8) 4. 5/8 log(3/8) – 3/8 log(5/8) target variable, case, 1. formula calculating entropy is putting p=5 n=8, entropy = = -(5/8 log(5/8) + 3/8 log(3/8)) 127. want predict probability death heart disease based risk factors age, gender, blood cholesterol level. appropriate algorithm case? choose correct option\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x 1. logistic regression 2. linear regression 3. k-means clustering 4. apriori algorithm appropriate algorithm case a, logistic regression. 128. studying behavior population, identified specific individual types valuable study. like find users similar individual type. algorithm appropriate study? choose correct option 1. k-means clustering 2. linear regression 3. association rules 4. decision trees looking grouping people specifically different similarities, indicates value k. therefore, k-means clustering (answer a) appropriate algorithm study. 129. run association rules algorithm dataset, rules {banana, apple} => {grape} {apple, orange} => {grape} found relevant. true? choose right 1. {banana, apple, grape, orange} frequent itemset 2. {banana, apple} => {orange} relevant rule 3. {grape} => {banana, apple} relevant rule 4. {grape, apple} frequent itemset answer a {grape, apple} frequent itemset 130. organization website visitors randomly receive coupons. possible visitors website receive coupon. asked determine offering coupon website visitors impact purchase decisions. analysis method use? 1. one-way anova 2. k-means clustering\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x 3. association rules 4. student's t-test answer a one-way anova additional data science interview s basic concepts 131. feature vectors? feature vector n-dimensional vector numerical features represent object. machine learning, feature vectors represent numeric symbolic characteristics (called features) object mathematical way that's easy analyze. 132. steps making decision tree? 1. entire data set input. 2. look split maximizes separation classes. split test divides data sets. 3. apply split input data (divide step). 4. re-apply steps divided data. 5. stop meet stopping criteria. 6. step called pruning. clean tree went far splits. 133. root cause analysis? root cause analysis initially developed analyze industrial accidents widely areas. problem-solving technique isolating root causes faults problems. factor called root cause deduction problem-fault-sequence averts final undesirable event recurring. 134. logistic regression? logistic regression known logit model. technique forecast binary outcome linear combination predictor variables. 135. recommender systems? recommender systems subclass information filtering systems meant predict preferences ratings user product. 136. explain cross-validation.\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x cross-validation model validation technique evaluating outcomes statistical analysis generalize independent data set. mainly backgrounds objective forecast wants estimate accurately model accomplish practice. goal cross-validation term data set test model training phase (i.e. validation data set) limit problems like overfitting gain insight model generalize independent data set. 137. collaborative filtering? recommender systems use filtering process find patterns information collaborating perspectives, numerous data sources, agents. 138. gradient descent methods converge similar points? not, cases, reach local minima local optima point. reach global optima point. governed data starting conditions. 139. goal a/b testing? statistical hypothesis testing randomized experiments variables, b. objective a/b testing detect changes web page maximize increase outcome strategy. 140. drawbacks linear model? • assumption linearity errors • can't count outcomes binary outcomes • overfitting problems can't solve 141. law large numbers? theorem describes result performing experiment frequently. theorem forms basis frequency-style thinking. states sample mean, sample variance sample standard deviation converge trying estimate. 142. confounding variables? extraneous variables statistical model correlates directly inversely dependent independent variable. estimate fails account confounding factor. 143. star schema? traditional database schema central table. satellite tables map ids physical names descriptions connected central fact table id fields; tables known\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x lookup tables principally useful real-time applications, save lot memory. sometimes, star schemas involve layers summarization recover information faster. 144. regularly algorithm updated? want update algorithm when • want model evolve data streams infrastructure • underlying data source changing • case non-stationarity 145. eigenvalue eigenvector? eigenvalues directions particular linear transformation acts flipping, compressing, stretching. eigenvectors understanding linear transformations. data analysis, usually calculate eigenvectors correlation covariance matrix. 146. resampling done? resampling cases • estimating accuracy sample statistics subsets accessible data, drawing randomly replacement set data points • substituting labels data points performing significance tests • validating models random subsets (bootstrapping, cross-validation) 147. selection bias? selection bias, general, problematic situation error introduced non-random population sample. 148. types biases occur sampling? 1. selection bias 2. undercoverage bias 3. survivorship bias 149. survivorship bias?\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x survivorship bias logical error focusing aspects support surviving process casually overlooking lack prominence. lead wrong conclusions numerous ways. 150. work random forest? underlying principle technique weak learners combine provide strong learner. steps involved are 1. build decision trees bootstrapped training samples data 2. tree, time split considered, random sample mm predictors chosen split candidates pp predictors 3. rule thumb split m=p√m=p 4. predictions majority rule 151. important skills python regard data analysis? following important skills possess come handy performing data analysis python. • good understanding built-in data types especially lists, dictionaries, tuples, sets. • mastery n-dimensional numpy arrays. • mastery pandas dataframes. • ability perform element-wise vector matrix operations numpy arrays. • knowing use anaconda distribution conda package manager. • familiarity scikit-learn. **scikit-learn cheat sheet** • ability write efficient list comprehensions instead traditional loops. • ability write small, clean functions (important developer), preferably pure functions don’t alter objects. • knowing profile performance python script optimize bottlenecks. credit kdnuggets, simplilearn, edureka, guru99, hackernoon, datacamp, nitin panwar, michael rundell\n",
      "\n",
      "deep learning interview s know 1.3k views kurt updated 22,2019 deep learning hottest topics 2018-19 good reason. advancements industry time come machines computer programs actually replacing humans. arti+cial intelligence going create 2.3 million jobs 2020 crack job interview come set deep learning interview s. divided article sections basic deep learning interview s advance deep learning interview s basics deep learning interview s q1. differentiate ai, machine learning deep learning. artificial intelligence technique enables machines mimic human behavior. machine learning subset ai technique uses statistical methods enable machines improve experience. deep learning subset ml computation multi-layer neural network feasible. uses neural networks simulate human-like decision making. q2. think deep learning better machine learning? so, why? traditional ml algorithms solve lot cases, useful working high dimensional data, large number inputs outputs. example, case handwriting recognition, large input different type inputs associated different type handwriting. second major challenge tell computer features look play important role predicting outcome achieve better accuracy so. q3. perceptron? work? focus structure biological neuron, dendrites receive inputs. inputs summed cell body axon passed biological neuron shown below. dendrite receives signals neurons cell body sums inputs axon transmit signals cells similarly, perceptron receives multiple inputs, applies transformations functions provides output. perceptron linear model binary classi+cation. models neuron set inputs, given specific weight. neuron computes function weighted inputs gives output.  subscribe \n",
      "\n",
      "q4. role weights bias? perceptron, input called bias. weights determine slope classifier line, bias allows shift line left right. normally bias treated weighted input input value x q5. activation functions? activation function translates inputs outputs. activation function decides neuron activated calculating weighted sum adding bias it. purpose activation function introduce non-linearity output neuron. activation functions like linear identity unit binary step sigmoid logistic tanh relu softmax q6. explain learning perceptron. 1. initializing weights threshold. 2. provide input calculate output. 3. update weights. 4. repeat steps 2 3 wj (t+1) – updated weight wj (t) – old weight d – desired output y – actual output x – input q7. significance cost/loss function? cost function measure accuracy neural network respect given training sample expected output. provides performance neural network whole. deep learning, goal minimize cost function. that, use concept gradient descent. q8. gradient descent? gradient descent optimization algorithm minimize function iteratively moving direction steepest descent defined negative gradient. stochastic gradient descent uses single training example calculate gradient update parameters. batch gradient descent calculate gradients dataset perform update iteration. mini-batch gradient descent mini-batch gradient variation stochastic gradient descent instead single training example, mini-batch samples used. it’s popular optimization algorithms. q9. benefits mini-batch gradient descent? efficient compared stochastic gradient descent. generalization finding flat minima. mini-batches allows help approximate gradient entire training set helps avoid local minima. 0.\n",
      "\n",
      "q10.what steps gradient descent algorithm? initialize random weight bias. pass input network values output layer. calculate error actual value predicted value. neuron contributes error change respective values reduce error. reiterate find best weights network. q11. create gradient descent python. q12. shortcomings single layer perceptron? well, major problems single-layer perceptrons classify non-linearly separable data points. complex problems, involve lot parameters solved single-layer perceptrons q13. multi-layer-perceptron multilayer perceptron (mlp) deep, arti+cial neural network. composed perceptron. composed input layer receive signal, output layer makes decision prediction input, two, arbitrary number hidden layers true computational engine mlp. q14. different parts multi-layer perceptron? input nodes input nodes provide information outside world network referred “input layer”. computation performed input nodes – pass information hidden nodes. hidden nodes hidden nodes perform computations transfer information input nodes output nodes. collection hidden nodes forms “hidden layer”. network single input layer single output layer, zero multiple hidden layers. output nodes output nodes collectively referred “output layer” responsible computations transferring information network outside world. q15. data normalization need it? data normalization important preprocessing step, rescale values +t speci+c range assure better convergence backpropagation. general, boils subtracting mean data point dividing standard deviation. basic deep learning interview s. now, let’s advanced ones. advance interview s q16. better deep networks shallow ones? why? networks, shallow deep capable approximating function. matters precise network terms getting results. shallow network works features, can’t extract more. deep network goes deep computing efficiently working features/parameters. q17. weight initialization important neural networks? weight initialization important steps. bad weight initialization prevent network learning good weight initialization helps giving quicker convergence better overall error. 1 2 3 4 5 6 7 8 9 10 11 12 13 params = [weights_hidden, weights_output, bias_hidden, bias_output] def sgd(cost, params, lr=0.05) grads = t.grad(cost=cost, wrt=params) updates = [] p, g zip(params, grads) updates.append([p, p - g * lr]) return updates updates = sgd(cost, params)\n",
      "\n",
      "biases generally initialized zero. rule setting weights close zero small. q18. what’s difference feed-forward backpropagation neural network? feed-forward neural network type neural network architecture connections “fed forward”, i.e. form cycles. term “feed-forward” input input layer travels input hidden hidden output layer. backpropagation training algorithm consisting 2 steps feed-forward values. calculate error propagate earlier layers. precise, forward-propagation backpropagation algorithm comes back-propagating. q19. hperparameteres? neural network. hyperparameters variables determine network structure(eg number hidden units) variables determine network trained(eg learning rate). hyperparameters set training. number hidden layers network weight initialization activation function learning rate momentum number epochs batch size q20. explain different hyperparameters related network training. network hyperparameters number hidden layers hidden units layer regularization techniques increase accuracy. smaller number units cause underfitting. network weight initialization ideally, better use different weight initialization schemes according activation function layer. uniform distribution used. activation function activation functions introduce nonlinearity models, allows deep learning models learn nonlinear prediction boundaries. training hyperparameters learning rate learning rate de+nes quickly network updates parameters. low learning rate slows learning process converges smoothly. larger learning rate speeds learning converge. momentum momentum helps know direction step knowledge previous steps. helps prevent oscillations. typical choice momentum 0.5 0.9. number epochs number epochs number times training data shown network training. increase number epochs validation accuracy starts decreasing training accuracy increasing(overfitting). batch size mini batch size number sub-samples given network parameter update happens. good default batch size 32. try 32, 64, 128, 256, on. q21. dropout? dropout regularization technique avoid over+tting increasing generalizing power. generally, use small dropout value 20%-50% neurons 20% providing good starting point. probability low minimal effect value high results under-learning network. use larger network. likely better performance dropout larger network, giving model opportunity learn independent representations. q22. training neural network, notice loss decrease starting epochs. reason? reasons be learning rate low regularization parameter high stuck local minima\n",
      "\n",
      "q23. deep learning frameworks tensorflow caffe microsoft cognitive toolkit/cntk torch/pytorch mxnet chainer keras q24. tensors? tensors de facto representing data deep learning. multidimensional arrays, allows represent data having higher dimensions. general, deep learning deal high dimensional data sets dimensions refer different features present data set. q25. list advantages tensorflow? platform flexibility easily trainable cpu gpu distributed computing. tensorflow auto differentiation capabilities advanced support threads, asynchronous computation, queue es. customizable open source. q26. computational graph? computational graph series tensorflow operations arranged nodes graph. node takes zero tensors input produces tensor output. basically, think computational graph alternative way conceptualizing mathematical calculations takes place tensorflow program. operations assigned different nodes computational graph performed parallel, thus, providing better performance terms computations. q27. cnn? convolutional neural network (cnn, convnet) class deep neural networks, commonly applied analyzing visual imagery. unlike neural networks, input vector, input multi-channeled image. cnns use variation multilayer perceptrons designed require minimal preprocessing. q28. explain different layers cnn. layered concepts understand convolutional neural networks convolution convolution layer comprises set independent +lters. +lters initialized randomly parameters learned network subsequently. relu layer convolutional layer.\n",
      "\n",
      "pooling function progressively reduce spatial size representation reduce number parameters computation network. pooling layer operates feature map independently. connectedness neurons fully connected layer connections activations previous layer, seen regular neural networks. activations computed matrix multiplication followed bias offset. q29. rnn? recurrent networks type arti+cial neural network designed recognize patterns sequences data, text, genomes, handwriting, spoken word, numerical times series data. recurrent neural networks use backpropagation algorithm training internal memory, rnn’s able remember important things input received, enables precise predicting what’s coming next. q30. issues faced training rnn? recurrent neural networks use backpropagation algorithm training, applied timestamp. commonly known back-propagation time (btt). issues back-propagation as vanishing gradient exploding gradient q31. vanishing gradient? harmful? back-propagation, gradients tend smaller smaller moving backward network. means neurons earlier layers learn slowly compared neurons later layers hierarchy. earlier layers network important responsible learn detecting simple patterns actually building blocks network. obviously, improper inaccurate results, expect layers complete network perform nicely produce accurate results. training process takes long prediction accuracy model decrease. q32. exploding gradient descent? exploding gradients problem large error gradients accumulate result large updates neural network model weights training. gradient descent process works best updates small controlled. magnitudes gradients accumulate, unstable network likely occur, cause poor prediction results model reports useful ever. q33. explain importance lstm. long short-term memory(lstm) arti+cial recurrent neural network architecture +eld deep learning. unlike standard feedforward neural networks, lstm feedback connections “general purpose computer”. process single data points, entire sequences data. special kind recurrent neural networks capable learning long-term dependencies. q34. capsules capsule neural network? capsules vector specifying features object likelihood. features instantiation parameters like position, size, orientation, deformation, velocity, hue, texture more.\n",
      "\n",
      "capsule specify attributes like angle size represent generic information. now, like neural network layers neurons, capsule network layers capsules. now, let’s continue deep learning interview s section autoencoders rbms. q35. explain autoencoders it’s uses. autoencoder neural network unsupervised machine learning algorithm applies backpropagation, setting target values equal inputs. autoencoders reduce size inputs smaller representation. needs original data, reconstruct compressed data. q36. terms dimensionality reduction, autoencoder differ pcas? autoencoder learn non-linear transformations non-linear activation function multiple layers. doesn’t learn dense layers. use convolutional layers learn better video, image series data. efficient learn layers autoencoder learn huge transformation pca. autoencoder provides representation layer output. use pre-trained layers model apply transfer learning enhance encoder/decoder. q37. real-life examples autoencoders applied. image coloring autoencoders converting black white picture colored image. depending picture, possible tell color be. feature variation extracts required features image generates output removing noise unnecessary interruption. dimensionality reduction reconstructed image input reduced dimensions. helps providing similar image reduced pixel value. denoising image input seen autoencoder raw input stochastically corrupted version. denoising autoencoder trained reconstruct original input noisy version. q38. different layers autoencoders? autoencoder consist layers encoder code decoder q39. explain architecture autoencoder. encoder network compresses input latent space representation. encoder layer encodes input image compressed representation reduced dimension. compressed image distorted version original image.\n",
      "\n",
      "code network represents compressed input fed decoder. decoder layer decodes encoded image original dimension. decoded image lossy reconstruction original image reconstructed latent space representation. q40. bottleneck autoencoder used? layer encoder decoder, ie. code known bottleneck. well-designed approach decide aspects observed data relevant information aspects discarded. balancing criteria compactness representation, measured compressibility. retains behaviourally relevant variables input. q41. variation autoencoders? convolution autoencoders sparse autoencoders deep autoencoders contractive autoencoders q42. deep autoencoders? extension simple autoencoder deep autoencoder. +rst layer deep autoencoder +rst-order features raw input. second layer second- order features corresponding patterns appearance first-order features. deeper layers deep autoencoder tend learn higher-order features. deep autoencoder composed two, symmetrical deep-belief networks shallow layers representing encoding half net. second set layers decoding half. q43. restricted boltzmann machine? restricted boltzmann machine undirected graphical model plays major role deep learning framework recent times. algorithm useful dimensionality reduction, classification, regression, collaborative filtering, feature learning, topic modeling. q44. rbm differ autoencoders? autoencoder simple 3-layer neural network output units directly connected input units. typically, number hidden units number visible ones. task training minimize error reconstruction, i.e. find efficient compact representation input data. rbm shares similar idea, uses stochastic units particular distribution instead deterministic distribution. task training +nd sets variables actually\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x interview series #2 python programming numpy 1. python numpy better lists? python numpy arrays considered instead list fast, consume memory convenient lots functionality. 2. describe map function python? map function executes function given argument elements iterable given second argument. 3. generate array ‘100’ random numbers sampled standard normal distribution numpy np.random.rand(100) create 100 random numbers generated standard normal distribution mean 0 standard deviation 1. 4. count occurrence value numpy array? use numpy.bincount() >>> arr = numpy.array([0, 5, 5, 0, 2, 4, 3, 0, 0, 5, 4, 1, 9, 9]) >>> numpy.bincount(arr) argument bincount() consist booleans positive integers. negative integers invalid. 5. numpy support nan? nan, short “not number”, special floating point value defined ieee-754 specification. python numpy supports nan definition nan system dependent systems don't round support like older cray vax computers. 6. ravel() function numpy do? combines multiple numpy arrays single array 7. meaning axis=0 axis=1? axis = 0 meant reading rows, axis = 1 meant reading columns\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x 8. numpy describe use cases? numpy package library python, adding support large, multi-dimensional arrays matrices, large collection high level mathematical functions. simple words, numpy optimized version python lists like financial functions, linear algebra, statistics, polynomials, sorting searching etc. 9. remove array items exist another? >>> = np.array([5, 4, 3, 2, 1]) >>> b = np.array([4, 8, 9, 10, 1]) # 'a' remove 'b' >>> np.setdiff1d(a,b) # output >>> array([5, 3, 2]) 10. sort numpy array specific column 2d array? #choose column 2 example >>> import numpy np >>> arr = np.array([[1, 2, 3], [4, 5, 6], [0,0,1]]) >>> arr[arr[,1].argsort()] # output >>> array([[0, 0, 1], [1, 2, 3], [4, 5, 6]]) 11. reverse numpy array efficient way? >>> import numpy np >>> arr = np.array([9, 10, 1, 2, 0]) >>> reverse_arr = arr[-1] 12. calculate percentiles numpy? >>> import numpy np >>> arr = np.array([11, 22, 33, 44 ,55 ,66, 77]) >>> perc = np.percentile(arr, 40) #returns 40th percentile >>> print(perc) 13. difference numpy scipy? numpy contain array data type basic operations indexing, sorting, reshaping, basic element wise functions, et cetera. numerical code reside scipy. scipy contains fully-featured versions linear algebra modules, numerical algorithms.\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x 14. preferred way check (zero element) array? numpy array, use size attribute. size attribute helpful determining length numpy array >>> arr = numpy.zeros((1,0)) >>> arr.size 15. difference matrices arrays? matrices two-dimensional, arrays number dimensions 16. find indices array condition true? given array a, condition arr > 3 returns boolean array false interpreted 0 python numpy. >>> import numpy np >>> arr = np.array([[9,8,7],[6,5,4],[3,2,1]]) >>> arr > 3 >>> array([[true, true, true], [ true, true, true], [false, false, false]], dtype=bool) 17. find maximum minimum value given flattened array? >>> import numpy np >>> = np.arange(4).reshape((2,2)) >>> max_val = np.amax(a) >>> min_val = np.amin(a) 18. write numpy program calculate difference maximum minimum values given array second axis. >>> import numpy np >>> arr = np.arange(16).reshape((4, 7)) >>> res = np.ptp(arr, 1) 19. find median numpy flattened array >>> import numpy np >>> arr = np.arange(16).reshape((4, 5)) >>> res = np.median(arr)\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x 20. write numpy program compute mean, standard deviation, variance given array second axis import numpy np >>> import numpy np >>> x = np.arange(16) >>> mean = np.mean(x) >>> std = np.std(x) >>> var= np.var(x) 21. calculate covariance matrix numpy arrays >>> import numpy np >>> x = np.array([2, 1, 0]) >>> y = np.array([2, 3, 3]) >>> cov_arr = np.cov(x, y) 22. compute compute pearson product-moment correlation coefficients given numpy arrays >>> import numpy np >>> x = np.array([0, 1, 3]) >>> y = np.array([2, 4, 5]) >>> cross_corr = np.corrcoef(x, y) 23. develop numpy program compute histogram nums bins >>> import numpy np >>> nums = np.array([0.5, 0.7, 1.0, 1.2, 1.3, 2.1]) >>> bins = np.array([0, 1, 2, 3]) >>> np.histogram(nums, bins) 24. powers array values element-wise >>> import numpy np >>> x = np.arange(7) >>> np.power(x, 3) 25. write numpy program true division element-wise array inputs >>> import numpy np >>> x = np.arange(10) >>> np.true_divide(x, 3)\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x pandas 26. series pandas? series defined one-dimensional array capable storing data types. row labels series called index. 'series' method, easily convert list, tuple, dictionary series. series contain multiple columns. 27. features pandas reliable option store tabular data? memory efficient, data alignment, reshaping, merge join time series. 28. reindexing pandas? reindexing conform dataframe new index optional filling logic. places na/nan location values present previous index. returns new object new index produced equivalent current one, value copy false. change index rows columns dataframe. 29. create series dict pandas? series defined one-dimensional array capable storing data types. >>> import pandas pd >>> info = {'x' 0., 'y' 1., 'z' 2.} >>> = pd.series(info) 30. create copy series pandas? use pandas.series.copy method >>> import pandas pd >>> pd.series.copy(deep=true) 31. groupby pandas? groupby split data groups. groups data based criteria. grouping provides mapping labels group names. lot variations defined parameters makes task splitting data quick easy. 32. vectorization pandas?\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x vectorization process running operations entire array. reduce iteration performed functions. pandas number vectorized functions like aggregations, string functions optimized operate specifically series dataframes. preferred use vectorized pandas functions execute operations quickly. 33. mention different types data structures pandas pandas provide data structures, supported pandas library, series, dataframes. data structures built numpy. 34. time series pandas time series ordered sequence data basically represents quantity changes time. pandas contains extensive capabilities features working time series data domains. 35. convert pandas dataframe numpy array? function to_numpy() convert dataframe numpy array. dataframe.to_numpy(self, dtype=none, copy=false) dtype parameter defines data type pass array copy ensures returned value view array. 36. write pandas program 5 rows given dataframe >>> import pandas pd >>> exam_data = {'name' ['anastasia', 'dima', 'katherine', 'james', 'emily', 'michael', 'matthew', 'laura', 'kevin', 'jonas'],} labels = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j'] >>> df = pd.dataframe(exam_data , index=labels) >>> df.iloc[5] 37. develop pandas program create display one-dimensional array- like object containing array data. >>> import pandas pd >>> pd.series([2, 4, 6, 8, 10]) 38. write python program convert panda module series python list it's type. >>> import pandas pd >>> ds = pd.series([2, 4, 6, 8, 10])\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x >>> type(ds) >>> ds.tolist() >>> type(ds.tolist()) 39. develop pandas program add, subtract, multiple divide pandas series. >>> import pandas pd >>> ds1 = pd.series([2, 4, 6, 8, 10]) >>> ds2 = pd.series([1, 3, 5, 7, 9]) >>> sum = ds1 + ds2 >>> sub = ds1 - ds2 >>> mul = ds1 * ds2 >>> div = ds1 / ds2 40. develop pandas program compare elements pandas series. >>> import pandas pd >>> ds1 = pd.series([2, 4, 6, 8, 10]) >>> ds2 = pd.series([1, 3, 5, 7, 10]) >>> ds1 == ds2 >>> ds1 > ds2 >>> ds1 < ds2 41. develop pandas program change data type given column series. >>> import pandas pd >>> s1 = pd.series(['100', '200', 'python', '300.12', '400']) >>> s2 = pd.to_numeric(s1, errors='coerce') >>> s2 42. write pandas program convert series lists series >>> import pandas pd >>> s = pd.series([ ['red', 'black'], ['red', 'green', 'white'] , ['yellow']]) >>> s = s.apply(pd.series).stack().reset_index(drop=true) 43. write pandas program create subset given series based value condition >>> import pandas pd >>> s = pd.series([0, 1,2,3,4,5,6,7,8,9,10]) >>> n = 6\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x >>> new_s = s[s < n] >>> new_s 44. develop pandas code alter order index given series >>> import pandas pd >>> s = pd.series(data = [1,2,3,4,5], index = ['a', 'b', 'c','d','e']) >>> s.reindex(index = ['b','a','c','d','e']) 45. write pandas code items given series present given series. >>> import pandas pd >>> sr1 = pd.series([1, 2, 3, 4, 5]) >>> sr2 = pd.series([2, 4, 6, 8, 10]) >>> result = sr1[~sr1.isin(sr2)] >>> result 46. difference data series df[‘name’] df.loc[, ‘name’]? >>> view original dataframe second copy original dataframe. 47. write pandas program display frequent value given series replace “replaced” series. >>> import pandas pd >>> import numpy np >>> np.random.randomstate(100) >>> num_series = pd.series(np.random.randint(1, 5, [15])) >>> result = num_series[~num_series.isin(num_series.value_counts().index[1])] = 'replaced' 48. write pandas program find positions numbers multiples 5 given series. >>> import pandas pd >>> import numpy np >>> num_series = pd.series(np.random.randint(1, 10, 9)) >>> result = np.argwhere(num_series % 5==0) 49. add column pandas dataframe?\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x # importing pandas library >>> import pandas pd >>> info = {'one' pd.series([1, 2, 3, 4, 5], index=['a', 'b', 'c', 'd', 'e']), 'two' pd.series([1, 2, 3, 4, 5, 6], index=['a', 'b', 'c', 'd', 'e', 'f'])} >>> info = pd.dataframe(info) # add new column existing dataframe object >>> info['three']=pd.series([20,40,60],index=['a','b','c']) 50. iterate pandas dataframe? iterate rows dataframe loop combination iterrows() dataframe. python language 51. type language python? programming scripting? python capable scripting, general sense, considered general-purpose programming language. 52. python case sensitive? yes, python case sensitive language. 53. lambda function python? anonymous function known lambda function. function number parameters statement. 54. difference xrange xrange python? xrange range exact terms functionality.the difference range returns python list object x range returns xrange object. 55. docstrings python? docstrings actually comments, documentation strings. docstrings triple quotes. assigned variable therefore, times, serve purpose comments well. 56. python exits, isn’t memory deallocated? python exits, especially python modules having circular references objects objects referenced global namespaces\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x de-allocated freed. impossible de-allocate portions memory reserved c library. exit, having efficient clean mechanism, python try de-allocate/destroy object. 57. mean *args, **kwargs? use it? use *args aren’t sure arguments going passed function, want pass stored list tuple arguments function. **kwargs don’t know keyword arguments passed function, pass values dictionary keyword arguments. 58. difference deep shallow copy? shallow copy new instance type gets created keeps values copied new instance. shallow copy copy reference pointers like copies values. deep copy store values copied. deep copy doesn’t copy reference pointers objects. makes reference object new object pointed object gets stored. 59. define encapsulation python? encapsulation means binding code data together. python class example encapsulation. 60. python use access specifiers? python deprive access instance variable function. python lays concept prefixing variable, function method single double underscore imitate behavior protected private access specifiers. 61. generators python? generators way implementing iterators. generator function normal function contains yield expression function definition making generator function. 62. remove duplicate elements given list? set type available python. doesn’t allow copies provides good functions perform set operations like union, difference etc. >>> list(set(a)) 63. python allow arguments pass value pass reference?\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x arguments pass value python supports pass reference. instead, pass assignment. parameter pass originally reference object reference fixed memory location. reference passed value. additionally, data types like strings tuples immutable mutable. 64. slicing python? slicing python mechanism select range items sequence types like strings, list, tuple, etc. 65. “pass” keyword python? “pass” keyword no-operation statement python. signals action required. works placeholder compound statements intentionally left blank. 66. pep8 important? pep stands python enhancement proposal. pep official design document providing information python community, describing new feature python processes. pep 8 especially important documents style guidelines python code. apparently contributing python open-source community requires follow style guidelines sincerely strictly. 67. decorators python? decorators python essentially functions add functionality existing function python changing structure function itself. represented @decorator_name python called bottom-up fashion 68. key difference lists tuples python? key difference lists mutable, tuples hand immutable objects. 69. self python? self keyword python define instance object class. python, explicitly parameter, unlike java optional. helps distinguishing methods attributes class local variables. 70. pythonpath python? pythonpath environment variable set add additional directories python look modules packages. especially useful maintaining python libraries wish install global default location. 71. difference .py .pyc files?\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x .py files contain source code program. whereas, .pyc file contains bytecode program. bytecode compilation .py file (source code). .pyc files created files run. created files import. 72. explain access module written python c? access module written python c following method, module = =pyimport_importmodule(\"<modulename>\"); 73. namespace python? python, introduced place lives hooked for. known namespace. like box variable mapped object placed. variable searched out, box searched, corresponding object. 74. pickling unpickling? pickle module accepts python object converts string representation dumps file dump function, process called pickling. process retrieving original python objects stored string representation called unpickling. 75. python interpreted? python language interpreted language. python program runs directly source code. converts source code written programmer intermediate language, translated machine language executed. jupyter notebook 76. main use jupyter notebook? jupyter notebook open-source web application allows create share codes documents. provides environment, document code, run it, look outcome, visualize data results leaving environment. 77. increase cell width jupyter/ipython notebook browser? >>> ipython.core.display import display, html >>> display(html(\"<style>.container { width100% !important; }</style>\"))\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x 78. convert ipython notebook python file command line? >>> jupyter nbconvert --to script [your_notebook].ipynb 79. measure execution time jupyter notebook? >>> %%time inbuilt magic command 80. run jupyter notebook command line? >>> jupyter nbconvert --to python nb.ipynb 81. inline plots larger jupyter notebooks? use figure size. >>> fig=plt.figure(figsize=(18, 16), dpi= 80, facecolor='w', edgecolor='k') 82. display multiple images jupyter notebook? >>>for ima images >>>plt.figure() >>>plt.imshow(ima) 83. jupyter notebook interactive code data exploration friendly? ipywidgets package provides common user interface controls exploring code data interactively. 84. default formatting option jupyter notebook? default formatting option markdown 85. kernel wrappers jupyter? jupyter brings lightweight interface kernel languages wrapped python. wrapper kernels implement optional methods, notably code completion code inspection. 86. advantages custom magic commands? create ipython extensions custom magic commands interactive computing easier. third-party extensions magic commands exist, example, %%cython magic allows write cython code directly notebook. 87. jupyter architecture language dependent? no. language independent.\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x 88. tools allow jupyter notebooks easily convert pdf html? nbconvert converts pdf html nbviewer renders notebooks web platforms. 89. major disadvantage jupyter notebook? hard run long asynchronous tasks. secure. 90. domain jupyter notebook widely used? mainly data analysis machine learning related tasks. 91. alternatives jupyter notebook? pycharm interact, vs code python interactive etc. 92. configuration changes jupyter notebook? config file located ~/.ipython/profile_default/ipython_config.py 93. magic command run python code jupyter notebook? %run execute python code .py files 94. pass variables notebooks? %store command lets pass variables different notebooks. >>> data = 'this string want pass different notebook' >>> %store data # stored 'data' (str) # new notebook >>> %store -r data >>> print(data) 95. export contents cell/show contents external script %%writefile magic saves contents cell external file. %pycat opposite shows (in popup) syntax highlighted contents external file. 96. inbuilt tool use debugging python code jupyter notebook? jupyter interface python debugger (pdb). makes possible inside function investigate happens there.\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x 97. high resolution plots jupyter notebook? >>> %config inlinebackend.figure_format ='retina' 98. use latex jupyter notebook? write latex markdown cell, rendered formula mathjax. 99. jupyter lab? generation user interface conventional jupyter notebooks. users drag drop cells, arrange code workspace live previews. it’s early stage development. 100. biggest limitation jupyter notebook? code versioning, management debugging scalable current jupyter notebook.\n",
      "\n",
      "follow steve nouri ai data science posts https//lnkd.in/gzu463x references [1] https//www.edureka.co [2] https//www.kausalvikash.in [3] https//www.wisdomjobs.com [4] https//blog.edugrad.com [5]https//stackoverflow.com [6]http//www.ezdev.org [7]https//www.techbeamers.com [8]https//www.w3resource.com [9]https//www.javatpoint.com [10]https//analyticsindiamag.com [11]https//www.onlineinterviews.com [12]https//www.geeksforgeeks.org [13]https//www.springpeople.com [14]https//atraininghub.com [15]https//www.interviewcake.com [16]https//www.techbeamers.com [17]https//www.tutorialspoint.com [18]https//programmingwithmosh.com [19]https//www.interviewbit.com [20]https//www.guru99.com [21]https//hub.packtpub.com [22]https//analyticsindiamag.com [23]https//www.dataquest.io [24]https//www.infoworld.com\n",
      "\n",
      "https//career.guru99.com/ 50 machine learning interview s & answers 1) machine learning? machine learning branch computer science deals system programming order automatically learn improve experience. example robots programed perform task based data gather sensors. automatically learns programs data. 2) mention diﬀerence data mining machine learning? machine learning relates study, design development algorithms computers capability learn explicitly programmed. while, data mining deﬁned process unstructured data tries extract knowledge unknown interesting patterns. process machine, learning algorithms used. 3) ‘overﬁtting’ machine learning? machine learning, statistical model describes random error noise instead underlying relationship ‘overﬁtting’ occurs. model excessively complex, overﬁtting normally observed, having parameters respect number training data types. model exhibits poor performance overﬁt. 4) overﬁtting happens? possibility overﬁtting exists criteria training model criteria judge eﬃcacy model. 5) avoid overﬁtting ? lot data overﬁtting avoided, overﬁtting happens relatively small dataset, try learn it. small database forced come model based that. situation, use technique known cross validation. method dataset splits section, testing training datasets, testing dataset test model while, training dataset, datapoints come model. technique, model usually given dataset known data training (training data set) run dataset unknown data model tested. idea cross validation deﬁne dataset “test” model training phase. 6) inductive machine learning? inductive machine learning involves process learning examples, system, set observed instances tries induce general rule.\n",
      "\n",
      "7) ﬁve popular algorithms machine learning? a) decision trees b) neural networks (back propagation) c) probabilistic networks d) nearest neighbor e) support vector machines 8) diﬀerent algorithm techniques machine learning? diﬀerent types techniques machine learning a) supervised learning b) unsupervised learning c) semi-supervised learning d) reinforcement learning e) transduction f) learning learn 9) stages build hypotheses model machine learning? a) model building b) model testing c) applying model 10) standard approach supervised learning? standard approach supervised learning split set example training set test. 11) ‘training set’ ‘test set’? areas information science like machine learning, set data discover potentially predictive relationship known ‘training set’. training set examples given learner, test set test accuracy hypotheses generated learner, set example held learner. training set distinct test set. 12) list approaches machine learning? diﬀerent approaches machine learning a) concept vs classiﬁcation learning b) symbolic vs statistical learning\n",
      "\n",
      "c) inductive vs analytical learning 13) machine learning? a) artiﬁcial intelligence b) rule based inference 14) explain function ‘unsupervised learning’? a) find clusters data b) find low-dimensional representations data c) find interesting directions data d) interesting coordinates correlations e) find novel observations/ database cleaning 15) explain function ‘supervised learning’? a) classiﬁcations b) speech recognition c) regression d) predict time series e) annotate strings 16) algorithm independent machine learning? machine learning mathematical foundations independent particular classiﬁer learning algorithm referred algorithm independent machine learning? 17) diﬀerence artiﬁcial learning machine learning? designing developing algorithms according behaviours based empirical data known machine learning. artiﬁcial intelligence addition machine learning, covers aspects like knowledge representation, natural language processing, planning, robotics etc. 18) classiﬁer machine learning? classiﬁer machine learning system inputs vector discrete continuous feature values outputs single discrete value, class. 19) advantages naive bayes? naïve bayes classiﬁer converge quicker discriminative models like logistic regression, need training data. main advantage can’t learn interactions features. 20) areas pattern recognition used? pattern recognition\n",
      "\n",
      "a) computer vision b) speech recognition c) data mining d) statistics e) informal retrieval f) bio-informatics 21) genetic programming? genetic programming techniques machine learning. model based testing selecting best choice set results. 22) inductive logic programming machine learning? inductive logic programming (ilp) subﬁeld machine learning uses logical programming representing background knowledge examples. 23) model selection machine learning? process selecting models diﬀerent mathematical models, describe data set known model selection. model selection applied ﬁelds statistics, machine learning data mining. 24) methods calibration supervised learning? methods predicting good probabilities supervised learning a) platt calibration b) isotonic regression methods designed binary classiﬁcation, trivial. 25) method frequently prevent overﬁtting? suﬃcient data ‘isotonic regression’ prevent overﬁtting issue. 26) diﬀerence heuristic rule learning heuristics decision trees? diﬀerence heuristics decision trees evaluate average quality number disjointed sets rule learners evaluate quality set instances covered candidate rule. 27) perceptron machine learning? machine learning, perceptron algorithm supervised classiﬁcation input possible non-binary outputs. 28) explain components bayesian logic program?\n",
      "\n",
      "bayesian logic program consists components. ﬁrst component logical ; consists set bayesian clauses, captures qualitative structure domain. second component quantitative one, encodes quantitative information domain. 29) bayesian networks (bn) ? bayesian network represent graphical model probability relationship set variables . 30) instance based learning algorithm referred lazy learning algorithm? instance based learning algorithm referred lazy learning algorithm delay induction generalization process classiﬁcation performed. 31) classiﬁcation methods svm ( support vector machine) handle? a) combining binary classiﬁers b) modifying binary incorporate multiclass learning 32) ensemble learning? solve particular computational program, multiple models classiﬁers experts strategically generated combined. process known ensemble learning. 33) ensemble learning used? ensemble learning improve classiﬁcation, prediction, function approximation etc model. 34) use ensemble learning? ensemble learning build component classiﬁers accurate independent other. 35) paradigms ensemble methods? paradigms ensemble methods a) sequential ensemble methods b) parallel ensemble methods 36) general principle ensemble method bagging boosting ensemble method? general principle ensemble method combine predictions models built given learning algorithm order improve robustness single model. bagging method ensemble improving unstable estimation classiﬁcation schemes. boosting method sequentially reduce bias combined model. boosting bagging reduce errors reducing variance term. 37) bias-variance decomposition classiﬁcation error ensemble method?\n",
      "\n",
      "expected error learning algorithm decomposed bias variance. bias term measures closely average classiﬁer produced learning algorithm matches target function. variance term measures learning algorithm’s prediction ﬂuctuates diﬀerent training sets. 38) incremental learning algorithm ensemble? incremental learning method ability algorithm learn new data available classiﬁer generated available dataset. 39) pca, kpca ica for? pca (principal components analysis), kpca ( kernel based principal component analysis) ica ( independent component analysis) important feature extraction techniques dimensionality reduction. 40) dimension reduction machine learning? machine learning statistics, dimension reduction process reducing number random variables considerations divided feature selection feature extraction 41) support vector machines? support vector machines supervised learning algorithms classiﬁcation regression analysis. 42) components relational evaluation techniques? important components relational evaluation techniques a) data acquisition b) ground truth acquisition c) cross validation technique d) query type e) scoring metric f) signiﬁcance test 43) diﬀerent methods sequential supervised learning? diﬀerent methods solve sequential supervised learning problems a) sliding-window methods b) recurrent sliding windows c) hidden markow models d) maximum entropy markow models e) conditional random ﬁelds\n",
      "\n",
      "f) graph transformer networks 44) areas robotics information processing sequential prediction problem arises? areas robotics information processing sequential prediction problem arises a) imitation learning b) structured prediction c) model based reinforcement learning 45) batch statistical learning? statistical learning techniques allow learning function predictor set observed data predictions unseen future data. techniques provide guarantees performance learned predictor future unseen data based statistical assumption data generating process. 46) pac learning? pac (probably approximately correct) learning learning framework introduced analyze learning algorithms statistical eﬃciency. 47) diﬀerent categories categorized sequence learning process? a) sequence prediction b) sequence generation c) sequence recognition d) sequential decision 48) sequence learning? sequence learning method teaching learning logical manner. 49) techniques machine learning ? techniques machine learning a) genetic programming b) inductive learning 50) popular application machine learning day day basis? recommendation engine implemented major ecommerce websites uses machine learning guru99 provides free online tutorial courses like\n",
      "\n",
      "java mis mongodb bigdata cassandra web services sqlite jsp informatica accounting sap training python excel asp net hbase project management test management business analyst ethical hacking pmp live project soapui photoshop manual testing mobile testing data warehouse r tutorial tableau devops aws jenkins agile testing rpa junit software engineering selenium ccna angularjs nodejs plsql\n",
      "\n",
      "1 digital notes machine learning (r20d5803) m.tech., ii year – sem (2021-2022) department computer science engineering malla reddy college engineering & technology (autonomous institution – ugc, govt. india) (affiliated jntuh, hyderabad, approved aicte - accredited nba & naac – ‘a’ grade - iso 90012015 certified) maisammaguda, dhulapally (post via. hakimpet), secunderabad – 500100, telangana state, india.\n",
      "\n",
      "2 malla reddy college engineering & technology department computer science engineering syllabus ii year m. tech. cse – sem l/t/p/ c 3 / - / - 3 (r20d5803) machine learning objectives 1. course explains machine learning techniques decision tree learning, bayesian learning etc. 2. understand computational learning theory. 3. study pattern comparison techniques. unit - introduction well-posed learning problems, designing learning system perspectives issues machine learning concept learning general specific ordering introduction,a concept learning task, concept learning search, find-s finding maximally specific hypothesis, version spaces candidate elimination algorithm, remarks version spaces candidate elimination, inductive bias. decision tree learning-introduction, decision tree representation, appropriate problems decision tree learning, basic decision tree learning algorithm hypothesis space search decision tree learning, inductive bias decision tree learning, issues decision tree learning. unit - ii artificial neural networks -introduction, neural network representation, appropriate problems neural network learning, perceptions, multilayer networks propagation algorithm. discussion propagation algorithm, illustrative example face recognition unit - iii bayesian learning-introduction, byes theorem, bayes theorem concept learning maximum likelihood squared error hypotheses, maximum likelihood hypotheses predicting probabilities, minimum description length principle, bayes optimal classifier, gibs algorithm, naïve bayes classifier, example learning classify text, bayesian belief networks, em algorithm. instance-based learning-introduction, k-nearest neighbor learning, locally weighted regression, radial basis functions, case-based reasoning, remarks lazy eager learning. unit -iv pattern comparison techniques-temporal patterns, dynamic time warping methods,clustering, introduction clustering, k-means clustering, k-mode clustering. codebook generation, vector quantization. unit - v genetic algorithms different search methods induction - explanation-based learning prior knowledge reduce sample complexity. dimensionality reduction feature selection, principal component analysis, linear discriminate analysis, factor analysis, independent component analysis, multidimensional scaling, manifold learning.\n",
      "\n",
      "3 textbooks 1. machine learning – tom m. mitchell, -mgh 2. fundamentals speech recognition lawrence rabiner biing – hwang juang .ethem alpaydin, ”introduction machine learning”, mit press, prentice hall india, 3 rd edition2014. 3. mehryar mohri, afshin rostamizadeh, ameet talwalkar ” foundations machine learning”,mit press,2012 references 1. machine learning algorithmic perspective, stephen marsland, taylor & francis .\n",
      "\n",
      "4 index s. unit topic page 1 introduction well-posed learning problems 1 2 concept learning task, concept learning search 6 3 find-s finding maximally specific hypothesis 15 4 version spaces candidate elimination algorithm 17 5 remarks version spaces candidate elimination, inductive bias 21 6 decision tree learning-introduction, decision tree representation 22 7 appropriate problems decision tree learning 23 8 decision tree learning algorithm, issues decision tree learning. 25 s. unit topic page 1 ii artificial neural networks -introduction, neural network representation 26 2 ii appropriate problems neural network learning 28 3 ii perceptions, multilayer networks & propagation algorithm. 29 4 ii discussion propagation algorithm 34 malla reddy college engineering & technology department computer science engineering\n",
      "\n",
      "5 s. unit topic page 1 iii bayesian learning-introduction ,bayes theorem & concept learning maximum 36 2 iii maximum likelihood hypotheses predicting probabilities(map) 42 3 iii gibs algorithm, naïve bayes classifier 46 4 iii minimum description length principle , bayes optimal classifier 47 5 iii example learning classify text, bayesian belief networks 50 6 iii em algorithm. instance-based learning-introduction 51 7 iii k-nearest neighbor learning, locally weighted regression 55 8 iii radial basis functions, case-based reasoning 56 9 iii remarks lazy eager learning. 57 malla reddy college engineering & technology department computer science engineering\n",
      "\n",
      "6 s. unit topic page 1 iv pattern comparison techniques-temporal patterns, 58 2 iv dynamic time warping methods 61 3 iv clustering 67 5 iv k-means clustering 69 6 iv k-mode clustering. codebook generation 70 7 iv vector quantization. 76 s. unit topic page 1 v genetic algorithms different search methods induction 78 2 v explanation-based learning prior knowledge reduce sample complexity. 79 3 v dimensionality reduction 82 4 v principal component analysis 84 5 v linear discriminate analysis, factor analysis, 85 6 v independent component analysis multidimensional scaling, manifold learning. 86 malla reddy college engineering & technology department computer science engineering\n",
      "\n",
      "department cse mrcet 1 unit-i machine learning field study gives computers capability learn explicitly programmed. ml exciting technologies come across. evident name, gives computer makes similar humans ability learn. machine learning actively today, places expect. machine learning broadly categorized following headings machine learning evolved left right shown diagram. • initially, researchers started supervised learning. case housing price prediction discussed earlier . • followed unsupervised learning, machine learn supervision. • scientists discovered good idea reward machine job expected way came reinforcement learning. • soon, data available days humongous conventional techniques developed far failed analyse big data provide predictions.\n",
      "\n",
      "department cse mrcet 2 • thus, came deep learning human brain simulated artificial neural networks (ann) created binary computers. • machine learns high computing power huge memory resources available today. • observed deep learning solved previously unsolvable problems. • technique advanced giving incentives deep learning networks awards finally comes deep reinforcement learning. let study categories details supervised learning supervised learning analogous training child walk. hold child’s hand, foot forward, walk demonstration on, child learns walk own. regression similarly, case supervised learning, concrete known examples computer. given feature value x1 output y1, x2 y2, x3 y3, on. based data, let computer figure empirical relationship x y. machine trained way sufficient number data points, ask machine predict y given x. assuming know real value y given x, able deduce machine’s prediction correct. thus, test machine learned known test data. satisfied machine able predictions desired level accuracy (say 80 90%) stop training machine. now, safely use machine predictions unknown data points, ask machine predict y given x know real value y. training comes regression talked earlier.\n",
      "\n",
      "department cse mrcet 3 classification use machine learning techniques classification problems. classification problems, classify objects similar nature single group. example, set 100 students say, like group groups based heights - short, medium long. measuring height student, place proper group. now, new student comes in, appropriate group measuring height. following principles regression training, train machine classify student based feature – height. machine learns groups formed, able classify unknown new student correctly. again, use test data verify machine learned technique classification putting developed model production. supervised learning ai began journey. technique applied successfully cases. model hand-written recognition machine. algorithms developed supervised learning. learn following chapters. unsupervised learning unsupervised learning, specify target variable machine, ask machine “what tell x?”. specifically, ask s given huge data set x, “what best groups x?” “what features occur frequently x?”. arrive answers s, understand number data points machine require deduce strategy large. case supervised learning, machine trained thousands data points. however, case unsupervised learning, number data points reasonably accepted learning starts millions. days, data generally abundantly available. data ideally requires curating. however, data continuously flowing social area network, cases data curation impossible task. following figure shows boundary yellow red dots determined unsupervised machine learning. clearly machine\n",
      "\n",
      "department cse mrcet 4 able determine class black dots fairly good accuracy. reinforcement learning consider training pet dog, train pet bring ball us. throw ball certain distance ask dog fetch us. time dog right, reward dog. slowly, dog learns job rightly gives reward dog starts job right way time future. exactly, concept applied “reinforcement” type learning. technique initially developed machines play games. machine given algorithm analyse possible moves stage game. machine select moves random. right, machine rewarded, penalized. slowly, machine start differentiating right wrong moves iterations learn solve game puzzle better accuracy. accuracy winning game improve machine plays games. entire process depicted following diagram\n",
      "\n",
      "department cse mrcet 5 deep learning deep learning model based artificial neural networks (ann), specifically convolutional neural networks (cnn)s. architectures deep learning deep neural networks, deep belief networks, recurrent neural networks, convolutional neural networks. networks successfully applied solving problems computer vision, speech recognition, natural language processing, bioinformatics, drug design, medical image analysis, games. fields deep learning proactively applied. deep learning requires huge processing power humongous data, generally easily available days. talk deep learning detail coming chapters. deep reinforcement learning deep reinforcement learning (drl) combines techniques deep reinforcement learning. reinforcement learning algorithms like q learning combined deep learning create powerful drl model. technique great success fields robotics, video games, finance healthcare. previously unsolvable problems solved creating drl models. lots research going area actively pursued industries. far,\n",
      "\n",
      "department cse mrcet 6 got brief introduction machine learning models, let explore slightly deeper algorithms available models. posed learning problems computer program said learn experience e context task t performance measure p, performance t, measured p, upgrades experience e. problem segregated well-posed learning problem traits – • task • performance measure • experience certain example efficiently defines well-posed learning problems are 1. better filter emails spam • task – classifying emails spam • performance measure – fraction emails accurately classified spam spam • experience – observing label emails spam spam 2. checkers learning problem • task – playing checkers game • performance measure – percent games won opposer • experience – playing implementation games 3. handwriting recognition problem • task – acknowledging handwritten words portrayal • performance measure – percent words accurately classified • experience – directory handwritten words given classifications 4. robot driving problem • task – driving public four-lane highways sight scanners • performance measure – average distance progressed fallacy • experience – order images steering instructions noted observing human driver 5. fruit prediction problem\n",
      "\n",
      "department cse mrcet 7 • task – forecasting different fruits recognition • performance measure – able predict maximum variety fruits • experience – training machine largest datasets fruits images 6. face recognition problem • task – predicting different types faces • performance measure – able predict maximum types faces • experience – training machine maximum datasets different face images 7. automatic translation documents • task – translating type language document language • performance measure – able convert language efficiently • experience – training machine large dataset different types languages design learning system looked learning process understood goal learning. want design learning system follows learning process, need consider design choices. design choices decide following key components 1. type training experience 2. choosing target function 3. choosing representation target function 4. choosing approximation algorithm target function 5. final design look game - checkers learning problem apply design choices. checkers learning problem, elements be, • task t play checkers • performance measure p total present game won tournament. • training experience e set games played itself. type training experience design checker's learning system, type training experience available learning system significant effect success failure learning.\n",
      "\n",
      "department cse mrcet 8 direct indirect training experience case direct training experience, individual board states correct board state given. case indirect training experience, sequences game final result (win, lose draw) given number games. assign credit blame individual moves credit assignment problem. 1. teacher not  supervised training experience labelled, means, board states labelled correct move. learning takes place presence supervisor teacher.  un-supervised training experience unlabelled, means, board states moves. learner generates random games plays supervision teacher involvement.  semi-supervised learner generates game states asks teacher help finding correct board state confusing. 2. training experience good  training examples represent distribution examples final system performance measured? performance best training examples test examples same/a similar distribution.  checker player learns playing oneself. experience indirect. encounter moves common human expert play. proper training experience available, design step choosing target function. choosing target function playing checkers game, moment time, decision choosing best different possibilities. think apply learning gained experience. learning is, specific board, checker board\n",
      "\n",
      "department cse mrcet 9 state tends winning situation. learning defined terms target function. 2 considerations — direct indirect experience. • direct experience checkers learning system, needs learn choose best large search space. need find target function help choose best alternatives. let function choose use notation choose move b →m indicate function accepts input board set legal board states b produces output set legal moves m. • indirect experience difficult learn function. assigning real score board state. function v b →r indicating accepts input board set legal board states b produces output real score. function assigns higher scores better board states system successfully learn target function v, easily use select best board position. let define target value v(b) arbitrary board state b b, follows\n",
      "\n",
      "department cse mrcet 10 1. b final board state won, v(b) = 100 2. b final board state lost, v(b) = -100 3. b final board state drawn, v(b) = 0 4. b final state game, v (b) = v (b’), b’ best final board state achieved starting b playing optimally end game. (4) recursive definition determine value v(b) particular board state, performs search ahead optimal line play, way end game. definition efficiently computable checkers playing program, non- operational definition. choosing representation target function specified ideal target function v, choose representation learning program use describe function ^v learn. earlier design choices, options. could, example, allow program represent large table distinct entry specifying value distinct board state. allow represent collection rules match features board state, quadratic polynomial function predefined board features, artificial neural network. general, choice representation involves crucial trade off. hand, wish pick expressive representation allow representing close approximation possible ideal target function v. hand, expressive representation, training data program require order choose alternative hypotheses represent. discussion brief, let choose simple representation given board state, function ^v calculated linear combination following board features • x1(b) — number black pieces board b • x2(b) — number red pieces b • x3(b) — number black kings b\n",
      "\n",
      "department cse mrcet 11 • x4(b) — number red kings b • x5(b) — number red pieces threatened black • x6(b) — number black pieces threatened red ^v = w0 + w1 · x1(b) + w2 · x2(b) + w3 · x3(b) + w4 · x4(b) +w5 · x5(b) + w6 · x6(b) w0 w6 numerical coefficients weights obtained learning algorithm. weights w1 w6 determine relative importance different board features. specification machine learning problem time till worked choosing type training experience, choosing target function representation. checkers learning task summarized below. • task t play checkers • performance measure % games won world tournament • training experience e opportunity play • target function v board → r • target function representation ^v = w0 + w1 · x1(b) + w2 · x2(b) + w3 · x3(b) + w4 · x4(b) +w5 · x5(b) + w6 · x6(b) items correspond specification learning task, final items constitute design choices implementation learning program. choosing approximation algorithm target function generating training data — train learning program, need set training data, describing specific board state b training value v_train (b) b. training example ordered pair <b,v_train(b)>.\n",
      "\n",
      "department cse mrcet 12 temporal difference (td) learning concept central reinforcement learning, learning happens iterative correction estimated returns accurate target return.  v_train(b) ← ^v(successor(b)) final design checkers learning system final design checkers learning system naturally described distinct program modules represent central components learning systems. 1. performance system takes new board input outputs trace game played itself. 2. critic takes trace game input outputs set training examples target function. 3. generalizer takes training examples input outputs hypothesis estimates target function. good generalization new cases crucial. 4. experiment generator takes current hypothesis (currently learned function) input outputs new problem (an initial board state) performance system explore. issues machine learning\n",
      "\n",
      "department cse mrcet 13 checkers example raises number generic s machine learning. field machine learning, book, concerned answering s following • algorithms exist learning general target functions specific training examples? settings particular algorithms converge desired function, given sufficient training data? algorithms perform best types problems representations? • training data sufficient? general bounds found relate confidence learned hypotheses training experience character learner's hypothesis space? • prior knowledge held learner guide process generalizing examples? prior knowledge helpful approximately correct? • best strategy choosing useful training experience, choice strategy alter complexity learning problem? • best way reduce learning task function approximation problems? way, specific functions system attempt learn? process automated? • learner automatically alter representation improve ability represent learn target function? concept learning • inducing general functions specific training examples main issue machine learning. • concept learning acquiring definition general category given sample positive negative training examples category. • concept learning problem searching predefined space potential hypotheses hypothesis best fits training examples. • hypothesis space general-to-specific ordering hypotheses, search efficiently organized taking advantage naturally occurring structure hypothesis space. formal definition concept learning\n",
      "\n",
      "department cse mrcet 14 inferring boolean-valued function training examples input output. • example concept-learning learning bird-concept given examples birds (positive examples) non-birds (negative examples). • trying learn definition concept given examples. concept learning task enjoy sport training examples set example days, described attributes. task learn predict value enjoy sport arbitrary day, based values attribute values. concept learning search • concept learning viewed task searching large space hypotheses implicitly defined hypothesis representation. • goal search find hypothesis best fits training examples. • selecting hypothesis representation, designer learning algorithm implicitly defines space hypotheses program represent learn.\n",
      "\n",
      "department cse mrcet 15 find-s • find-s algorithm starts specific hypothesis generalize considering positive examples. • find-s algorithm ignores negative example long hypothesis space contains hypothesis describes true target concept, training data contains errors, ignoring negative examples cause problem. • find-s algorithm finds specific hypothesis h consistent positive training examples. – final hypothesis consistent negative examples correct target concept h, training examples correct. find-s algorithm 1. initialize h specific hypothesis h 2. positive training instance x attribute constraint a, h constraint a, satisfied x 3. replace a, h general constraint satisfied x 4. output hypothesis h find-s algorithm – example important-representation 1. ? indicates value acceptable attribute. 2. specify single required value (e.g., cold) attribute. 3. φ indicates value acceptable. 4. general hypothesis represented by {?, ?, ?, ?, ?, ?} 5. specific hypothesis represented by {ϕ, ϕ, ϕ, ϕ, ϕ, ϕ} steps involved find-s 1. start specific hypothesis. h = {ϕ, ϕ, ϕ, ϕ, ϕ, ϕ}\n",
      "\n",
      "department cse mrcet 16 2. example negative, changes occur hypothesis. 3. example positive find initial hypothesis specific update current hypothesis general condition. 4. repeating steps till training examples complete. 5. completed training examples final hypothesis use classify new examples. example consider following data set having data particular seeds poisonous. first, consider hypothesis specific hypothesis. hence, hypothesis be h = {ϕ, ϕ, ϕ, ϕ, ϕ, ϕ} consider example 1 data example 1 {green, hard, no, wrinkled}. initial hypothesis specific generalize example. hence, hypothesis becomes h = {green, hard, no, wrinkled} consider example 2\n",
      "\n",
      "department cse mrcet 17 example negative outcome. neglect example hypothesis remains same. h = {green, hard, no, wrinkled} consider example 3 example negative outcome. neglect example hypothesis remains same. h = {green, hard, no, wrinkled} consider example 4 data present example 4 {orange, hard, no, wrinkled}. compare single attribute initial data mismatch found replace particular attribute general case (“ ?”). process hypothesis becomes h = {?, hard, no, wrinkled } consider example 5 data present example 5 {green, soft, yes, smooth}. compare single attribute initial data mismatch found replace particular attribute general case ( “?” ). process hypothesis becomes h = {?, ?, ?, ? } reached point attributes hypothesis general condition, example 6 example 7 result hypothesizes general attributes. h = {?, ?, ?, ? } hence, given data final hypothesis be final hypothesis h = { ?, ?, ?, ? }. version spaces definition(version space). concept complete covers positive examples. concept consistent covers negative examples. version space set complete consistent concepts. set convex fully defined general elements. candidate-elimination learning algorithm\n",
      "\n",
      "department cse mrcet 18 candidate-elimintion algorithm computes version space containing hypotheses h consistent observed sequence training examples. initialize g set maximally general hypotheses h initialize s set maximally specific hypotheses h training example d, • d positive example • remove g hypothesis inconsistent d • hypothesis s s consistent d • remove s s • add s minimal generalizations h s h consistent d, member g general h • remove s hypothesis general hypothesis s • d negative example • remove s hypothesis inconsistent d • hypothesis g g consistent d • remove g g 18\\ • add g minimal specializations h g • h consistent d, member s specific h • remove g hypothesis general hypothesis g. candidate- elimintion algorithm version spaces illustrative example\n",
      "\n",
      "department cse mrcet 19 candidate-elimintion algorithm begins initializing version space set hypotheses h; boundary set contain general hypothesis h, g0 ?, ?, ?, ?, ?, training example presented, candidateelimintion algorithm checks s boundary finds overly specific fails cover positive example. • boundary revised moving general hypothesis covers new example. • update g boundary needed response training example correctly covers example. • second training example observed, similar effect generalizing s s2, leaving g unchanged i.e., g2 = g1 =g0\n",
      "\n",
      "department cse mrcet 20 • consider training example. negative example reveals boundary version space overly general, is, hypothesis g incorrectly predicts new example positive example. • hypothesis g boundary specialized correctly classifies new negative example. given attributes specified specialize g2, new hypotheses g3? example, hypothesis h = (?, ?, normal, ?, ?, ?) minimal specialization g2 correctly labels new example negative example, included g3. reason hypothesis excluded inconsistent previously encountered positive examples. consider fourth training example.\n",
      "\n",
      "department cse mrcet 21 • positive example generalizes s boundary version space. results removing member g boundary, member fails cover new positive example processing examples, boundary sets s4 g4 delimit version space hypotheses consistent set incrementally observed training examples. • processing examples, boundary sets s4 g4 delimit version space hypotheses consistent set incrementally observed training examples. inductive bias\n",
      "\n",
      "department cse mrcet decision tree decision trees type supervised machine learning (that explain input corresponding output training data) th e data continuously split according certain parameter. tree explained entities, decision nodes leaves. leaves decisions final outcomes. decision nodes data split. decision tree representation example decision tree explained binary tree. let’s want predict person fit given information like age, eating habit, physical activity, etc. decision nodes s like ‘what’s age?’, ‘does exercise?’, ‘does eat lot pizzas’? leaves, outcomes like ‘fit’, ‘unfit’. case binary classification problem (a yes type problem). main types decision trees 1. classification trees (yes/no types) seen example classification tree, outcome variable like ‘fit’ ‘unfit’. decision variable categorical. inductive bias refers restriction2s2 imposed assumptions\n",
      "\n",
      "department cse mrcet 23 decision outcome variable continuous, e.g. number like 123. working know decision tree is, we’ll works internally. algorithms construct decision trees, best called id3 algorithm. id3 stands iterative dichotomiser3. discussing id3 algorithm, we’ll definitions. entropy, called shannon entropy denoted h(s) finite set s, measure uncertainty randomness data. appropriate problems decision tree learning • instances represented attribute-value pair • target function discrete output values • disjunctive descriptions required • training data contain errors • training data contain missing attribute values. • suitable classifications. hypothesis space search set possible decision tree, simple complex, hill climbing search. capability • hypothesis space decision trees complete space finite discrete valued functions. • id3 maintains single current hypothesis. • determine alternative decision trees consistent available training data.\n",
      "\n",
      "department cse mrcet 24 • id3 uses training example step statistically based decisions refine current hypothesis. • resulting search sensitive errors individual training examples. inductive bias decision tree learning note h power set instances x • inductive bias id3 – approximate inductive bias id3  shorter trees preferred larger tress  bfs-id3 difference (id3 & c-e) && restriction bias preference bias id3 candidate-elimination searches complete hypothesis space incompletely searches incomplete hypothesis space completely inductive bias solely consequence ordering hypotheses search strategy inductive bias solely consequence expressive power hypothesis representation sss restriction bias preference bias candidate-elimination id3 categorical restriction set hypotheses considered preference certain hypotheses\n",
      "\n",
      "department cse mrcet 25 possibility excluding unknown target function work complete hypothesis space issues decision tree learning • determine deeply grow decision tree • handling continuous attributes • choosing appropriate attribute selection measure • handling training data missing attribute values • handling attributes differing costs • improving computational efficiency\n",
      "\n",
      "department cse mrcet 26 unit-ii artificial neural networks introduction artificial neural networks (ann) algorithms based brain function model complicated patterns forecast issues. artificial neural network (ann) deep learning method arose concept human brain biological neural networks. development ann result attempt replicate workings human brain. workings ann extremely similar biological neural networks, identical. ann algorithm accepts numeric structured data. ann applications classification, aim predict class input vector • pattern matching, aim produce pattern best associated given input vector. • pattern completion, aim complete missing parts given input vector. • optimization, aim find optimal values parameters optimization problem. • control, appropriate action suggested based given input vectors • function approximation/times series modelling, aim learn functional relationships input desired output vectors. • data mining, aim discovering hidden patterns data (knowledge discovery). ann architectures • neural networks known universal function approximators • architectures available approximate nonlinear function • different architectures allow generation functions different complexity power  feed forward networks  feedback networks  lateral networks\n",
      "\n",
      "department cse mrcet 27 advantages artificial neural networks attribute-value pairs represent problems ann. 1. output anns discrete-valued, real-valued, vector multiple real discrete-valued characteristics, target function discrete-valued, real-valued, vector numerous real discrete- valued attributes. 2. noise training data problem ann learning techniques. mistakes training samples, affect final result. 3. it’s utilized quick assessment taught target function necessary. 4. number weights network. 5. number training instances evaluated, settings different learning algorithm parameters contribute extended training periods anns. disadvantages artificial neural networks 1. hardware dependence • construction artificial neural networks necessitates use parallel processors. • result, equipment’s realization contingent. 2. understanding network’s operation • issue ann. • ann provides probing answer, explain chosen. • result, network’s confidence eroded. 3. assured network structure\n",
      "\n",
      "department cse mrcet 28 • precise rule determine structure artificial neural networks. • experience trial error develop suitable network structure. 4. difficulty presenting issue network • anns capable working numerical data. • introduced ann, problems converted numerical values. • display method chosen direct impact network’s performance. • user’s skill factor here. 5. network’s lifetime unknown • network’s error sample decreased specific amount, training complete. • value produce best outcomes. appropriate problems neural network learning 1. instances represented attribute-value pairs (e.g., pixels picture. alvinn [mitchell, p. 84]). 2. target function output discrete-valued, real-valued, vector real- discrete-valued attributes. 3. training examples contain errors. 4. long training times acceptable. 5. fast evaluation learned target function required. 6. ability humans understand learned target function important. history neural networks 1. 1943 mcculloch pitts proposed model neuron perceptron (read [mitchell, section 4.4]) 2. 1960s widrow hoff explored perceptron networks (which called “adelines”) delta rule. 3. 1962 rosenblatt proved convergence perceptron training rule.\n",
      "\n",
      "department cse mrcet 29 4. 1969 minsky papert showed perceptron deal nonlinearly-separable data sets---even represent simple function x-or. 5. 1970-1985 little research neural nets 6. 1986 invention backpropagation rumelhart mcclelland, parker earlier on werbos learn nonlinearly-separable data sets. 7. 1985 lot research neural nets! multilayer neural network • multiplayer perceptron feed forward neural network hidden layers • network consists input layer source neurons, hidden layer computational neurons, output layer computational neurons. • input signals propagated forward direction layer-by-layer basis. • neurons hidden layer observed input/output behaviour network. • obvious way know desired output hidden layer be.\n",
      "\n",
      "department cse mrcet 30\n",
      "\n",
      "department cse mrcet 31\n",
      "\n",
      "department cse mrcet 32 propagation overview • propagation works applying gradient descent rule feed forward network. • algorithm composed parts repeated pre-set maximal number epochs, ep max. • i, feed forward pass activation values hidden output units computed. • ii, propagation pass weights network updated- starting hidden output weights followed input hidden weights--with respect sum squares error series weight update rules called delta rule. definition propagation algorithm neural network computes gradient loss function single weight chain rule. efficiently computes layer time, unlike native direct computation. computes gradient, define gradient used. generalizes computation delta rule. consider following propagation neural network example diagram understand\n",
      "\n",
      "department cse mrcet 33 • inputs x, arrive preconnected path • input modelled real weights w. weights usually randomly selected. • calculate output neuron input layer, hidden layers, output layer. • calculate error outputs errorb= actual output – desired output • travel output layer hidden layer adjust weights error decreased. • repeating process desired output achieved need propagation? • prominent advantages propagation are • propagation fast, simple easy program • parameters tune apart numbers input • flexible method require prior knowledge network • standard method generally works • need special mention features function learned. 1. inputs x, arrive preconnected path\n",
      "\n",
      "department cse mrcet 34 types propagation networks types propagation networks are • static back-propagation • recurrent propagation static back-propagation kind propagation network produces mapping static input static output. useful solve static classification issues like optical character recognition. recurrent propagation recurrent propagation data mining fed forward fixed value achieved. that, error computed propagated backward. disadvantages propagation • actual performance propagation specific problem dependent input data. • propagation algorithm data mining sensitive noisy data • need use matrix-based approach propagation instead mini-batch. propagation algorithm • initialize weights small random values; create random pool training patterns; set ep, number epochs training 0. • 2. pick training pattern remaining pool patterns propagate forward network. • 3. compute deltas, k output layer. • 4. compute deltas, backward. hidden layer propagating error • update connections • w newji = wjiold + wji w newkj = wkjold + wkj j\n",
      "\n",
      "department cse mrcet 35 • pattern remains pool, step 2. training patterns pool used, set ep = ep+1, ep epmax, create random pool patterns step 2. ep = epmax, stop. propagation momentum • point, propagation disadvantage slow small oscillate widely large. • solve problem, add momentum connection inertia, forcing change direction downhill “force”. • new delta rule wpq(t+1) = - e/ wpq + wpq(t) • p q input hidden, or, hidden output units; t time step epoch; momentum parameter regulates inertia weights.\n",
      "\n",
      "department cse mrcet 36 unit - iii introduction bayesian learning imagine situation friend gives new coin asks fairness coin (or probability observing heads) flipping coin once. fact, aware friend coin biased. general, seen coins fair, expect probability observing heads 0.50.5. absence observations, assert fairness coin past experiences observations coins. suppose allowed flip coin 1010 times order determine fairness coin. observations experiment fall following cases • case 1 observing 55 heads 55 tails. • case 2 observing hh heads 10−h10−h tails, h≠10−hh≠10−h. case 1 observed, certain coin fair coin, decide probability observing heads 0.50.5 confidence. case 2 observed either 1. neglect prior beliefs new data, decide probability observing heads h/10h/10 solely depending recent observations. 2. adjust belief accordingly value hh observed, decide probability observing heads recent observations. method suggests use frequentist method, omit beliefs making decisions. however, second method convenient 1010 coins insufficient determine fairness coin. therefore, better decisions combining recent observations beliefs gained past experiences. thinking model uses recent observations beliefs inclination critical thinking known bayesian thinking.\n",
      "\n",
      "department cse mrcet 37 moreover, assume friend allows conduct 1010 coin flips. use new observations update beliefs. gain data, incrementally update beliefs increasing certainty conclusions. known incremental learning, update knowledge incrementally new evidence. bayesian learning comes play occasions, unable use frequentist statistics drawbacks discussed above. use bayesian learning address drawbacks additional capabilities (such incremental updates posterior) testing hypothesis estimate unknown parameters machine learning models. bayesian learning uses bayes’ theorem determine conditional probability hypotheses given evidence observations. famous coin flip experiment flip coin, possible outcomes - heads tails. course, rare possibility coin balances edge falling side, assume possible outcome coin flip discussion. conduct series coin flips record observations i.e. number heads (or tails) observed certain number coin flips. experiment, trying determine fairness coin, number heads (or tails) observe. frequentist statistics let think determine fairness coin observations mentioned experiment. conducted sufficient number coin flip trials, determine frequency probability observing heads (or tails). observed heads tails equal frequencies probability observing heads (or tails) 0.50.5, established coin fair coin. failing that, biased coin. let's denote pp probability observing heads. consequently, quantity pp deviates 0.50.5 indicates biased coin is, pp considered degree-of-fairness coin.\n",
      "\n",
      "department cse mrcet 38 testing hypothesis true false calculating probability event prolonged experiment known frequentist statistics. such, determining fairness coin probability observing heads example frequentist statistics (a.k.a. frequentist approach). let investigate coin flip example frequentist approach. intentionally altered coin, reasonable assume unbiased coin experiment. flip coin 1010 times, observe heads 66 times. therefore, pp 0.60.6 (note pp number heads observed number total coin flips). hence, according frequencies statistics, coin biased coin — opposes assumption fair coin. friends skeptical extends experiment 100100 trails coin. observes heads 5555 times, results different pp 0.550.55. new value pp change previous conclusion (i.e. coin biased), observation raises s • confident pp 0.60.6? • confident pp 0.550.55? • values accurate estimation pp?\n",
      "\n",
      "39 department cse mrcet pp continue change increase number coin flip trails? find exact answers s frequentist statistics. assume true value pp closer 0.550.55 0.60.6 computed observations considerable number trials compared compute latter. way confirming hypothesis. however, increase number trials, different probability values observing heads eventually, discover coin fair coin. number coin number heads probability observing heads flips 10 6 0.6 50 29 0.58 100 55 0.55 200 94 0.47 500 245 0.49 table 1 - coin flip experiment results increasing number trials table 1 presents possible outcomes hypothetical coin flip experiment increasing number trials. fairness (pp) coin changes increasing number coin-flips experiment. confidence estimated pp increase increasing number coin-flips, frequentist statistic facilitate indication confidence estimated pp value. attempt understand importance confident measure studying following cases • experiment infinite number trials guarantees pp absolute accuracy (100% confidence). yet, practical conduct experiment infinite number trials stop experiment sufficiently large number trials. however, deciding value sufficient number trials challenge frequentist statistics. determine confidence estimated pp value inferred conclusion, situation number trials limited, allow\n",
      "\n",
      "department cse mrcet 40 decide accept conclusion extend experiment trials achieves sufficient confidence. moreover, valuable insights prior beliefs (for example, coins usually fair coin biased intentionally, p≈0.5p≈0.5) describes value pp. embedding information significantly improve accuracy final conclusion. beliefs play significant role shaping outcome hypothesis test especially limited data. however, frequentist statistics, possible incorporate beliefs past experience increase accuracy hypothesis test. terms understand delving bayesian learning, essential understand definition terminologies used. provide lengthy explanations mathematical definition lot widely available content use understand concepts. • random variable (stochastic variable) - statistics, random variable variable possible values result random event. therefore, possible value random variable probability attached represent likelihood values. • probability distribution - function defines probability different outcomes/values random variable. continuous probability distributions described probability density functions discrete probability distributions represented probability mass functions. conditional probability - measure probability p(a|b)p(a|b) event given event b occurred. • joint probability distribution bayes’ theorem bayes’ theorem describes conditional probability event hypothesis computed evidence prior knowledge. similar concluding code bugs given evidence passed\n",
      "\n",
      "department cse mrcet 41 test cases, including prior belief rarely observed bugs code. however, intuition goes simple hypothesis test multiple events hypotheses involved (let worry moment). bayes’ theorem given by p(θ|x)=p(x|θ)p(θ)p(x)p(θ|x)=p(x|θ)p(θ)p(x) explain term bayes’ theorem example. consider hypothesis bugs code. θθ xx denote code bug free passes test cases respectively. • p(θ)p(θ) - prior probability probability hypothesis θθ true applying bayes’ theorem. prior represents beliefs gained past experience, refers common sense outcome bayes’ theorem past observations. example given, prior probability denotes probability observing bugs code. however, time applying bayes’ theorem, decide priors means (otherwise use previous posterior new prior). let assume unlikely find bugs code rarely observed bugs code past. past experience observing fewer bugs code, assign prior p(θ)p(θ) higher probability. however, now, let assume p(θ)=pp(θ) term depends test coverage test cases. know value term proper measurements, order continue discussion let assume p(x|¬θ)=0.5p(x|¬θ)=0.5. accordingly, p(x)=1×p+0.5×(1−p)=0.5(1+p)p(x)=1×p+0.5×(1−p)=0.5(1+p) • p(θ|x)p(θ|x) - posteriori probability denotes conditional probability hypothesis θθ observing evidence xx. probability observing bugs code given passes test cases.\n",
      "\n",
      "department cse mrcet 42 know values terms bayes’ theorem, calculate posterior probability following formula p(θ|x)=1×p0.5(1+p)p(θ|x)=1×p0.5(1+p) calculate probability observing bug, given code passes test cases p(¬θ|x)p(¬θ|x) . p(¬θ|x)=p(x|¬θ).p(¬θ)p(x)=0.5×(1−p)0.5×(1+p)=(1−p)(1+p)p(¬θ|x)=p(x|¬ θ).p(¬θ) p(x)=0.5×(1−p)0.5×(1+p)=(1−p)(1+p) know conditional probabilities observing bug code observing bug code. going confirm valid hypothesis posterior probabilities? maximum posteriori (map) use map determine valid hypothesis set hypotheses. according map, hypothesis maximum posterior probability considered valid hypothesis. therefore, express hypothesis θmapθmap concluded map follows θmap=argmaxθp(θi|x)=argmaxθ(p(x|θi)p(θi)p(x))θmap=argmaxθp(θi|x) =argmaxθ(p(x|θ i)p(θi)p(x)) argmaxθargmaxθ operator estimates event hypothesis θiθi maximizes posterior probability p(θi|x)p(θi|x). let apply map example order determine true hypothesis θmap=argmaxθ{θp(θ|x)=p0.5(1+p),¬θp(¬θ|x)=(1−p)(1+p)}θmap=argma xθ{θp(θ|x)=p0.5(1+p),¬θp(¬θ|x)=(1−p)(1+p)}\n",
      "\n",
      "department cse mrcet 43 figure 1 - p(θ|x)p(θ|x) p(¬θ|x)p(¬θ|x) changing p(θ)=pp(θ)=p figure 1 illustrates posterior probabilities possible hypotheses change value prior probability. unlike frequentist statistics belief past experience influence concluded hypothesis, bayesian learning capable incorporating belief improve accuracy predictions. assuming fairly good programmers probability observing bug p(θ)=0.4p(θ)=0.4 , find θmapθmap map=argmaxθ{θp(|x)=0.40.5(1+0.4),¬θp(¬θ|x)=0.5(1−0.4)0.5(1+0.4)}=ar gmaxθ{θp(θ|x)=0.57,¬θp(¬θ|x)=0.43}=θ⟹no bugs present codemap=argmaxθ{θp(|x)=0.40.5(1+0.4),¬θp(¬θ|x)=0.5(1−0.4)0.5(1+0.4 )}=argmaxθ{θp(θ|x)=0.57,¬θp(¬θ|x)=0.43}=θ⟹no bugs present code\n",
      "\n",
      "department cse mrcet 44 however, p(x)p(x) independent θθ, p(x)p(x) events hypotheses. therefore, simplify θmapθmap estimation, denominator posterior computation shown below θmap=argmaxθ(p(x|θi)p(θi))θmap=argmaxθ(p(x|θi)p(θi)) notice map estimation algorithms compute posterior probability hypothesis decide probable hypothesis. assuming hypothesis space continuous (i.e. fairness coin encoded probability observing heads, coefficient regression model, etc.), endless possible hypotheses present smallest range human mind think of, discrete hypothesis space large number possible outcomes event, need find posterior hypothesis order decide probable hypothesis. therefore, practical implementation map estimation algorithms use approximation techniques, capable finding probable hypothesis computing posteriors computing them. bayesian theorem, incorporate belief prior probability, possible frequentist statistics. however, problem deciding sufficiently large number trials attaching confidence concluded hypothesis. example solely designed introduce bayesian theorem terms. let gain better understanding bayesian learning learn potential bayes’ theorem. binomial likelihood likelihood coin flip experiment given probability observing heads coin flips given fairness coin. defined fairness coins (θθ) probability observing heads coin flip, define probability observing heads\n",
      "\n",
      "department cse mrcet 45 tails given fairness coin p(y|θ)p(y|θ) y=1y=1 observing heads y=0y=0 observing tails. accordingly p(y=1|θ)=θp(y=0|θ)=(1−θ)p(y=1|θ)=θp(y=0|θ)=(1−θ) defined conditional probabilities outcome above, let try find p(y=y|θ)p(y=y|θ) joint probability observing heads tails p(y=y|θ)={θ, y=11−θ, p(y=y|θ)={θ, y=11−θ, note yy 00 11, θθ lie range [0,1][0,1]. rewrite expression single expression follows p(y=y|θ)=θy×(1−θ)1−yp(y=y|θ)=θy×(1−θ)1−y equation represents likelihood single test coin flip experiment. interestingly, likelihood function single coin flip experiment similar bernoulli probability distribution. bernoulli distribution probability distribution single trial experiment opposite outcomes. bernoulli probability distribution simplification binomial probability distribution single trail, represent likelihood coin flip experiment observe kk number heads nn number trials binomial probability distribution shown below p(k,n|θ)=(nk)θk(1−θ)n−k\n",
      "\n",
      "department cse mrcet 46 maximum likelihood estimation method (mle) likelihood function indicates likely observed sample function possible parameter values. therefore, maximizing likelihood function determines parameters likely produce observed data. statistical point view, mle usually recommended large samples versatile, applicable models different types data, produces precise estimates. squares estimation method (lse) squares estimates calculated fitting regression line points data set minimal sum deviations squared (least square error). reliability analysis, line data plotted probability plot. bayes optimal classifier bayes optimal classifier probabilistic model makes probable prediction new example, given training dataset. model referred bayes optimal learner, bayes classifier, bayes optimal decision boundary, bayes optimal discriminant function. gibbs sampling algorithm start selecting initial value random variables x & y. then, sample conditional probability distribution x given y = y⁰ denoted p(x|y⁰). step, sample new value y conditional x¹, computed. repeat procedure additional n - 1 iterations, alternating drawing new sample conditional probability distribution x conditional probability distribution y, given current value random variable.\n",
      "\n",
      "department cse mrcet 47 let’s look example. suppose following posterior conditional probability distributions. naive bayes classifier algorithm • naïve bayes algorithm supervised learning algorithm, based bayes theorem solving classification problems.\n",
      "\n",
      "department cse mrcet 48 • mainly text classification includes high-dimensional training dataset. • naïve bayes classifier simple effective classification algorithms helps building fast machine learning models quick predictions. • probabilistic classifier, means predicts basis probability object. • popular examples naïve bayes algorithm spam filtration, sentimental analysis, classifying articles. example suppose dataset weather conditions corresponding target variable \"play\". dataset need decide play particular day according weather conditions. solve problem, need follow steps 1. convert given dataset frequency tables. 2. generate likelihood table finding probabilities given features. 3. now, use bayes theorem calculate posterior probability. problem weather sunny, player play not? solution solve this, consider dataset outlook play 0 rainy yes 1 sunny yes 2 overcast yes 3 overcast yes 4 sunny 5 rainy yes 6 sunny yes\n",
      "\n",
      "department cse mrcet frequency table weather conditions likelihood table weather condition weather yes overcast 0 5 5/14= 0.35 rainy 2 2 4/14=0.29 sunny 2 3 5/14=0.35 4/14=0.29 10/14=0.71 applying bayes'theorem p(yes|sunny)= p(sunny|yes)*p(yes)/p(sunny) 49 7 overcast yes 8 rainy 9 sunny 10 sunny yes 11 rainy 12 overcast yes 13 overcast yes weather yes overcast 5 0 rainy 2 2 sunny 3 2 total 10 5\n",
      "\n",
      "department cse mrcet 50 p(sunny|yes)= 3/10= 0.3 p(sunny)= 0.35 p(yes)=0.71 p(yes|sunny) = 0.3*0.71/0.35= 0.60 p(no|sunny)= p(sunny|no)*p(no)/p(sunny) p(sunny|no)= 2/4=0.5 p(no)= 0.29 p(sunny)= 0.35 p(no|sunny)= 0.5*0.29/0.35 = 0.41 bayesian belief network graphical representation different probabilistic relationships random variables particular set. classifier dependency attributes i.e condition independent. feature joint probability, probability bayesian belief network derived, based condition — p(attribute/parent) i.e probability attribute, true parent attribute. consider example • figure, alarm ‘a’ – node, installed house person ‘gfg’, rings probabilities i.e burglary ‘b’ fire\n",
      "\n",
      "department cse mrcet 51 ‘f’, – parent nodes alarm node. alarm parent node probabilities p1 calls ‘p1’ & p2 calls ‘p2’ person nodes. • instance burglary fire, ‘p1’ ‘p2’ person ‘gfg’, respectively. but, drawbacks case, ‘p1’ forget person ‘gfg’, hearing alarm, tendency forget things, quick. similarly, ‘p2’, fails person ‘gfg’, able hear alarm, certain distance. expectation-maximization algorithm real-world applications machine learning, common relevant features available learning small subset observable. so, variables observable not, use instances variable visible observed purpose learning predict value instances observable. hand, expectation-maximization algorithm latent variables (variables directly observable actually inferred values observed variables) order predict values condition general form probability distribution governing latent variables known us. algorithm actually base unsupervised clustering algorithms field machine learning. explained, proposed given paper published 1977 arthur dempster, nan laird, donald rubin. find local maximum likelihood parameters statistical model cases latent variables involved data missing incomplete. algorithm 1. given set incomplete data, consider set starting parameters. 2. expectation step (e – step) observed available data dataset, estimate (guess) values missing data. 3. maximization step (m – step) complete data generated expectation (e) step order update parameters. 4. repeat step 2 step 3 convergence.\n",
      "\n",
      "department cse mrcet 52 essence expectation-maximization algorithm use available observed data dataset estimate missing data data update values parameters. let understand em algorithm detail. • initially, set initial values parameters considered. set incomplete observed data given system assumption observed data comes specific model. • step known “expectation” – step e-step. step, use observed data order estimate guess values missing incomplete data. basically update variables. • step known “maximization”-step m-step. step, use complete data generated preceding “expectation” – step order update values parameters. basically update hypothesis. • now, fourth step, checked values converging not, yes, stop repeat step-2 step-3 i.e. “expectation” – step “maximization” – step convergence occurs. flow chart em algorithm\n",
      "\n",
      "department cse mrcet 53 usage em algorithm • fill missing data sample. • basis unsupervised learning clusters. • purpose estimating parameters hidden markov model (hmm). • discovering values latent variables. advantages em algorithm • guaranteed likelihood increase iteration. • e-step m-step pretty easy problems terms implementation. • solutions m-steps exist closed form.\n",
      "\n",
      "department cse mrcet 54 instance-based learning machine learning systems categorized instance-based learning systems learn training examples heart generalizes new instances based similarity measure. called instance-based builds hypotheses training instances. known memory-based learning lazy-learning. time complexity algorithm depends size training data. worst-case time complexity algorithm o (n), n number training instances. example, create spam filter instance-based learning algorithm, instead flagging emails marked spam emails, spam filter programmed flag emails similar them. requires measure resemblance emails. similarity measure emails sender repetitive use keywords else. advantages 1. instead estimating entire instance set, local approximations target function. 2. algorithm adapt new data easily, collected go. disadvantages 1. classification costs high 2. large memory required store data, query involves starting identification local model scratch. instance-based learning algorithms 1. k nearest neighbor (knn) 2. self-organizing map (som) 3. learning vector quantization (lvq) 4. locally weighted learning (lwl)\n",
      "\n",
      "department cse mrcet 55 k-nearest neighbor(knn) algorithm • k-nearest neighbour simplest machine learning algorithms based supervised learning technique. • k-nn algorithm assumes similarity new case/data available cases new case category similar available categories. • k-nn algorithm stores available data classifies new data point based similarity. means new data appears easily classified suite category k- nn algorithm. • k-nn algorithm regression classification classification problems. • k-nn non-parametric algorithm, means assumption underlying data. • called lazy learner algorithm learn training set immediately instead stores dataset time classification, performs action dataset. • knn algorithm training phase stores dataset gets new data, classifies data category similar new data. working knn algorithm k-nearest neighbours (knn) algorithm uses ‘feature similarity’ predict values new data points means new data point assigned value based closely matches points training set. understand working help following steps − step 1 − implementing algorithm, need dataset. step knn, load training test data. step 2 − next, need choose value k i.e. nearest data points. k integer. step 3 − point test data following\n",
      "\n",
      "department cse mrcet 56 • 3.1 − calculate distance test data row training data help method namely euclidean, manhattan hamming distance. commonly method calculate distance euclidean. • 3.2 − now, based distance value, sort ascending order. • 3.3 − next, choose k rows sorted array. • 3.4 − now, assign class test point based frequent class rows. step 4 – end example case based reasoning know nearest neighbour classifiers stores training tuples points euclidean space. case-based reasoning classifiers (cbr) use database problem solutions solve new problems. stores tuples cases problem-solving complex symbolic descriptions. cbr works? new case arrises classify, case-based reasoner(cbr) check identical training case exists. found, accompanying solution case returned. identical case found, cbr search training cases having components similar new case. conceptually, training cases considered neighbours new case. cases represented graphs, involves searching subgraphs similar subgraphs new case. cbr tries combine solutions neighbouring training cases propose solution new case. compatibilities arise individual solutions, backtracking search solutions\n",
      "\n",
      "department cse mrcet 57 necessary. cbr employ background knowledge problem-solving strategies propose feasible solution. applications cbr includes 1. problem resolution customer service help desks, cases describe product-related diagnostic problems. 2. applied areas engineering law, cases technical designs legal rulings, respectively. 3. medical educations, patient case histories treatments help diagnose treat new patients. challenges cbr • finding good similarity metric (eg matching subgraphs) suitable methods combining solutions. • selecting salient features indexing training cases development efficient indexing techniques. cbr intelligent number trade-off accuracy efficiency evolves number stored cases large. certain point, system’s efficiency suffer time required search process relevant cases increases. differences eager lazy learning • eager learning methods construct general, explicit description target function based provided training examples. • lazy learning methods simply store data generalizing data postponed explicit request made. • lazy learning methods construct different approximation target function encountered query instance. lazy learning suitable complex incomplete problem domains, complex target function represented collection complex local approximations. eager learning methods use approximation target function, learned based training examples input queries observed.\n",
      "\n",
      "department cse mrcet 58 unit - iv pattern comparison techniques pattern recognition process finding regularities similarities data machine learning data. now, similarities found based statistical analysis, historical data, gained knowledge machine itself. pattern regularity world abstract notions. discuss sports, description type pattern. person keeps watching videos related cricket, youtube wouldn’t recommend chess tutorials videos. examples speech recognition, speaker identification, multimedia document recognition (mdr), automatic medical diagnosis. searching pattern certain steps collect data real world. collected data needs filtered preprocessed system extract features data. based type data system choose appropriate algorithm classification, regression, regression recognize pattern. • classification. classification, algorithm assigns labels data based predefined features. example supervised learning. • clustering. algorithm splits data number clusters based similarity features. example unsupervised learning. • regression. regression algorithms try find relationship variables predict unknown dependent variables based known data. based supervised learning. [2] • features represented continuous, discrete, discrete binary variables. feature basically function measurements, computed quantify significant characteristics object. feature important components pattern recognition system. example consider football, shape, size color, etc. features football.\n",
      "\n",
      "department cse mrcet 59 feature vector set features taken together. example example football, features (shape, size, color etc.) taken sequence feature vector ([shape, size, color]). feature vector sequence features represented n-dimensional column vector. case speech, mfcc (mel-frequency cepstral coefficient) spectral features speech. sequence 13 features forms feature vector. temporal patterns temporal patterns pattern comparison techniques defined segment signals recurs frequently temporal signal sequence. example, temporal signal sequences movements head, hand, body, piece music, on. temporal abstraction data mining research fields tried synthesis time oriented data bring understanding hidden relationships exist time oriented events. clinical settings, having ability know hidden relationships patient data unfold help save life aiding detection conditions obvious clinicians healthcare workers. understanding hidden patterns huge challenge exponential search space unique time-series data. paper, propose temporal pattern recognition model based dimension reduction similarity measures maintaining temporal nature raw data introduction temporal pattern processing important intelligent behaviours, including hearing, vision, speech, music motor control. live ever-changing environment, intelligent system, human robot, encode patterns time, recognize generate temporal patterns. time embodied temporal pattern different ways • temporal order. refers ordering components sequence. example, sequence n-e-t different t-e-n. temporal order\n",
      "\n",
      "department cse mrcet 60 refer syntactic structure, subject-verb-object, component category possible symbols • time duration. duration play critical role temporal processing. speech recognition, example, want rate invariance distinguishing relative durations vowel /i/ (as beet) /i/ (as bit) temporal pattern recognition shared goal stm models input history available simultaneously recognition takes place. stm model place, recognition different recognition static patterns. template matching hebbian learning architecture type recognition simply two-layer network input layer incorporates stm, sequence recognition layer unit encodes individual sequence. recognition scheme essentially template matching, templates formed following hebbian learning wij(t) = wij(t–1) + c si (t)[xj (t) – wij(t–1)] wij connection weight unit xj input layer sequence recognizer si recognition layer. parameter c controls learning rate. hebbian learning applied presentation entire sequence completed. templates formed recognize specific input sequences. recognition layer typically includes recurrent connections selecting winner self-organization (e.g. winner-take-all) training recognition. associative memory approach dynamics hopfield associative memory model characterized evolving memory state similar current input pattern.\n",
      "\n",
      "department cse mrcet 61 views memory state category, hopfield net performs pattern recognition recalled category recognized pattern. process dynamic evolution viewed optimization process, minimizes cost function equilibrium reached. normalized exponential kernel stm, tank hopfield (1987) described recognition network based associative memory dynamics. layer sequence recognizers receives inputs stm model. recognizer encodes different template sequence unique weight vector acting inputs stm. addition, recognizers form competitive network. recognition process uses current input sequence (evidence) bias minimization process similar template wins competition, activating corresponding recognizer. exponential kernels, demonstrated recognition fairly robust time warping, distortions duration. similar architecture later applied speakerindependent spoken digit recognition. multilayer perceptrons popular approach temporal pattern learning multilayer perceptrons (mlp). mlps demonstrated effective static pattern recognition. natural combine mlp stm model temporal pattern recognition. example, delay line stm waibel et al. (1989) reported architecture called time delay neural networks (tdnn) spoken phoneme recognition. input layer, tdnn uses 2 hidden layers output layer unit encodes phoneme. feed forward connections converge input layer successive layer unit specific layer receives inputs limited time window previous layer. demonstrated good recognition performance stop consonants /b/, /d/, /g/, accuracy speaker dependent recognition reached 98.5%. dynamic time warping sounds like time traveling kind future technic, however, not. dynamic time warping compare similarity calculate\n",
      "\n",
      "department cse mrcet 62 distance arrays time series different length. suppose want calculate distance equal-length arrays = [1, 2, 3] b = [3, 2, 2] that? obvious way match b 1-to-1 fashion sum total distance component. sounds easy, b different lengths? = [1, 2, 3] b = [2, 2, 2, 3, 4] match up? map which? solve problem, comes dynamic time warping. indicates, warp series match up. use cases digging algorithm, useful? need compare distance unequal-length time series? yes, lot scenarios dtw playing key role. sound pattern recognition use case detect sound pattern kind. suppose want recognise voice person analysing sound track, able collect sound track saying hello scenario. however, people speak word different ways, speaks hello slower pace like heeeeeeelloooooo , need algorithm match sound track different lengths able identify come person.\n",
      "\n",
      "department cse mrcet 63 stock market stock market, people hope able predict future, general machine learning algorithms exhaustive, prediction task requires test training set dimension features. however, speculate stock market, know pattern stock different length reflection klines indicators.\n",
      "\n",
      "department cse mrcet 64 time series analysis, dynamic time warping (dtw) algorithms measuring similarity temporal sequences, vary speed. dtw applied temporal sequences video, audio, graphics data — indeed, data turned linear sequence analysed dtw. idea compare arrays different length build one-to-many many-to-one matches total distance minimised two. suppose different arrays red blue different length\n",
      "\n",
      "department cse mrcet 65 clearly series follow pattern, blue curve longer red. apply one-to-one match, shown top, mapping perfectly synced tail blue curve left out. dtw overcomes issue developing one-to-many match troughs peaks pattern perfectly matched, left curves(shown top).\n",
      "\n",
      "department cse mrcet 66 l > k rules general, dtw method calculates optimal match given sequences (e.g. time series) certain restriction rules(comes wiki) • index sequence matched indices sequence vice versa • index sequence matched index sequence (but match) • index sequence matched index sequence (but match) • mapping indices sequence indices sequence monotonically increasing, vice versa, i.e. sequence, indices indices sequence, index matched index l index j matched index k , vice versa. optimal match denoted match satisfies restrictions rules minimal cost, cost computed sum absolute differences, matched pair indices, values. j >\n",
      "\n",
      "department cse mrcet 67 introduction clustering basically type unsupervised learning method. unsupervised learning method method draw references datasets consisting input data labelled responses. generally, process find meaningful structure, explanatory underlying processes, generative features, groupings inherent set examples. clustering task dividing population data points number groups data points groups similar data points group dissimilar data points groups. basically collection objects basis similarity dissimilarity them. ex– data points graph clustered classified single group. distinguish clusters, identify 3 clusters picture. necessary clusters spherical. as\n",
      "\n",
      "department cse mrcet 68 dbscan density-based spatial clustering applications noise data points clustered basic concept data point lies given constraint cluster center. distance methods techniques calculation outliers. clustering? clustering important determines intrinsic grouping unlabelled data present. criteria good clustering. depends user, criteria use satisfy need. instance, interested finding representatives homogeneous groups (data reduction), finding “natural clusters” describe unknown properties (“natural” data types), finding useful suitable groupings (“useful” data classes) finding unusual data objects (outlier detection). algorithm assumptions constitute similarity points assumption different equally valid clusters. clustering methods • density-based methods methods consider clusters dense region having similarities differences lower dense region space. methods good accuracy ability merge clusters. example dbscan (density-based spatial clustering applications noise), optics (ordering points identify clustering structure), etc. • hierarchical based methods clusters formed method form treetype structure based hierarchy. new clusters formed previously formed one. divided category • agglomerative (bottom-up approach) • divisive (top-down approach)\n",
      "\n",
      "department cse mrcet 69 examples cure (clustering representatives), birch (balanced iterative reducing clustering hierarchies), etc. • partitioning methods methods partition objects k clusters partition forms cluster. method optimize objective criterion similarity function distance major parameter example k-means, clarans (clustering large applications based randomized search), etc. • grid-based methods method, data space formulated finite number cells form grid-like structure. clustering operations grids fast independent number data objects example sting (statistical information grid), wave cluster, clique (clustering quest), etc. k means clustering simplest unsupervised learning algorithm solves clustering problem.k-means algorithm partitions n observations k clusters observation belongs cluster nearest mean serving prototype cluster. applications clustering different fields • marketing characterize & discover customer segments marketing purposes. • biology classification different species plants animals. • libraries clustering different books basis topics information. • insurance acknowledge customers, policies identifying frauds. • city planning groups houses study values based geographical locations factors present.\n",
      "\n",
      "department cse mrcet 70 • earthquake studies learning earthquake-affected areas determine dangerous zones. algorithm categorize items k groups similarity. calculate similarity, use euclidean distance measurement. algorithm works follows 1. first, initialize k points, called means, randomly. 2. categorize item closest mean update mean’s coordinates, averages items categorized mean far. 3. repeat process given number iterations end, clusters. “points” mentioned called means hold mean values items categorized them. initialize means, lot options. intuitive method initialize means random items data set. method initialize means random values boundaries data set (if feature x items values [0,3], initialize means values x [0,3]). algorithm pseudocode k-mode clustering kmodes clustering unsupervised machine learning algorithms cluster categorical variables. kmodes algorithm work? 1. pick k observations random use leaders/clusters 2. calculate dissimilarities assign observation closest cluster 3. define new modes clusters\n",
      "\n",
      "department cse mrcet 71 4. repeat 2–3 steps re-assignment required example imagine dataset information hair color, eye color, skin color persons. aim group based available information(maybe want suggest styling ideas) hair color, eye color, skin color categorical variables. dataset looks like. alright, sample data now. let proceed defining number clusters(k)=3 step 1 pick k observations random use leaders/clusters choosing p1, p7, p8 leaders/clusters step 2 calculate dissimilarities(no. mismatches) assign observation closest cluster iteratively compare cluster data points observations. similar data points 0, dissimilar data points 1.\n",
      "\n",
      "department cse mrcet 72 comparing leader/cluster p1 observation p1 gives 0 dissimilarities comparing leader/cluster p1 observation p2 gives 3(1+1+1) dissimilarities. likewise, calculate dissimilarities matrix shown assign observations closest cluster (cluster dissimilarity)\n",
      "\n",
      "department cse mrcet 73 step 2, observations p1, p2, p5 assigned cluster 1; p3, p7 assigned cluster 2; p4, p6, p8 assigned cluster 3. step 3 define new modes clusters mode simply observed value. mark observations according cluster belong to. observations cluster 1 marked yellow, cluster 2 marked brick red, cluster 3 marked purple. considering cluster time, feature, look mode update new leaders. explanation cluster 1 observations(p1, p2, p5) brunette observed hair color, amber observed eye color, fair observed skin color. new leaders update. repeat steps 2–4 obtaining new leaders, calculate dissimilarities observations newly obtained leaders.\n",
      "\n",
      "department cse mrcet 74 comparing cluster 1 observation p1 gives 1 dissimilarity. comparing cluster 1 observation p2 gives 2 dissimilarities. likewise, calculate dissimilarities matrix. assign observation closest cluster.\n",
      "\n",
      "department cse mrcet 75 observations p1, p2, p5 assigned cluster 1; p3, p7 assigned cluster 2; p4, p6, p8 assigned cluster 3. stop change assignment observations. implementation kmodes python begin importing necessary libraries\n",
      "\n",
      "department cse mrcet 76 vector quantization learning vector quantization ( lvq ) type artificial neural network inspired biological models neural systems. based prototype supervised learning classification algorithm trained network competitive learning algorithm similar self organizing map. deal multiclass classification problem. lvq layers, input layer output layer. architecture learning vector quantization number classes input data n number input features sample given below\n",
      "\n",
      "department cse mrcet 77 let input data size ( m, n ) m number training example n number features example label vector size ( m, 1 ). first, initializes weights size ( n, c ) c number training samples different labels discarded training samples. here, c number classes. iterate remaining input data, training example, updates winning vector ( weight vector shortest distance ( e.g euclidean distance ) training example ). weight updation rule given wij = wij(old) - alpha(t) * (x k - wij(old)) alpha learning rate time t, j denotes winning vector, denotes ith feature training example k denotes kth training example input data. training lvq network, trained weights classifying new examples. new example labeled class winning vector. algorithm steps involved • weight initialization • 1 n number epochs • select training example • compute winning vector • update winning vector • repeat steps 3, 4, 5 training example. • classify test sample\n",
      "\n",
      "department cse mrcet genetic algorithms unit- v genetic algorithms(gas) adaptive heuristic search algorithms belong larger evolutionary algorithms. genetic algorithms based ideas natural selection genetics. intelligent exploitation random search provided historical data direct search region better performance solution space. commonly generate high-quality solutions optimization problems search problems. genetic algorithms simulate process natural selection means species adapt changes environment able survive reproduce generation. simple words, simulate “survival fittest” individual consecutive generation solving problem. generation consist population individuals individual represents point search space possible solution. individual represented string character/integer/float/bits. string analogous chromosome. different search methods induction field machine learning, induction algorithm represents example mathematical principles development sophisticated computing systems. machine learning systems simple “rote input/output” function, evolve results supply continued use. induction algorithms help real-time handling sophisticated data sets, long-term efforts. induction algorithm applies systems complex results depending set for. fundamental ways engineers use induction algorithm enhance knowledge acquisition given system. words, algorithm place, set “knowledge data” end users improved, that’s quantity data, filtering noise undesirable results, refinement data points. machine learning r20d5803\n",
      "\n",
      "department cse mrcet 79 technical descriptions induction algorithms largely territory mathematical scientific journals, basic ideas induction algorithm organize “classification rules” according induction principle separate corollary results different kinds system noise exceptions. filtering noise domain prominent use induction algorithm general. idea real-world data filtering, induction algorithms compose different sets rules legitimate results system noise, order distinguish other. setting induction algorithms according certain training examples, stakeholders looking ability systems identify assess consistent rules data represents exceptions rules. sense, use induction algorithm uses induction principle “prove” certain results aid knowledge, provide marked delineations data set (or multiple data sets) – distinctions drive sorts end user capabilities. like kinds machine learning software, induction algorithms thought form “decision support.” “we consider principal task real-world induction system assisting expert expressing expertise,” write authors turing institute paper induction machine learning 1980s. “consequently, require induced rules highly predictive easily comprehensible expert.” mind, induction algorithms kinds software products seek refine data produce evolving results human users. general, machine learning use visual dashboards generating new tools users rapidly develop in-depth knowledge given system, it's related marine research, medical diagnosis, e-commerce, kind data-rich system. explanation-based learning (ebl)\n",
      "\n",
      "department cse mrcet 80 simple terms, ability gain basic problem-solving techniques observing analysing solutions specific problems. terms machine learning, algorithm aims understand example particular concept generalizations form concepts training examples. example, ebl uses domain theory creates program learns play chess. ebl involves 2 steps 1. explanation — domain theory eliminate unimportant training example retaining important ones best describe goal concept. 2. generalization — explanation goal concept general widely applicable possible. ensures cases covered, certain specific ones. ebl architecture • ebl model training • training, model generalizes training example way scenarios lead goal concept, specific cases. (as shown fig 1)\n",
      "\n",
      "department cse mrcet 81 • ebl model training • post training, ebl model tends directly reach hypothesis space involving goal concept. (as shown fig 2)\n",
      "\n",
      "department cse mrcet 82 dimensionality reduction intuitive example dimensionality reduction discussed simple e-mail classification problem, need classify e-mail spam not. involve large number features, e-mail generic title, content e- mail, e-mail uses template, etc. however, features overlap. condition, classification problem relies humidity rainfall collapsed underlying feature, aforementioned correlated high degree. hence, reduce number features problems. 3d classification problem hard visualize, 2-d mapped simple 2 dimensional space, 1-d problem simple line. figure illustrates concept, 3-d feature space split 1-d feature spaces, later, found correlated, number features reduced further.\n",
      "\n",
      "department cse mrcet 83 components dimensionality reduction components dimensionality reduction • feature selection this, try find subset original set variables, features, smaller subset model problem. usually involves ways 1. filter 2. wrapper 3. embedded • feature extraction reduces data high dimensional space lower dimension space, i.e. space lesser no. dimensions. methods dimensionality reduction methods dimensionality reduction include • principal component analysis (pca) • linear discriminant analysis (lda) • generalized discriminant analysis (gda) dimensionality reduction linear non-linear, depending method used. prime linear method, called principal component analysis, pca, discussed below.\n",
      "\n",
      "department cse mrcet 84 principal component analysis method introduced karl pearson. works condition data higher dimensional space mapped data lower dimension space, variance data lower dimensional space maximum. involves following steps • construct covariance matrix data. • compute eigenvectors matrix. • eigenvectors corresponding largest eigenvalues reconstruct large fraction variance original data. hence, left lesser number eigenvectors, data loss process. but, important variances retained remaining eigenvectors. advantages dimensionality reduction • helps data compression, reduced storage space. • reduces computation time. • helps remove redundant features, any. disadvantages dimensionality reduction • lead data loss. • pca tends find linear correlations variables, undesirable. • pca fails cases mean covariance define datasets.\n",
      "\n",
      "department cse mrcet 85 • know principal components keep- practice, thumb rules applied. factor analysis. factor analysis statistical method describe variability observed, correlated variables terms potentially lower number observed variables called factors. example, possible variations observed variables mainly reflect variations unobserved (underlying) variables. factor analysis searches joint variations response unobserved latent variables. observed variables modelled linear combinations potential factors plus \"error\" terms, factor analysis thought special case errors-invariables models. here,there party going room people. ‘n’ number speakers room speaking simultaneously party. room, ‘n’ number microphones placed different\n",
      "\n",
      "department cse mrcet 86 distances speakers recording ‘n’ speakers’ voice signals. hence, number speakers equal number microphones room. now, microphones’ recordings, want separate ‘n’ speakers’ voice signals room given microphone recorded voice signals coming speaker different intensity difference distances them. decomposing mixed signal microphone’s recording independent source’s speech signal machine learning technique, independent component analysis. [ x1, x2, ….., xn ] => [ y1, y2, ….., yn ] where, x1, x2, …, xn original signals present mixed signal y1, y2, …, yn new features independent components independent other. restrictions ica 1. independent components generated ica assumed statistically independent other. 2. independent components generated ica non-gaussian distribution. 3. number independent components generated ica equal number observed mixtures. multidimensional scaling multidimensional scaling visual representation distances dissimilarities sets objects. “objects” colors, faces, map coordinates, political persuasion, kind real conceptual stimuli (kruskal wish, 1978). objects similar (or shorter distances) closer graph objects similar (or longer distances). interpreting dissimilarities distances\n",
      "\n",
      "department cse mrcet 87 graph, mds serve dimension reduction technique high- dimensional data (buja et. al, 2007). term scaling comes psychometrics, abstract concepts (“objects”) assigned numbers according rule (trochim, 2006). example, want quantify person’s attitude global warming. assign “1” “doesn’t believe global warming”, 10 “firmly believes global warming” scale 2 9 attitudes between. think “scaling” fact you’re essentially scaling data (i.e. making simpler creating lower-dimensional data). data scaled dimension keeps similar properties. example, data points close high-dimensional space close low- dimensional space (martinez, 2005). “multidimensional” fact aren’t limited dimensional graphs data. three- dimensional, four-dimensional higher plots possible. mds wide variety disciplines. it’s use isn’t limited specific matrix set data; fact, matrix analyzed technique long matrix contains type relational data (young, 2013). examples relational data include correlations, distances, multiple rating scales similarities. manifold learning manifold? two-dimensional manifold 2-d shape fit higher dimensional space twisting bending it, loosely speaking.\n",
      "\n",
      "department cse mrcet 88 manifold hypothesis? “the manifold hypothesis states real-world high-dimensional data lie low dimensional manifolds embedded high-dimensional space.” simpler terms, means higher-dimensional data time lies closer lower-dimensional manifold. process modelling manifold training instances lie called manifold learning. locally linear embedding (lle) locally linear embedding (lle) manifold learning technique non-linear dimensionality reduction. unsupervised learning algorithm produces low-dimensional embeddings high-dimensional inputs, relating training instance closest neighbor. lle work? training instance x(i), algorithm finds k nearest neighbors tries express x(i) linear function them. general, m training instances total, tries find set weights w minimizes squared distance x(i) linear representation. so, cost function given wi,j =0, j included k closest neighbors i. also, normalizes weights training instance x(i),\n",
      "\n",
      "department cse mrcet 89 finally, high-dimensional training instance x(i) mapped low- dimensional (say, d dimensions) vector y(i) preserving neighborhood relationships. choosing d-dimensional coordinates minimize cost function, weights wi,j kept fixed try find optimum coordinates y(i)\n",
      "\n",
      "introduction machine learning early draft proposed textbook nils j. nilsson robotics laboratory department computer science stanford university stanford, 94305 e-mail nilsson@cs.stanford.edu november 3, 1998 copyright c⃝2005 nils j. nilsson material copied, reproduced, distributed written permission copyright holder.\n",
      "\n",
      "ii\n",
      "\n",
      "contents 1 preliminaries 1 1.1 introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1.1.1 machine learning? . . . . . . . . . . . . . . . . . 1 1.1.2 wellsprings machine learning . . . . . . . . . . . . . . 3 1.1.3 varieties machine learning . . . . . . . . . . . . . . . . 4 1.2 learning input-output functions . . . . . . . . . . . . . . . . . . 5 1.2.1 types learning . . . . . . . . . . . . . . . . . . . . . . 5 1.2.2 input vectors . . . . . . . . . . . . . . . . . . . . . . . . . 7 1.2.3 outputs . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 1.2.4 training regimes . . . . . . . . . . . . . . . . . . . . . . . 8 1.2.5 noise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 1.2.6 performance evaluation . . . . . . . . . . . . . . . . . . . 9 1.3 learning requires bias . . . . . . . . . . . . . . . . . . . . . . . . 9 1.4 sample applications . . . . . . . . . . . . . . . . . . . . . . . . . 11 1.5 sources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 1.6 bibliographical historical remarks . . . . . . . . . . . . . . 13 2 boolean functions 15 2.1 representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 2.1.1 boolean algebra . . . . . . . . . . . . . . . . . . . . . . . 15 2.1.2 diagrammatic representations . . . . . . . . . . . . . . . 16 2.2 classes boolean functions . . . . . . . . . . . . . . . . . . . . 17 2.2.1 terms clauses . . . . . . . . . . . . . . . . . . . . . . 17 2.2.2 dnf functions . . . . . . . . . . . . . . . . . . . . . . . . 18 2.2.3 cnf functions . . . . . . . . . . . . . . . . . . . . . . . . 21 2.2.4 decision lists . . . . . . . . . . . . . . . . . . . . . . . . . 22 2.2.5 symmetric voting functions . . . . . . . . . . . . . . 23 2.2.6 linearly separable functions . . . . . . . . . . . . . . . . 23 2.3 summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 2.4 bibliographical historical remarks . . . . . . . . . . . . . . 25 iii\n",
      "\n",
      "3 version spaces learning 27 3.1 version spaces mistake bounds . . . . . . . . . . . . . . . . 27 3.2 version graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 3.3 learning search version space . . . . . . . . . . . . . . . 32 3.4 candidate elimination method . . . . . . . . . . . . . . . . 32 3.5 bibliographical historical remarks . . . . . . . . . . . . . . 34 4 neural networks 35 4.1 threshold logic units . . . . . . . . . . . . . . . . . . . . . . . . 35 4.1.1 deﬁnitions geometry . . . . . . . . . . . . . . . . . . 35 4.1.2 special cases linearly separable functions . . . . . . . 37 4.1.3 error-correction training tlu . . . . . . . . . . . . 38 4.1.4 weight space . . . . . . . . . . . . . . . . . . . . . . . . . 40 4.1.5 widrow-hoﬀprocedure . . . . . . . . . . . . . . . . . 42 4.1.6 training tlu non-linearly-separable training sets 44 4.2 linear machines . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 4.3 networks tlus . . . . . . . . . . . . . . . . . . . . . . . . . . 46 4.3.1 motivation examples . . . . . . . . . . . . . . . . . . 46 4.3.2 madalines . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 4.3.3 piecewise linear machines . . . . . . . . . . . . . . . . . . 50 4.3.4 cascade networks . . . . . . . . . . . . . . . . . . . . . . 51 4.4 training feedforward networks backpropagation . . . . . . . 52 4.4.1 notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 4.4.2 backpropagation method . . . . . . . . . . . . . . . . 53 4.4.3 computing weight changes final layer . . . . . . 56 4.4.4 computing changes weights intermediate layers 58 4.4.5 variations backprop . . . . . . . . . . . . . . . . . . . 59 4.4.6 application steering van . . . . . . . . . . . . . . . 60 4.5 synergies neural network knowledge-based methods 61 4.6 bibliographical historical remarks . . . . . . . . . . . . . . 61 5 statistical learning 63 5.1 statistical decision theory . . . . . . . . . . . . . . . . . . 63 5.1.1 background general method . . . . . . . . . . . . . . 63 5.1.2 gaussian (or normal) distributions . . . . . . . . . . . . 65 5.1.3 conditionally independent binary components . . . . . . 68 5.2 learning belief networks . . . . . . . . . . . . . . . . . . . . . . 70 5.3 nearest-neighbor methods . . . . . . . . . . . . . . . . . . . . . . 70 5.4 bibliographical historical remarks . . . . . . . . . . . . . . 72 iv\n",
      "\n",
      "6 decision trees 73 6.1 deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73 6.2 supervised learning univariate decision trees . . . . . . . . . 74 6.2.1 selecting type test . . . . . . . . . . . . . . . . . . 75 6.2.2 uncertainty reduction select tests . . . . . . . 75 6.2.3 non-binary attributes . . . . . . . . . . . . . . . . . . . . 79 6.3 networks equivalent decision trees . . . . . . . . . . . . . . . 79 6.4 overﬁtting evaluation . . . . . . . . . . . . . . . . . . . . . 80 6.4.1 overﬁtting . . . . . . . . . . . . . . . . . . . . . . . . . . 80 6.4.2 validation methods . . . . . . . . . . . . . . . . . . . . . 81 6.4.3 avoiding overﬁtting decision trees . . . . . . . . . . . 82 6.4.4 minimum-description length methods . . . . . . . . . . . 83 6.4.5 noise data . . . . . . . . . . . . . . . . . . . . . . . . . 84 6.5 problem replicated subtrees . . . . . . . . . . . . . . . . 84 6.6 problem missing attributes . . . . . . . . . . . . . . . . . 86 6.7 comparisons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86 6.8 bibliographical historical remarks . . . . . . . . . . . . . . 87 7 inductive logic programming 89 7.1 notation deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . 90 7.2 generic ilp algorithm . . . . . . . . . . . . . . . . . . . . . . 91 7.3 example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94 7.4 inducing recursive programs . . . . . . . . . . . . . . . . . . . . 98 7.5 choosing literals add . . . . . . . . . . . . . . . . . . . . . . 100 7.6 relationships ilp decision tree induction . . . . . 101 7.7 bibliographical historical remarks . . . . . . . . . . . . . . 104 8 computational learning theory 107 8.1 notation assumptions pac learning theory . . . . . . . 107 8.2 pac learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109 8.2.1 fundamental theorem . . . . . . . . . . . . . . . . . 109 8.2.2 examples . . . . . . . . . . . . . . . . . . . . . . . . . . . 111 8.2.3 properly pac-learnable classes . . . . . . . . . . . 112 8.3 vapnik-chervonenkis dimension . . . . . . . . . . . . . . . . 113 8.3.1 linear dichotomies . . . . . . . . . . . . . . . . . . . . . . 113 8.3.2 capacity . . . . . . . . . . . . . . . . . . . . . . . . . . . 115 8.3.3 general capacity result . . . . . . . . . . . . . . 116 8.3.4 facts speculations vc dimension . 117 8.4 vc dimension pac learning . . . . . . . . . . . . . . . . . 118 8.5 bibliographical historical remarks . . . . . . . . . . . . . . 118 v\n",
      "\n",
      "9 unsupervised learning 119 9.1 unsupervised learning? . . . . . . . . . . . . . . . . . . 119 9.2 clustering methods . . . . . . . . . . . . . . . . . . . . . . . . . . 120 9.2.1 method based euclidean distance . . . . . . . . . . 120 9.2.2 method based probabilities . . . . . . . . . . . . . . 124 9.3 hierarchical clustering methods . . . . . . . . . . . . . . . . . . 125 9.3.1 method based euclidean distance . . . . . . . . . . 125 9.3.2 method based probabilities . . . . . . . . . . . . . . 126 9.4 bibliographical historical remarks . . . . . . . . . . . . . . 130 10 temporal-diﬀerence learning 131 10.1 temporal patterns prediction problems . . . . . . . . . . . . 131 10.2 supervised temporal-diﬀerence methods . . . . . . . . . . . 131 10.3 incremental computation (∆w)i . . . . . . . . . . . . . . 134 10.4 experiment td methods . . . . . . . . . . . . . . . . . 135 10.5 theoretical results . . . . . . . . . . . . . . . . . . . . . . . . . . 138 10.6 intra-sequence weight updating . . . . . . . . . . . . . . . . . . 138 10.7 example application td-gammon . . . . . . . . . . . . . . . 140 10.8 bibliographical historical remarks . . . . . . . . . . . . . . 141 11 delayed-reinforcement learning 143 11.1 general problem . . . . . . . . . . . . . . . . . . . . . . . . 143 11.2 example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144 11.3 temporal discounting optimal policies . . . . . . . . . . . . 145 11.4 q-learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147 11.5 discussion, limitations, extensions q-learning . . . . . . 150 11.5.1 illustrative example . . . . . . . . . . . . . . . . . . . 150 11.5.2 random actions . . . . . . . . . . . . . . . . . . . 152 11.5.3 generalizing inputs . . . . . . . . . . . . . . . . . . 153 11.5.4 partially observable states . . . . . . . . . . . . . . . . . 154 11.5.5 scaling problems . . . . . . . . . . . . . . . . . . . . . . . 154 11.6 bibliographical historical remarks . . . . . . . . . . . . . . 155 vi\n",
      "\n",
      "12 explanation-based learning 157 12.1 deductive learning . . . . . . . . . . . . . . . . . . . . . . . . . . 157 12.2 domain theories . . . . . . . . . . . . . . . . . . . . . . . . . . . 158 12.3 example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159 12.4 evaluable predicates . . . . . . . . . . . . . . . . . . . . . . . . . 162 12.5 general proofs . . . . . . . . . . . . . . . . . . . . . . . . . 164 12.6 utility ebl . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164 12.7 applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164 12.7.1 macro-operators planning . . . . . . . . . . . . . . . . 164 12.7.2 learning search control knowledge . . . . . . . . . . . . 167 12.8 bibliographical historical remarks . . . . . . . . . . . . . . 168 vii\n",
      "\n",
      "viii\n",
      "\n",
      "preface notes process textbook. process unﬁnished, author solicits corrections, criticisms, suggestions students readers. tried eliminate errors, un- doubtedly remain—caveat lector. typographical infelicities doubt persist ﬁnal version. material added. let plans additions reminders mentioned marginal notes. suggestions topics important left out. hope future versions cover hopﬁeld nets, elman nets re- current nets, radial basis functions, grammar automata learning, genetic algorithms, bayes networks . . .. collecting exercises project suggestions appear future versions. intention pursue middle ground theoretical textbook focusses applications. book concentrates important ideas machine learning. proofs theorems state, plausibility arguments citations formal proofs. and, treat matters practical importance applications; book handbook machine learning practice. instead, goal reader suﬃcient preparation extensive literature machine learning accessible. students stanford courses machine learning useful suggestions, colleague, pat langley, teaching assistants, ron kohavi, karl pﬂeger, robert allen, lise getoor. ix\n",
      "\n",
      "chapter 1 preliminaries 1.1 introduction 1.1.1 machine learning? learning, like intelligence, covers broad range processes dif- ﬁcult deﬁne precisely. dictionary deﬁnition includes phrases “to gain knowledge, understanding of, skill in, study, instruction, expe- rience,” “modiﬁcation behavioral tendency experience.” zoologists psychologists study learning animals humans. book fo- cus learning machines. parallels animal machine learning. certainly, techniques machine learning derive eﬀorts psychologists precise theories animal human learning computational models. likely concepts techniques explored researchers machine learning illuminate certain aspects biological learning. regards machines, say, broadly, machine learns changes structure, program, data (based inputs response external information) manner expected future performance improves. changes, addition record data base, fall comfortably province disciplines necessarily better understood called learning. but, example, performance speech-recognition machine improves hearing samples person’s speech, feel justiﬁed case machine learned. machine learning usually refers changes systems perform tasks associated artiﬁcial intelligence (ai). tasks involve recognition, diag- nosis, planning, robot control, prediction, etc. “changes” enhancements performing systems ab initio synthesis new sys- tems. slightly speciﬁc, architecture typical ai 1\n",
      "\n",
      "2 chapter 1. preliminaries “agent” fig. 1.1. agent perceives models environment com- putes appropriate actions, anticipating eﬀects. changes components shown ﬁgure count learning. diﬀerent learning mechanisms employed depending subsystem changed. study diﬀerent learning methods book. sensory signals perception actions action computation model planning reasoning goals figure 1.1 ai system ask “why machines learn? design ma- chines perform desired ﬁrst place?” reasons machine learning important. course, mentioned achievement learning machines help understand animals humans learn. important engineering reasons well. are • tasks deﬁned example; is, able specify input/output pairs concise relationship inputs desired outputs. like machines able adjust internal structure produce correct outputs large number sample inputs suitably constrain input/output function approximate relationship implicit examples. • possible hidden large piles data important rela- tionships correlations. machine learning methods extract relationships (data mining).\n",
      "\n",
      "1.1. introduction 3 • human designers produce machines work desired environments used. fact, certain char- acteristics working environment completely known design time. machine learning methods on-the-job improvement existing machine designs. • knowledge available certain tasks large explicit encoding humans. machines learn knowledge gradually able capture humans want write down. • environments change time. machines adapt changing environment reduce need constant redesign. • new knowledge tasks constantly discovered humans. vocabulary changes. constant stream new events world. continuing redesign ai systems conform new knowledge impractical, machine learning methods able track it. 1.1.2 wellsprings machine learning work machine learning converging sources. dif- ferent traditions bring diﬀerent methods diﬀerent vocabulary assimilated uniﬁed discipline. brief listing separate disciplines contributed machine learning; details follow appropriate chapters • statistics long-standing problem statistics best use sam- ples drawn unknown probability distributions help decide distribution new sample drawn. related problem estimate value unknown function new point given values function set sample points. statistical methods dealing problems considered instances machine learning decision estimation rules depend corpus samples drawn problem environment. explore statistical methods later book. details statistical the- ory underlying methods found statistical textbooks [anderson, 1958]. • brain models non-linear elements weighted inputs suggested simple models biological neu- rons. networks elements studied sev- eral researchers including [mcculloch & pitts, 1943, hebb, 1949, rosenblatt, 1958] and, recently [gluck & rumelhart, 1989, sejnowski, koch, & churchland, 1988]. brain modelers interested closely networks approximate learning phenomena\n",
      "\n",
      "4 chapter 1. preliminaries living brains. shall important machine learning techniques based networks nonlinear elements—often called neural networks. work inspired school called connectionism, brain-style computation, sub-symbolic processing. • adaptive control theory control theorists study problem con- trolling process having unknown parameters estimated operation. often, parameters change operation, control process track changes. aspects controlling robot based sensory inputs represent instances sort problem. introduction [bollinger & duﬃe, 1988]. • psychological models psychologists studied performance humans learning tasks. early example epam net- work storing retrieving member pair words given [feigenbaum, 1961]. related work led number early decision tree [hunt, marin, & stone, 1966] semantic network [anderson & bower, 1973] methods. recent work sort inﬂuenced activities artiﬁcial intelligence pre- senting. work reinforcement learning traced eﬀorts model reward stimuli inﬂuence learning goal-seeking behavior animals [sutton & barto, 1987]. reinforcement learning important theme machine learning research. • artiﬁcial intelligence beginning, ai research con- cerned machine learning. samuel developed prominent early pro- gram learned parameters function evaluating board posi- tions game checkers [samuel, 1959]. ai researchers explored role analogies learning [carbonell, 1983] fu- ture actions decisions based previous exemplary cases [kolodner, 1993]. recent work directed discovering rules expert systems decision-tree methods [quinlan, 1990] in- ductive logic programming [muggleton, 1991, lavraˇc & dˇzeroski, 1994]. theme saving generalizing results prob- lem solving explanation-based learning [dejong & mooney, 1986, laird, et al., 1986, minton, 1988, etzioni, 1993]. • evolutionary models nature, individual animals learn perform better, species evolve better ﬁt individual niches. distinc- tion evolving learning blurred computer systems, techniques model certain aspects biological evolution proposed learning methods improve performance computer programs. genetic algorithms [holland, 1975] genetic programming [koza, 1992, koza, 1994] prominent computational tech- niques evolution.\n",
      "\n",
      "1.2. learning input-output functions 5 1.1.3 varieties machine learning orthogonal historical source learning technique important learned. book, thing learned computational structure sort. consider variety diﬀerent computational structures • functions • logic programs rule sets • finite-state machines • grammars • problem solving systems present methods synthesis structures examples changing existing structures. case, change existing structure simply computationally eﬃcient increase coverage situations handle. terminology shall book best introduced discussing problem learning functions, turn matter ﬁrst. 1.2 learning input-output functions use fig. 1.2 help deﬁne terminology describing problem learning function. imagine function, f, task learner guess is. hypothesis function learned denoted h. f h functions vector-valued input x = (x1, x2, . . . , xi, . . . , xn) n components. think h implemented device x input h(x) output. f h vector-valued. assume priori hypothesized function, h, selected class functions h. know f belongs class subset class. select h based training set, ξ, m input vector examples. important details depend nature assumptions entities. 1.2.1 types learning major settings wish learn function. one, called supervised learning, know (sometimes approximately) values f m samples training set, ξ. assume ﬁnd hypothesis, h, closely agrees f members ξ, hypothesis good guess f—especially ξ large.\n",
      "\n",
      "6 chapter 1. preliminaries h(x) h u = {x1, x2, . . . xi, . . ., xm} training set x = x1 . . . xi . . . xn h d h figure 1.2 input-output function curve-ﬁtting simple example supervised learning function. sup- pose given values two-dimensional function, f, sample points shown solid circles fig. 1.3. want ﬁt points function, h, drawn set, h, second-degree functions. two-dimensional parabolic surface x1, x2 plane ﬁts points. parabolic function, h, hypothesis function, f, produced samples. case, h = f samples, need required exact matches. setting, termed unsupervised learning, simply train- ing set vectors function values them. problem case, typically, partition training set subsets, ξ1, . . . , ξr, ap- propriate way. (we regard problem learning function; value function subset input vector be- longs.) unsupervised learning methods application taxonomic problems desired invent ways classify data meaningful categories. shall describe methods intermediate supervised unsupervised learning. trying ﬁnd new function, h, modify existing one. interesting special case changing existing function equivalent computationally eﬃcient. type learning called speed-up learning. simple example speed-up learning involves deduction processes. formulas ⊃b b ⊃c, deduce c given a. deductive process, create formula ⊃c—a new formula sanction con-\n",
      "\n",
      "1.2. learning input-output functions 7 -10 -5 0 5 10-10 -5 0 5 10 0 500 1000 1500 -10 -5 0 5 10-10 -5 0 5 10 0 00 00 0 x1 x2 h sample f-value figure 1.3 surface fits points clusions derived formulas previously had. new formula derive c quickly, given a, before. contrast speed-up learning methods create genuinely new functions—ones diﬀerent results learning before. methods involve inductive learning. opposed deduction, correct inductions—only useful ones. 1.2.2 input vectors machine learning methods derive diﬀerent traditions, terminology rife synonyms, book. example, input vector called variety names. are input vector, pattern vector, feature vector, sample, example, instance. components, xi, input vector variously called features, attributes, input variables, components. values components main types. real-valued numbers, discrete-valued numbers, categorical values. example illustrating categorical values, information student represented values attributes class, major, sex, adviser. par- ticular student represented vector as (sophomore, history, male, higgins). additionally, categorical values ordered (as {small, medium, large}) unordered (as example given). course, mixtures types values possible. cases, possible represent input unordered form listing names attributes values. vector form assumes attributes ordered given implicitly form. example attribute-value representation, have (major history, sex male,\n",
      "\n",
      "8 chapter 1. preliminaries class sophomore, adviser higgins, age 19). vector form exclusively. important specialization uses boolean values, regarded special case discrete numbers (1,0) categorical variables (true, false). 1.2.3 outputs output real number, case process embodying function, h, called function estimator, output called output value estimate. alternatively, output categorical value, case pro- cess embodying h variously called classiﬁer, recognizer, categorizer, output called label, class, category, decision. classi- ﬁers application number recognition problems, example recognition hand-printed characters. input case suitable representation printed character, classiﬁer maps input of, say, 64 categories. vector-valued outputs possible components real numbers categorical values. important special case boolean output values. case, training pattern having value 1 called positive instance, training sample having value 0 called negative instance. input boolean, classiﬁer implements boolean function. study boolean case detail allows important general points simpliﬁed setting. learning boolean function called concept learning, function called concept. 1.2.4 training regimes ways training set, ξ, produce hypothesized function. batch method, entire training set available compute function, h. variation method uses entire training set modify current hypothesis iteratively acceptable hypothesis obtained. contrast, incremental method, select member time training set use instance modify current hypothesis. member training set selected, on. selection method random (with replacement) cycle training set iteratively. entire training set available member time, use incremental method—selecting training set members arrive. (alterna- tively, stage training set members far available “batch” process.) training set members available called online method. online methods used, example,\n",
      "\n",
      "1.3. learning requires bias 9 training instance function current hypothesis previ- ous instance—as classiﬁer decide robot’s action given current set sensory inputs. set sensory inputs depend action selected. 1.2.5 noise vectors training set corrupted noise. kinds noise. class noise randomly alters value function; attribute noise randomly alters values components input vector. case, inappropriate insist hypothesized function agree precisely values samples training set. 1.2.6 performance evaluation correct answer inductive learning, important methods evaluate result learning. discuss matter detail later, but, brieﬂy, supervised learning induced function usually evaluated separate set inputs function values called testing set . hypothesized function said generalize guesses testing set. mean-squared-error total number errors common measures. 1.3 learning requires bias long reader undoubtedly asked learning function possible all? certainly, example, uncountable number diﬀerent functions having values agree samples shown fig. 1.3. learning procedure happen select quadratic shown ﬁgure? order selection limit priori set hypotheses quadratic functions insist chose passed sample points. kind priori information called bias, useful learning bias impossible. gain insight role bias considering special case learning boolean function n dimensions. 2n diﬀerent boolean inputs possible. suppose bias; h set 22n boolean functions, preference ﬁt samples training set. case, presented member training set value rule precisely one-half members h—those boolean functions misclassify labeled sample. remaining functions constitute called “version space;” we’ll explore concept detail later. present members training set, graph number hypotheses ruled function number diﬀerent patterns presented shown fig. 1.4. stage process,\n",
      "\n",
      "10 chapter 1. preliminaries half remaining boolean functions value 1 half value 0 training pattern seen. generalization possible case training patterns clue value pattern seen. memorization possible here, trivial sort learning. log2|hv| 2n 2n j = no. labeled patterns seen 0 0 2n < j (generalization possible) |hv| = no. functions ruled figure 1.4 hypotheses remaining function labeled patterns presented suppose limited h subset, hc, boolean functions. depending subset order presentation training patterns, curve hypotheses ruled look like shown fig. 1.5. case possible seeing fewer 2n labeled samples, hypothesis agrees training set. certainly, hypothesis remaining, value patterns seen! theory probably approximately correct (pac) learning makes intuitive idea precise. we’ll examine theory later. let’s look speciﬁc example bias aids learning. boolean function represented hypercube vertices represents diﬀerent input pattern. 3-dimensional version fig. 1.6. there, training set sample patterns marked having value 1 small square having value 0 small circle. hypothesis set consists linearly separable functions—those positive negative instances separated linear surface, function remaining hypothsis set consistent training set. so, case, training set contain possible patterns, pin function be—given bias.\n",
      "\n",
      "1.4. sample applications 11 log2|hv| 2n 2n j = no. labeled patterns seen 0 0 |hv| = no. functions ruled depends order presentation log2|hc| figure 1.5 hypotheses remaining restricted subset machine learning researchers identiﬁed main varieties bias, ab- solute preference. absolute bias (also called restricted hypothesis-space bias), restricts h deﬁnite subset functions. example fig. 1.6, restriction linearly separable boolean functions. preference bias, selects hypothesis minimal according ordering scheme hypotheses. example, way measuring complex- ity hypothesis, select simplest performed satisfactorily training set. principle occam’s razor, science prefer simple explanations complex ones, type preference bias. (william occam, 1285-?1349, english philosopher said “non sunt multiplicanda entia praeter necessitatem,” means “entities multiplied unnecessarily.”) 1.4 sample applications main emphasis book concepts machine learning—not applications. nevertheless, concepts irrelevant real-world problems probably interest. motivation, short summary areas machine learning techniques successfully applied. [langley, 1992] cites following applications others a. rule discovery variant id3 printing industry problem\n",
      "\n",
      "12 chapter 1. preliminaries x1 x2 x3 figure 1.6 training set completely determines linearly separable function [evans & fisher, 1992]. b. electric power load forecasting k-nearest-neighbor rule system [jabbour, k., et al., 1987]. c. automatic “help desk” assistant nearest-neighbor system [acorn & walden, 1992]. d. planning scheduling steel mill expertease, marketed (id3-like) system [michie, 1992]. e. classiﬁcation stars galaxies [fayyad, et al., 1993]. application-oriented papers presented annual conferences neural information processing systems. papers on speech recognition, dolphin echo recognition, image processing, bio-engineering, diag- nosis, commodity trading, face recognition, music composition, optical character recognition, control applications [various editors, 1989-1994]. additional examples, [hammerstrom, 1993] mentions a. sharp’s japanese kanji character recognition system processes 200 char- acters second 99+% accuracy. recognizes 3000+ characters. b. neuroforecasting centre’s (london business school university col- lege london) trading strategy selection network earned average annual proﬁt 18% conventional system’s 12.3%.\n",
      "\n",
      "1.5. sources 13 c. fujitsu’s (plus partner’s) neural network monitoring continuous steel casting operation successful operation early 1990. summary, easy nowadays ﬁnd applications machine learn- ing techniques. fact come surprise inasmuch machine learning techniques viewed extensions known statistical meth- ods successfully applied years. 1.5 sources rich literature machine learning (a small referenced bibliography), text- books worth mentioning [hertz, krogh, & palmer, 1991, weiss & kulikowski, 1991, natarjan, 1991, fu, 1994, langley, 1996]. [shavlik & dietterich, 1990, buchanan & wilkins, 1993] edited vol- umes containing important papers. survey paper [dietterich, 1990] gives good overview important topics. established conferences publications papers given appear including • annual conferences advances neural information processing systems • annual workshops computational learning theory • annual international workshops machine learning • annual international conferences genetic algorithms (the proceedings above-listed conferences published morgan kaufmann.) • journal machine learning (published kluwer academic publish- ers). information, programs datasets, available internet world wide web. 1.6 bibliographical historical remarks added. chapter contain brief survey history material covered chapter.\n",
      "\n",
      "14 chapter 1. preliminaries\n",
      "\n",
      "chapter 2 boolean functions 2.1 representation 2.1.1 boolean algebra important ideas learning functions easily presented special case boolean functions. important sub- classes boolean functions hypothesis classes function learning. therefore, digress chapter present review boolean functions properties. (for thorough treatment see, example, [unger, 1989].) boolean function, f(x1, x2, . . . , xn) maps n-tuple (0,1) values {0, 1}. boolean algebra convenient notation representing boolean func- tions. boolean algebra uses connectives ·, +, . example, function variables written x1 · x2. convention, connective, “·” usually suppressed, function written x1x2. x1x2 value 1 x1 x2 value 1; x1 x2 value 0, x1x2 value 0. (inclusive) function variables written x1 + x2. x1 + x2 value 1 x1 x2 value 1; x1 x2 value 0, x1 + x2 value 0. complement negation variable, x, written x. x value 1 x value 0; x value 1, x value 0. deﬁnitions compactly given following rules boolean algebra 1 + 1 = 1, 1 + 0 = 1, 0 + 0 = 0, 1 · 1 = 1, 1 · 0 = 0, 0 · 0 = 0, 1 = 0, 0 = 1. arguments values boolean functions expressed terms constants t (true) f (false) instead 1 0, respectively. 15\n",
      "\n",
      "16 chapter 2. boolean functions connectives · + commutative associative. thus, example, x1(x2x3) = (x1x2)x3, written simply x1x2x3. similarly +. boolean formula consisting single variable, x1 called atom. consisting single variable complement, x1, called literal. operators · + commute themselves. instead, demorgan’s laws (which veriﬁed deﬁnitions) x1x2 = x1 + x2, x1 + x2 = x1 x2. 2.1.2 diagrammatic representations saw chapter boolean function represented labeling vertices cube. function n variables, need n-dimensional hypercube. fig. 2.1 2- 3-dimensional examples. vertices having value 1 labeled small square, vertices having value 0 labeled small circle. x1 x2 x1 x2 x1 x2 xor (exclusive or) x1x2 x1 + x2 x1x2 + x1x2 parity function x1 x2 x3 x1x2x3 + x1x2x3 + x1x2x3 + x1x2x3 figure 2.1 representing boolean functions cubes hypercube representations, easy boolean functions n dimensions are. 3-dimensional cube 23 = 8 vertices, labeled diﬀerent ways; 2(23) = 256\n",
      "\n",
      "2.2. classes boolean functions 17 diﬀerent boolean functions 3 variables. general, 22n boolean functions n variables. 2- 3-dimensional cubes later provide intuition properties certain boolean functions. course, visualize hypercubes (for n > 3), surprising properties higher dimensional spaces, careful intuitions gained low dimensions. diagrammatic technique dimensions slightly higher 3 karnaugh map. karnaugh map array values boolean function horizontal rows indexed values variables vertical columns indexed rest. rows columns arranged way entries adjacent map correspond vertices adjacent hypercube representation. example 4-dimensional parity function fig. 2.2. (an parity function boolean function value 1 number arguments value 1; value 0.) note adjacent cells table correspond inputs diﬀering component. describe general logic diagrams, [wnek, et al., 1990]. 00 01 10 11 00 01 10 11 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 x1,x2 x3,x4 figure 2.2 karnaugh map 2.2 classes boolean functions 2.2.1 terms clauses use absolute bias machine learning, limit class hypotheses. learning boolean functions, frequently use common sub-classes functions. therefore, important know subclasses. basic subclass called terms. term function written form l1l2 · · · lk, li literals. form called conjunction literals. example terms x1x7 x1x2x4. size term number literals contains. examples sizes 2 3, respectively. (strictly speaking, class conjunctions literals called monomials,\n",
      "\n",
      "18 chapter 2. boolean functions conjunction literals called term. distinction ﬁne elect blur here.) easy exactly 3n possible terms n variables. number terms size k bounded pk i=0 c(2n, i) = o(nk), c(i, j) = i! (i−j)!j! binomial coeﬃcient. probably i’ll simple term-learning algorithm here—so started learning! dnf functions decision lists—as deﬁned pages. clause function written form l1 +l2 +· · ·+lk, li literals. form called disjunction literals. example clauses x3 + x5 + x6 x1 + x4. size clause number literals contains. 3n possible clauses fewer pk i=0 c(2n, i) clauses size k less. f term, (by de morgan’s laws) f clause, vice versa. thus, terms clauses duals other. psychological experiments, conjunctions literals easier humans learn disjunctions literals. 2.2.2 dnf functions boolean function said disjunctive normal form (dnf) written disjunction terms. examples dnf are f = x1x2+x2x3x4 f = x1x3 + x2 x3 + x1x2x3. dnf expression called k-term dnf expression disjunction k terms; class k-dnf size largest term k. examples 2-term 3-term expressions, respectively. expressions class 3-dnf. term dnf expression function called implicant “implies” function (if term value 1, function). general, term, t, implicant function, f, f value 1 t does. term, t, prime implicant f term, t′, formed taking literal implicant t longer implicant f. (the implicant “divided” term remain implicant.) thus, x2x3 x1 x3 prime implicants f = x2x3+x1 x3+x2x1x3, x2x1x3 not. relationship implicants prime implicants geometri- cally illustrated cube representation boolean functions. consider, example, function f = x2x3 + x1 x3 + x2x1x3. illustrate fig. 2.3. note planes ﬁgure “cuts oﬀ” group vertices having value 1, cuts oﬀany vertices having value 0. planes pictorial devices isolate certain lower dimensional subfaces cube. isolate one-dimensional edges, isolates zero-dimensional vertex. group vertices subface corresponds implicants function, f, implicant corresponds subface dimension. k-dimensional subface corresponds (n −k)-size implicant term. function written disjunction implicants—corresponding union vertices cut oﬀby planes. geometrically, implicant prime corresponding subface largest dimensional subface includes vertices\n",
      "\n",
      "2.2. classes boolean functions 19 vertices having value 0. note term x2x1x3 prime implicant f. (in case, don’t include term function vertex cut oﬀby plane corresponding x2x1x3 cut oﬀby plane corresponding x2x3.) implicants prime corresponding subfaces expanded including vertices having value 0. x2 x1 x3 1, 0, 0 1, 0, 1 1, 1, 1 0, 0, 1 f = x2x3 + x1x3 + x2x1x3 = x2x3 + x1x3 x2x3 x1x3 prime implicants figure 2.3 function implicants note boolean functions represented dnf—trivially disjunctions terms size n term corresponds vertices value 1. 22n functions n dimensions dnf (since boolean function written dnf), 2o(nk) functions k-dnf. boolean functions represented dnf term prime implicant, representation unique, shown fig. 2.4. express function dnf form, use consensus method ﬁnd expression function term prime implicant. consensus method relies results replace section describing quine-mccluskey method instead. • consensus\n",
      "\n",
      "20 chapter 2. boolean functions x2 x1 x3 1, 0, 0 1, 0, 1 1, 1, 1 0, 0, 1 f = x2x3 + x1x3 + x1x2 = x1x2 + x1x3 terms prime implicants, unique representation figure 2.4 non-uniqueness representation prime implicants xi · f1 + xi · f2 = xi · f1 + xi · f2 + f1 · f2 f1 f2 terms literal appearing f1 appears complemented f2. f1 · f2 called consensus xi · f1 xi · f2. readers familiar resolution rule inference note consensus dual resolution. examples x1 consensus x1x2 x1x2. terms x1x2 x1x2 consensus term literal appearing complemented other. • subsumption xi · f1 + f1 = f1 f1 term. f1 subsumes xi · f1. example x1 x4x5 subsumes x1 x4 x2x5\n",
      "\n",
      "2.2. classes boolean functions 21 consensus method ﬁnding set prime implicants function, f, iterates following operations terms dnf expression f operations applied a. initialize process set, t , terms dnf expression f, b. compute consensus pair terms t add result t , c. eliminate terms t subsumed terms t . process halts, terms remaining t prime implicants f. example let f = x1x2 + x1 x2x3 + x1 x2 x3 x4x5. derivation set prime implicants consensus tree fig. 2.5. circled numbers adjoining terms indicate order consensus subsumption operations performed. shaded boxes surrounding term indicate subsumed. ﬁnal form function terms prime implicants is f = x1x2 +x1x3 +x1 x4x5. terms non-subsumed terms consensus tree. x1x2 x1x2x3 x1x2x3x4x5 x1x3 x1x2x4x5 x1x4x5 f = x1x2 + + x1x3 x1x4x5 1 2 6 4 5 3 figure 2.5 consensus tree 2.2.3 cnf functions disjunctive normal form dual conjunctive normal form (cnf). boolean function said cnf written conjunction clauses.\n",
      "\n",
      "22 chapter 2. boolean functions example cnf is f = (x1 +x2)(x2 +x3 +x4). cnf expression called k-clause cnf expression conjunction k clauses; class k-cnf size largest clause k. example 2-clause expression 3-cnf. f written dnf, application de morgan’s law renders f cnf, vice versa. cnf dnf duals, 2o(nk) functions k-cnf. 2.2.4 decision lists rivest proposed class boolean functions called decision lists [rivest, 1987]. decision list written ordered list pairs (tq, vq) (tq−1, vq−1) · · · (ti, vi) · · · (t2, v2) (t, v1) vi 0 1, ti terms (x1, . . . , xn), t term value 1 (regardless values xi). value decision list value vi ﬁrst ti list value 1. (at ti value 1, does; v1 regarded default value decision list.) decision list size k, size largest term k. class decision lists size k called k-dl. example decision list is f = (x1x2, 1) (x1 x2x3, 0) x2x3, 1) (1, 0) f value 0 x1 = 0, x2 = 0, x3 = 1. value 1 x1 = 1, x2 = 0, x3 = 1. function 3-dl. shown class k-dl strict superset union k-dnf k-cnf. 2o[nkk log(n)] functions k-dl [rivest, 1987]. interesting generalizations decision lists use boolean functions place terms, ti. example use linearly separable functions place ti (see [marchand & golea, 1993]).\n",
      "\n",
      "2.2. classes boolean functions 23 2.2.5 symmetric voting functions boolean function called symmetric invariant permutations input variables. example, function dependent number input variables values 1 symmetric function. parity functions, value 1 depending number input variables value 1 odd symmetric function. (the exclusive function, illustrated fig. 2.1, odd-parity function dimensions. functions dimensions symmetric.) important subclass symmetric functions class voting func- tions (also called m-of-n functions). k-voting function value 1 k n inputs value 1. k = 1, voting function n-sized clause; k = n, voting function n-sized term; k = (n + 1)/2 n odd k = 1 + n/2 n even, majority function. 2.2.6 linearly separable functions linearly separable functions expressed follows f = thresh( n x i=1 wixi, θ) wi, = 1, . . . , n, real-valued numbers called weights, θ real-valued number called threshold, thresh(σ, θ) 1 σ ≥θ 0 otherwise. (note concept linearly separable functions extended non- boolean inputs.) k-voting functions members class linearly separable functions weights unit value threshold depends k. thus, terms clauses special cases linearly separable functions. convenient way write linearly separable functions uses vector notation f = thresh(x · w, θ) x = (x1, . . . , xn) n-dimensional vector input variables, w = (w1, . . . , wn) n-dimensional vector weight values, x · w dot (or inner) product vectors. input vectors f value 1 lie half-space (and on) hyperplane orientation normal w position (with respect origin) determined θ. saw example separating plane fig. 1.6. idea mind, easy functions fig. 2.1 linearly separable, not. note terms figs. 2.3 2.4 linearly separable functions evidenced separating planes shown. closed-form expression number linearly separable func- tions n dimensions, following table gives numbers n 6.\n",
      "\n",
      "24 chapter 2. boolean functions n boolean linearly separable functions functions 1 4 4 2 16 14 3 256 104 4 65,536 1,882 5 ≈4.3 × 109 94,572 6 ≈1.8 × 1019 15,028,134 [muroga, 1971] shown (for n > 1) 2n2 linearly separable functions n dimensions. (see [winder, 1961, winder, 1962].) 2.3 summary diagram fig. 2.6 shows set inclusions classes boolean functions considered. confronting classes later chapters. dnf (all) k-dl k-dnf k-size- terms terms lin sep figure 2.6 classes boolean functions sizes classes given following table (adapted [dietterich, 1990, page 262])\n",
      "\n",
      "2.4. bibliographical historical remarks 25 class size class terms 3n clauses 3n k-term dnf 2o(kn) k-clause cnf 2o(kn) k-dnf 2o(nk) k-cnf 2o(nk) k-dl 2o[nkk log(n)] lin sep 2o(n2) dnf 22n 2.4 bibliographical historical remarks added.\n",
      "\n",
      "26 chapter 2. boolean functions\n",
      "\n",
      "chapter 3 version spaces learning 3.1 version spaces mistake bounds ﬁrst learning methods present based concepts version spaces version graphs. ideas clearly explained case boolean function learning. given initial hypothesis set h (a subset boolean functions) values f(x) x training set, ξ, version space subset hypotheses, hv, consistent values. hypothesis, h, consistent values x ξ h(x) = f(x) x ξ. hypotheses h consistent values training set ruled training set. imagine (conceptually only!) devices implement- ing function h. incremental training procedure deﬁned presented pattern ξ functions eliminated functions values pattern agree given value. stage process left subset functions consistent patterns presented far; subset version space patterns presented. idea illustrated fig. 3.1. consider following procedure classifying arbitrary input pattern, x pattern class (0 1) majority outputs functions version space. learning procedure, majority equal value pattern presented, mistake made, revise version space accordingly—eliminating (majority the) functions voting incorrectly. thus, mistake made, rule half functions remaining version space. mistakes procedure make? obviously, log2(|h|) mistakes, |h| number hypotheses 27\n",
      "\n",
      "28 chapter 3. version spaces learning h1 h2 hi hk x subset, h, boolean functions rule hypotheses consistent training patterns hj hypotheses ruled constitute version space k = |h| 1 0 figure 3.1 implementing version space original hypothesis set, h. (note, though, number training patterns seen maximum number mistakes greater.) theoretical (and impractical!) result (due [littlestone, 1988]) example mistake bound—an important concept machine learning theory. shows exist learning procedure makes mistakes upper bound. later, we’ll derive mistake bounds. special case, bias limit h terms, log2(3n) = n log2(3) = 1.585n mistakes exhausting version space. result means f term, 1.585n mistakes learning f, number mistakes able decide f term. suﬃcient training patterns reduce version space single function, training patterns reduce version space set functions assign values patterns henceforth. select remaining functions random reasonably assured generalize satisfactorily. discuss computationally feasible method representing version space.\n",
      "\n",
      "3.2. version graphs 29 3.2 version graphs boolean functions ordered generality. boolean function, f1, general function, f2, (and f2 speciﬁc f1), f1 value 1 arguments f2 value 1, f1 ̸= f2. example, x3 general x2x3 general x3 + x2. form graph hypotheses, {hi}, version space nodes. node graph, hi, arc directed node, hj, hj general hi. graph version graph. fig. 3.2, example version graph 3-dimensional input space hypotheses restricted terms (with ruled out). 0 x1 x2 x3 x2 x3 1 x1x2 x3 x1x2 x1 version graph terms x1 x2 x3 (for simplicity, arcs graph shown) (none ruled out) (k = 1) (k = 2) (k = 3) x1 x3 figure 3.2 version graph terms function, denoted “1,” value 1 inputs, corre- sponds node graph. (it general term.) similarly, function “0” graph. “1” row nodes corresponding terms having literal, row nodes corresponding terms having literals,\n",
      "\n",
      "30 chapter 3. version spaces learning on. 33 = 27 functions altogether (the function “0,” included graph, technically term). portrayal graph cluttered arcs shown; node actual graph arc directed nodes general. use example version graph changes consider set labeled samples training set, ξ. suppose ﬁrst consider training pattern (1, 0, 1) value 0. functions version graph fig. 3.2 inconsistent training pattern. ruled nodes longer version graph shown shaded fig. 3.3. three-dimensional cube representation vertex (1, 0, 1) value 0. 0 x1 x2 x3 x2 x3 1 x1x2 x3 x1x2 x1 new version graph 1, 0, 1 value 0 x1x3 x1x2 x2x3 x1x2x3 x1 x2 x3 x1x3 (only arcs graph shown) ruled nodes figure 3.3 version graph seeing (1, 0, 1) version graph, set hypotheses maximally general set hypotheses maximally speciﬁc. called general boundary set (gbs) speciﬁc boundary set (sbs), respectively. fig. 3.4, version graph exists learning (1,0,1) value 0 (1, 0, 0) value 1. gbs sbs shown.\n",
      "\n",
      "3.2. version graphs 31 0 x1 x2 x3 x2 x3 1 x1x2 x3 x1 x2x3 x1x3 general boundary set (gbs) specific boundary set (sbs) x1x2 specific gbs, general sbs 1, 0, 1 value 0 x1 x2 x3 1, 0, 0 value 1 figure 3.4 version graph seeing (1, 0, 1) (1, 0, 0) boundary sets important provide alternative repre- senting entire version space explicitly, impractical. given boundary sets, possible determine hypoth- esis (in prescribed class boolean functions using) member version space. determination possible fact member version space (that member boundary sets) speciﬁc member general boundary set general member speciﬁc boundary set. limit boolean functions version space terms, simple matter determine maximally general maximally speciﬁc functions (assuming term version space). maximally speciﬁc corresponds subface minimal dimension contains members training set labelled 1 members labelled 0. maximally general corresponds subface maximal dimension contains members training set labelled 1 members labelled 0. looking fig. 3.4, subface minimal dimension contains (1, 0, 0) contain (1, 0, 1) vertex (1, 0, 0) itself—corresponding function x1x2 x3. subface\n",
      "\n",
      "32 chapter 3. version spaces learning maximal dimension contains (1, 0, 0) contain (1, 0, 1) face cube—corresponding function x3. figs. 3.2 3.4 sbs singular. version spaces terms singular speciﬁc boundary sets. seen fig. 3.3, however, gbs version space terms need singular. 3.3 learning search version space [to written. relate term learning algorithm presented chapter two. discuss best-ﬁrst search methods. pat langley’s example us- ing “pseudo-cells” generate eliminate hypotheses.] selecting hypothesis version space thought search problem. start general function specialize specialization operators ﬁnds function consistent (or adequately so) set training patterns. procedures usually called top-down methods. or, start special function generalize it—resulting bottom-up methods. shall instances styles learning book. compare view top-down versus bottom-up divide-and-conquer covering (or aq) methods decision-tree induction. 3.4 candidate elimination method candidate elimination method, incremental method computing boundary sets. quoting [hirsh, 1994, page 6] “the candidate-elimination algorithm manipulates boundary-set representation version space create boundary sets rep- resent new version space consistent previous instances plus new one. positive exmple algorithm generalizes elements [sbs] little possible cover new instance remain consistent past data, removes elements [gbs] cover new instance. negative instance algorithm specializes elements [gbs] longer cover new instance remain consis- tent past data, removes [sbs] elements mistakenly cover new, negative instance.” method uses following deﬁnitions (adapted [genesereth & nilsson, 1987]) • hypothesis called suﬃcient value 1 training samples labeled 1, • hypothesis called necessary value 0 training samples labeled 0.\n",
      "\n",
      "3.4. candidate elimination method 33 think deﬁnitions hypothesis implements suﬃ- cient condition training sample value 1 hypothesis value 1 positive instances; hypothesis implements necessary condition training sample value 1 hypothesis value 0 negative instances. hypothesis consistent training set (and version space) suﬃcient necessary. start (before receiving members training set) function “0” singleton element speciﬁc boundary set function “1” singleton element general boundary set. receiving new labeled input vector, boundary sets changed follows a. new vector labelled 1 new general boundary set obtained previous ex- cluding elements suﬃcient. (that is, exclude elements value 0 new vector.) new speciﬁc boundary set obtained previous re- placing element, hi, generalizations. hypothesis hg generalization h if a) h speciﬁc hg, b) hg suﬃcient, c) function (including h) speciﬁc hg suﬃcient, d) hg speciﬁc member new general boundary set. hg = h. also, generalizations diﬀerent functions speciﬁc boundary set identical. b. new vector labelled 0 new speciﬁc boundary set obtained previous ex- cluding elements necessary. (that is, exclude elements value 1 new vector.) new general boundary set obtained previous re- placing element, hi, specializations. hypothesis hs specialization h if a) h general hs, b) hs necessary, c) function (including h) general hs necessary, d) hs general member new speciﬁc boundary set. again, hs = h, specializations diﬀerent functions general boundary set identical. example, suppose present vectors following order vector label (1, 0, 1) 0 (1, 0, 0) 1 (1, 1, 1) 0 (0, 0, 1) 0\n",
      "\n",
      "34 chapter 3. version spaces learning start general boundary set, “1”, speciﬁc boundary set, “0.” seeing ﬁrst sample, (1, 0, 1), labeled 0, speciﬁc boundary set stays “0” (it necessary), change general boundary set {x1, x2, x3}. functions, x1, x2, x3, specializations “1” (they necessary, “1” not, general “0”, functions general necessary). then, seeing (1, 0, 0), labeled 1, general boundary set changes {x3} (because x1 x2 suﬃcient), speciﬁc boundary set changed {x1x2 x3}. single function generalization “0” (it suﬃcient, “0” speciﬁc it, function (including “0”) speciﬁc suﬃcient, speciﬁc member general boundary set. (1, 1, 1), labeled 0, change speciﬁc boundary set function necessary. change general boundary set x3 necessary. finally, (0, 0, 1), labeled 0, change speciﬁc boundary set function necessary. change general boundary set x3 necessary. maybe i’ll example version graph non-boolean functions. 3.5 bibliographical historical remarks concept version spaces role learning ﬁrst investigated tom mitchell [mitchell, 1982]. ideas prac- tical machine learning procedures, provide insight nature hypothesis selection. order accomodate noisy data, version spaces generalized [hirsh, 1994] allow hypotheses necessarily consistent training set. added.\n",
      "\n",
      "chapter 4 neural networks chapter deﬁned important subsets boolean functions. sup- pose decide use subsets hypothesis set supervised function learning. best implement function device gives outputs prescribed function arbi- trary inputs. chapter describe networks non-linear elements implement input-output functions trained supervised learning methods. networks non-linear elements, interconnected adjustable weights, play prominent role machine learning. called neural networks be- cause non-linear elements inputs weighted sum outputs elements—much like networks biological neurons do. networks commonly use threshold element encountered chapter study linearly separable boolean functions. begin treatment neural nets studying threshold element simplest networks, ones composed single threshold element. 4.1 threshold logic units 4.1.1 deﬁnitions geometry linearly separable (threshold) functions implemented straightforward way summing weighted inputs comparing sum threshold value shown fig. 4.1. structure threshold logic unit (tlu). output 1 0 depending weighted sum inputs greater equal threshold value, θ. called adaline (for adaptive linear element) [widrow, 1962, widrow & lehr, 1990], ltu (linear threshold unit), perceptron, neuron. (although word “per- ceptron” nowadays refer single tlu, rosenblatt originally deﬁned class networks threshold elements [rosenblatt, 1958].) 35\n",
      "\n",
      "36 chapter 4. neural networks ! x1 x2 xn+1 = 1 xi w1 w2 wn+1 wi wn x threshold weight xn w threshold \" = 0 f f = thresh( ! wi xi, 0) = 1 n+1 figure 4.1 threshold logic unit (tlu) n-dimensional feature input vector denoted x = (x1, . . . , xn). want distinguish diﬀerent feature vectors, attach subscripts, xi. components x real-valued numbers, specialize binary numbers 0 1. weights tlu represented n-dimensional weight vector, w = (w1, . . . , wn). components real-valued numbers (but specialize integers). tlu output 1 pn i=1 xiwi ≥θ; output 0. weighted sum calculated tlu simply represented vector dot product, x•w. (if pattern weight vectors thought “column” vectors, dot product written xtw, “row” vector xt transpose x.) often, threshold, θ, tlu ﬁxed 0; case, arbitrary thresholds achieved (n + 1)- dimensional “augmented” vectors, y, v, ﬁrst n components x w, respectively. (n + 1)-st component, xn+1, augmented feature vector, y, value 1; (n + 1)-st component, wn+1, augmented weight vector, v, set equal negative desired threshold value. (when want emphasize use augmented vectors, we’ll use y,v notation; context discussion makes clear sort vectors talking about, we’ll lapse familiar x,w notation.) y,v notation, tlu output 1 y•v ≥0. otherwise, output 0. intuitively useful geometric description tlu. tlu divides input space hyperplane sketched fig. 4.2. hyperplane boundary patterns x•w + wn+1 > 0 patterns x•w + wn+1 < 0. thus, equation hyperplane x•w+wn+1 = 0. unit vector normal hyperplane n = w |w|, |w| = p (w2 1 + . . . + w2n) length vector w. (the normal\n",
      "\n",
      "4.1. threshold logic units 37 form hyperplane equation x•n + w |w| = 0.) distance hyperplane origin wn+1 |w| , distance arbitrary point, x, hyperplane x•w+wn+1 |w| . distance hyperplane origin negative (that is, wn+1 < 0), origin negative hyperplane (that is, x•w + wn+1 < 0). x.w + wn+1 > 0 w x w n = w |w| origin unit vector normal hyperplane w + wn+1 = 0 x n + = 0 x equations hyperplane wn+1 |w| wn+1 w + wn+1 x x.w + wn+1 < 0 figure 4.2 tlu geometry adjusting weight vector, w, changes orientation hyperplane; adjusting wn+1 changes position hyperplane (relative origin). thus, training tlu achieved adjusting values weights. way hyperplane moved tlu implements diﬀerent (linearly separable) functions input. 4.1.2 special cases linearly separable functions terms term size k implemented tlu weight inputs corresponding variables occurring term. weight +1 input corresponding positive literal, weight −1 input corresponding negative literal. (literals mentioned term weights zero—that is, connection all—from inputs.) threshold, θ, set equal kp −1/2, kp number positive literals term. tlu implements hyperplane boundary\n",
      "\n",
      "38 chapter 4. neural networks parallel subface dimension (n −k) unit hypercube. three-dimensional example fig. 4.3. thus, linearly separable functions superset terms. (1,1,1) (1,1,0) x2 x1 x3 f = x1x2 x1 + x2 - 3/2 = 0 equation plane is figure 4.3 implementing term clauses negation clause term. example, negation clause f = x1 + x2 + x3 term f = x1 x2 x3. hyperplane implement term. “invert” hyperplane, implement clause instead. inverting hyperplane multiplying tlu weights—even wn+1—by −1. process simply changes orientation hyperplane—ﬂipping 180 degrees changing “positive side.” therefore, linearly separable functions superset clauses. example fig. 4.4. 4.1.3 error-correction training tlu procedures proposed adjusting weights tlu. present family incremental training procedures parameter c. methods adjustments weight vector tlu trained makes error training pattern; called error-correction procedures. use augmented feature weight vectors describing them. a. start ﬁnite training set, ξ, vectors, yi , binary labels.\n",
      "\n",
      "4.1. threshold logic units 39 f = x1 + x2 + x3 x1 x1 + x2 + x3 < 1/2 = 0 f = x1x2x3 equation plane is x2 x3 figure 4.4 implementing clause b. compose inﬁnite training sequence, σ, vectors ξ labels member ξ occurs inﬁnitely σ. set initial weight values tlu arbitrary values. c. repeat forever present vector, yi, σ tlu note response. (a) tlu responds correctly, change weight vector. (b) yi supposed produce output 0 produces output 1 instead, modify weight vector follows v ←−v −ciyi ci positive real number called learning rate parame- ter (whose value diﬀererent diﬀerent instances family procedures depend i). note adjustment new dot product (v − ciyi)•yi = v•yi −ciyi•yi, smaller weight adjustment. (c) yi supposed produce output 1 produces output 0 instead, modify weight vector follows v ←−v + ciyi case, new dot product (v + ciyi)•yi = v•yi + ciyi•yi, larger weight adjustment. note cases combined following rule\n",
      "\n",
      "40 chapter 4. neural networks v ←−v + ci(di −fi)yi di desired response (1 0) yi , fi actual response (1 0) yi.] note weight vector v includes wn+1 thresh- old component, threshold tlu changed adjust- ments. identify versions procedure 1) ﬁxed-increment procedure, learning rate parameter, ci, ﬁxed, positive constant i. depending value constant, weight adjustment correct response erroneously classiﬁed feature vector. 2) fractional-correction procedure, parameter ci set λ yi•v yi•yi , v weight vector changed. note λ = 0, correction takes place all. λ = 1, correction suﬃcient yi•v = 0. λ > 1, error corrected. proved weight vector, v, produces correct output feature vectors ξ, ﬁnite number feature vector presentations, ﬁxed-increment procedure ﬁnd weight vector weight changes. result holds fractional-correction procedure 1 < λ ≤2. additional background, proofs, examples error-correction proce- dures, [nilsson, 1990]. [maass & tur´an, 1994] hyperplane-ﬁnding procedure makes o(n2 log n) mistakes. 4.1.4 weight space intuitive idea procedures work considering happens augmented weight vector “weight space” corrections made. use augmented vectors discussion threshold function compares dot product, yi•v, threshold 0. particular weight vector, v, corresponds point (n + 1)-dimensional weight space. now, pattern vector, yi, consider locus points weight space corresponding weight vectors yielding yi•v = 0. locus hyperplane passing origin (n + 1)-dimensional space. pattern vector hyperplane corresponding it. weight points half-spaces deﬁned hyperplane cause corresponding pattern yield dot product 0, weight points half- space cause corresponding pattern yield dot product greater 0. schematic representation weight space fig. 4.5. pattern hyperplanes, 1, 2, 3, 4 , corresponding patterns y1,\n",
      "\n",
      "4.1. threshold logic units 41 y2, y3, y4, respectively, indicate arrow half-space weight vectors dot products greater 0. suppose wanted weight values positive responses patterns y1, y3, y4, negative response pattern y2. weight point, v, indicated ﬁgure set weight values. 2 3 4 1 v figure 4.5 weight space exists weight vector gives desired responses given set patterns given geometric interpretation. involves reversing “polarity” hyperplanes corresponding patterns negative response desired. example above, weight space diagram shown fig. 4.6. 2 3 4 1 v 0 1 1 2 3 2 3 4 figure 4.6 solution region weight space\n",
      "\n",
      "42 chapter 4. neural networks weight vector exists correctly classiﬁes set patterns, half-spaces deﬁned correct responses patterns non- intersection, called solution region. solution region “hyper-wedge” region vertex origin weight space cross-section increases increasing distance origin. region shown shaded fig. 4.6. (the boxed numbers show, later purposes, number errors weight vectors regions.) ﬁxed-increment error-correction procedure changes weight vector moving normal pattern hyperplane weight vector gives incorrect response. suppose example present patterns sequence y1, y2, y3, y4, start process weight point v1, shown fig. 4.7. starting v1, gives incorrect response pattern y1, v1 v2 direction normal plane 1. (that adding y1 v1 does.) y2 gives incorrect response pattern y2, on. ultimately, responses incorrect planes bounding solution region. subsequent corrections overshoot solution region, eventually work way far solution region corrections (for ﬁxed increment size) it. proofs convergence ﬁxed-increment rule intuitive argument precise. 2 3 4 1 v v1 v2 v3 v4 v5 v6 figure 4.7 moving solution region 4.1.5 widrow-hoﬀprocedure widrow-hoﬀprocedure (also called lms delta procedure) at- tempts ﬁnd weights minimize squared-error function pat- tern labels dot product computed tlu. purpose, pattern labels assumed +1 −1 (instead 1 0).\n",
      "\n",
      "4.1. threshold logic units 43 squared error pattern, xi, label di (for desired output) is εi = (di − n+1 x j=1 xijwj)2 xij j-th component xi. total squared error (over patterns training set, ξ, containing m patterns) then ε = m x i=1 (di − n+1 x j=1 xijwj)2 want choose weights wj minimize squared error. way ﬁnd set weights start arbitrary weight vector negative gradient ε function weights. ε quadratic wj, know global minimum, steepest descent procedure guaranteed ﬁnd minimum. component gradient partial derivative ε respect weights. problem taking partial derivative ε ε depends input vectors ξ. often, preferable use incremental procedure try tlu element, xi, ξ time, compute gradient single- pattern squared error, εi, appropriate adjustment weights, try member ξ. course, results incremental version approximate batch one, approximation usually eﬀective. describing incremental version here. j-th component gradient single-pattern error is ∂εi ∂wj = −2(di − n+1 x j=1 xijwj)xij adjustment direction negative gradient change weight follows wj ←−wj + ci(di −fi)xij fi = pn+1 j=1 xijwj, ci governs size adjustment. entire weight vector (in augmented, v, notation) adjusted according following rule v ←−v + ci(di −fi)yi where, before, yi i-th augmented pattern vector. widrow-hoﬀprocedure makes adjustments weight vector when- dot product itself, yi•v, equal speciﬁed desired target\n",
      "\n",
      "44 chapter 4. neural networks value, di (which 1 −1). learning-rate factor, ci, de- crease time 0 achieve asymptotic convergence. widrow- hoﬀformula changing weight vector form standard ﬁxed-increment error-correction formula. diﬀerence fi thresholded response tlu error-correction case dot product widrow-hoﬀprocedure. finding weight values desired dot products corresponds solv- ing set linear equalities, widrow-hoﬀprocedure interpreted descent procedure attempts minimize mean-squared-error be- tween actual desired values dot product. (for widrow- hoﬀand related procedures, [duda & hart, 1973, pp. 151ﬀ].) examples training curves tlu’s; performance training set; performance test set; cumulative number corrections. 4.1.6 training tlu non-linearly-separable training sets training set linearly separable (perhaps noise inherently), desired ﬁnd “best” separating hy- perplane. typically, error-correction procedures non- linearly-separable training sets continue attempt correct inevitable errors, hyperplane settle acceptable place. methods proposed deal case. first, use widrow-hoﬀprocedure, (although converge zero error non-linearly separable problems) weight vector min- imizes mean-squared-error. mean-squared-error criterion gives un- satisfactory results, however, prefers small errors large ones. alternative, error correction continuous decrease zero value learning rate constant, c, result decreasing changes hyperplane. duda [duda, 1966] suggested keeping track average value weight vector error correction average separating hyperplane performs reasonably non-linearly-separable problems. gallant [gallant, 1986] proposed called “pocket algo- rithm.” described [hertz, krogh, & palmer, 1991, p. 160] . . . pocket algorithm . . . consists simply storing (or “putting pocket”) set weights longest un- modiﬁed run successes far. algorithm stopped chosen time t . . . stopping, weights pocket set small number errors training set. error-correction proceeds usual ordinary set weights. methods proposed [john, 1995] [marchand & golea, 1993]. claimed outperform pocket algorithm. 4.2 linear machines natural generalization (two-category) tlu r-category classiﬁer structure, shown fig. 4.8, called linear machine. here, use\n",
      "\n",
      "4.2. linear machines 45 familiar notation, ws x meant augmented vectors (with (n+1)-st component). structure called “competitive” net “winner-take-all” net. output linear machine numbers, {1, . . . , r}, corresponding dot product largest. note r = 2, linear machine reduces tlu weight vector w = (w1 −w2). x w1 wr . . . y y argmax w1.x wr.x figure 4.8 linear machine diagram fig. 4.9 shows character regions 2-dimensional space created linear machine r = 5. n dimensions, pair regions separated section hyperplane non-adjacent. r1 r3 r4 r5 x.w4 * x.wi & 4 r2 region figure 4.9 regions linear machine train linear machine, straightforward generalization 2-category error-correction rule. assemble patterns training set sequence before. a. machine classiﬁes pattern correctly, change\n",
      "\n",
      "46 chapter 4. neural networks weight vectors. b. machine mistakenly classiﬁes category u pattern, xi, category v (u ̸= v), then wu ←−wu + cixi wv ←−wv −cixi weight vectors changed. correction increases value u-th dot product decreases value v-th dot product. 2-category ﬁxed increment proce- dure, procedure guaranteed terminate, constant ci, exists weight vectors correct separations training set. note r = 2, procedure reduces ordinary tlu error-correction procedure. proof procedure terminates given [nilsson, 1990, pp. 88-90] [duda & hart, 1973, pp. 174-177]. 4.3 networks tlus 4.3.1 motivation examples layered networks classify correctly patterns non-linearly-separable training sets re- quires separating surfaces complex hyperplanes. way achieve complex surfaces networks tlus. consider, example, 2- dimensional, parity function, f = x1x2 + x1 x2. single line 2-dimensional square separate vertices (1,1) (0,0) vertices (1,0) (0,1)—the function linearly separable im- plemented single tlu. but, network tlus shown fig. 4.10 implement function. ﬁgure, weight values input lines tlu threshold value inside circle representing tlu. function implemented network tlus depends topology weights individual tlus. feedforward networks cycles; feedforward network tlu’s input depends (through zero intermediate tlus) tlu’s output. (networks feedforward called recurrent networks). tlus feedforward network arranged layers, elements layer j receiving inputs tlus layer j −1, network layered, feedforward\n",
      "\n",
      "4.3. networks tlus 47 f x1 x2 1.5 -0.5 0.5 1 1 -1 -1 1 1 figure 4.10 network parity function network. network shown fig. 4.10 layered, feedforward network having layers (of weights). (some people count layers tlus include inputs layer also; network three-layer network.) general, feedforward, layered network structure shown fig. 4.11. tlus “output” units called hidden units (they “hidden” output). x hidden units output units figure 4.11 layered, feedforward network implementing dnf functions two-layer networks deﬁned k-term dnf functions—they dnf functions having k terms. k-term dnf function implemented two-layer network k units hidden layer—to implement k terms—and output unit implement disjunction terms. boolean function dnf form, boolean function implemented two-layer network tlus. example, consider function f = x1x2 + x2x3 + x1x3. form network implements function shown fig. 4.12. (we leave reader calculate appropriate values weights\n",
      "\n",
      "48 chapter 4. neural networks thresholds.) 3-cube representation function shown fig. 4.13. network fig. 4.12 designed hidden unit implements planar boundaries shown fig. 4.13. x conjuncts disjunct feedforward, 2-layer network tlus disjunction terms conjunctions literals (terms) figure 4.12 two-layer network x2 x1 x3 f = x1x2 + x2x3 + x1x3 figure 4.13 planes implemented hidden units train two-layer network implements k-term dnf function, ﬁrst note output unit implements disjunction, weights ﬁnal layer ﬁxed. weights ﬁrst layer (except “threshold weights”) values 1, −1, 0. later, present training procedure ﬁrst layer weights. discuss half-space intersections, half-space unions, np-hardness optimal versions, single-side-error-hypeplane methods, relation “aq” methods.\n",
      "\n",
      "4.3. networks tlus 49 important comment layered networks adding additional layers compensate inadequate ﬁrst layer tlus. ﬁrst layer tlus partitions feature space dif- ferently labeled vectors region (that is, vectors yield set outputs ﬁrst-layer units). ﬁrst layer partition feature space way, regardless subse- quent layers do, ﬁnal outputs consistent labeled training set. add diagrams showing non-linear transformation performed layered network. 4.3.2 madalines two-category networks interesting example layered, feedforward network two-layer odd number hidden units, “vote-taking” tlu output unit. network called “madaline” (for adalines widrow. typically, response vote taking unit deﬁned response majority hidden units, output logics possible. ridgway [ridgway, 1962] proposed following error-correction rule adjusting weights hidden units madaline • madaline correctly classiﬁes pattern, xi, corrections hidden units’ weight vectors, • madaline incorrectly classiﬁes pattern, xi, determine minimum number hidden units responses need changed (from 0 1 1 0—depending type error) order madaline correctly classify xi. suppose minimum number ki. hidden units voting incorrectly, change weight vectors ki dot products closest 0 error correction rule w ←−w + ci(di −fi)xi di desired response hidden unit (0 1) fi actual response (0 1). (we assume augmented vectors x, w notation.) is, perform error-correction hidden units correct vote majority voting correctly, change easiest change. example problems set weight values exists given madaline structure classify members training set correctly, procedure fail ﬁnd them. nevertheless, procedure works eﬀectively experiments it. leave reader think training procedure modiﬁed output tlu implemented function (or function).\n",
      "\n",
      "50 chapter 4. neural networks r-category madalines error-correcting output codes k hidden units (k > 1) two-layer network, responses correspond vertices k-dimensional hypercube. ordinary two-category madaline identiﬁes special points space, vertex consisting k 1’s vertex consisting k 0’s. madaline’s response 1 point “hidden-unit-space” closer 1’s vertex 0’s vertex. design r-category madaline identifying r vertices hidden-unit space classifying pattern according vertices hidden-unit response closest to. machine idea implemented early 1960s sri [brain, et al., 1962]. fact 2p so-called maximal-length shift-register sequences [peterson, 1961, pp. 147ﬀ] (2p −1)-dimensional boolean space mutually equidistant (for integer p). similar, recent work [dietterich & bakiri, 1991]. 4.3.3 piecewise linear machines two-category training set linearly separable exists threshold func- tion correctly classiﬁes members training set. similarly, r-category training set linearly separable exists linear machine correctly classiﬁes members training set. r- category problem linearly separable, need powerful classiﬁer. candidate structure called piecewise linear (pwl) machine illustrated fig. 4.14. x w1 w1 . . . y y max . . . y y max . . . wr wr arg max 1 r 1 n1 1 nr figure 4.14 piecewise linear machine\n",
      "\n",
      "4.3. networks tlus 51 pwl machine groups weighted summing units r banks corre- sponding r categories. input vector x assigned category corresponding bank largest weighted sum. use error- correction training algorithm similar linear machine. pattern classiﬁed incorrectly, subtract (a constant times) pattern vec- tor weight vector producing largest dot product (it incorrectly largest) add (a constant times) pattern vector weight vector correct bank weight vectors dot product locally largest bank. (again, use augmented vectors here.) unfortunately, example training sets separable given pwl machine structure error-correction training method fails ﬁnd solution. method appear work situations [duda & fossum, 1966], al- [nilsson, 1965, page 89] observed “it probably eﬀective method training pwl machines having [weight vectors] bank.” 4.3.4 cascade networks interesting class feedforward networks tlus ordered tlu receives inputs pattern components tlus lower ordering. network called cascade network. example shown fig. 4.15 tlus labeled linearly separable functions (of inputs) implement. tlu network implements set 2k parallel hyperplanes, k number tlus receives inputs. (each k preceding tlus output 1 0; that’s 2k diﬀerent combinations—resulting 2k diﬀerent positions parallel hyperplanes.) 3-dimensional sketch network tlus fig. 4.16. reader consider n-dimensional parity function implemented cascade network having log2 n tlus. x l1 l2 output l3 figure 4.15 cascade network\n",
      "\n",
      "52 chapter 4. neural networks l1 l2 l2 figure 4.16 planes implemented cascade network tlus cascade networks trained ﬁrst training l1 good job possible separating training patterns (perhaps pocket algorithm, example), training l2 (including weight l1 l2) good job possible separating training patterns, resulting network classiﬁes patterns training set satisfactorily. mention “cascade-correlation” method [fahlman & lebiere, 1990]. 4.4 training feedforward networks back- propagation 4.4.1 notation general problem training network tlus diﬃcult. consider, example, layered, feedforward network fig. 4.11. network makes error pattern, usually diﬀerent ways error corrected. diﬃcult assign “blame” error particular tlu network. intuitively, looks weight-adjusting procedures network correct direction (relative error) making minimal changes. spirit, widrow-hoﬀmethod gradient descent generalized deal multilayer networks. explaining generalization, use fig. 4.17 introduce nota- tion. network output unit, but, course, possible tlus output layer—each implementing diﬀerent function. layers tlus outputs components vectors, input features components input vector. j-th layer tlus (1 ≤j < k) outputs vector x(j). input feature vector denoted x(0), ﬁnal output (of k-th layer tlu) f. tlu layer weight vector (connecting inputs) threshold; i-th tlu j-th layer weight vector denoted w(j) . (we assume “threshold weight” component associated weight vector; v notation instead include\n",
      "\n",
      "4.4. training feedforward networks backpropagation53 threshold component, chosen use familiar x,w notation, assuming vectors “augmented” appropriate.) denote weighted sum input i-th threshold unit j-th layer s(j) . (that is, s(j) = x(j−1)•w(j) .) number tlus j-th layer given mj. vector w(j) components w(j) l,i l = 1, . . . , m(j−1) + 1. x(0) . . . . . . . . . . . . wi(1) w(k) x(1) m1 tlus . . . wi(j) . . . x(j) . . . wi(k-1) x(k-1) mj tlus m(k-1) tlus wli(j) wl(k) layer j-th layer (k-1)-th layer k-th layer . . . f si(1) si(j) si(k-1) s(k) figure 4.17 k-layer network 4.4.2 backpropagation method gradient descent method, similar widrow hoﬀmethod, proposed authors training multi-layer, feedforward network. before, deﬁne error function ﬁnal output network adjust weight network minimize error. desired response, di, i-th input vector, xi, training set, ξ, compute squared error entire training set be ε = x xi ϵ ξ (di −fi)2 fi actual response network input xi. gradient descent squared error, adjust weight network proportional negative partial derivative ε respect weight. again, use single-pattern error function use incremental weight adjustment procedure. squared error single input vector, x, evoking output f desired output d is\n",
      "\n",
      "54 chapter 4. neural networks ε = (d −f)2 convenient partial derivatives ε respect weights groups corresponding weight vectors. deﬁne partial derivative quantity φ, say, respect weight vector, w(j) , thus ∂φ ∂w(j) def = \" ∂φ ∂w(j) 1i , . . . , ∂φ ∂w(j) li , . . . , ∂φ ∂w(j) mj−1+1,i # w(j) li l-th component w(j) . vector partial derivative φ called gradient φ respect w denoted ∇wφ. ε’s dependence w(j) entirely s(j) , use chain rule write ∂ε ∂w(j) = ∂ε ∂s(j) ∂s(j) ∂w(j) s(j) = x(j−1)•w(j) , ∂s(j) ∂w(j) = x(j−1). substituting yields ∂ε ∂w(j) = ∂ε ∂s(j) x(j−1) note ∂ε ∂s(j) = −2(d −f) ∂f ∂s(j) . thus, ∂ε ∂w(j) = −2(d −f) ∂f ∂s(j) x(j−1) quantity (d−f) ∂f ∂s(j) plays important role calculations; shall denote δ(j) . δ(j) tells sensitive squared error network output changes input threshold function. changing weight vectors directions negative gradient, fundamental rule weight changes network be w(j) ←w(j) + c(j) δ(j) x(j−1) c(j) learning rate constant weight vector. (usually, learning rate constants weight vectors network same.) rule similar error correction procedure\n",
      "\n",
      "4.4. training feedforward networks backpropagation55 single tlu. weight vector changed addition constant times vector (unweighted) inputs. now, turn attention calculation δ(j) ’s. deﬁnition, have δ(j) = (d −f) ∂f ∂s(j) problem, however, attempting carry partial deriva- tives f respect s’s. network output, f, continuously diﬀerentiable respect s’s presence threshold functions. small changes sums change f all, f change, changes abruptly 1 0 vice versa. way diﬃculty proposed werbos [werbos, 1974] (perhaps independently) pursued researchers, example [rumelhart, hinton, & williams, 1986]. trick involves replacing threshold functions diﬀerentiable functions called sigmoids.1 output sigmoid function, superimposed threshold function, shown fig. 4.18. usually, sigmoid function f(s) = 1 1+e−s , s input f output. sigmoid threshold function f (s) s f (s) = 1/[1 + e<s] figure 4.18 sigmoid function 1[russell & norvig 1995, page 595] attributes use idea [bryson & ho 1969].\n",
      "\n",
      "56 chapter 4. neural networks network containing sigmoid units place tlus fig. 4.19. output i-th sigmoid unit j-th layer denoted f (j) . (that is, f (j) = 1 1+e−s(j) .) x(0) . . . . . . . . . . . . wi(1) si(1) w(k) x(1) fi(1) m1 sigmoids . . . wi(j) fi(j) si(j) . . . x(j) . . . wi(k-1) fi(k-1) si(k-1) f(k) s(k) x(k-1) mj sigmoids m(k-1) sigmoids wli(j) wl(k) bi(j) bi(1) bi(k-1) b(k) layer j-th layer (k-1)-th layer k-th layer . . . figure 4.19 network sigmoid units 4.4.3 computing weight changes final layer ﬁrst calculate δ(k) order compute weight change ﬁnal sigmoid unit\n",
      "\n",
      "4.4. training feedforward networks backpropagation57 δ(k) = (d −f (k))∂f (k) ∂s(k) given sigmoid function using, f(s) = 1 1+e−s , ∂f ∂s = f(1 −f). substituting gives us δ(k) = (d −f (k))f (k)(1 −f (k)) rewriting general rule weight vector changes, weight vector ﬁnal layer changed according rule w(k) ←w(k) + c(k)δ(k)x(k−1) δ(k) = (d −f (k))f (k)(1 −f (k)) interesting compare backpropagation error-correction rule widrow-hoﬀrule. backpropagation weight adjustment single element ﬁnal layer written as w ←−w + c(d −f)f(1 −f)x written format, error-correction rule is w ←−w + c(d −f)x widrow-hoﬀrule is w ←−w + c(d −f)x diﬀerence (except fact f thresholded widrow- hoﬀ) f(1 −f) term presence sigmoid function. sigmoid function, f(1 −f) vary value 0 1. f 0, f(1 −f) 0; f 1, f(1 −f) 0; f(1 −f) obtains maximum value 1/4 f 1/2 (that is, input sigmoid 0). sigmoid function thought implementing “fuzzy” hyperplane. pattern far away fuzzy hyperplane, f(1 −f) value close 0, backpropagation rule makes little change weight values regardless desired output. (small changes weights little eﬀect output inputs far hyperplane.) weight changes region “fuzz” surrounding hyperplane, changes direction correcting error, error-correction widrow-hoﬀrules.\n",
      "\n",
      "58 chapter 4. neural networks 4.4.4 computing changes weights intermediate layers expression δ’s, similarly compute change weight vectors network. recall δ(j) = (d −f) ∂f ∂s(j) use chain rule. ﬁnal output, f, depends s(j) summed inputs sigmoids (j + 1)-th layer. so δ(j) = (d −f) ∂f ∂s(j) = (d −f) \" ∂f ∂s(j+1) 1 ∂s(j+1) 1 ∂s(j) + · · · + ∂f ∂s(j+1) l ∂s(j+1) l ∂s(j) + · · · + ∂f ∂s(j+1) mj+1 ∂s(j+1) mj+1 ∂s(j) # = mj+1 x l=1 (d −f) ∂f ∂s(j+1) l ∂s(j+1) l ∂s(j) = mj+1 x l=1 δ(j+1) l ∂s(j+1) l ∂s(j) remains compute ∂s(j+1) l ∂s(j) ’s. ﬁrst write s(j+1) l = x(j)•w(j+1) l = mj+1 x ν=1 f (j) ν w(j+1) νl then, weights depend s’s ∂s(j+1) l ∂s(j) = ∂ hpmj+1 ν=1 f (j) ν w(j+1) νl ∂s(j) = mj+1 x ν=1 w(j+1) νl ∂f (j) ν ∂s(j) now, note ∂f (j) ν ∂s(j) = 0 ν = i, case ∂f (j) ν ∂s(j) ν = f (j) ν (1 −f (j) ν ). therefore ∂s(j+1) l ∂s(j) = w(j+1) il f (j) (1 −f (j) )\n",
      "\n",
      "4.4. training feedforward networks backpropagation59 use result expression δ(j) give δ(j) = f (j) (1 −f (j) ) mj+1 x l=1 δ(j+1) l w(j+1) il equation recursive δ’s. (it interesting note expression independent error function; error function explicitly aﬀects computation δ(k).) having computed δ(j+1) layer j + 1, use equation compute δ(j) ’s. base case δ(k), computed δ(k) = (d −f (k))f (k)(1 −f (k)) use expression δ’s generic weight changing rule, namely w(j) ←w(j) + c(j) δ(j) x(j−1) rule appears complex, intuitively reasonable explanation. quantity δ(k) = (d −f)f(1 −f) controls overall sign weight adjustments network. (adjustments diminish ﬁnal output, f, approaches 0 1, vanishing eﬀect f then.) recursion equation δ’s shows, adjustments weights going sigmoid unit j-th layer proportional eﬀect adjustments sigmoid unit’s output (its f (j)(1−f (j)) factor). proportional kind “average” eﬀect change output sigmoid unit ﬁnal output. average eﬀect depends weights going sigmoid unit j-th layer (small weights produce little downstream eﬀect) eﬀects changes outputs (j + 1)-th layer sigmoid units ﬁnal output (as measured δ(j+1)’s). calculations simply implemented “backpropagating” δ’s weights reverse direction (thus, backprop algorithm). 4.4.5 variations backprop [to written problem local minima, simulated annealing, momemtum (plaut, et al., 1986, [hertz, krogh, & palmer, 1991]), quickprop, regulariza- tion methods] simulated annealing apply simulated annealing, value learning rate constant gradually decreased time. fall early error-function valley deep (a local minimum), typically broad, soon\n",
      "\n",
      "60 chapter 4. neural networks subsequent large correction jostle it. likely deep valleys, end process (with small values learning rate constant), descend deepest point. process gets analogy annealing metallurgy, material’s temperature gradually decreased allowing crystalline structure reach minimal energy state. 4.4.6 application steering van neural network system called alvinn (autonomous land vehicle neural network) trained steer chevy van successfully ordinary roads highways speeds 55 mph [pomerleau, 1991, pomerleau, 1993]. input network derived low-resolution (30 x 32) television image. tv camera mounted van looks road straight ahead. image sampled produces stream 960-dimensional input vectors neural network. network shown fig. 4.20. 960 inputs 30 x 32 retina . . . 5 hidden units connected 960 inputs 30 output units connected hidden units . . . sharp left sharp right straight ahead centroid outputs steers vehicle figure 4.20 alvinn network network ﬁve hidden units ﬁrst layer 30 output units second layer; sigmoid units. output units arranged linear order control van’s steering angle. unit near array output units higher output units, van steered left; unit near array high output, van steered right. “centroid” responses output\n",
      "\n",
      "4.5. synergies neural network knowledge-based methods61 units computed, van’s steering angle set corresponding value hard left hard right. system trained modiﬁed on-line training regime. driver drives van, actual steering angles taken correct labels corresponding inputs. network trained incrementally backprop produce driver-speciﬁed steering angles response visual pattern occurs real time driving. simple procedure augmented avoid potential problems. first, driver usually driving well, network experience far-from-center vehicle positions and/or incorrect vehicle orien- tations. also, long, straight stretches road, network trained long time produce straight-ahead steering angles; training swamp earlier training follow curved road. wouldn’t want try avoid problems instructing driver drive erratically occasionally, system learn mimic erratic behavior. instead, original image shifted rotated software create 14 additional images vehicle appears situated diﬀerently relative road. model tells system steering angle ought shifted images, given driver-speciﬁed steering angle original image, system constructs additional 14 labeled training patterns add encountered ordinary driver training. 4.5 synergies neural network knowledge-based methods written; discuss rule-generating procedures (such [towell & shavlik, 1992]) expert-provided rules aid neural net training vice-versa [towell, shavlik, & noordweier, 1990]. 4.6 bibliographical historical remarks added.\n",
      "\n",
      "62 chapter 4. neural networks\n",
      "\n",
      "chapter 5 statistical learning 5.1 statistical decision theory 5.1.1 background general method suppose pattern vector, x, random variable probability distri- bution category 1 diﬀerent category 2. (the treatment given easily generalized r-category problems.) speciﬁcally, suppose probability distributions (perhaps probability density functions), p(x | 1) p(x | 2). given pattern, x, want use statistical tech- niques determine category—that is, determine distribution drawn. techniques based idea minimizing ex- pected value quantity similar error function deriving weight-changing rules backprop. developing decision method, necessary know relative serious- ness kinds mistakes made. (we decide pattern category 1 category 2, vice versa.) describe information loss function, λ(i | j), i, j = 1, 2. λ(i | j) represents loss incurred decide pattern category category j. assume λ(1 | 1) λ(2 | 2) 0. given pattern, x, want decide category way minimizes expected value loss. given pattern, x, decide category i, expected value loss be lx(i) = λ(i | 1)p(1 | x) + λ(i | 2)p(2 | x) p(j | x) probability given pattern x, category j. decision rule decide x belongs category 1 lx(1) ≤lx(2), decide category 2 otherwise. 63\n",
      "\n",
      "64 chapter 5. statistical learning use bayes’ rule expressions p(j | x) terms p(x | j), assume known (or estimatible) p(j | x) = p(x | j)p(j) p(x) p(j) (a priori) probability category j (one category probable other); p(x) (a priori) probability pattern x pattern asked classify. performing substitutions given bayes’ rule, decision rule becomes decide category 1 iﬀ λ(1 | 1)p(x | 1)p(1) p(x) + λ(1 | 2)p(x | 2)p(2) p(x) ≤λ(2 | 1)p(x | 1)p(1) p(x) + λ(2 | 2)p(x | 2)p(2) p(x) fact λ(i | i) = 0, noticing p(x) common expressions, obtain, decide category 1 iﬀ λ(1 | 2)p(x | 2)p(2) ≤λ(2 | 1)p(x | 1)p(1) λ(1 | 2) = λ(2 | 1) p(1) = p(2), decision particu- larly simple decide category 1 iﬀ p(x | 2) ≤p(x | 1) p(x | j) called likelihood j respect x, simple decision rule implements called maximum-likelihood decision. generally, deﬁne k(i | j) λ(i | j)p(j), decision rule simply, decide category1 iﬀ k(1 | 2)p(x | 2) ≤k(2 | 1)p(x | 1) case, need compare (perhaps weighted) quantities p(x | i) = 1 2. exact decision rule depends probability distributions assumed. treat interesting distributions.\n",
      "\n",
      "5.1. statistical decision theory 65 5.1.2 gaussian (or normal) distributions multivariate (n-dimensional) gaussian distribution given proba- bility density function p(x) = 1 (2π)n/2|σ|1/2 e −(x−m)tς −1 (x−m) 2 n dimension column vector x, column vector m called mean vector, (x −m)t transpose vector (x −m), σ covariance matrix distribution (an n × n symmetric, positive deﬁnite matrix), σ−1 inverse covariance matrix, |σ| determinant covariance matrix. mean vector, m, components (m1, . . . , mn), expected value x (using distribution); is, m = e[x]. components covariance matrix given by σ2 ij = e[(xi −mi)(xj −mj)] particular, σ2 ii called variance xi. formula appears complex, intuitive idea gaussian dis- tributions given n = 2. two-dimensional gaussian distribution fig. 5.1. three-dimensional plot distribution shown ﬁgure, contours equal probability shown bot- tom. case, covariance matrix, σ, elliptical contours equal probability skewed. covariance matrix diagonal, oﬀ-diagonal terms 0, major axes elliptical contours aligned coordinate axes. general principal axes given eigenvectors σ. case, equi-probability contours centered mean vector, m, ﬁgure happens origin. general, formula exponent gaussian distribution positive deﬁnite quadratic form (that is, value positive); equi-probability contours hyper-ellipsoids n-dimensional space. suppose assume classes pattern vectors want distinguish distributed according gaussian distribution diﬀerent means covariance matrices. is, class tends patterns clustered point n-dimensional space, class tends patterns clustered point. two-dimensional instance problem fig. 5.2. (in ﬁgure, plotted sum distributions.) decision rule use separate patterns appropriate categories? substituting gaussian distributions maximum likelihood for- mula yields\n",
      "\n",
      "66 chapter 5. statistical learning -5 0 5 -5 0 5 0 0.25 0.5 0.75 1 -5 0 5 -5 0 5 0 25 .5 75 1 -6 -4 -2 0 2 4 6 -6 -4 -2 0 2 4 6 x1 x2 p(x1,x2) 2 4 6 2 4 6 x1 x2 figure 5.1 two-dimensional gaussian distribution decide category 1 iﬀ 1 (2π)n/2|σ2|1/2 e−1/2(x−m2)tς −1 2 (x−m2) equal 1 (2π)n/2|σ1|1/2 e−1/2(x−m1)tς −1 1 (x−m1) category 1 patterns distributed mean covariance m1 σ1, respectively, category 2 patterns distributed mean covariance m2 σ2. result comparison isn’t changed compare logarithms instead. manipulation, decision rule then\n",
      "\n",
      "5.1. statistical decision theory 67 -5 0 5 10 -5 0 5 10 0 0.25 0.5 0.75 1 -5 0 5 10 -5 0 5 10 0 25 .5 75 1 x1 x2 p(x1,x2) -5 -2.5 0 2.5 5 7.5 10 -5 -2.5 0 2.5 5 7.5 10 figure 5.2 sum gaussian distributions decide category 1 iﬀ (x −m1)tς−1 1 (x −m1) < (x −m2)tς−1 2 (x −m2) + b b, constant bias term, incorporates logarithms fractions preceding exponential, etc. quadratic forms multiplied represented terms components xi, decision rule involves quadric surface (a hyperquadric) n-dimensional space. exact shape position hyperquadric determined means covariance matrices. surface separates space parts, contains points assigned category 1 contains points assigned category 2. interesting look special case surface. covariance matrices category identical diagonal, σii equal other, contours equal probability distributions\n",
      "\n",
      "68 chapter 5. statistical learning hyperspherical. quadric forms (1/|σ|)(x−mi)t(x−mi), decision rule is decide category 1 iﬀ (x −m1)t(x −m1) < (x −m2)t(x −m2) multiplying yields x•x −2x•m1 + m1•m1 < x•x −2x•m2 + m2•m2 ﬁnally, decide category 1 iﬀ x•m1 ≥x•m2 + constant x•(m1 −m2) ≥constant constant depends lengths mean vectors. optimal decision surface special case hyperplane. fact, hyperplane perpendicular line joining means. weights tlu implementation equal diﬀerence mean vectors. parameters (mi, σi) probability distributions categories known, techniques estimating them, estimates decision rule. example, suﬃcient training patterns, use sample means sample covariance matrices. (caution sample covariance matrix singular training patterns happen lie subspace n-dimensional space—as certainly will, example, number training patterns n.) 5.1.3 conditionally independent binary components suppose vector x random variable having binary (0,1) components. continue denote probability distributions p(x | 1) p(x | 2). suppose components vectors conditionally independent given category. conditional independence case, mean formulas distribution expanded follows\n",
      "\n",
      "5.1. statistical decision theory 69 p(x | i) = p(x1 | i)p(x2 | i) · · · p(xn | i) = 1, 2 recall minimum-average-loss decision rule, decide category 1 iﬀ λ(1 | 2)p(x | 2)p(2) ≤λ(2 | 1)p(x | 1)p(1) assuming conditional independence components λ(1 | 2) = λ(2 | 1), obtain, decide category 1 iﬀ p(1)p(x1 | 1)p(x2 | 1) · · · p(xn | 1) ≥p(x1 | 2)p(x2 | 2) · · · p(xn | 2)p(2) iﬀ p(x1 | 1)p(x2 | 1) . . . p(xn | 1) p(x1 | 2)p(x2 | 2) . . . p(xn | 2) ≥p(2) p(1) iﬀ log p(x1 | 1) p(x1 | 2) + log p(x2 | 1) p(x2 | 2) + · · · + log p(xn | 1) p(xn | 2) + log p(1) p(2) ≥0 let deﬁne values components distribution speciﬁc values arguments, xi p(xi = 1 | 1) = pi p(xi = 0 | 1) = 1 −pi p(xi = 1 | 2) = qi p(xi = 0 | 2) = 1 −qi now, note xi assume values 1 0 log p(xi | 1) p(xi | 2) = xi log pi qi + (1 −xi) log (1 −pi) (1 −qi)\n",
      "\n",
      "70 chapter 5. statistical learning = xi log pi(1 −qi) qi(1 −pi) + log (1 −pi) (1 −qi) substituting expressions decision rule yields decide category 1 iﬀ n x i=1 xi log pi(1 −qi) qi(1 −pi) + n x i=1 log (1 −pi) (1 −qi) + log p(1) p(2) ≥0 achieve decision tlu weight values follows wi = log pi(1 −qi) qi(1 −pi) = 1, . . . , n, wn+1 = log p(1) 1 −p(1) + n x i=1 log (1 −pi) (1 −qi) know pi, qi p(1), use sample labeled training patterns estimate parameters. 5.2 learning belief networks added. 5.3 nearest-neighbor methods class methods related statistical ones. called nearest-neighbor methods or, sometimes, memory-based methods. (a collection papers subject [dasarathy, 1991].) given training set ξ m labeled patterns, nearest-neighbor procedure decides new pattern, x, belongs category closest neighbors ξ. precisely, k-nearest-neighbor method assigns new pattern, x, category plurality k closest neighbors belong. relatively large values k decreases chance decision unduly inﬂuenced noisy training pattern close x. large values k reduce acuity method. k-nearest-neighbor method thought estimating values probabilities classes given x. course denser points x, larger value k, better estimate.\n",
      "\n",
      "5.3. nearest-neighbor methods 71 distance metric nearest-neighbor methods (for numerical at- tributes) simple euclidean distance. is, distance patterns (x11, x12, . . . , x1n) (x21, x22, . . . , x2n) qpn j=1(x1j −x2j)2. distance measure modiﬁed scaling features spread attribute values dimension approximately same. case, distance vectors qpn j=1 a2 j(x1j −x2j)2, aj scale factor dimension j. example nearest-neighbor decision problem shown fig. 5.3. ﬁgure class training pattern indicated number it. k = 8 x (a pattern classified) 1 1 1 1 1 1 1 1 2 1 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 training pattern class training pattern patterns category 1 patterns category 2 patterns category 3 plurality category 1, decide x category 1 figure 5.3 8-nearest-neighbor decision [baum, 1994] theoretical analysis error rate function number training patterns case points randomly distributed surface unit sphere underlying function linearly separable. nearest-neighbor methods memory intensive large number training patterns stored achieve good generalization. memory cost reasonably low, method derivatives seen practical applications. (see, example, [moore, 1992, moore, et al., 1994]. also, distance calculations required ﬁnd nearest neighbors eﬃciently computed kd-tree methods [friedman, et al., 1977]. theorem cover hart [cover & hart, 1967] relates performance 1-nearest-neighbor method performance minimum-probability- of-error classiﬁer. mentioned earlier, minimum-probability-of-error clas- siﬁer assign new pattern x category maximized p(i)p(x | i), p(i) priori probability category i, p(x | i) probability (or probability density function) x given x belongs category i, categories = 1, . . . , r. suppose probability error classifying patterns minimum-probability-of-error classiﬁer ε. cover-hart theo- rem states mild conditions (having smoothness\n",
      "\n",
      "72 chapter 5. statistical learning probability density functions) probability error, εnn, 1-nearest- neighbor classiﬁer bounded by ε ≤εnn ≤ε \u0012 2 −ε r r −1 \u0013 ≤2ε r number categories. [aha, 1991]. 5.4 bibliographical historical remarks added.\n",
      "\n",
      "chapter 6 decision trees 6.1 deﬁnitions decision tree (generally deﬁned) tree internal nodes tests (on input patterns) leaf nodes categories (of patterns). example fig. 6.1. decision tree assigns class number (or output) input pattern ﬁltering pattern tests tree. test mutually exclusive exhaustive outcomes. example, test t2 tree fig. 6.1 outcomes; left-most assigns input pattern class 3, middle sends input pattern test t4, right-most assigns pattern class 1. follow usual convention depicting leaf nodes class number.1 note discussing decision trees limited implementing boolean functions—they useful general, categorically valued functions. dimensions decision trees diﬀer a. tests multivariate (testing features input once) univariate (testing features). b. tests outcomes two. (if tests outcomes, binary decision tree.) c. features attributes categorical numeric. (binary-valued ones regarded either.) 1one researchers lot work learning decision trees ross quinlan. quinlan distinguishes classes categories. calls subsets patterns ﬁlter tip categories subsets patterns having label classes. quinlan’s terminology, example tree categories classes. distinction, however, use words “category” “class” interchangeably refer quinlan calls “class.” 73\n",
      "\n",
      "74 chapter 6. decision trees t1 t2 t3 t4 t4 t4 3 1 3 2 1 2 3 2 1 figure 6.1 decision tree d. classes two. classes binary inputs, tree implements boolean function, called boolean decision tree. straightforward represent function implemented univariate boolean decision tree dnf form. dnf form implemented tree obtained tracing path leading tip node corresponding output value 1, forming conjunction tests path, taking disjunction conjunctions. example fig. 6.2. drawing univariate decision trees, non-leaf node depicted single attribute. attribute value 0 input pattern, branch left; value 1, branch right. k-dl class boolean functions implemented multivariate decision tree having (highly unbalanced) form shown fig. 6.3. test, ci, term size k less. vi values 0 1. 6.2 supervised learning univariate decision trees systems learning decision trees proposed. prominent id3 new version, c4.5 [quinlan, 1986, quinlan, 1993], cart [breiman, et al., 1984] discuss batch methods, al- incremental ones proposed [utgoﬀ, 1989].\n",
      "\n",
      "6.2. supervised learning univariate decision trees 75 x3 x2 x4 x1 1 0 1 1 0 0 0 1 x3x2 x3x2 x3x4 x3x4x1 x3x4x1 f = x3x2 + x3x4x1 1 0 0 1 0 figure 6.2 decision tree implementing dnf function 6.2.1 selecting type test usual, n features attributes. attributes binary, tests simply attribute’s value 0 1. attributes categorical, non-binary, tests formed dividing attribute values mutually exclusive exhaustive subsets. decision tree tests shown fig. 6.4. attributes numeric, tests involve “interval tests,” example 7 ≤xi ≤13.2. 6.2.2 uncertainty reduction select tests main problem learning decision trees binary-attribute case selecting order tests. categorical numeric attributes, decide tests (besides selecting order). techniques tried; popular stage select test maximally reduces entropy-like measure. technique works simple case tests binary outcomes. extension multiple-outcome tests straightforward computation- ally gives poor results entropy decreased having outcomes. entropy uncertainty remaining class pattern— knowing set, ξ, patterns deﬁned as h(ξ) = − x p(i|ξ) log2 p(i|ξ)\n",
      "\n",
      "76 chapter 6. decision trees cq cq-1 ci 1 vn vn-1 vi v1 figure 6.3 decision tree implementing decision list p(i|ξ) probability pattern drawn random ξ belongs class i, summation classes. want select tests node travel decision tree, uncertainty class pattern less. general probabilities p(i|ξ), estimate sample statistics. estimates errorful, never- theless useful estimating uncertainties. let ˆp(i|ξ) number patterns ξ belonging class divided total number patterns ξ. estimate uncertainty is ˆh(ξ) = − x ˆp(i|ξ) log2 ˆp(i|ξ) simplicity, we’ll drop “hats” use sample statistics real probabilities. perform test, t, having k possible outcomes patterns ξ, create k subsets, ξ1, ξ2, . . . , ξk. suppose ni patterns ξ ξi = 1, ..., k. (some ni 0.) knew t applied pattern ξ resulted j-th outcome (that is, knew pattern ξj), uncertainty class be h(ξj) = − x p(i|ξj) log2 p(i|ξj) reduction uncertainty (beyond knowing pattern ξ) be\n",
      "\n",
      "6.2. supervised learning univariate decision trees 77 x3 = a, b, c, d {a, c} {b} x1 = e, b, d {e,b} {d} x4 = a, e, f, g {a, g} {e, f} x2 = a, g {a} {g} 1 2 1 1 2 {d} 2 figure 6.4 decision tree categorical attributes h(ξ) −h(ξj) course test t guaranteed produce reduction uncertainty don’t know result test j-th outcome. estimate average uncertainty ξj, by e[ht (ξ)] = x j p(ξj)h(ξj) ht (ξ) mean average uncertainty performing test t patterns ξ, p(ξj) probability test outcome j, sum taken 1 k. again, don’t know probabilities p(ξj), use sample values. estimate ˆp(ξj) p(ξj) number patterns ξ outcome j divided total number patterns ξ. average reduction uncertainty achieved test t (applied patterns ξ) then rt (ξ) = h(ξ) −e[ht (ξ)] important family decision tree learning algorithms selects root tree test gives maximum reduction uncertainty, applies criterion recursively termination condition met (which shall discuss detail later). uncertainty calculations particu- larly simple tests binary outcomes attributes\n",
      "\n",
      "78 chapter 6. decision trees binary values. we’ll simple example illustrate test selection mechanism works case. suppose want use uncertainty-reduction method build decision tree classify following patterns pattern class (0, 0, 0) 0 (0, 0, 1) 0 (0, 1, 0) 0 (0, 1, 1) 0 (1, 0, 0) 0 (1, 0, 1) 1 (1, 1, 0) 0 (1, 1, 1) 1 single test, x1, x2, x3, performed ﬁrst? illustration fig. 6.5 gives geometric intuition problem. x1 x2 x3 test x1 figure 6.5 patterns classiﬁed decision tree initial uncertainty set, ξ, containing points is h(ξ) = −(6/8) log2(6/8) −(2/8) log2(2/8) = 0.81 next, calculate uncertainty reduction perform x1 ﬁrst. left- hand branch patterns belonging class 0 (we set ξl), right-hand-branch (ξr) patterns class. so, uncertainty left-hand branch is\n",
      "\n",
      "6.3. networks equivalent decision trees 79 hx1(ξl) = −(4/4) log2(4/4) −(0/4) log2(0/4) = 0 uncertainty right-hand branch is hx1(ξr) = −(2/4) log2(2/4) −(2/4) log2(2/4) = 1 half patterns “go left” half “go right” test x1. thus, average uncertainty performing x1 test is 1/2hx1(ξl) + 1/2hx1(ξr) = 0.5 uncertainty reduction ξ achieved x1 is rx1(ξ) = 0.81 −0.5 = 0.31 similar calculations, test x3 achieves exactly uncertainty reduction, x2 achieves reduction whatsoever. thus, “greedy” algorithm selecting ﬁrst test select x1 x3. suppose x1 selected. uncertainty-reduction procedure select x3 test. decision tree procedure creates implements boolean function f = x1x3. [quinlan, 1986, sect. 4] example. 6.2.3 non-binary attributes attributes non-binary, use uncertainty-reduction tech- nique select tests. now, addition selecting attribute, select test attribute. suppose example value at- tribute real number test performed set threshold test number greater threshold. principle, given set labeled patterns, measure uncertainty reduc- tion test achieved possible threshold (there ﬁnite number thresholds diﬀerent test results ﬁnite number training patterns). similarly, attribute categorical (with ﬁnite number categories), ﬁnite number mutually exclusive exhaustive subsets values attribute split. calculate uncertainty reduction split. 6.3 networks equivalent decision trees univariate boolean decision trees implementations dnf functions, equivalent two-layer, feedforward neural networks. example fig. 6.6. decision tree left ﬁgure implements\n",
      "\n",
      "80 chapter 6. decision trees function network right ﬁgure. course, implemented network, features evaluated parallel input pattern, implemented decision tree features branch traveled input pattern need evaluated. decision-tree induction methods discussed chapter thought particular ways establish structure weight values networks. x x1 x2 x3 x4 terms -1 +1 disjunction x3x2 x3x4x1 +1 -1 +1 f 1.5 0.5 x3 x2 x4 x1 1 0 1 1 0 0 0 1 x3x2 x3x2 x3x4 x3x4x1 x3x4x1 f = x3x2 + x3x4x1 1 0 0 1 0 figure 6.6 univariate decision tree equivalent network multivariate decision trees linearly separable functions node implemented feedforward networks—in case three-layer ones. example fig. 6.7 linearly separable functions, im- plemented tlu, indicated l1, l2, l3, l4. again, ﬁnal layer ﬁxed weights, weights ﬁrst layers trained. dif- ferent approaches training procedures discussed [brent, 1990], [john, 1995], (for special case) [marchand & golea, 1993]. 6.4 overﬁtting evaluation 6.4.1 overﬁtting supervised learning, choose function ﬁt training set set hypotheses. showed generalization impossible bias. know priori function trying guess belongs small subset possible functions, then, incomplete set training samples, possible reduce subset functions consistent training set suﬃciently useful guesses value function inputs training set. and,\n",
      "\n",
      "6.4. overfitting evaluation 81 l1 l2 l3 l4 1 0 1 1 0 0 0 1 1 0 0 1 0 x l1 l2 l3 l4 conjunctions l1l2 l1 l3 l4 < + + + disjunction < f figure 6.7 multivariate decision tree equivalent network larger training set, likely randomly selected consistent function appropriate outputs patterns seen. however, bias, training set suﬃciently large compared size hypothesis space, consistent functions useful guesses, generalization performance poor. hypotheses consistent training set, overﬁtting training data. overﬁtting problem address learning methods. decision tree suﬃcient size implement boolean function danger overﬁtting—especially training set small. is, decision tree synthesized classify members training set correctly, perform poorly new patterns build decision tree. techniques proposed avoid overﬁtting, shall examine here. use methods estimating given decision tree generalize—methods shall describe next. 6.4.2 validation methods straightforward way estimate hypothesized function (such decision tree) performs test set test test set! but, comparing learning systems (for example, comparing diﬀerent decision trees) select performs best test set, comparison amounts “training test data.” true, training test data enlarges training set, consequent ex- pected improvement generalization, danger overﬁtting comparing diﬀerent learning systems. technique\n",
      "\n",
      "82 chapter 6. decision trees split training set—using (say) two-thirds training estimating generalization performance. splitting reduces size training set increases possibility overﬁtting. describe validation techniques attempt avoid problems. cross-validation cross-validation, divide training set ξ k mutually exclusive exhaustive equal-sized subsets ξ1, . . . , ξk. subset, ξi, train union subsets, empirically determine error rate, εi, ξi. (the error rate number classiﬁcation errors ξi divided number patterns ξi.) estimate error rate expected new patterns classiﬁer trained patterns ξ average εi. leave-one-out validation leave-one-out validation cross validation special case k equals number patterns ξ, ξi consists single pattern. testing ξi, simply note mistake made. count total number mistakes divide k estimated error rate. type validation is, course, expensive computationally, useful accurate estimate error rate classiﬁer needed. describe “bootstrapping” [efron, 1982]. 6.4.3 avoiding overﬁtting decision trees near tips decision tree patterns node. nodes, selecting test based small sample, likely overﬁtting. problem dealt terminating test-generating procedure patterns perfectly split separate categories. is, leaf node contain patterns class, decide favor numerous class. procedure result errors accepting small number errors training set results fewer errors testing set. behavior illustrated fig. 6.8. use cross-validation techniques determine stop splitting nodes. cross validation error increases consequence node split, don’t split. careful stop, though, underﬁtting usually leads errors test sets overﬁtting. general rule lowest error-rate attainable sub-tree fully expanded tree 1/2 error rate fully expanded tree [weiss & kulikowski, 1991, page 126].\n",
      "\n",
      "6.4. overfitting evaluation 83 (from weiss, s., kulikowski, c., computer systems learn, morgan kaufmann, 1991) training errors validation errors 1 2 3 4 5 6 7 8 9 0.2 0.4 0.6 0.8 1.0 0 0 error rate number terminal nodes iris data decision tree figure 6.8 determining overﬁtting begins stopping growth decision tree, grow size prune away leaf nodes ancestors cross- validation accuracy longer increases. technique called post-pruning. techniques pruning discussed [weiss & kulikowski, 1991]. 6.4.4 minimum-description length methods important tree-growing pruning technique based minimum- description-length (mdl) principle. (mdl important idea extends decision-tree methods [rissanen, 1978].) idea simplest decision tree predict classes training patterns best one. consider problem transmitting labels training set patterns, assuming receiver information ordered set patterns. m patterns, labeled r classes, transmit list m r-valued numbers. assuming equally probable classes, transmission require m log2 r bits. or, transmit decision tree correctly labelled patterns. number bits transmission require depends technique encoding decision trees size tree. tree small accurately classiﬁes patterns, economical transmit tree transmit labels directly. extremes, transmit tree plus list labels patterns tree misclassiﬁes. general, number bits (or description length binary encoded message) t + d, t length message required transmit tree, d length message required transmit labels\n",
      "\n",
      "84 chapter 6. decision trees patterns misclassiﬁed tree. sense, tree associated smallest value t + d best economical tree. mdl method way adhering occam’s razor principle. quinlan rivest [quinlan & rivest, 1989] proposed techniques encoding decision trees lists exception labels calculating description length (t+d) trees labels. use description length measure quality tree ways a. growing tree, use reduction description length select tests (instead reduction uncertainty). b. pruning tree grown zero error, prune away nodes (starting tips) achieve decrease description length. techniques compare favorably uncertainty-reduction method, sensitive coding schemes used. 6.4.5 noise data noise data means inevitably accept number errors—depending noise level. refusal tolerate errors training set noise leads problem “ﬁtting noise.” dealing noise, then, requires accepting errors leaf nodes fact small number patterns leaf nodes. 6.5 problem replicated subtrees decision trees economical means implementing boolean functions. consider, example, function f = x1x2 +x3x4. decision tree function shown fig. 6.9. notice replicated subtrees shown circled. dnf-form equivalent function implemented decision tree f = x1x2 + x1x2x3x4 + x1x3x4. dnf form non-minimal (in number disjunctions) equivalent f = x1x2 + x3x4. need replication means takes longer learn tree subtrees replicated tree learned smaller training subset. problem called fragmentation problem. approaches suggested dealing fragmenta- tion. attempt build decision graph instead tree [oliver, dowe, & wallace, 1992, kohavi, 1994]. decision graph imple- ments decisions decision tree fig. 6.9 shown fig. 6.10. approach use multivariate (rather univariate tests node). example learning f = x1x2 + x3x4, test x1x2\n",
      "\n",
      "6.6. problem missing attributes 85 x1 x3 x2 1 0 x4 0 1 x3 0 x4 0 1 figure 6.9 decision tree subtree replication test x3x4, decision tree simpliﬁed, shown fig. 6.11. researchers proposed techniques learning decision trees tests node linearly separable functions. [john, 1995] gives nice overview (with citations) learning linear discriminant trees presents method based “soft entropy.” method dealing replicated subtree problem involves ex- tracting propositional “rules” decision tree. rules an- tecedents conjunctions lead leaf nodes, consequents class corresponding leaf node. example rule tree repeating subtree example be x1 ∧¬x2 ∧x3 ∧x4 ⊃1. quinlan [quinlan, 1987] discusses methods reducing set rules sim- pler set 1) eliminating antecedent rule “unnecessary” conjuncts, 2) eliminating “unnecessary” rules. conjunct rule determined unnecessary elimination little eﬀect classiﬁcation accuracy—as determined chi-square test, example. rule set processed, case rule “active” given pattern, care taken active rules conﬂict decision class pattern.\n",
      "\n",
      "86 chapter 6. decision trees x1 x3 x2 1 0 x4 0 1 figure 6.10 decision graph 6.6 problem missing attributes added. 6.7 comparisons experimenters compared decision-tree, neural-net, nearest- neighbor classiﬁers wide variety problems. comparison neural nets versus decision trees, example, [dietterich, et al., 1990, shavlik, mooney, & towell, 1991, quinlan, 1994]. statlog project, [taylor, michie, & spiegalhalter, 1994] thorough comparisons machine learning algorithms diﬀerent types problems. x1x2 1 0 x3x4 1 figure 6.11 multivariate decision tree\n",
      "\n",
      "6.8. bibliographical historical remarks 87 single type classiﬁer best problems. and, general conclusions enable classiﬁer method best sorts classiﬁcation problems, [quinlan, 1994] provide intuition properties problems render ill suited decision trees, hand, backpropa- gation, other. 6.8 bibliographical historical remarks added.\n",
      "\n",
      "88 chapter 6. decision trees\n",
      "\n",
      "chapter 7 inductive logic programming diﬀerent representational forms functions input vari- ables. far, seen (boolean) algebraic expressions, decision trees, neural networks, plus computational mechanisms techniques computing nearest neighbors. course, representation important computer science computer program. example, lisp predicate binary-valued inputs computes boolean function inputs. similarly, logic program (whose ordinary application compute bindings variables) simply decide predicate value true (t) false (f). example, boolean exclusive-or (odd parity) function variables computed following logic program parity(x,y) - true(x), ¬ true(y) - true(y), ¬ true(x) follow prolog syntax (see, example, [mueller & page, 1988]), convention write variables strings beginning lower-case letters predicates strings beginning upper-case letters. unary function “true” returns t value argument t. (we think boolean functions arguments having values t f instead 0 1.) programs written “typewriter” font. chapter, consider matter learning logic programs given set variable values logic program return t (the positive instances) set variable values return f (the negative instances). subspecialty machine learning deals learning logic programs called inductive logic programming (ilp) [lavraˇc & dˇzeroski, 1994]. learning problem, complex intractably diﬃcult constrain biases sort. 89\n",
      "\n",
      "90 chapter 7. inductive logic programming ilp, variety possible biases (called language biases). restrict program horn clauses, allow recursion, allow functions, on. example ilp problem, suppose trying induce func- tion nonstop(x,y), value t pairs cities connected non-stop air ﬂight f pairs cities. given training set consisting positive negative examples. positive examples, (a,b), (a, a1), pairs; negative examples, (a1, a2), pairs. ilp, usually additional infor- mation examples, called “background knowledge.” air-ﬂight problem, background information ground facts hub(a), hub(b), satellite(a1,a), plus others. (hub(a) intended mean city denoted hub city, satellite(a1,a) intended mean city denoted a1 satellite city denoted a.) train- ing facts, want induce program nonstop(x,y), written terms background relations hub satellite, value t positive instances value f negative instances. depending exact set examples, induce program nonstop(x,y) - hub(x), hub(y) - satellite(x,y) - satellite(y,x) value t cities hub cities satellite other. learning problems, want induced program generalize well; is, presented arguments represented training set (but needed background knowledge), like function guess well. 7.1 notation deﬁnitions evaluating logic programs ilp, implicitly append background facts program adopt usual convention program value t set inputs program interpreter returns t actually running program (with background facts appended) inputs; oth- erwise value f. given background facts, program return t input (a, a1), example. logic program, π, returns t set arguments x, program covers arguments write covers(π, x). following terminology introduced connection version spaces, program suﬃcient covers positive instances necessary cover neg- ative instances. (that is, program implements suﬃcient condition training instance positive covers positive training instances;\n",
      "\n",
      "7.2. generic ilp algorithm 91 implements necessary condition covers negative instances.) noiseless case, want induce program suﬃcient nec- essary, case consistent. imperfect (noisy) training sets, relax criterion settle program covers fraction positive instances allowing cover fraction negative instances. illustrate deﬁnitions schematically fig. 7.1. < < < < < < < /1 necessary program /2 sufficient program /3 consistent program + + + + + + + + + + < < positive instance covered /2 /3 figure 7.1 suﬃcient, necessary, consistent programs version spaces, program suﬃcient necessary cover fewer examples specializing it. conversely, necessary suﬃcient, cover examples generalizing it. suppose attempting induce logic program compute relation ρ. general logic program, certainly suﬃcient, value t inputs, single clause body, [ρ - ], called fact prolog. special logic program, certainly necessary, value f inputs, [ρ - f ]. diﬀerent ways search consistent logic program are 1) start [ρ - ] specialize program consistent, 2) start [ρ - f ] generalize program consistent. discussing method starts [ρ - ], specializes program necessary (but longer suﬃcient), reachieves suﬃciency stages generalizing—ensuring stage program remains necessary (by specializing). 7.2 generic ilp algorithm primary operators search consistent program special- ization generalization, discuss operations.\n",
      "\n",
      "92 chapter 7. inductive logic programming major ways logic program generalized a. replace terms program clause variables. (readers familiar substitutions predicate calculus note process inverse substitution.) b. remove literals body clause. c. add clause program analogously, ways logic program specialized a. replace variables program clause terms (a substitution). b. add literals body clause. c. remove clause program presenting ilp learning method adds clauses program generalizing adds literals body clause special- izing. add clause, add clause [ρ - ] specialize adding literals body. thus, need describe process adding literals. clauses partially ordered specialization relation. general, clause c1 special clause c2 c2 |= c1. special case, use here, clause c1 special clause c2 set literals body c2 subset c1. ordering relation structure partially ordered clauses, called reﬁnement graph, similar version space. clause c1 immediate successor clause c2 graph clause c1 obtained clause c2 adding literal body c2. reﬁnement graph tells ways specialize clause adding literal it. course unlimited possible literals add body clause. practical ilp systems restrict literals ways. typical allowed additions are a. literals background knowledge. b. literals arguments subset head clause. c. literals introduce new distinct variable diﬀerent head clause. d. literal equates variable head clause variable term mentioned background knowledge. (this possibility equivalent forming specialization making substitution.)\n",
      "\n",
      "7.2. generic ilp algorithm 93 e. literal (except arguments) head clause. (this possibility admits recursive programs, dis- allowed systems.) illustrate possibilities air-ﬂight example. start program [nonstop(x,y) - ]. literals background knowledge hub satellite. literals consider adding are hub(x) hub(y) hub(z) satellite(x,y) satellite(y,x) satellite(x,z) satellite(z,y) (x = y) (if recursive programs allowed, add literals nonstop(x,z) nonstop(z,y).) possibilities illustrated re- ﬁnement graph shown fig. 7.2. restrictions additional literals imposed, syntactic ones successors reﬁne- ment graph easily computed. ilp programs follow approach discussing (of specializing clauses adding literal) deﬁned methods computing possible literals add clause. ready write simple generic algorithm inducing logic program, π inducing relation ρ. given training set, ξ argument sets known relation ρ ρ; ξ+ positive instances, ξ−are negative instances. algorithm outer loop successively adds clauses π suﬃcient. inner loop constructing clause, c, necessary refers subset, ξcur, training instances. (the positive instances ξcur denoted ξ+ cur, negative ones ξ− cur.) algorithm given background relations means adding literals clause. uses logic program interpreter compute program inducing covers training instances. algorithm written follows generic ilp algorithm (adapted [lavraˇc & dˇzeroski, 1994, p. 60].)\n",
      "\n",
      "94 chapter 7. inductive logic programming nonstop(x,y) - nonstop(x,y) - hub(x) nonstop(x,y) - satellite(x,y) nonstop(x,y) - (x = y) . . . . . . . . . . . . nonstop(x,y) - hub(x), hub(y) . . . . . . . . . figure 7.2 reﬁnement graph initialize ξcur = ξ. initialize π = set clauses. repeat [the outer loop works π suﬃcient.] initialize c = ρ −. repeat [the inner loop makes c necessary.] select literal l add c. [this nondeterministic choice point.] assign c = c, l. c necessary. [that is, c covers negative instances ξcur.] assign π = π, c. [we add clause c program.] assign ξcur = ξcur −(the positive instances ξcur covered π). π suﬃcient. (the termination tests inner outer loops relaxed appro- priate case noisy instances.) 7.3 example illustrate algorithm works returning example airline ﬂights. consider portion airline route map, shown fig. 7.3. cities a, b, c “hub” cities, know nonstop ﬂights hub cities (even shown portion route map).\n",
      "\n",
      "7.3. example 95 cities “satellites” hubs, know nonstop ﬂights satellite city hub. learning program given set positive instances, ξ+, pairs cities nonstop ﬂights set negative instances, ξ−, pairs cities nonstop ﬂights. ξ+ contains pairs {< a, b >, < a, c >, < b, c >, < b, >, < c, >, < c, b >, < a, a1 >, < a, a2 >, < a1, >, < a2, >, < b, b1 >, < b, b2 >, < b1, b >, < b2, b >, < c, c1 >, < c, c2 >, < c1, c >, < c2, c >} example, assume ξ−contains pairs cities shown fig. 7.3 ξ+ (a type closed-world assumption). are {< a, b1 >, < a, b2 >, < a, c1 >, < a, c2 >, < b, c1 >, < b, c2 >, < b, a1 >, < b, a2 >, < c, a1 >, < c, a2 >, < c, b1 >, < c, b2 >, < b1, >, < b2, >, < c1, >, < c2, >, < c1, b >, < c2, b >, < a1, b >, < a2, b >, < a1, c >, < a2, c >, < b1, c >, < b2, c >} cities shown map, training set necessarily exhaust cities. b c c1 c2 b1 b2 a1 a2 figure 7.3 airline route map want learning program induce program computing value relation nonstop. training set, ξ, thought partial\n",
      "\n",
      "96 chapter 7. inductive logic programming description relation extensional form—it explicitly names pairs relation pairs relation. desire learn nonstop relation logic program terms background relations, hub satellite, given extensional form. compact, intensional, description relation, description generalize usefully cities mentioned map. assume learning program following extensional deﬁnitions relations hub satellite hub {< >, < b >, < c >} cities mentioned map assumed relation hub. use notation hub(x) express city named x relation hub. satellite {< a1, a, >, < a2, >, < b1, b >, < b2, b >, < c1, c >, < c2, c >} pairs cities mentioned map relation satellite. use notation satellite(x,y) express pair < x, y > relation satellite. knowing predicate nonstop two-place predicate, inner loop algorithm initializes ﬁrst clause nonstop(x,y) - . clause necessary covers negative examples (since covers examples). add literal (empty) body. suppose (selecting literal reﬁnement graph) algorithm adds hub(x). following positive instances ξ covered nonstop(x,y) - hub(x) {< a, b >, < a, c >, < b, c >, < b, >, < c, >, < c, b >, < a, a1 >, < a, a2 >, < b, b1 >, < b, b2 >, < c, c1 >, < c, c2 >} compute covering, interpret logic program nonstop(x,y) - hub(x) pairs cities ξ, pairs given background relation hub ground facts. following negative instances covered\n",
      "\n",
      "7.3. example 97 {< a, b1 >, < a, b2 >, < a, c1 >, < a, c2 >, < c, a1 >, < c, a2 >, < c, b1 >, < c, b2 >, < b, a1 >, < b, a2 >, < b, c1 >, < b, c2 >} thus, clause necessary literal added. sup- pose add hub(y). following positive instances covered nonstop(x,y) - hub(x), hub(y) {< a, b >, < a, c >, < b, c >, < b, >, < c, >, < c, b >} longer negative instances ξ covered clause nonstop(x,y) - hub(x), hub(y) necessary, terminate ﬁrst pass inner loop. program, π, consisting clause suﬃcient. positive instances covered clause {< a, a1 >, < a, a2 >, < a1, >, < a2, >, < b, b1 >, < b, b2 >, < b1, b >, < b2, b >, < c, c1 >, < c, c2 >, < c1, c >, < c2, c >} positive instances covered nonstop(x,y) - hub(x), hub(y) removed ξ form ξcur pass inner loop. ξcur consists negative instances ξ plus positive instances (listed above) covered. order attempt cover them, inner loop creates clause c, initially set nonstop(x,y) - . clause covers negative instances, add liter- als necessary. suppose add literal satellite(x,y). clause nonstop(x,y) - satellite(x,y) covers negative instances, necessary. cover following positive instances ξcur {< a1, >, < a2, >, < b1, b >, < b2, b >, < c1, c >, < c2, c >} instances removed ξcur pass inner loop. program contains clauses nonstop(x,y) - hub(x), hub(y) - satellite(x,y) program suﬃcient cover following positive instances {< a, a1 >, < a, a2 >, < b, b1 >, < b, b2 >, < c, c1 >, < c, c2 >}\n",
      "\n",
      "98 chapter 7. inductive logic programming pass inner loop, add clause nonstop(x,y) - satellite(y,x). clause necessary, program containing clauses suﬃcient, procedure terminates with nonstop(x,y) - hub(x), hub(y) - satellite(x,y) - satellite(y,x) clause necessary, program suﬃcient, pro- gram consistent instances training set. note program applied (perhaps good generalization) cities be- sides partial map—so long evaluate relations hub satellite cities. section, technique extended use recursion relation inducing. extension, method induce general logic programs. 7.4 inducing recursive programs induce recursive program, allow addition literal having predicate letter head clause. mechanisms ensure program terminate; sure new literal diﬀerent variables head literal. process best illustrated example. example continues airline map, map somewhat simpler order reduce size extensional relations used. consider map shown fig. 7.4. again, b c hub cities, b1 b2 satellites b, c1 c2 satellites c. introduced new cities, b3 c3. ﬂights exist cities cities—perhaps bus routes shown grey lines map. seek learn program canfly(x,y) covers pairs cities reached nonstop ﬂights. relation canfly satisﬁed following pairs postive instances {< b1, b >, < b1, b2 >, < b1, c >, < b1, c1 >, < b1, c2 >, < b, b1 >, < b2, b1 >, < c, b1 >, < c1, b1 >, < c2, b1 >, < b2, b >, < b2, c >, < b2, c1 >, < b2, c2 >, < b, b2 >, < c, b2 >, < c1, b2 >, < c2, b2 >, < b, c >, < b, c1 >, < b, c2 >, < c, b >, < c1, b >, < c2, b >, < c, c1 >, < c, c2 >, < c1, c >, < c2, c >, < c1, c2 >, < c2, c1 >}\n",
      "\n",
      "7.4. inducing recursive programs 99 b c c1 c2 b1 b2 b3 c3 figure 7.4 airline route map closed-world assumption map, negative instances canfly be {< b3, b2 >, < b3, b >, < b3, b1 >, < b3, c >, < b3, c1 >, < b3, c2 >, < b3, c3 >, < b2, b3 >, < b, b3 >, < b1, b3 >, < c, b3 >, < c1, b3 >, < c2, b3 >, < c3, b3 >, < c3, b2 >, < c3, b >, < c3, b1 >, < c3, c >, < c3, c1 >, < c3, c2 >, < b2, c3 >, < b, c3 >, < b1, c3 >, < c, c3 >, < c1, c3 >, < c2, c3 >} induce canfly(x,y) extensionally deﬁned background relation nonstop given earlier (modiﬁed required reduced airline map) canfly (recursively). before, start program proceed inner loop construct clause necessary. suppose inner loop adds background literal nonstop(x,y). clause canfly(x,y) - nonstop(x,y) necessary; covers negative instances. suﬃcient cover following positive instances {< b1, b2 >, < b1, c >, < b1, c1 >, < b1, c2 >, < b2, b1 >, < c, b1 >, < c1, b1 >, < c2, b1 >, < b2, c >, < b2, c1 >, < b2, c2 >, < c, b2 >, < c1, b2 >, < c2, b2 >, < b, c1 >,\n",
      "\n",
      "100 chapter 7. inductive logic programming < b, c2 >, < c1, b >, < c2, b >, < c1, c2 >, < c2, c1 >} thus, add clause program. inner loop, ﬁrst create clause canfly(x,y) - nonstop(x,z) introduces new variable z. digress brieﬂy describe program containing clause unbound variables body interpreted. suppose try inter- pret positive instance canfly(b1,b2). interpreter attempts establish nonstop(b1,z) z. nonstop(b1, b), example, background fact, interpreter returns t—which means instance < b1, b2 > covered. suppose now, attempt interpret clause negative instance canfly(b3,b). interpreter attempts estab- lish nonstop(b3,z) z. background facts match, clause cover < b3, b >. interpreter, clause canfly(x,y) - nonstop(x,z) covers positive instances covered ﬁrst clause, covers negative instances < b2, b3 >, < b, b3 >. inner loop add literal. time, suppose adds canfly(z,y) yield clause canfly(x,y) - nonstop(x,z), canfly(z,y). clause necessary; negative instances covered. program suﬃcient consistent; is canfly(x,y) - nonstop(x,y) - nonstop(x,z), canfly(z,y) 7.5 choosing literals add ﬁrst practical ilp systems quinlan’s foil [quinlan, 1990]. major problem involves deciding select literal add inner loop (from literals allowed). foil, quinlan suggested candidate literals compared information-like measure—similar measures inducing decision trees. measure gives comparison quinlan’s based adding literal increases odds instance drawn random covered new clause positive instance odds adding literal. let p estimate probability instance drawn random covered clause adding literal positive instance. is, p =(number positive instances covered clause)/(total number instances covered clause). convenient express probability “odds form.” odds, o, covered instance positive deﬁned o = p/(1 −p). expressing probability terms odds, obtain p = o/(1 + o).\n",
      "\n",
      "7.6. relationships ilp decision tree induction101 selecting literal, l, add clause, instances previously covered covered; positive negative. let pl denote probability instance drawn random instances covered new clause (with l added) positive. odds denoted ol. want select literal, l, gives maximal increase odds. is, deﬁne λl = ol/o, want literal gives high value λl. specializing clause way fails cover negative instances previously covered covers positive instances previously covered result high value λl. (it turns value quinlan’s information theoretic measure increases monotonically λl, use instead.) ﬁnding literal high value λl, quinlan’s foil system restricts choice literals that a) contain variable used, b) place restrictions variables literal selected predicate letter literal induced (in order prevent inﬁnite recursion), c) survive pruning test based values λl literals selected far. refer reader quinlan’s paper discussion points. quinlan discusses post-processing pruning methods presents experi- mental results method applied learning recursive relations lists, learning rules chess endgames card game eleusis, standard tasks mentioned machine learning literature. reader refer [pazzani & kibler, 1992, lavraˇc & dˇzeroski, 1994, muggleton, 1991, muggleton, 1992]. discuss preprocessing, postprocessing, bottom-up methods, linus. 7.6 relationships ilp decision tree induction generic ilp algorithm understood type decision tree induction. recall problem inducing decision trees values attributes categorical. splitting single variable, split node involves asking mutually exclusive exhaustive subsets value variable belongs. example, node tested variable xi, xi values drawn {a, b, c, d, e, f}, possible split (among many) according value xi value {a, b, c} {d, e, f}. possible multi-variate split—testing values variables time. categorical variables, n-variable split based n-ary relations values variables satisﬁed. example, node tested variables xi xj, xi xj values drawn {a, b, c, d, e, f}, possible binary split\n",
      "\n",
      "102 chapter 7. inductive logic programming (among many) according < xi, xj > satisﬁed relation {< a, c >, < c, d >}. (note subset method forming single- variable splits equivalently framed 1-ary relations—which usually called properties.) framework, ilp problem follows given training set, ξ, positively negatively labeled patterns components drawn set variables {x, y, z, . . .}. positively labeled patterns ξ form extensional deﬁnition relation, r. given background relations, r1, . . . , rk, subsets variables. (that is, given sets tuples relations.) desire construct intensional deﬁnition r terms r1, . . . , rk, positively labeled patterns ξ satisﬁed r negatively labeled patterns are. intensional deﬁnition terms logic program relation r head set clauses bodies involve background relations. generic ilp algorithm understood decision tree induction, node decision tree sub-decision tree, sub- decision tree consists nodes binary splits variables background relations, ri. speak top-level decision tree sub-decision trees. (actually, decision trees decision lists—a special case decision trees, refer trees discussions.) broad outline, method inducing intensional version rela- tion r illustrated considering decision tree shown fig. 7.5. diagram, patterns ξ ﬁrst ﬁltered decision tree top- level node 1. background relation r1 satisﬁed patterns; ﬁltered right (to relation r2), rest ﬁltered left (more happens later). right-going patterns ﬁltered sequence relational tests positively labeled patterns sat- isfy relation—in case r3. is, subset patterns satisfying relations, r1, r2, r3 contains positive instances ξ. (we combination tests necessary. correspond clause created ﬁrst pass inner loop generic ilp algo- rithm.) let subset patterns satisfying relations, ξ1; satisfy node 1 level. patterns, {ξ −ξ1} = ξ2 ﬁltered left node 1. ξ2 ﬁltered top-level node 2 manner, node 2 satisﬁed positively labeled samples ξ2. continue ﬁltering top-level nodes negatively labeled patterns fail satisfy node. example, ξ4 contains negatively labeled patterns union ξ1 ξ3 contains positively labeled patterns. relation, r, distinguishes positive negative patterns ξ given terms following logic program r - r1, r2, r3\n",
      "\n",
      "7.6. relationships ilp decision tree induction103 r1 r2 r3 t t t f f f t f r4 r5 t t f f t f u u1 u2 = u < u1 u3 u4= u2 < u3 node 1 node 2 (only positive instances satisfy tests) (only positivel instances satisfy tests) (only negative instances) figure 7.5 decision tree ilp - r4, r5 apply sort decision-tree induction procedure problem generating logic program relation nonstop (refer fig. 7.3), obtain decision tree shown fig. 7.6. logic program resulting decision tree produced generic ilp algorithm. setting problem, training set, ξ expressed set 2- dimensional vectors components x y. values components range cities {a, b, c, a1, a2, b1, b2, c1, c2} (for simplicity) allow patterns x y value. before, relation, nonstop, contains following pairs cities, positive instances {< a, b >, < a, c >, < b, c >, < b, >, < c, >, < c, b >, < a, a1 >, < a, a2 >, < a1, >, < a2, >, < b, b1 >, < b, b2 >, < b1, b >, < b2, b >, < c, c1 >, < c, c2 >, < c1, c >, < c2, c >} pairs cities named map fig. 7.3 (using closed world assumption) relation nonstop negative instances. values x y categorical, decision-tree induction diﬃcult task—involving need invent relations\n",
      "\n",
      "104 chapter 7. inductive logic programming x y tests. background relations, ri (in case hub satellite), problem easier. select relations way select literals; available tests, selection based leads largest value λri. 7.7 bibliographical historical remarks added.\n",
      "\n",
      "7.7. bibliographical historical remarks 105 hub(x) t f u node 1 (top level) {<a,b>, <a,c>, <b,c>, <b,a>, <c,a>, <c,b>} hub(y) t t f node 2 (top level) satellite(x,y) f t t {<a1,a>, <a2,a>, <b1,b>, <b2,b>, <c1,c>, <c2,c>} f {<a,a1>, <a,a2>,<b,b1>, <b,b2>, <c,c1>, <c,c2>} satellite(y,x) f f t node 3 (top level) t {only negative instances} (only positive instances) (only positive instances) (only positive instances) f figure 7.6 decision tree airline route problem\n",
      "\n",
      "106 chapter 7. inductive logic programming\n",
      "\n",
      "chapter 8 computational learning theory chapter posed problem guessing function given set sample inputs values. gave intuitive arguments support claim seeing small fraction possible inputs (and values) guess correctly values subsequent inputs—if knew function trying guess belonged appropriately restricted subset functions. is, given training set sample patterns adequate allow select function, consistent labeled samples, restricted set hypotheses high probability function select approximately correct (small probability error) subsequent samples drawn random according distribution labeled samples drawn. insight led theory probably approximately correct (pac) learning—initially developed leslie valiant [valiant, 1984]. present brief description theory case boolean functions. [dietterich, 1990, haussler, 1988, haussler, 1990] nice surveys important results. overviews? 8.1 notation assumptions pac learn- ing theory assume training set ξ n-dimensional vectors, xi, = 1, . . . , m, labeled (by 1 0) according target function, f, unknown learner. probability given vector x ξ, later presented learner, p(x). probability distribution, p, arbitrary. (in literature pac learning theory, target function usually called target concept denoted c, consistent previous notation continue denote f.) problem guess 107\n",
      "\n",
      "108 chapter 8. computational learning theory function, h(x), based labeled samples ξ. pac theory guessed function called hypothesis. assume target function element set functions, c. assume hypothesis, h, element set, h, hypotheses, includes set, c, target functions. h called hypothesis space. general, h won’t identical f, strive value h(x) = value f(x) x’s. is, want h approximately correct. quantify notion, deﬁne error h, εh, probability x drawn randomly according p misclassiﬁed εh = x [xh(x)̸=f(x)] p(x) boldface symbols need smaller subscripts math environments. h approximately (except ε ) correct εh ≤ε, ε accuracy parameter. suppose able ﬁnd h classiﬁes m randomly drawn training samples correctly; is, h consistent randomly selected training set, ξ. m large enough, h approximately correct (and value ε)? training occasions, m randomly drawn training samples, h turn approximately correct (for given value ε), not. h probably (except δ) approximately correct (pac) probability approximately correct greater 1−δ, δ conﬁdence parameter. shall m greater bound value depends ε δ, h guaranteed probably approximately correct. general, learning algorithm pac-learns functions c terms h iﬀfor function fϵ c, outputs hypothesis hϵ h, probability (1 −δ), εh ≤ε. hypothesis called probably (except δ) approximately (except ε) correct. want learning algorithms tractable, want algorithm pac-learns functions polynomial time. certain classes functions. ﬁnite number hypotheses hypothesis set (as hypothesis sets considered), produce consistent hypothesis set testing training data. exponential number hypotheses, exponential time. seek training methods produce consistent hypotheses time. time complexities hypothesis sets determined, summarized table presented later. class, c, polynomially pac learnable terms h provided exists polynomial-time learning algorithm (polynomial number samples needed, m, dimension, n, 1/ε, 1/δ) pac-learns functions c terms h. initial work pac assumed h = c, later shown func- tions polynomially pac-learned assumption (assuming\n",
      "\n",
      "8.2. pac learning 109 p ̸= np)—but polynomially pac-learned h strict superset c! deﬁnition specify distribution, p, patterns drawn properties learning algo- rithm. c h identical, restrictive deﬁnition properly pac-learnable class class c exists algorithm polynomially pac-learns functions c terms c. 8.2 pac learning 8.2.1 fundamental theorem suppose learning algorithm selects h randomly consistent values f m training patterns. probability error randomly selected h greater ε, h consis- tent values f(x) m instances x (drawn according arbitrary p), equal |h|e−εm, |h| number hypotheses h. state result theorem [blumer, et al., 1987] theorem 8.1 (blumer, et al.) let h set hypotheses, ξ set m ≥1 training examples drawn independently according distribution p, f classiﬁcation function h, ε > 0. then, probability exists hypothesis h consistent f members ξ error greater ε |h|e−εm. proof consider set hypotheses, {h1, h2, . . . , hi, . . . , hs}, h, s = |h|. error hi εhi= probability hi classify pattern error (that is, diﬀerently f classify it). probability hi classify pattern correctly (1−εhi). subset, hb, h error greater ε. hypotheses subset bad. probability particular bad hypotheses, hb, classify pattern correctly (1−εhb). εhb > ε, probability hb (or bad hypothesis) classify pattern correctly (1 −ε). probability classify m independently drawn patterns correctly (1 −ε)m. is, prob[hb classiﬁes m patterns correctly |hb ϵ hb] ≤(1 −ε)m. prob[some h ϵ hb classiﬁes m patterns correctly] = p hb ϵ hb prob[hb classiﬁes m patterns correctly |hb ϵ hb] ≤k(1 −ε)m, k = |hb|.\n",
      "\n",
      "110 chapter 8. computational learning theory is, prob[there bad hypothesis classiﬁes m patterns correctly] ≤k(1 −ε)m. k ≤|h| (1 −ε)m ≤e−εm, have prob[there bad hypothesis classiﬁes m patterns correctly] = prob[there hypothesis error > ε classiﬁes m patterns correctly] ≤|h|e−εm. qed corollary theorem is corollary 8.2 given m ≥(1/ε)(ln |h| + ln(1/δ)) independent samples, probability exists hypothesis h consistent f samples error greater ε δ. proof ﬁnd bound m guarantees prob[there hypothesis error > ε classiﬁes m patterns correctly] ≤δ. thus, result theorem, |h|e−εm ≤δ. taking natural logarithm sides yields ln |h| −εm ≤ln δ m ≥(1/ε)(ln |h| + ln(1/δ)) qed corollary important reasons. clearly states select hypothesis consistent m samples assured probability (1 −δ) error ε. also, shows order m increase polynomially n, |h| larger 2o(nk). class larger guaranteed properly pac learnable. possible point confusion bound given corollary upper bound value m needed guarantee polynomial probably ap- proximately correct learning. values m greater bound suﬃcient (but necessary). present lower (necessary) bound later chapter.\n",
      "\n",
      "8.2. pac learning 111 8.2.2 examples terms let h set terms (conjunctions literals). then, |h| = 3n, m ≥(1/ε)(ln(3n) + ln(1/δ)) ≥(1/ε)(1.1n + ln(1/δ)) note bound m increases polynomially n, 1/ε, 1/δ. n = 50, ε = 0.01 δ = 0.01, m ≥5, 961 guarantees pac learnability. order terms properly pac learnable, additionally ﬁnd time polynomial m n hypothesis h consistent set m patterns labeled value term. following procedure ﬁnding consistent hypothesis requires o(nm) steps (adapted [dietterich, 1990, page 268]) given training sequence, ξ, m examples. find ﬁrst pattern, x1, list labeled 1. initialize boolean function, h, conjunction n literals corresponding values n components x1. (components value 1 corresponding positive literals; components value 0 corresponding negative literals.) patterns labeled 1, exit null concept (h ≡0 patterns). then, additional pattern, xi, labeled 1, delete h boolean variables appearing xi sign diﬀerent sign h. processing patterns labeled 1, check patterns labeled 0 sure assigned value 1 h. if, stage algorithm, patterns labeled 0 assigned 1 h, exists term consistently classiﬁes patterns ξ, exit failure. otherwise, exit h. change paragraph algorithm presented chapter three. example, consider following patterns, labeled 1 (from [dietterich, 1990]) (0, 1, 1, 0) (1, 1, 1, 0) (1, 1, 0, 0) processing ﬁrst pattern, h = x1x2x3x4; processing second pattern, h = x2x3x4; ﬁnally, pattern, h = x2x4. linearly separable functions let h set linearly separable functions. then, |h| ≤2n2,\n",
      "\n",
      "112 chapter 8. computational learning theory m ≥(1/ε) \u0000n2 ln 2 + ln(1/δ) \u0001 again, note bound m increases polynomially n, 1/ε, 1/δ. n = 50, ε = 0.01 δ = 0.01, m ≥173, 748 guarantees pac learnabil- ity. linearly separable functions properly pac learnable, additionally ﬁnd time polynomial m n hypothesis h consistent set m labeled linearly separable patterns. linear programming polynomial. 8.2.3 properly pac-learnable classes properly pac-learnable classes functions given following table. (adapted [dietterich, 1990, pages 262 268] gives references proofs time complexities.) h |h| time complexity p. learnable? terms 3n polynomial yes k-term dnf 2o(kn) np-hard (k disjunctive terms) k-dnf 2o(nk) polynomial yes (a disjunction k-sized terms) k-cnf 2o(nk) polynomial yes (a conjunction k-sized clauses) k-dl 2o(nkk lg n) polynomial yes (decision lists k-sized terms) lin. sep. 2o(n2) polynomial yes lin. sep. (0,1) weights ? np-hard k-2nn ? np-hard dnf 22n polynomial (all boolean functions) (members class k-2nn two-layer, feedforward neural networks exactly k hidden units output unit.) summary order class functions properly pac- learnable a. algorithm produces consistent hypothesis m n-dimensional samples time polynomial m n. b. sample size, m, needed ensure pac learnability polyno- mial (or better) (1/ε), (1/δ), n showing ln |h| polynomial better number dimensions.\n",
      "\n",
      "8.3. vapnik-chervonenkis dimension 113 hinted earlier, enlarging class hypotheses makes learning easier. example, table shows k-cnf pac learnable, k-term-dnf not. yet, k-term-dnf subclass k-cnf! so, target function k-term-dnf, able ﬁnd hypothesis k-cnf probably approximately correct target function. sim- ilarly, linearly separable functions implemented tlus weight values restricted 0 1 properly pac learnable, unrestricted linearly separable functions are. possible enlarging space hy- potheses makes ﬁnding consistent training examples easier. interesting class functions k-2nn poly- nomially pac learnable hypotheses drawn k′-2nn k′ > k. (at time writing, matter undecided.) pac learning theory powerful analytic tool, (like complexity theory) deals mainly worst-case results. fact class two- layer, feedforward neural networks polynomially pac learnable attack theory networks, successful applications. [baum, 1994, page 416-17] says “ . . . humans capable learning natural world. therefore, proof model learning learning feasible indictment model. examine model constraints relaxed realistic.” 8.3 vapnik-chervonenkis dimension 8.3.1 linear dichotomies consider set, h, functions, set, ξ, (unlabeled) patterns. measure expressive power set hypotheses, relative ξ, ability arbitrary classiﬁcations patterns ξ.1 m patterns ξ, 2m diﬀerent ways divide patterns disjoint exhaustive subsets. 2m diﬀerent dichotomies ξ. ξ include 2n boolean patterns, example, 22n ways dichotomize them, (of course) set possible boolean functions dichotomizes ways. subset, h, boolean functions able dichotomize arbitrary set, ξ, m boolean patterns 2m ways. general (that is, non-boolean case), subset, h, functions dichotomize set, ξ, m patterns 2m ways, h shatters ξ. example, consider set ξ m patterns n-dimensional space, rn. (that is, n components patterns real numbers.) deﬁne linear dichotomy implemented (n−1)-dimensional hyperplane n-dimensional space. linear dichotomies m patterns n di- mensions there? example, shown fig. 8.1, 14 dichotomies 1and, course, hypothesis drawn set arbitrary classiﬁcations set training patterns, little likelihood hypothesis generalize training set.\n",
      "\n",
      "114 chapter 8. computational learning theory points dimensions (each separating line yields dichotomies depending points line classiﬁed 1 0). (note inﬁnite number hyperplanes, are, nevertheless, ﬁnite number ways hyperplanes dichotomize ﬁnite number patterns. small movements hyperplane typically change classiﬁcations patterns.) 1 2 3 4 14 dichotomies 4 points 2 dimensions 5 6 7 figure 8.1 dichotomizing points dimensions number dichotomies achievable hyperplanes depends patterns disposed. maximum number linear dichotomies, points called general position. m > n, set m points general position n-dimensional space subset (n+1) points lies (n−1)-dimensional hyperplane. m ≤n, set m points general position (m −2)-dimensional hyperplane contains set. thus, example, set m ≥4 points general position three-dimensional space lie (two-dimensional) plane. denote number linear dichotomies m points general position n-dimensional space expression πl(m, n). diﬃcult verify that include derivation. πl(m, n) = 2 n x i=0 c(m −1, i) m > n, = 2m m ≤n\n",
      "\n",
      "8.3. vapnik-chervonenkis dimension 115 c(m −1, i) binomial coeﬃcient (m−1)! (m−1−i)!i!. table shows values πl(m, n). m n (no. patterns) (dimension) 1 2 3 4 5 1 2 2 2 2 2 2 4 4 4 4 4 3 6 8 8 8 8 4 8 14 16 16 16 5 10 22 30 32 32 6 12 32 52 62 64 7 14 44 84 114 126 8 16 58 128 198 240 note class linear dichotomies shatters m patterns m ≤n + 1. bold-face entries table correspond highest values m linear dichotomies shatter m patterns n dimensions. 8.3.2 capacity let pm,n = πl(m,n) 2m = probability randomly selected dichotomy (out 2m possible dichotomies m patterns n dimensions) linearly separable. fig. 8.2 plot pλ(n+1),n versus λ n, λ = m/(n + 1). note large n (say n > 30) quickly pm,n falls 1 0 m goes 2(n + 1). m < 2(n + 1), dichotomy m points certainly linearly separable. m > 2(n + 1), randomly selected dichotomy m points certainly linearly separable. reason m = 2(n + 1) called capacity tlu [cover, 1965]. number training patterns exceeds capacity, fact tlu separates training patterns according labels means terms tlu generalize new patterns. special separation found m < 2(n + 1) patterns—almost dichotomy patterns linearly separable. sure separation found forced training set generalizes well, case linearly separable functions separate m training patterns. analogous results generalizing abilities neural networks developed [baum & haussler, 1989] given intuitive experimen- tal justiﬁcation [baum, 1994, page 438] “the results indicate following heuristic rule holds. m examples [can correctly classiﬁed by] net w weights (for m >> w), net fraction ε errors new examples chosen [uniform] distribution ε = w/m.”\n",
      "\n",
      "116 chapter 8. computational learning theory 0 1 2 3 4 10 20 30 40 50 0 0.25 0.5 0.75 1 0 1 2 3 4 10 20 30 40 50 0 25 .5 75 1 ph(n + 1), n h n figure 8.2 probability random dichotomy linearly separable 8.3.3 general capacity result corollary 7.2 gave expression number training patterns suﬃcient guarantee required level generalization—assuming function guessing function belonging class known ﬁnite cardinality. capacity result presented applies linearly separable functions non- binary patterns. extend ideas general dichotomies non-binary patterns. general, let denote maximum number dichotomies set m n-dimensional patterns hypotheses h πh(m, n). number dichotomies will, course, depend disposition m points n-dimensional space; πh(m, n) maximum possible arrangements m points. (in case class linearly separable functions, maximum number achieved m points general position.) class, h, maximum value m πh(m, n) = 2m, is, h shatters m patterns. maximum number called vapnik-chervonenkis (vc) dimension denoted vcdim(h) [vapnik & chervonenkis, 1971]. saw class linear dichotomies, vcdim(linear) = (n + 1). example, let calculate vc dimension hypothesis space single intervals real line—used classify points real line. example points line dichotomized single interval fig. 8.3. set ξ be, example, {0.5, 2.5, - 2.3, 3.14}, hypotheses set [1, 4.5]. hypothesis label points 2.5 3.14 1 points - 2.3 0.5 0.\n",
      "\n",
      "8.3. vapnik-chervonenkis dimension 117 set hypotheses (single intervals real line) arbitrarily classify points. single interval classify points outer classiﬁed 1 inner 0. vc dimension single intervals real line 2. soon 2 training patterns real line provided know classiﬁcation function trying guess single interval, begin good generalization. figure 8.3 dichotomizing points interval vc dimension useful measure expressive power hypothesis set. dichotomy vcdim(h) fewer patterns general position n dimensions achieved hypothesis h, vcdim(h) patterns training set order hypothesis consistent training set suﬃciently constrained imply good generalization. examples shown concept vc dimension restricted boolean functions. 8.3.4 facts speculations vc dimen- sion • ﬁnite number, |h|, hypotheses h, then vcdim(h) ≤log(|h|) • vc dimension terms n dimensions n. • suppose generalize example hypothesis set single intervals real line. let consider n-dimensional feature space tests form li ≤xi ≤hi. allow test dimension. hypothesis space consisting conjunctions tests (called axis-parallel hyper-rectangles) vc dimension bounded by n ≤vcdim ≤2n • seen, tlus n inputs vc dimension n + 1. • [baum, 1994, page 438] gives experimental evidence proposition “ . . . multilayer [neural] nets vc dimension roughly equal total number [adjustable] weights.”\n",
      "\n",
      "118 chapter 8. computational learning theory 8.4 vc dimension pac learning theorems connect idea vc dimension pac learn- ing [blumer, et al., 1990]. state proof. theorem 8.3 (blumer, et al.) hypothesis space h pac learnable iﬀit ﬁnite vc dimension. theorem 8.4 set hypotheses, h, properly pac learnable if a. m ≥(1/ε) max [4 lg(2/δ), 8 vcdim lg(13/ε)], b. algorithm outputs hypothesis h ϵ h consistent training set polynomial (in m n) time. second theorems improves bound number training patterns needed linearly separable functions linear n. previous example training patterns needed ensure pac learnability linearly separable function n = 50, ε = 0.01, δ = 0.01, obtained m ≥173, 748. blumer, et al. result m ≥52, 756. example second theorem, let h set closed intervals real line. vc dimension 2 (as shown previously). n = 50, ε = 0.01, δ = 0.01, m ≥16, 551 ensures pac learnability. theorem gives lower (necessary) bound number training patterns required pac learning [ehrenfeucht, et al., 1988] theorem 8.5 pac learning algorithm examine ω(1/ε lg(1/δ) + vcdim(h)) training patterns. diﬀerence lower upper bounds o(log(1/ε)vcdim(h)/ε). 8.5 bibliographical historical remarks added.\n",
      "\n",
      "chapter 9 unsupervised learning 9.1 unsupervised learning? consider sets points two-dimensional space illustrated fig. 9.1. ﬁrst set (a) naturally partitionable classes, second (b) diﬃcult partition all, (c) problematic. unsupervised learning uses procedures attempt ﬁnd natural partitions patterns. stages • form r-way partition set ξ unlabeled training patterns (where value r, itself, need induced patterns). partition separates ξ r mutually exclusive exhaustive subsets, ξ1, . . . , ξr, called clusters. • design classiﬁer based labels assigned training patterns partition. explain shortly methods deciding clusters separating set patterns clusters. base methods, motivation, minimum-description- length (mdl) principles. setting, assume want encode description set points, ξ, message minimal length. encoding involves description point separately; other, shorter, encodings involve description clusters points point cluster described given cluster belongs to. speciﬁc techniques described chapter explicitly use mdl principles, mdl method applied success. mdl-based methods, autoclass ii [cheeseman, et al., 1988] discovered new classiﬁcation stars based properties infrared sources. type unsupervised learning involves ﬁnding hierarchies par- titionings clusters clusters. hierarchical partition ξ 119\n",
      "\n",
      "120 chapter 9. unsupervised learning a) clusters b) cluster c) ? figure 9.1 unlabeled patterns divided mutually exclusive exhaustive subsets, ξ1, . . . , ξr; set, ξi, (i = 1, . . . , r) divided mutually exclusive exhaustive subsets, on. example hierarchical partition fig. 9.2. hierarchical form best displayed tree, shown fig. 9.3. tip nodes tree expanded individual pattern elements. application hierarchical partitions organizing individuals taxonomic hierarchies botany zoology. 9.2 clustering methods 9.2.1 method based euclidean distance unsupervised learning methods use measure similarity patterns order group clusters. simplest involves deﬁning distance patterns. patterns features numeric, distance measure ordinary euclidean distance points n-dimensional space. simple, iterative clustering method based distance. described follows. suppose r randomly chosen cluster seekers, c1, . . . , cr. points n-dimensional space want adjust center clusters patterns. present (unlabeled) patterns training set, ξ, algorithm\n",
      "\n",
      "9.2. clustering methods 121 u11 u12 u21 u22 u23 u31 u32 u11 f u12 = u1 u21 f u22 f u23 = u2 u31 f u32 = u3 u1 f u2 f u3 = u figure 9.2 hierarchy clusters one-by-one. pattern, xi, presented, ﬁnd cluster seeker, cj, closest xi closer xi cj ←−(1 −αj)cj + αjxi αj learning rate parameter j-th cluster seeker; determines far cj moved xi. reﬁnements procedure cluster seekers far training proceeds. suppose cluster seeker, cj, mass, mj, equal number times moved. cluster seeker’s mass increases moves far pattern. example, set αj = 1/(1 + mj) use rule mj ←−mj +1. adjustment rule, cluster seeker center gravity (sample mean) set patterns far moved. intuitively, cluster seeker gets reasonably clustered set patterns (and cluster seeker located), converge center gravity cluster.\n",
      "\n",
      "122 chapter 9. unsupervised learning u u2 u11 u12 u31 u32 u21 u22 u23 u1 u3 figure 9.3 displaying hierarchy tree cluster seekers converged, classiﬁer implied now- labeled patterns ξ based voronoi partitioning space (based distances cluster seekers). kind classiﬁcation, ex- ample shown fig. 9.4, implemented linear machine. georgy fedoseevich voronoi, russian mathematician lived 1868 1909. basing partitioning distance, seek clusters patterns close possible. measure badness, v , cluster patterns, {xi}, computing sample variance deﬁned by v = (1/k) x (xi −m)2 m sample mean cluster, deﬁned be m = (1/k) x xi k number points cluster. like partition set patterns clusters sum sample variances (badnesses) clusters small. course cluster pattern, sample variances zero, arrange measure badness partition increase number clusters. way, seek trade-oﬀbetween variances\n",
      "\n",
      "9.2. clustering methods 123 c1 c2 c3 separating boundaries figure 9.4 minimum-distance classiﬁcation clusters number way somewhat similar principle minimal description length discussed earlier. elaborations basic cluster-seeking procedure allow number clus- ter seekers vary depending distances depending sample variances clusters. example, distance, dij, cluster seekers, ci cj, falls threshold ε, replace single cluster seeker placed center gravity (taking account respective masses). way decrease overall badness partition reducing number clusters compara- tively little penalty increased variance. hand, cluster seekers, ci, deﬁnes cluster sample variance larger δ, place new cluster seeker, cj, random location somewhat adjacent ci reset masses ci cj zero. way badness par- tition ultimately decrease decreasing total sample variance comparatively little penalty additional cluster seeker. values parameters ε δ set depending relative weights given sample variances numbers clusters. distance-based methods, important scale components pattern vectors. variation values dimensions pattern vector diﬀerent dimensions. commonly technique compute standard deviation (i.e., square root variance) components entire training set normalize values components adjusted standard deviations equal.\n",
      "\n",
      "124 chapter 9. unsupervised learning 9.2.2 method based probabilities suppose partition training set, ξ, r mutually exclusive exhaustive clusters, c1, . . . , cr. decide clusters arbitrary pattern, x, assigned selecting ci probability, p(ci|x), largest, providing p(ci|x) larger ﬁxed threshold, δ. saw earlier, use bayes rule base decision maximizing p(x|ci)p(ci). assuming conditional independence pattern components, xi, quantity maximized is s(x, ci) = p(x1|ci)p(x2|ci) · · · p(xn|ci)p(ci) p(xj|ci) estimated sample statistics patterns clusters expression. (recall linear form formula took case binary-valued components.) s(x, ci) similarity x cluster, ci, patterns. thus, assign x cluster similar, providing similarity larger δ. before, deﬁne sample mean cluster, ci, be mi = (1/ki) x xjϵ ci xj ki number patterns ci. base iterative clustering algorithm measure similarity [mahadevan & connell, 1992]. described follows a. begin set unlabeled patterns ξ list, l, clusters. b. pattern, x, ξ, compute s(x, ci) cluster, ci. (initially, similarities zero.) suppose largest similarities s(x, cmax). (a) s(x, cmax) > δ, assign x cmax. is, cmax ←−cmax ∪{x} update sample statistics p(x1|cmax), p(x2|cmax), . . . , p(xn|cmax), p(cmax) new pattern account. 3. (b) s(x, cmax) ≤δ, create new cluster, cnew = {x} add cnew l. 3. c. merge existing clusters, ci cj (mi −mj)2 < ε. compute new sample statistics p(x1|cmerge), p(x2|cmerge), . . . , p(xn|cmerge), p(cmerge) merged cluster, cmerge = ci ∪cj.\n",
      "\n",
      "9.3. hierarchical clustering methods 125 d. sample statistics clusters changed entire iteration ξ, terminate clusters l; 2. value parameter δ controls number clusters. δ high, large number clusters patterns cluster. small values δ, small number clusters patterns cluster. similarly, larger value ε, smaller number clusters found. designing classiﬁer based patterns labeled partitioning straightforward. assign pattern, x, category maximizes s(x, ci). mention “k-means “em” methods. 9.3 hierarchical clustering methods 9.3.1 method based euclidean distance suppose set, ξ, unlabeled training patterns. form hi- erarchical classiﬁcation patterns ξ simple agglomerative method. (the description algorithm based unpublished manuscript pat langley.) description gives general idea; leave reader generate precise algorithm. ﬁrst compute euclidean distance pairs patterns ξ. (again, appropriate scaling dimensions assumed.) suppose smallest distance patterns xi xj. collect xi xj cluster, c, eliminate xi xj ξ replace cluster vector, c, equal average xi xj. compute euclidean distance pairs points ξ. smallest distance pairs patterns, form new cluster, c, replace pair patterns ξ average. shortest distance pattern, xi, cluster vector, cj (representing cluster, cj), form new cluster, c, consisting union cj {xi}. case, replace cj xi ξ (appropriately weighted) average continue. shortest distance cluster vectors, ci cj, form new cluster, c, consisting union ci cj. case, replace ci cj (appropriately weighted) average continue. reduce number points ξ time, ultimately terminate tree clusters rooted cluster containing points original training set. example method aggregates set dimensional patterns shown fig. 9.5. numbers associated cluster indicate order formed. clusters organized hierarchically binary tree cluster 9 root, clusters 7 8 descendants root, on. ternary tree formed instead searches points ξ triangle deﬁned patterns minimal area.\n",
      "\n",
      "126 chapter 9. unsupervised learning 1 2 3 5 4 6 7 8 9 figure 9.5 agglommerative clustering 9.3.2 method based probabilities probabilistic quality measure partitions develop measure goodness partitioning based accurately guess pattern given partition in. suppose given partitioning ξ r classes, c1, . . . , cr. before, compute sample statistics p(xi|ck) probability values component given class assigned partitioning. suppose component xi x values vij, index j steps domain component. use notation pi(vij|ck) = probability(xi = vij|ck). suppose use following probabilistic guessing rule values components vector x given class k. guess xi = vij probability pi(vij|ck). then, probability guess i-th component correctly is x j probability(guess vij)pi(vij|ck) = x j [pi(vij|ck)]2 average number (the n) components values guessed correctly method given sum probabilities components x x x j [pi(vij|ck)]2\n",
      "\n",
      "9.3. hierarchical clustering methods 127 given partitioning r classes, goodness measure, g, parti- tioning average expression classes g = x k p(ck) x x j [pi(vij|ck)]2 p(ck) probability pattern class ck. order penalize measure having large number classes, divide r overall “quality” measure partitioning z = (1/r) x k p(ck) x x j [pi(vij|ck)]2 example use measure trivially simple clustering three-dimensional patterns shown fig. 9.6. diﬀerent partitionings. let’s evaluate z values follow- ing ones p1 = {a, b, c, d}, p2 = {{a, b}, {c, d}}, p3 = {{a, c}, {b, d}}, p4 = {{a}, {b}, {c}, {d}}. ﬁrst, p1, puts patterns single cluster. sample probabilities pi(vi1 = 1) pi(vi0 = 0) equal 1/2 components. summing values components (0 1) gives (1/2)2 + (1/2)2 = 1/2. summing components gives 3/2. averaging clusters (there one) gives 3/2. finally, dividing number clusters produces ﬁnal z value partition, z(p1) = 3/2. second partition, p2, gives following sample probabilities p1(v11 = 1|c1) = 1 p2(v21 = 1|c1) = 1/2 p3(v31 = 1|c1) = 1 summing values components (0 1) gives (1)2 + (0)2 = 1 component 1, (1/2)2 + (1/2)2 = 1/2 component 2, (1)2 + (0)2 = 1 component 3. summing components gives 2 1/2 class 1. similar calculation gives 2 1/2 class 2. averaging clusters gives 2 1/2. finally, dividing number clusters produces ﬁnal z value partition, z(p2) = 1 1/4, high z(p1). similar calculations yield z(p3) = 1 z(p4) = 3/4, method evaluating partitions favor placing patterns single cluster.\n",
      "\n",
      "128 chapter 9. unsupervised learning x2 x3 x1 b c d figure 9.6 patterns 3-dimensional space iterative method hierarchical clustering evaluating partitionings m patterns selecting best computationally intractable. following iterative method based hi- erarchical clustering procedure called cobweb [fisher, 1987]. procedure grows tree node labeled set patterns. end process, root node contains patterns ξ. successors root node contain mutually exclusive exhaustive subsets ξ. general, successors node, η, labeled mutually exclusive exhaustive subsets pattern set labelling node η. tips tree contain singleton sets. method uses z values place patterns vari- ous nodes; sample statistics update z values pattern placed node. algorithm follows a. start tree root node contains patterns ξ single successor node. arrange times dur- ing process non-empty node tree (besides successors) exactly successor. b. select pattern xi ξ (if patterns select, terminate). c. set µ root node. d. successors µ (including successor!), calculate best host xi. best host determined tentatively placing xi successors calculating resulting z value\n",
      "\n",
      "9.3. hierarchical clustering methods 129 ways accomodating xi. best host corresponds assignment highest z value. e. best host node, η, place xi η, generate successor node η, generate sibling node η, 2. f. best host non-empty, singleton (tip) node, η, place xi η, create successor node η containing singleton pattern η, create successor node η containing xi, create successor node η, create successor nodes new non-empty successors η, 2. g. best host non-empty, non-singleton node, η, place xi η, set µ η, 4. process sensitive order patterns presented. ﬁnal classiﬁcation tree order dependent, cobweb proce- dure incorporates node merging splitting. node merging happen nodes having parent merged overall increase quality resulting classiﬁcation performed successors parent. try pairs merge, good heuristic attempt merge best hosts. merging improves z value, new node containing union patterns merged nodes replaces merged nodes, nodes merged installed successors new node. node splitting heuristic node splitting consider replacing best host group siblings host’s successors. operation performed increases z value classiﬁcation performed group siblings. example results cobweb mention experiments cobweb. ﬁrst, program at- tempted ﬁnd categories (we class 1 class 2) united states senators based votes (yes no) issues. clus- ters established, majority vote class computed. shown table below. issue class 1 class 2 toxic waste yes budget cuts yes sdi reduction yes contra aid yes line-item veto yes mx production yes\n",
      "\n",
      "130 chapter 9. unsupervised learning second experiment, program attempted classify soybean dis- eases based characteristics. cobweb grouped diseases taxonomy shown fig. 9.7. n0 soybean diseases n1 diaporthe stem canker n2 charcoal rot n3 n31 rhizoctonia rot n32 phytophthora rot figure 9.7 taxonomy induced soybean diseases 9.4 bibliographical historical remarks added.\n",
      "\n",
      "chapter 10 temporal-diﬀerence learning 10.1 temporal patterns prediction prob- lems chapter, consider problems wish learn predict future value quantity, z, n-dimensional input pattern, x. problems, patterns occur temporal sequence, x1, x2, . . ., xi, xi+1, . . ., xm, generated dynamical process. components xi features values available time, t = i. distinguish kinds prediction problems. one, desire predict value z time t = + 1 based input xi i. example, wish predict aspects tomorrow’s weather based set measurements today. kind prediction problem, desire sequence predictions value z ﬁxed time, t = m + 1, based xi, = 1, . . . , m. example, wish series predictions aspect weather new year’s day, based measurements taken day new year’s. sutton [sutton, 1988] called problem, multi-step prediction, problem consider here. multi-step prediction, expect prediction accuracy better better increases m. 10.2 supervised temporal-diﬀerence meth- ods training method naturally suggests use actual value z time m + 1 (once known) supervised learning procedure 131\n",
      "\n",
      "132 chapter 10. temporal-difference learning sequence training patterns, {x1, x2, . . ., xi, xi+1, . . ., xm}. is, seek learn function, f, f(xi) close possible z i. typically, need training set, ξ, consisting sequences. method better supervised learning important problems base learning diﬀerence f(xi+1) f(xi) diﬀerence z f(xi). methods involve called temporal-diﬀerence (td) learning. assume prediction, f(x), depends vector modiﬁable weights, w. dependence explicit, write f(x, w). su- pervised learning, consider procedures following type xi, prediction f(xi, w) computed compared z, learning rule (whatever is) computes change, (∆wi), w. then, taking account weight changes pattern sequence having predictions old weight vector, change w follows w ←−w + m x i=1 (∆w)i attempting minimize squared error z f(xi, w) gradient descent, weight-changing rule pattern is (∆w)i = c(z −fi) ∂fi ∂w c learning rate parameter, fi prediction z, f(xi, w), time t = i, ∂fi ∂w is, deﬁnition, vector partial derivatives ( ∂fi ∂w1 , . . . , ∂fi ∂wi , . . . , ∂fi ∂wn ) wi individual components w. (the expression ∂fi ∂w written ∇wfi.) reader recall equivalent expression (∆w)i deriving backpropagation formulas training multi-layer neural networks. widrow-hoﬀrule results f(x, w) = x • w. then (∆w)i = c(z −fi)xi interesting form (∆w)i developed note (z −fi) = m x k=i (fk+1 −fk) deﬁne fm+1 = z. substituting formula (∆w)i yields (∆w)i = c(z −fi) ∂fi ∂w\n",
      "\n",
      "10.2. supervised temporal-difference methods 133 = c ∂fi ∂w m x k=i (fk+1 −fk) form, instead diﬀerence prediction value z, use diﬀerences successive predictions—thus phrase temporal-diﬀerence (td) learning. case f(x, w) = x • w, temporal diﬀerence form widrow-hoﬀrule is (∆w)i = cxi m x k=i (fk+1 −fk) reason writing (∆w)i temporal-diﬀerence form permit interesting generalization follows (∆w)i = c ∂fi ∂w m x k=i λ(k−i)(fk+1 −fk) 0 < λ ≤1. here, λ term gives exponentially decreasing weight diﬀerences later time t = i. λ = 1, rule began—weighting diﬀerences equally, λ →0, weight (fi+1 −fi) diﬀerence. λ term, method called td(λ). interesting compare extreme cases td(0) (∆w)i = c(fi+1 −fi) ∂fi ∂w td(1) (∆w)i = c(z −fi) ∂fi ∂w extremes handled learning mechanism; error term diﬀerent. td(0), error diﬀerence successive predic- tions, td(1), error diﬀerence ﬁnally revealed value z prediction. intermediate values λ account diﬀerently weighted diﬀerences future pairs successive predictions. td(1) considered pure supervised learning procedure, sensitive ﬁnal value z provided teacher. λ < 1, degrees unsupervised learning, prediction function strives prediction like successive ones (whatever be). shall soon unsupervised procedures result better learning supervised ones important class problems.\n",
      "\n",
      "134 chapter 10. temporal-difference learning 10.3 incremental computation (∆w)i rewrite formula (∆w)i, (∆w)i = c ∂fi ∂w m x k=i λ(k−i)(fk+1 −fk) allow type incremental computation. write expression weight change rule takes account (∆w)i w ←−w + m x i=1 c ∂fi ∂w m x k=i λ(k−i)(fk+1 −fk) interchanging order summations yields w ←−w + m x k=1 c k x i=1 λ(k−i)(fk+1 −fk) ∂fi ∂w = w + m x k=1 c(fk+1 −fk) k x i=1 λ(k−i) ∂fi ∂w interchanging indices k ﬁnally yields w ←−w + m x i=1 c(fi+1 −fi) x k=1 λ(i−k) ∂fk ∂w if, earlier, want use expression form w ←−w+pm i=1(∆w)i, write (∆w)i = c(fi+1 −fi) x k=1 λ(i−k) ∂fk ∂w now, let ei = pi k=1 λ(i−k) ∂fk ∂w, develop computationally eﬃcient recurrence equation ei+1 follows ei+1 = i+1 x k=1 λ(i+1−k) ∂fk ∂w = ∂fi+1 ∂w + x k=1 λ(i+1−k) ∂fk ∂w\n",
      "\n",
      "10.4. experiment td methods 135 = ∂fi+1 ∂w + λei rewriting (∆w)i terms, obtain (∆w)i = c(fi+1 −fi)ei where e1 = ∂f1 ∂w e2 = ∂f2 ∂w + λe1 etc. quoting sutton [sutton, 1988, page 15] (about diﬀerent equation, quote applies equally one) “. . . equation computed incrementally, (∆w)i depends pair successive predictions [weighted] sum past values ∂fi ∂w. saves substantially memory, longer necessary individually remember past values ∂fi ∂w.” 10.4 experiment td methods td prediction methods [especially td(0)] suited situations patterns generated dynamic process. case, sequences temporally presented patterns contain important information ignored conventional supervised method widrow-hoﬀrule. sutton [sutton, 1988, page 19] gives interesting example involving random walk, repeat here. fig. 10.1, sequences vectors, x, generated follows start vector xd; vector sequence equally likely adjacent vectors diagram. vector xc (or xe), equally likely vectors adjacent xc (or xe). xb sequence, equally likely sequence terminates z = 0 vector xc. similarly, xf sequence, equally likely sequence terminates z = 1 vector xe. sequences random, start xd. sample sequences shown ﬁgure.\n",
      "\n",
      "136 chapter 10. temporal-difference learning 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 z = 0 z = 1 xb xc xd xe xf typical sequences xdxcxdxexf 1 xdxcxbxcxdxexdxexf 1 xdxexdxcxb 0 figure 10.1 markov process random walk example markov process; transitions state state j occur probabilities depend j. given set sequences generated process training set, want able predict value z x test sequence. assume learning system know transition probabilities. experiments process, sutton linear predictor, f(x, w) = x • w. learning problem ﬁnd weight vector, w, minimizes mean-squared error z predicted value z. given ﬁve diﬀerent values x on, following predictions f(xb) = w1, f(xc) = w2, f(xd) = w3, f(xe) = w4, f(xf ) = w5, wi i-th component weight vector. (note values predictions limited 1 0—even z values—because minimizing mean-squared error.) training, predictions compared optimal ones—given transition probabilities. experimental setup follows random sequences generated transition probabilities. sequences presented turn td(λ) method values λ. weight vector increments, (∆w)i, computed pattern presentation weight changes sequences presented. weight vector increments summed sequences presented, sum change weight vector pass sequences. process repeated (using training sequences) (quoting sutton) “the procedure longer produced signiﬁcant changes weight vector. small c, weight vector converged way,\n",
      "\n",
      "10.4. experiment td methods 137 ﬁnal value [for 100 diﬀerent training sets random sequences], independent initial value.” (even though, ﬁxed, small c, weight vector converged vector, converge somewhat diﬀerent vector diﬀerent values c.) convergence, predictions ﬁnal weight vector com- pared optimal predictions transition probabilities. optimal predictions simply p(z = 1|x). compute proba- bilities 1/6, 1/3, 1/2, 2/3, 5/6 xb, xc, xd, xe, xf , respectively. root-mean-squared diﬀerences best learned predictions (over c) optimal ones plotted fig. 10.2 seven diﬀerent values λ. (for data point, standard error approximately σ = 0.01.) 0.10 0.12 0.14 0.16 0.18 0.20 0.0 0.1 0.3 0.5 0.7 0.9 1.0 h error best c widrow-hoff td(1) td(0) (adapted sutton, p. 20, 1988) figure 10.2 prediction errors td(λ) notice widrow-hoﬀprocedure perform versions td(λ) λ < 1! quoting [sutton, 1988, page 21] “this result contradicts conventional wisdom. known that, repeated presentations, widrow-hoﬀprocedure minimizes rms error predictions actual outcomes training set ([widrow & stearns, 1985]). optimal method peformed worse td methods λ < 1? answer widrow-hoﬀprocedure minimizes error training set; necessarily minimize error future experience. [later] prove fact linear td(0) converges considered optimal estimates\n",
      "\n",
      "138 chapter 10. temporal-difference learning matching future experience—those consistent maximum- likelihood estimate underlying markov process.” 10.5 theoretical results possible analyze performance linear-prediction td(λ) methods markov processes. state theorems proof. theorem 10.1 (sutton, page 24, 1988) absorbing markov chain, linearly independent set observation vectors {xi} non- terminal states, exists ε > 0 positive c < ε initial weight vector, predictions linear td(0) (with weight updates sequence) converge expected value optimal (maximum likelihood) predictions true process. expected values predictions converge, predictions converge vary expected values depending recent experience. sutton conjectures c approach 0 training progresses, variance predictions approach 0 also. dayan [dayan, 1992] extended result theorem 9.1 td(λ) arbitrary λ 0 1. (also [dayan & sejnowski, 1994].) 10.6 intra-sequence weight updating standard weight updating rule td(λ) methods is w ←−w + m x i=1 c(fi+1 −fi) x k=1 λ(i−k) ∂fk ∂w weight update occurs entire sequence observed. method truly incremental (in analogy weight updating rules neural nets), desirable change weight vector pattern presentation. obvious extension is wi+1 ←−wi + c(fi+1 −fi) x k=1 λ(i−k) ∂fk ∂w fi+1 computed making weight change; is, fi+1 = f(xi+1, wi). fi = f(xi, wi−1), rule prediction diﬀerence, (fi+1 −fi), sensitive changes x changes w lead instabilities. instead, modify rule that, pair predictions, fi+1 = f(xi+1, wi) fi = f(xi, wi). version rule practice excellent results.\n",
      "\n",
      "10.6. intra-sequence weight updating 139 td(0) linear predictors, rule is wi+1 = wi + c(fi+1 −fi)xi rule implemented follows a. initialize weight vector, w, arbitrarily. b. = 1, ..., m, do (a) fi ←−xi • w (we compute fi anew time use value fi+1 previous time through.) (b) fi+1 ←−xi+1 • w (c) di+1 ←−fi+1 −fi (d) w ←−w + c di+1xi (if fi computed changed weight vector, value closer fi+1 desired.) linear td(0) method regarded technique training simple network consisting single dot product unit (and threshold sigmoid function). td methods combination back- propagation train neural networks. td(0) change network weights according expression wi+1 = wi + c(fi+1 −fi) ∂fi ∂w change standard backpropagation weight- changing rule diﬀerence term desired output output unit ﬁnal (k-th) layer, (d −f (k)), replaced diﬀerence term successive outputs, (fi+1 −fi). change direct eﬀect expression δ(k) becomes δ(k) = 2(f ′(k) −f (k))f (k)(1 −f (k)) f ′(k) f (k) successive outputs network. weight changing rule i-th weight vector j-th layer weights form before, namely w(j) ←−w(j) + cδ(j) x(j−1) δ(j) given recursively by δ(j) = f (j) (1 −f (j) ) mj+1 x l=1 δ(j+1) l w(j+1) il w(j+1) il l-th component i-th weight vector (j +1)-th layer weights. course, assumed f ′(k) f (k) computed weights weights changed. section shall interesting example application td learning.\n",
      "\n",
      "140 chapter 10. temporal-difference learning 10.7 example application td-gammon program called td-gammon [tesauro, 1992] learns play backgammon training neural network temporal-diﬀerence methods. structure neural net, coding shown fig. 10.3. network trained minimize error actual payoﬀand estimated payoﬀ, actual payoﬀis deﬁned df = p1 + 2p2 −p3 −2p4, pi actual probabilities outcomes deﬁned ﬁgure. . . . p3 = pr(black wins) p4 = pr(black gammons) p1 = pr(white wins) p2 = pr(white gammons) estimated payoff d = p1 + 2p2 < p3 < 2p4 no. white cell 1 no. bar, board, moves 198 inputs 1 2 3 # > 3 . . . 40 hidden units 2 x 24 cells 4 output units hidden output units sigmoids learning rate c = 0.1; initial weights chosen randomly <0.5 +0.5. estimated probabilities figure 10.3 td-gammon network td-gammon learned network select results best predicted payoﬀ. is, stage game ﬁnite set moves possible lead set, {x}, new board positions. member set evaluated network, largest\n",
      "\n",
      "10.8. bibliographical historical remarks 141 predicted payoﬀis selected white’s (and smallest black’s). made, network weights adjusted predicted payoﬀfrom original position closer resulting position. weight adjustment procedure combines temporal-diﬀerence (td(λ)) learning backpropagation. dt network’s estimate payoﬀ time t (before made), dt+1 estimate time t + 1 (after made), weight adjustment rule is ∆wt = c(dt+1 −dt) t x k=1 λt−k ∂dk ∂w wt vector weights network time t, ∂dk ∂w gradient dk weight space. (for layered, feedforward network, td-gammon, weight changes weight vectors layer expressed usual manner.) special cases clear, recall td(0), network trained that, t, output, dt, input xt tended expected output, dt+1, input xt+1. td(1), network trained that, t, output, dt, input xt tended expected ﬁnal payoﬀ, df, given input. case widrow-hoﬀrule. 200,000 games following results obtained. td-gammon (with 40 hidden units, λ = 0.7, c = 0.1) won 66.2% 10,000 games sun microsystems gammontool 55% 10,000 games neural network trained expert moves. commenting later version td- gammon, incorporating special features inputs, tesauro said “it appears strongest program seen author.” 10.8 bibliographical historical remarks added.\n",
      "\n",
      "142 chapter 10. temporal-difference learning\n",
      "\n",
      "chapter 11 delayed-reinforcement learning 11.1 general problem imagine robot exists environment sense act. suppose (as extreme case) idea eﬀects actions. is, doesn’t know acting change sensory inputs. sensory inputs “rewards,” occasionally receives. choose actions maximize rewards long run? maximize rewards, need able predict actions change inputs, particular, actions lead rewards. formalize problem following way robot exists environment consisting set, s, states. assume robot’s sensory apparatus constructs input vector, x, environment, informs robot state environment in. moment, assume mapping states vectors one-to-one, and, fact, use notation x refer state environment input vector. presented input vector, robot decides action set, a, actions perform. performing action produces eﬀect environment—moving new state. new state results robot perceiving new input vector, cycle repeats. assume discrete time model; input vector time t = xi, action taken time ai, expected reward, ri, received t = depends action taken state, ri = r(xi, ai). learner’s goal ﬁnd policy, π(x), maps input vectors actions way maximizes rewards accumulated time. type learning called reinforcement learning. learner ﬁnd policy trial error; initial knowledge eﬀects actions. situation shown fig. 11.1. 143\n",
      "\n",
      "144 chapter 11. delayed-reinforcement learning xi ri learner environment (reward) (state) (action) ai figure 11.1 reinforcement learning 11.2 example “grid world,” shown fig. 11.2 illustrate reinforcement learning. imagine robot initially cell (2,3). robot receives input vector (x1, x2) telling cell in; capable actions, n, e, s, w moving robot cell up, right, down, left, respectively. rewarded negative unit bumps wall blocked cells. example, input robot (1,3), robot chooses action w, input robot (1,3) receives reward −1. robot lands cell marked g (for goal), receives reward +10. let’s suppose robot lands goal cell gets reward, immediately transported random cell, quest reward continues. policy robot speciﬁcation action inputs, is, cells grid. example, com- ponent policy “when cell (3,1), right.” optimal policy policy maximizes long-term reward. way displaying policy grid-world robot arrow cell indicating direc- tion robot cell. fig. 11.3, optimal policy displayed manner. chapter describe methods learning optimal policies based reward values received learner.\n",
      "\n",
      "11.3. temporal discounting optimal policies 145 r g 1 2 3 4 5 6 7 1 2 3 4 5 6 7 8 figure 11.2 grid world 11.3 temporal discounting optimal poli- cies delayed reinforcement learning, assumes rewards distant future valuable immediate rewards. preference accomodated temporal discount factor, 0 ≤γ < 1. present value reward, ri, occuring time units future, taken γiri. suppose policy π(x) maps input vectors actions, let rπ(x) reward received i-th time step begins executing policy π starting state x. total reward accumulated time steps policy π beginning state x is v π(x) = ∞ x i=0 γirπ(x) reason temporal discount factor sum ﬁnite. optimal policy maximizes v π(x) inputs, x. general, want consider case rewards, ri, random variables eﬀects actions environmental states random. markovian environments, example, probability action state xi lead state xj given transition probability p[xj|xi, a]. then, want maximize expected future reward deﬁne v π(x) as v π(x) = e \" ∞ x i=0 γirπ(x) # case, v π(x) value policy π input x.\n",
      "\n",
      "146 chapter 11. delayed-reinforcement learning r g 1 2 3 4 5 6 7 1 2 3 4 5 6 7 8 figure 11.3 optimal policy grid world action prescribed π taken state x leads state x′ (randomly according transition probabilities), write v π(x) terms v π(x′) follows v π(x) = r[x, π(x)] + γ x x′ p[x′|x, π(x)]v π(x′) (in summary) γ = discount factor, v π(x) = value state x policy π, r[x, π(x)] = expected immediate reward received execute action prescribed π state x, p[x′|x, π(x)] = probability environment transitions state x′ execute action prescribed π state x. words, value state x policy π expected value immediate reward received executing action recommended π plus average value (under π) states accessible x. optimal policy, π∗(and others!), famous “optimality equation” v π∗(x) = max \" r(x, a) + γ x x′ p[x′|x, a]v π∗(x′) # theory dynamic programming (dp) [bellman, 1957, ross, 1983] assures optimal policy, π∗, satisﬁes equation. dp\n",
      "\n",
      "11.4. q-learning 147 provides methods calculating v π∗(x) π∗, assuming know average rewards transition probabilities. knew transition probabilities, average rewards, v π∗(x) x a, easy implement optimal policy. simply select maximizes r(x, a) + γ p x′ p[x′|x, a]v π∗(x′). is, π∗(x) = arg max \" r(x, a) + γ x x′ p[x′|x, a]v π∗(x′) # but, course, assuming know average rewards transition probabilities, ﬁnd method eﬀectively learns them. model actions, is, knew state, x, action a, state, x′ resulted, use method called value iteration ﬁnd optimal policy. value iteration works follows begin assigning, randomly, estimated value ˆv (x) state, x. i-th step process, suppose state xi (that is, input i-th step xi), estimated value state xi i-th step ˆvi(xi). select action maximizes estimated value predicted subsequent state. suppose subsequent state having highest estimated value x′ i. update estimated value, ˆvi(xi), state xi follows ˆvi(x) = (1 −ci) ˆvi−1(x) + ci h ri + γ ˆvi−1(x′ i) x = xi, = ˆvi−1(x) otherwise. adjustment moves value ˆvi(xi) increment (depend- ing ci) closer h ri + γ ˆvi(x′ i) . assuming ˆvi(x′ i) good estimate vi(x′ i), adjustment helps estimates consistent. providing 0 < ci < 1 visit state inﬁnitely often, process value iteration converge optimal values. discuss synchronous dynamic programming, asynchronous dynamic programming, policy iteration. 11.4 q-learning watkins [watkins, 1989] proposed technique calls incremental dynamic programming. let a; π stand policy chooses action once, chooses actions according policy π. deﬁne qπ(x, a) = v a;π(x)\n",
      "\n",
      "148 chapter 11. delayed-reinforcement learning optimal value state x given by v π∗(x) = max qπ∗(x, a) equation holds optimal policy, π∗. optimal policy given by π∗(x) = arg max qπ∗(x, a) note action makes qπ(x, a) larger v π(x), improve π changing π(x) = a. making change basis powerful learning rule shall describe shortly. suppose action state x leads state x′. deﬁnitions q v , easy that qπ(x, a) = r(x, a) + γe[v π(x′)] r(x, a) average value immediate reward received execute action state x. optimal policy (and others), version optimality equation terms q values qπ∗(x, a) = max h r(x, a) + γe h qπ∗(x′, a) ii actions, a, states, x. now, optimal q values (for x), implement optimal policy simply selecting action maximized r(x, a) + γe \u0002 qπ∗(x′, a) \u0003 . is, π∗(x) = arg max h r(x, a) + γe h qπ∗(x′, a) ii watkins’ proposal amounts td(0) method learning q values. quote (with minor notational changes) [watkins & dayan, 1992, page 281] “in q-learning, agent’s experience consists sequence dis- tinct stages episodes. i-th episode, agent • observes current state xi, • selects [using method described below] performs action ai, • observes subsequent state x′ i, • receives immediate reward ri,\n",
      "\n",
      "11.4. q-learning 149 • adjusts qi−1 values learning factor ci, according to qi(x, a) = (1 −ci)qi−1(x, a) + ci[ri + γvi−1(x′ i)] x = xi = ai, = qi−1(x, a) otherwise, vi−1(x′) = max b [qi−1(x′, b)] best agent thinks state x′. . . . initial q values, q0(x, a), states actions assumed given.” current q values, qi(x, a), agent selects action maximizes qi(x, a). note q value corresponding state exited action taken adjusted. q value adjusted closer (by determined ci) sum immediate reward plus discounted maximum (over actions) q values state entered. imagine q values predictions ultimate (inﬁnite horizon) total reward, learning procedure described exactly td(0) method learning predict q values. q learning strengthens usual td methods, however, td (applied reinforcement problems value iteration) requires one-step lookahead, model eﬀects actions, q learning not. convenient notation (proposed [schwartz, 1993]) representing change q value is q(x, a) β ←−r + γv (x′) q(x, a) new q value input x action a, r immediate reward action taken response input x, v (x′) maximum (over actions) q value state reached action taken state x, β fraction way new q value, q(x, a), adjusted equal r + γv (x′). watkins dayan [watkins & dayan, 1992] prove that, certain con- ditions, q values computed learning procedure converge optimal ones (that is, ones optimal policy based). deﬁne ni(x, a) index (episode number) i-th time action tried state x. then, have\n",
      "\n",
      "150 chapter 11. delayed-reinforcement learning theorem 11.1 (watkins dayan) markov problems states {x} actions {a}, given bounded rewards |rn| ≤r, learning rates 0 ≤cn < 1, ∞ x i=0 cni(x,a) = ∞, ∞ x i=0 \u0002 cni(x,a) \u00032 < ∞ x a, qn(x, a) →q∗ n(x, a) n →∞, x a, probability 1, q∗ n(x, a) corresponds q values optimal policy. again, quote [watkins & dayan, 1992, page 281] “the important condition implicit convergence theorem . . . sequence episodes forms basis learning include inﬁnite number episodes starting state action. considered strong condition way states actions selected—however, stochastic con- ditions theorem, method guaranteed ﬁnd optimal policy weaker conditions. note, however, episodes need form continuous sequence—that x′ episode need x episode.” relationships q learning, dynamic programming, control described [barto, bradtke, & singh, 1994]. q learning best thought stochastic approximation method calculating q values. deﬁnition optimal q values state depends recursively expected values q values subsequent states (and expected values rewards), expected values explicitly computed procedure. instead, values approximated iterative sampling actual stochastic mechanism produces successor states. 11.5 discussion, limitations, extensions q-learning 11.5.1 illustrative example q-learning procedure requires maintain table q(x, a) values state-action pairs. grid world described earlier, table excessively large. start random entries table; portion intial table follows\n",
      "\n",
      "11.5. discussion, limitations, extensions q-learning151 x q(x, a) r(x, a) (2,3) w 7 0 (2,3) n 4 0 (2,3) e 3 0 (2,3) s 6 0 (1,3) w 4 -1 (1,3) n 5 0 (1,3) e 2 0 (1,3) s 4 0 suppose robot cell (2,3). maximum q value occurs = w, robot moves west cell (1,3)—receiving immediate reward. maximum q value cell (1,3) 5, learning mechanism attempts value q((2, 3), w) closer discounted value 5 plus immediate reward (which 0 case). learning rate parameter c = 0.5 γ = 0.9, q value q((2, 3), w) adjusted 7 5.75. changes table episode. reader try learning procedure grid world simple computer program. notice optimal policy discovered cells visited actions tried frequently enough. learning problem faced agent associate speciﬁc actions speciﬁc input patterns. q learning gradually reinforces actions con- tribute positive rewards increasing associated q values. typically, example, rewards occur somewhat actions lead them— phrase delayed-reinforcement learning. imagine better better approximations optimal q values gradually propagate states producing rewards states agent fre- quently visits. random q values begin, agent’s actions random walk space states. random walk happens stumble rewarding states q learning begin produce q values useful, and, then, q values work way outward rewarding states. general problem associating rewards state-action pairs called temporal credit assignment problem—how credit reward apportioned actions leading it? q learning is, date, successful technique temporal credit assignment, related method, called bucket brigade algorithm, proposed [holland, 1986]. learning problems similar faced agent grid world thoroughly studied sutton proposed architecture, called dyna, solving [sutton, 1990]. dyna combines reinforcement learning planning. sutton characterizes planning learning simulated world models world agent inhabits. agent’s model world obtained q learning actual world, planning accomplished q learning model world. note learning problem faced grid-world robot\n",
      "\n",
      "152 chapter 11. delayed-reinforcement learning modiﬁed places grid positive rewards. possibility presents interesting way generalize classical notion “goal” ai planning systems—even learning. instead representing goal condition achieved, represent “goal struc- ture” set rewards given achieving conditions. then, generalized “goal” maximizing discounted future reward instead simply achieving particular condition. generalization encompass so-called goals maintenance goals avoidance. exam- ple presented included avoiding bumping grid-world boundary. goal maintenance, particular state, expressed terms reward earned agent state performed action transitioned state step. 11.5.2 random actions pattern presentation sequence patterns caused agent’s action response pattern, called on-line learning method. watkins dayan’s terminology, on-line learning episodes form continous sequence. mentioned, convergence theorem q learning require on-line learning; indeed, special precautions taken ensure on-line learning meets conditions theorem. on-line learning discovers good paths rewards, agent ﬁxate discover policy leads possibly greater long-term reward. reinforcement learning phraseology, problem referred problem exploitation (of learned behavior) versus exploration (of possibly better behavior). way force exploration perform occasional random actions (in- stead single action prescribed current q values). example, grid-world problem, imagine selecting action randomly ac- cording probability distribution actions (n, e, s, w). distribution, turn, depend q values. example, ﬁrst ﬁnd action prescribed q values choose action probability 1/2, choose orthogonal actions probability 3/16 each, choose opposite action probability 1/8. policy modiﬁed “simulated annealing” gradually increase probabil- ity action prescribed q values time goes on. strategy favor exploration beginning learning exploitation later. methods, also, proposed dealing exploration, in- cluding making unvisited states intrinsically rewarding “interval estimate,” related uncertainty estimate state’s value [kaelbling, 1993].\n",
      "\n",
      "11.5. discussion, limitations, extensions q-learning153 11.5.3 generalizing inputs large problems impractical maintain table like grid-world example. researchers suggested mechanisms computing q values, given pattern inputs actions. method sug- gests use neural network. example, consider simple linear machine shown fig. 11.4. x . . . . . . y y y trainable weights y wi r dot product units q(ai, x) = x . wi q(a1, x) q(a2, x) q(ar, x) figure 11.4 net computes q values neural net agent r actions select from. q values (as function input pattern x action ai) computed dot products weight vectors (one action) input vector. weight adjustments according td(0) procedure bring q value action selected closer sum immediate reward (if any) (discounted) maximum q value input pattern. optimum q values problem (whatever be) complex computed linear machine, layered neural network used. sigmoid units ﬁnal layer compute q values range 0 1. td(0) method updating q values combined multi-layer weight-changing rule, backpropagation. networks sort able aggregate diﬀerent input vectors regions action performed. kind aggregation example called structural credit assignment. combining td(λ) backpropagation method dealing temporal structural credit assignment problems.\n",
      "\n",
      "154 chapter 11. delayed-reinforcement learning interesting examples delayed-reinforcement training simulated actual robots requiring structural credit assignment reported [lin, 1992, mahadevan & connell, 1992]. 11.5.4 partially observable states far, identiﬁed input vector, x, actual state envi- ronment. input vector results agent’s perceptual apparatus (as assume does), reason suppose uniquely identiﬁes environmental state. inevitable perceptual limitations, diﬀerent environmental states rise input vector. phenomenon referred perceptual aliasing. perceptual alias- ing, longer guarantee q learning result useful action policies, let optimal ones. researchers attempted deal problem variety methods including attempting model “hid- den” states internal memory [lin, 1993]. is, aspect environment sensed currently, sensed remembered agent. case, longer markov problem; is, x vector, given action, depend sequence previous ones immediately preceding one. possible reinstate markov framework (over x’s) x includes current sensory precepts information agent’s memory. 11.5.5 scaling problems diﬃculties far prohibited wide application reinforcement learn- ing large problems. (the td-gammon program, mentioned chap- ter, probably unique terms success high-dimensional problem.) touched diﬃculties; summarized references attempts overcome them. a. exploration versus exploitation. • use random actions • favor states visited recently • separate learning phase use phase • employ teacher guide exploration b. slow time convergence • combine learning prior knowledge; use estimates q values (rather random values) initially • use hierarchy actions; learn primitive actions ﬁrst freeze useful sequences macros learn use macros\n",
      "\n",
      "11.6. bibliographical historical remarks 155 • employ teacher; use graded “lessons”—starting near rewards backing away, use examples good behavior [lin, 1992] • use eﬃcient computations; e.g. updates episode [moore & atkeson, 1993] c. large state spaces • use hand-coded features • use neural networks • use nearest-neighbor methods [moore, 1990] d. temporal discounting problems. small γ learner greedy present rewards indiﬀerent future; large γ slows learning. • use learning method based average rewards [schwartz, 1993] e. “transfer” learning . learned depends reward struc- ture; rewards change, learning start over. • separate learning parts learn “action model” predicts actions change states (and constant prob- lems), learn “values” states reinforcement learn- ing diﬀerent set rewards. reinforcement learning replaced “planner” uses action model produce plans achieve goals. articles special issue reinforcement learning machine learning, 8, may, 1992. 11.6 bibliographical historical remarks added.\n",
      "\n",
      "156 chapter 11. delayed-reinforcement learning\n",
      "\n",
      "chapter 12 explanation-based learning 12.1 deductive learning learning methods studied far, typically training set ex- haust version space. logical terminology, classi- ﬁer’s output logically follow training set. sense, methods inductive. logic, deductive system conclusions logically follow set input facts, system sound.1 contrast inductive deductive systems logical setting, suppose set facts (the training set) includes following formulas {round(obj1), round(obj2), round(obj3), round(obj4), ball(obj1), ball(obj2), ball(obj3), ball(obj4)} learning system forms conclusion (∀x)[ball(x) ⊃round(x)] in- ductive. conclusion useful (if facts form ball(σ) ∧¬round(σ)), logically follow facts. hand, facts green(obj5) green(obj5) ⊃round(obj5), logically conclude round(obj5). making conclusion sav- ing instance deductive learning—a topic study chapter. suppose logical proposition, φ, logically follows set facts, ∆. circumstances process deducing φ ∆results learning φ? sense, implicitly knew φ along, inherent knowing ∆. yet, φ obvious given ∆, 1logical reasoning systems sound, example non-monotonic reasoning, produce inductive conclusions logically follow input facts. 157\n",
      "\n",
      "158 chapter 12. explanation-based learning deduction process establish φ arduous. deduce φ again, want save it, deduction, case needed later. shouldn’t process count learning? dietterich [dietterich, 1990] called type learning speed-up learning. strictly speaking, speed-up learning result system able decisions that, principle, learning took place. speed-up learning simply makes possible decisions eﬃciently. but, practice, type learning possible certain decisions infeasible. extreme case, chess player said learn chess optimal play inherent rules chess. surface, real diﬀerence experience-based hypotheses chess player makes constitutes good play kind learning studying far. example, suppose given theorems geometry asked prove sum angles right triangle 180 degrees. let suppose proof constructed depend given triangle right triangle; case learn general fact. learning technique going study related example. called explanation-based learning (ebl). ebl thought process implicit knowledge converted explicit knowledge. ebl, specialize parts domain theory explain particular ex- ample, generalize explanation produce element domain theory useful similar examples. process illustrated fig. 12.1. 12.2 domain theories types information present inductive methods studied information inherent training samples information domain implied “bias” (for example, hypothesis set choose functions). learning methods successful hypothesis set appropriate problem. typically, smaller hypothesis set (that is, priori information function sought), dependent information supplied training set (that is, fewer samples). priori information problem expressed ways. methods studied far restrict hypotheses direct way. direct method involves making assertions logical language property trying learn. set assertions usually called “domain theory.” suppose, example, wanted classify people according good credit risks. represent person set properties (income, marital status, type employment, etc.), assemble\n",
      "\n",
      "12.3. example 159 domain theory example (x p) prove x p specialize explanation (proof) generalize new domain rule things \"like\" x p y like x complex proof process trivial proof y p figure 12.1 ebl process data people known good bad credit risks train classiﬁer decisions. or, loan oﬃcer bank, ask sorts things s/he looks making decision loan, encode knowledge set rules expert system, use expert system decisions. knowledge loan oﬃcer originated set “policies” (the domain theory), application policies specialized eﬃcient experience special cases loans district. 12.3 example discussion concrete, let’s consider following fanciful exam- ple. want ﬁnd way classify robots “robust” not. attributes use represent robot include relevant decision not.\n",
      "\n",
      "160 chapter 12. explanation-based learning suppose domain theory logical sentences taken together, help deﬁne robot classiﬁed robust. (the domain theory useful purposes also, things, describes concept “robust.”) example, let’s suppose domain theory includes sentences fixes(u, u) ⊃robust(u) (an individual ﬁx robust.) sees(x, y) ∧habile(x) ⊃fixes(x, y) (a habile individual entity ﬁx entity.) robot(w) ⊃sees(w, w) (all robots themselves.) r2d2(x) ⊃habile(x) (r2d2-class individuals habile.) c3po(x) ⊃habile(x) (c3po-class individuals habile.) . . . (by convention, variables assumed universally quantiﬁed.) use theorem-proving methods operating domain theory conclude certain robots robust. methods computationally expensive extensive search performed derive conclusion. having found proof particular robot, able derive new sentence use allows faster conclusion. new rule derived example. suppose given number facts num5, as robot(num5) r2d2(num5) age(num5, 5) manufacturer(num5, gr) . . .\n",
      "\n",
      "12.3. example 161 fixes(u, u) => robust(u) robust(num5) fixes(num5, num5) sees(num5,num5) habile(num5) sees(x,y) & habile(x) => fixes(x,y) robot(w) => sees(w,w) robot(num5) r2d2(x) => habile(x) r2d2(num5) figure 12.2 proof tree told robust(num5) true, attempt ﬁnd proof assertion facts num5 domain theory. facts num5 correspond features use represent num5. example, relevant decision robust(num5). relevant ones needed proving robust(num5) domain theory. proof tree fig. 12.2 typical theorem-proving system produce. language ebl, proof explanation fact robust(num5). explanation facts num5 robot(num5) r2d2(num5). fact, con- struct following rule explanation robot(num5) ∧r2d2(num5) ⊃robust(num5) explanation allowed prune attributes num5 irrelevant (at deciding robust(num5)). type pruning ﬁrst sense explanation generalize classiﬁcation problem. ([dejong & mooney, 1986] aspect explanation-based learning feature elimination.) rule extracted explanation applies num5. little value learning rule speciﬁc. generalized applied individuals well?\n",
      "\n",
      "162 chapter 12. explanation-based learning examination proof shows proof structure, sentences domain theory, independently talking num5 individual. generalize proof process replaces constants tip nodes proof tree variables works upward—using uniﬁcation constrain values variables needed obtain proof. example, replace robot(num5) robot(r) r2d2(num5) r2d2(s) redo proof—using explanation proof template. note use diﬀerent values diﬀerent occurrences num5 tip nodes. results general, valid rules. apply rules proof forward direction, keeping track substitutions imposed general uniﬁers proof. (note substitute terms tree variables rules.) process results generalized proof tree shown fig. 12.3. note occurrence sees(r, r) node tree forces uniﬁcation x y domain rule, sees(x, y)∧habile(y) ⊃fixes(x, y). substitutions applied variables tip nodes root node yield general rule robot(r) ∧r2d2(r) ⊃robust(r). rule end result ebl example. process num5 example generalized variable [dejong & mooney, 1986] identity elimination (the precise identity num5 turned irrelevant). (the generalization process described ex- ample based [dejong & mooney, 1986] diﬀers [mitchell, et al., 1986]. similar [fikes, et al., 1972].) clearly, certain assumptions, general rule easily con- clude robust individual original proof process was. important note derived general rule domain theory example. (in literature, called static analysis [etzioni, 1991].) fact, example told new told num5. sole role example instance ebl provide template proof help guide generalization process. basing generalization process examples helps insure learn rules matched distribution problems occur. number qualiﬁcations elaborations ebl need mentioned. 12.4 evaluable predicates domain theory includes number predicates occuring formula trying prove custom- arily describe individual. note, example, habile(num5) describe num5, proof shorter. didn’t we? situation analogous data base augmented logical rules. application, formulas actual data base\n",
      "\n",
      "12.4. evaluable predicates 163 robust(r) fixes(r, r) sees(r,r) habile(s) robot(r) r2d2(s) {r/w} {s/x} {r/x, r/y, r/s} {r/u} robot(w) => sees(w,w) r2d2(x) => habile(x) sees(x,y) & habile(x) => fixes(x,y) fixes(u, u) => robust(u) r2d2(r) applying {r/s} figure 12.3 generalized proof tree “extensional,” logical rules “intensional.” usage reﬂects fact predicates data base deﬁned extension—we explicitly list tuples sastisfying relation. logical rules serve connect data base predicates higher level abstractions described (if deﬁned) rules. typically look truth values formulas containing intensional predicates; derived rules database. ebl process assumes similar. domain theory useful connecting formulas want prove truth values “looked up” evaluated. ebl literature, formulas satisfy called operationality criterion. analogy neural networks. evaluable predicates correspond components input pattern vector; predicates domain theory correspond hidden units. finding new rule corresponds ﬁnding simpler expression formula proved terms evaluable predicates.\n",
      "\n",
      "164 chapter 12. explanation-based learning 12.5 general proofs examining domain theory example reveals alternative rule been robot(u) ∧c3po(u) ⊃robust(u). rule resulted given {c3po(num6), robot(num6), . . .} proved robust(num6). considering examples (num5 num6), arises, want generalize rules like robot(u)∧[c3po(u)∨r2d2(u)] ⊃robust(u)? example [dejong & mooney, 1986] structural generalization (via disjunctive augmen- tation ). adding disjunctions alternative proof soon cumbersome destroy eﬃciency advantage ebl. example, eﬃciency retrieved evaluable predicate, say, bionic(u) domain theory contained r2d2(x) ⊃bionic(x) c3po(x) ⊃ bionic(x). seeing number similar examples, willing induce formula bionic(u) ⊃[c3po(u) ∨r2d2(u)] case rule disjunction replaced robot(u) ∧bionic(u) ⊃robust(u). 12.6 utility ebl known theorem proving complexity ﬁnding proof depends number formulas domain theory depth shortest proof. adding new rule decreases depth shortest proof increases number formulas domain theory. realistic applications, added rules relevant tasks others. thus, unclear overall utility new rules turn positive. ebl methods applied settings, usually positive utility. (see [minton, 1990] analysis). 12.7 applications applications ebl methods. mention here, formation macro-operators automatic plan generation learning control search. 12.7.1 macro-operators planning automatic planning systems, eﬃciency enhanced chain- ing sequence operators macro-operators. exam- ple process creating macro-operators based techniques explored [fikes, et al., 1972]. referring fig. 12.4, consider problem ﬁnding plan robot room r1 fetch box, b1, going adjacent room, r2, pushing\n",
      "\n",
      "12.7. applications 165 r1. goal robot inroom(b1, r1), facts true initial state listed ﬁgure. r1 r2 r3 d1 d2 b1 initial state inroom(robot, r1) inroom(b1,r2) connects(d1,r1,r2) connects(d1,r2,r1) . . . figure 12.4 initial state robot problem construct plan set strips operators include gothru(d, r1, r2) preconditions inroom(robot, r1), connects(d, r1, r2) delete list inroom(robot, r1) add list inroom(robot, r2) pushthru(b, d, r1, r2) preconditions inroom(robot, r1), connects(d, r1, r2), inroom(b, r1) delete list inroom(robot, r1), inroom(b, r1) add list inroom(robot, r2), inroom(b, r2) backward-reasoning strips system produce plan shown fig. 12.5. main goal subgoals solution path. (the conditions subgoal true initial state shown underlined.) preconditions plan, true initial state, are inroom(robot, r1)\n",
      "\n",
      "166 chapter 12. explanation-based learning connects(d1, r1, r2) connects(d1, r2, r1) inroom(b1, r2) saving speciﬁc plan, valid speciﬁc constants mentions, useful saving general one. ﬁrst generalize preconditions substituting variables constants. follow structure speciﬁc plan produce generalized plan shown fig. 12.6 achieves inroom(b1, r4). note generalized plan require pushing box place robot started. preconditions generalized plan are inroom(robot, r1) connects(d1, r1, r2) connects(d2, r2, r4) inroom(b, r4) inroom(b1,r1) pushthru(b1,d,r1,r1) inroom(robot, r1), connects(d, r1, r1), inroom(b1, r1) inroom(robot, r2), connects(d1, r2, r1), inroom(b1, r2) {r2/r1, d1/d} gothru(d2, r3, r2) inroom(robot, r3), connects(d2, r3, r2), connects(d1, r2, r1), inroom(b1, r2) {r1/r3, d1/d2} inroom(robot, r1), connects(d1, r1, r2), connects(d1, r2, r1), inroom(b1, r2) r1 r2 r3 d1 d2 gothru(d1,r1,r2) pushthru(b1,d1,r2,r1) b1 plan figure 12.5 plan robot problem related technique chains sequences operators form general ones chunking mechanism soar [laird, et al., 1986].\n",
      "\n",
      "12.7. applications 167 inroom(b1,r4) pushthru(b1,d2,r2,r4) inroom(robot, r2), connects(d1, r1, r2), connects(d2, r2, r4), inroom(b1, r4) gothru(d1, r1, r2) inroom(robot, r1), connects(d1, r1, r2), connects(d2, r2, r4), inroom(b1, r4) figure 12.6 generalized plan 12.7.2 learning search control knowledge use creating macro-operators, ebl methods improve eﬃciency planning way also. system called prodigy, minton proposed ebl learn eﬀective ways control search [minton, 1988]. prodigy strips-like system solves planning problems blocks-world, simple mobile robot world, job-shop scheduling. prodigy domain theory involving domain problem simple (meta) theory planning. meta theory includes statements control choice subgoal work on, oper- ator apply, etc. succeeds fails. producing plan, analyzes successful unsuccessful choices attempts explain terms domain theory. ebl-like process, able produce useful control rules as\n",
      "\n",
      "168 chapter 12. explanation-based learning (and (current −node node) (candidate −goal node (on x y)) (candidate −goal node (on y z))) (prefer goal (on y z) (on x y)) prodigy keeps statistics learned rules used, savings (in time ﬁnd plans), cost application. saves rules utility, measured, judged high. minton [minton, 1990] shown overall advantage rules (as having rules hand-coded search control rules). 12.8 bibliographical historical remarks added.\n",
      "\n",
      "bibliography [acorn & walden, 1992] acorn, t., walden, s., “smart support man- agement automated reasoning technology compaq customer ser- vice,” proc. fourth annual conf. innovative applications artiﬁcial intelligence, menlo park, ca aaai press, 1992. [aha, 1991] aha, d., kibler, d., albert, m., “instance-based learning algorithms,” machine learning, 6, 37-66, 1991. [anderson & bower, 1973] anderson, j. r., bower, g. h., human asso- ciative memory, hillsdale, nj erlbaum, 1973. [anderson, 1958] anderson, t. w., introduction multivariate statistical analysis, new york john wiley, 1958. [barto, bradtke, & singh, 1994] barto, a., bradtke, s., singh, s., “learn- ing act real-time dynamic programming,” appear ar- tiﬁcial intelligence, 1994. [baum & haussler, 1989] baum, e, haussler, d., “what size net gives valid generalization?” neural computation, 1, pp. 151-160, 1989. [baum, 1994] baum, e., “when k-nearest neighbor backpropagation accurate feasible-sized sets examples?” hanson, s., drastal, g., rivest, r., (eds.), computational learning theory natural learning systems, volume 1 constraints prospects, pp. 415-442, cambridge, ma mit press, 1994. [bellman, 1957] bellman, r. e., dynamic programming, princeton princeton university press, 1957. [blumer, et al., 1987] blumer, a., et al., “occam’s razor,” info. process. lett., vol 24, pp. 377-80, 1987. [blumer, et al., 1990] blumer, a., et al., “learnability vapnik- chervonenkis dimension,” jacm, 1990. [bollinger & duﬃe, 1988] bollinger, j., duﬃe, n., computer control machines processes, reading, ma addison-wesley, 1988. 169\n",
      "\n",
      "170 bibliography [brain, et al., 1962] brain, a. e., et al., “graphical data processing research study experimental investigation,” report no. 8 (pp. 9-13) no. 9 (pp. 3-10), contract da 36-039 sc-78343, sri international, menlo park, ca, june 1962 september 1962. [breiman, et al., 1984] breiman, l., friedman, j., olshen, r., stone, c., classiﬁcation regression trees, monterey, ca wadsworth, 1984. [brent, 1990] brent, r. p., “fast training algorithms multi-layer neural nets,” numerical analysis project manuscript na-90-03, computer sci- ence department, stanford university, stanford, 94305, march 1990. [bryson & ho 1969] bryson, a., ho, y.-c., applied optimal control, new york blaisdell. [buchanan & wilkins, 1993] buchanan, b. wilkins, d., (eds.), readings knowledge acquisition learning, san francisco morgan kaufmann, 1993. [carbonell, 1983] carbonell, j., “learning analogy,” machine learning artiﬁcial intelligence approach, michalski, r., carbonell, j., mitchell, t., (eds.), san francisco morgan kaufmann, 1983. [cheeseman, et al., 1988] cheeseman, p., et al., “autoclass bayesian clas- siﬁcation system,” proc. fifth intl. workshop machine learning, morgan kaufmann, san mateo, ca, 1988. reprinted shavlik, j. dietterich, t., readings machine learning, morgan kaufmann, san francisco, pp. 296-306, 1990. [cover & hart, 1967] cover, t., hart, p., “nearest neighbor pattern clas- siﬁcation,” ieee trans. information theory, 13, 21-27, 1967. [cover, 1965] cover, t., “geometrical statistical properties systems linear inequalities applications pattern recognition,” ieee trans. elec. comp., ec-14, 326-334, june, 1965. [dasarathy, 1991] dasarathy, b. v., nearest neighbor pattern classiﬁcation techniques, ieee computer society press, 1991. [dayan & sejnowski, 1994] dayan, p., sejnowski, t., “td(λ) converges probability 1,” machine learning, 14, pp. 295-301, 1994. [dayan, 1992] dayan, p., “the convergence td(λ) general λ,” machine learning, 8, 341-362, 1992. [dejong & mooney, 1986] dejong, g., mooney, r., “explanation-based learning alternative view,” machine learning, 1145-176, 1986. reprinted shavlik, j. dietterich, t., readings machine learn- ing, san francisco morgan kaufmann, 1990, pp 452-467.\n",
      "\n",
      "bibliography 171 [dietterich & bakiri, 1991] dietterich, t. g., bakiri, g., “error-correcting output codes general method improving multiclass induc- tive learning programs,” proc. ninth nat. conf. a.i., pp. 572-577, aaai-91, mit press, 1991. [dietterich, et al., 1990] dietterich, t., hild, h., bakiri, g., “a compara- tive study id3 backpropagation english text-to-speech map- ping,” proc. seventh intl. conf. mach. learning, porter, b. mooney, r. (eds.), pp. 24-31, san francisco morgan kaufmann, 1990. [dietterich, 1990] dietterich, t., “machine learning,” annu. rev. comput. sci., 4255-306, palo alto annual reviews inc., 1990. [duda & fossum, 1966] duda, r. o., fossum, h., “pattern classiﬁcation iteratively determined linear piecewise linear discriminant functions,” ieee trans. elect. computers, vol. ec-15, pp. 220-232, april, 1966. [duda & hart, 1973] duda, r. o., hart, p.e., pattern classiﬁcation scene analysis, new york wiley, 1973. [duda, 1966] duda, r. o., “training linear machine mislabeled patterns,” sri tech. report prepared onr contract 3438(00), sri in- ternational, menlo park, ca, april 1966. [efron, 1982] efron, b., jackknife, bootstrap resampling plans, philadelphia siam, 1982. [ehrenfeucht, et al., 1988] ehrenfeucht, a., et al., “a general lower bound number examples needed learning,” proc. 1988 workshop computational learning theory, pp. 110-120, san francisco morgan kaufmann, 1988. [etzioni, 1991] etzioni, o., “static problem-space compiler prodigy,” proc. ninth national conf. artiﬁcial intelligence, pp. 533-540, menlo park aaai press, 1991. [etzioni, 1993] etzioni, o., “a structural theory explanation-based learn- ing,” artiﬁcial intelligence, 601, pp. 93-139, march, 1993. [evans & fisher, 1992] evans, b., fisher, d., process delay analyses decision-tree induction, tech. report cs92-06, department com- puter science, vanderbilt university, tn, 1992. [fahlman & lebiere, 1990] fahlman, s., lebiere, c., “the cascade- correlation learning architecture,” touretzky, d., (ed.), advances neural information processing systems, 2, pp. 524-532, san francisco morgan kaufmann, 1990.\n",
      "\n",
      "172 bibliography [fayyad, et al., 1993] fayyad, u. m., weir, n., djorgovski, s., “skicat machine learning system automated cataloging large scale sky surveys,” proc. tenth intl. conf. machine learning, pp. 112- 119, san francisco morgan kaufmann, 1993. (for longer version paper see fayyad, u. djorgovski, g., weir, n., “automating analysis cataloging sky surveys,” fayyad, u., et al.(eds.), advances knowledge discovery data mining, chapter 19, pp. 471ﬀ., cambridge mit press, march, 1996.) [feigenbaum, 1961] feigenbaum, e. a., “the simulation verbal learning be- havior,” proceedings western joint computer conference, 19121- 132, 1961. [fikes, et al., 1972] fikes, r., hart, p., nilsson, n., “learning execut- ing generalized robot plans,” artiﬁcial intelligence, pp 251-288, 1972. reprinted shavlik, j. dietterich, t., readings machine learn- ing, san francisco morgan kaufmann, 1990, pp 468-486. [fisher, 1987] fisher, d., “knowledge acquisition incremental conceptual clustering,” machine learning, 2139-172, 1987. reprinted shavlik, j. dietterich, t., readings machine learning, san francisco morgan kaufmann, 1990, pp. 267–283. [friedman, et al., 1977] friedman, j. h., bentley, j. l., finkel, r. a., “an algorithm finding best matches logarithmic expected time,” acm trans. math. software, 3(3)209-226, september 1977. [fu, 1994] fu, l., neural networks artiﬁcial intelligence, new york mcgraw-hill, 1994. [gallant, 1986] gallant, s. i., “optimal linear discriminants,” eighth inter- national conf. pattern recognition, pp. 849-852, new york ieee, 1986. [genesereth & nilsson, 1987] genesereth, m., nilsson, n., logical founda- tions artiﬁcial intelligence, san francisco morgan kaufmann, 1987. [gluck & rumelhart, 1989] gluck, m. rumelhart, d., neuroscience connectionist theory, developments connectionist theory, hills- dale, nj erlbaum associates, 1989. [hammerstrom, 1993] hammerstrom, d., “neural networks work,” ieee spectrum, pp. 26-32, june 1993. [haussler, 1988] haussler, d., “quantifying inductive bias ai learning al- gorithms valiant’s learning framework,” artiﬁcial intelligence, 36177-221, 1988. reprinted shavlik, j. dietterich, t., readings machine learning, san francisco morgan kaufmann, 1990, pp. 96-107.\n",
      "\n",
      "bibliography 173 [haussler, 1990] haussler, d., “probably approximately correct learning,” proc. eighth nat. conf. ai, pp. 1101-1108. cambridge, ma mit press, 1990. [hebb, 1949] hebb, d. o., organization behaviour, new york john wiley, 1949. [hertz, krogh, & palmer, 1991] hertz, j., krogh, a, palmer, r., introduc- tion theory neural computation, lecture notes, vol. 1, santa fe inst. studies sciences complexity, new york addison- wesley, 1991. [hirsh, 1994] hirsh, h., “generalizing version spaces,” machine learning, 17, 5-45, 1994. [holland, 1975] holland, j., adaptation natural artiﬁcial systems, ann arbor university michigan press, 1975. (second edition printed 1992 mit press, cambridge, ma.) [holland, 1986] holland, j. h., “escaping brittleness; possibilities general-purpose learning algorithms applied parallel rule-based systems.” michalski, r., carbonell, j., mitchell, t. (eds.) , ma- chine learning artiﬁcial intelligence approach, volume 2, chapter 20, san francisco morgan kaufmann, 1986. [hunt, marin, & stone, 1966] hunt, e., marin, j., stone, p., experiments induction, new york academic press, 1966. [jabbour, k., et al., 1987] jabbour, k., et al., “alfa automated load fore- casting assistant,” proc. ieee pwer engineering society summer meeting, san francisco, ca, 1987. [john, 1995] john, g., “robust linear discriminant trees,” proc. conf. artiﬁcial intelligence statistics, ft. lauderdale, fl, january, 1995. [kaelbling, 1993] kaelbling, l. p., learning embedded systems, cambridge, ma mit press, 1993. [kohavi, 1994] kohavi, r., “bottom-up induction oblivious read-once de- cision graphs,” proc. european conference machine learning (ecml-94), 1994. [kolodner, 1993] kolodner, j., case-based reasoning, san francisco morgan kaufmann, 1993. [koza, 1992] koza, j., genetic programming programming comput- ers means natural selection, cambridge, ma mit press, 1992. [koza, 1994] koza, j., genetic programming ii automatic discovery reusable programs, cambridge, ma mit press, 1994.\n",
      "\n",
      "174 bibliography [laird, et al., 1986] laird, j., rosenbloom, p., newell, a., “chunking soar anatomy general learning mechanism,” machine learn- ing, 1, pp. 11-46, 1986. reprinted buchanan, b. wilkins, d., (eds.), readings knowledge acquisition learning, pp. 518-535, morgan kaufmann, san francisco, ca, 1993. [langley, 1992] langley, p., “areas application machine learning,” proc. fifth int’l. symp. knowledge engineering, sevilla, 1992. [langley, 1996] langley, p., elements machine learning, san francisco morgan kaufmann, 1996. [lavraˇc & dˇzeroski, 1994] lavraˇc, n., dˇzeroski, s., inductive logic pro- gramming, chichester, england ellis horwood, 1994. [lin, 1992] lin, l., “self-improving reactive agents based reinforcement learning, planning, teaching,” machine learning, 8, 293-321, 1992. [lin, 1993] lin, l., “scaling reinforcement learning robot control,” proc. tenth intl. conf. machine learning, pp. 182-189, san francisco morgan kaufmann, 1993. [littlestone, 1988] littlestone, n., “learning quickly irrelevant at- tributes abound new linear-threshold algorithm,” machine learn- ing 2 285-318, 1988. [maass & tur´an, 1994] maass, w., tur´an, g., “how fast thresh- old gate learn?,” hanson, s., drastal, g., rivest, r., (eds.), computational learning theory natural learning systems, volume 1 constraints prospects, pp. 381-414, cambridge, ma mit press, 1994. [mahadevan & connell, 1992] mahadevan, s., connell, j., “automatic programming behavior-based robots reinforcement learn- ing,” artiﬁcial intelligence, 55, pp. 311-365, 1992. [marchand & golea, 1993] marchand, m., golea, m., “on learning sim- ple neural concepts halfspace intersections neural decision lists,” network, 467-85, 1993. [mcculloch & pitts, 1943] mcculloch, w. s., pitts, w. h., “a logical cal- culus ideas immanent nervous activity,” bulletin mathe- matical biophysics, vol. 5, pp. 115-133, chicago university chicago press, 1943. [michie, 1992] michie, d., “some directions machine intelligence,” unpub- lished manuscript, turing institute, glasgow, scotland, 1992. [minton, 1988] minton, s., learning search control knowledge explanation-based approach, kluwer academic publishers, boston, ma, 1988.\n",
      "\n",
      "bibliography 175 [minton, 1990] minton, s., “quantitative results concerning utility explanation-based learning,” artiﬁcial intelligence, 42, pp. 363-392, 1990. reprinted shavlik, j. dietterich, t., readings machine learning, san francisco morgan kaufmann, 1990, pp. 573-587. [mitchell, et al., 1986] mitchell, t., et al., “explanation-based generalization unifying view,” machine learning, 11, 1986. reprinted shavlik, j. dietterich, t., readings machine learning, san francisco morgan kaufmann, 1990, pp. 435-451. [mitchell, 1982] mitchell, t., “generalization search,” artiﬁcial intelligence, 18203-226, 1982. reprinted shavlik, j. dietterich, t., readings machine learning, san francisco morgan kaufmann, 1990, pp. 96–107. [moore & atkeson, 1993] moore, a., atkeson, c., “prioritized sweeping reinforcement learning data time,” machine learn- ing, 13, pp. 103-130, 1993. [moore, et al., 1994] moore, a. w., hill, d. j., johnson, m. p., “an em- pirical investigation brute force choose features, smoothers, function approximators,” hanson, s., judd, s., petsche, t., (eds.), computational learning theory natural learning systems, vol. 3, cambridge mit press, 1994. [moore, 1990] moore, a., eﬃcient memory-based learning robot control, phd. thesis; technical report no. 209, computer laboratory, univer- sity cambridge, october, 1990. [moore, 1992] moore, a., “fast, robust adaptive control learning forward models,” moody, j., hanson, s., lippman, r., (eds.), advances neural information processing systems 4, san francisco morgan kaufmann, 1992. [mueller & page, 1988] mueller, r. page, r., symbolic computing lisp prolog, new york john wiley & sons, 1988. [muggleton, 1991] muggleton, s., “inductive logic programming,” new gen- eration computing, 8, pp. 295-318, 1991. [muggleton, 1992] muggleton, s., inductive logic programming, london aca- demic press, 1992. [muroga, 1971] muroga, s., threshold logic applications, new york wiley, 1971. [natarjan, 1991] natarajan, b., machine learning theoretical approach, san francisco morgan kaufmann, 1991.\n",
      "\n",
      "176 bibliography [nilsson, 1965] nilsson, n. j., “theoretical experimental investigations trainable pattern-classifying systems,” tech. report no. radc-tr- 65-257, final report contract af30(602)-3448, rome air develop- ment center (now rome laboratories), griﬃss air force base, new york, september, 1965. [nilsson, 1990] nilsson, n. j., mathematical foundations learning ma- chines, san francisco morgan kaufmann, 1990. (this book reprint learning machines foundations trainable pattern-classifying systems, new york mcgraw-hill, 1965.) [oliver, dowe, & wallace, 1992] oliver, j., dowe, d., wallace, c., “infer- ring decision graphs minimum message length principle,” proc. 1992 australian artiﬁcial intelligence conference, 1992. [pagallo & haussler, 1990] pagallo, g. haussler, d., “boolean feature dis- covery empirical learning,” machine learning, vol.5, no.1, pp. 71-99, march 1990. [pazzani & kibler, 1992] pazzani, m., kibler, d., “the utility knowl- edge inductive learning,” machine learning, 9, 57-94, 1992. [peterson, 1961] peterson, w., error correcting codes, new york john wiley, 1961. [pomerleau, 1991] pomerleau, d., “rapidly adapting artiﬁcial neural net- works autonomous navigation,” lippmann, p., et al. (eds.), ad- vances neural information processing systems, 3, pp. 429-435, san francisco morgan kaufmann, 1991. [pomerleau, 1993] pomerleau, d, neural network perception mobile robot guidance, boston kluwer academic publishers, 1993. [quinlan & rivest, 1989] quinlan, j. ross, rivest, ron, “inferring deci- sion trees minimum description length principle,” informa- tion computation, 80227–248, march, 1989. [quinlan, 1986] quinlan, j. ross, “induction decision trees,” machine learning, 181–106, 1986. reprinted shavlik, j. dietterich, t., readings machine learning, san francisco morgan kaufmann, 1990, pp. 57–69. [quinlan, 1987] quinlan, j. r., “generating production rules decision trees,” ijcai-87 proceedings tenth intl. joint conf. ar- tiﬁcial intelligence, pp. 304-7, san francisco morgan-kaufmann, 1987. [quinlan, 1990] quinlan, j. r., “learning logical deﬁnitions relations,” machine learning, 5, 239-266, 1990.\n",
      "\n",
      "bibliography 177 [quinlan, 1993] quinlan, j. ross, c4.5 programs machine learning, san francisco morgan kaufmann, 1993. [quinlan, 1994] quinlan, j. r., “comparing connectionist symbolic learn- ing methods,” hanson, s., drastal, g., rivest, r., (eds.), com- putational learning theory natural learning systems, volume 1 constraints prospects, pp. 445-456,, cambridge, ma mit press, 1994. [ridgway, 1962] ridgway, w. c., adaptive logic system generalizing properties, phd thesis, tech. rep. 1556-1, stanford electronics labs., stanford, ca, april 1962. [rissanen, 1978] rissanen, j., “modeling shortest data description,” auto- matica, 14465-471, 1978. [rivest, 1987] rivest, r. l., “learning decision lists,” machine learning, 2, 229-246, 1987. [rosenblatt, 1958] rosenblatt, f., principles neurodynamics, washington spartan books, 1961. [ross, 1983] ross, s., introduction stochastic dynamic programming, new york academic press, 1983. [rumelhart, hinton, & williams, 1986] rumelhart, d. e., hinton, g. e., williams, r. j., “learning internal representations error propa- gation,” rumelhart, d. e., mcclelland, j. l., (eds.) parallel distributed processing, vol 1, 318–362, 1986. [russell & norvig 1995] russell, s., norvig, p., artiﬁcial intelligence modern approach, englewood cliﬀs, nj prentice hall, 1995. [samuel, 1959] samuel, a., “some studies machine learning game checkers,” ibm journal research development, 3211-229, july 1959. [schwartz, 1993] schwartz, a., “a reinforcement learning method max- imizing undiscounted rewards,” proc. tenth intl. conf. machine learning, pp. 298-305, san francisco morgan kaufmann, 1993. [sejnowski, koch, & churchland, 1988] sejnowski, t., koch, c., church- land, p., “computational neuroscience,” science, 241 1299-1306, 1988. [shavlik, mooney, & towell, 1991] shavlik, j., mooney, r., towell, g., “symbolic neural learning algorithms experimental compar- ison,” machine learning, 6, pp. 111-143, 1991. [shavlik & dietterich, 1990] shavlik, j. dietterich, t., readings ma- chine learning, san francisco morgan kaufmann, 1990.\n",
      "\n",
      "178 bibliography [sutton & barto, 1987] sutton, r. s., barto, a. g., “a temporal- diﬀerence model classical conditioning,” proceedings ninth annual conference cognitive science society, hillsdale, nj erl- baum, 1987. [sutton, 1988] sutton, r. s., “learning predict methods temporal diﬀerences,” machine learning 3 9-44, 1988. [sutton, 1990] sutton, r., “integrated architectures learning, planning, reacting based approximating dynamic programming,” proc. seventh intl. conf. machine learning, pp. 216-224, san francisco morgan kaufmann, 1990. [taylor, michie, & spiegalhalter, 1994] taylor, c., michie, d., spiegal- halter, d., machine learning, neural statistical classiﬁcation, paramount publishing international. [tesauro, 1992] tesauro, g., “practical issues temporal diﬀerence learn- ing,” machine learning, 8, nos. 3/4, pp. 257-277, 1992. [towell & shavlik, 1992] towell g., shavlik, j., “interpretation artiﬁ- cial neural networks mapping knowledge-based neural networks rules,” moody, j., hanson, s., lippmann, r., (eds.), advances neural information processing systems, 4, pp. 977-984, san francisco morgan kaufmann, 1992. [towell, shavlik, & noordweier, 1990] towell, g., shavlik, j., noordweier, m., “reﬁnement approximate domain theories knowledge-based artiﬁcial neural networks,” proc. eighth natl., conf. artiﬁcial in- telligence, pp. 861-866, 1990. [unger, 1989] unger, s., essence logic circuits, englewood cliﬀs, nj prentice-hall, 1989. [utgoﬀ, 1989] utgoﬀ, p., “incremental induction decision trees,” machine learning, 4161–186, nov., 1989. [valiant, 1984] valiant, l., “a theory learnable,” communications acm, vol. 27, pp. 1134-1142, 1984. [vapnik & chervonenkis, 1971] vapnik, v., chervonenkis, a., “on uniform convergence relative frequencies, theory probability applications, vol. 16, no. 2, pp. 264-280, 1971. [various editors, 1989-1994] advances neural information processing sys- tems, vols 1 6, san francisco morgan kaufmann, 1989 -1994. [watkins & dayan, 1992] watkins, c. j. c. h., dayan, p., “technical note q-learning,” machine learning, 8, 279-292, 1992.\n",
      "\n",
      "bibliography 179 [watkins, 1989] watkins, c. j. c. h., learning delayed rewards, phd thesis, university cambridge, england, 1989. [weiss & kulikowski, 1991] weiss, s., kulikowski, c., computer systems learn, san francisco morgan kaufmann, 1991. [werbos, 1974] werbos, p., regression new tools prediction analysis behavioral sciences, ph.d. thesis, harvard university, 1974. [widrow & lehr, 1990] widrow, b., lehr, m. a., “30 years adaptive neural networks perceptron, madaline backpropagation,” proc. ieee, vol. 78, no. 9, pp. 1415-1442, september, 1990. [widrow & stearns, 1985] widrow, b., stearns, s., adaptive signal pro- cessing, englewood cliﬀs, nj prentice-hall. [widrow, 1962] widrow, b., “generalization storage networks ada- line neurons,” yovits, jacobi, goldstein (eds.), self-organizing systems—1962, pp. 435-461, washington, dc spartan books, 1962. [winder, 1961] winder, r., “single stage threshold logic,” proc. aiee symp. switching circuits logical design, conf. paper cp-60- 1261, pp. 321-332, 1961. [winder, 1962] winder, r., threshold logic, phd dissertation, princeton uni- versity, princeton, nj, 1962. [wnek, et al., 1990] wnek, j., et al., “comparing learning paradigms di- agrammatic visualization,” proc. fifth intl. symp. methodologies intelligent systems, pp. 428-437, 1990. (also tech. report mli90-2, university illinois urbana-champaign.)\n",
      "\n",
      "100 machine learning s & answers steve nouri q1 explain difference supervised unsupervised machine learning? supervised machine learning algorithms, provide labeled data, example, prediction stock market prices, unsupervised need labeled data, example, classification emails spam non-spam. q2 parametric models? example. parametric models finite number parameters. predict new data, need know parameters model. examples include linear regression, logistic regression, linear svms. non-parametric models unbounded number parameters, allowing flexibility. predict new data, need know parameters model state data observed. examples include decision trees, k-nearest neighbors, topic models latent dirichlet analysis. q3 difference classification​ ​and regression? classification produce discrete results, classification classify data specific categories. example, classifying emails spam non-spam categories. whereas, use regression analysis dealing continuous data, example predicting stock prices certain point time. q4 overfitting, avoid it? overfitting situation occurs model learns training set well, taking random fluctuations training data concepts. impact model’s ability generalize don’t apply new data. model given training data, shows 100 percent accuracy—technically slight loss. but, use test data, error low efficiency. condition known overfitting. multiple ways avoiding overfitting, as ● regularization. involves cost term features involved objective function ● making simple model. lesser variables parameters, variance reduced ● cross-validation methods like k-folds ● model parameters likely cause overfitting, techniques regularization like lasso penalize parameters steve nouri ​ ​https//www.linkedin.com/in/stevenouri/\n",
      "\n",
      "q5 meant ‘training set’ ‘test set’? split given data set different sections namely,’training set’ ‘test set’. ‘training set’ portion dataset train model. ‘testing set’ portion dataset test trained model. q6 handle missing corrupted data dataset? easiest ways handle missing corrupted data drop rows columns replace entirely value. useful methods pandas ● isnull() dropna() help find columns/rows missing data drop ● fillna() replace wrong values placeholder value q7 explain ensemble learning. ensemble learning, base models like classifiers regressors generated combined better results. build component classifiers accurate independent. sequential parallel ensemble methods. q8 explain bias-variance tradeoff. predictive models tradeoff bias (how model fits data) variance (how model changes based changes inputs). simpler models stable (low variance) don't close truth (high bias). complex models prone overfitting (high variance) expressive close truth (low bias). best model given problem usually lies middle. q9 difference stochastic gradient descent (sgd) gradient descent (gd)? algorithms methods finding set parameters minimize loss function evaluating parameters data making adjustments. standard gradient descent, you'll evaluate training samples set parameters. akin taking big, slow steps solution. stochastic gradient descent, you'll evaluate 1 training sample set parameters updating them. akin taking small, quick steps solution. q10 choose classifier based training set data size? training set small, model right bias low variance work better likely overfit. steve nouri ​ ​https//www.linkedin.com/in/stevenouri/\n",
      "\n",
      "example, naive bayes works best training set large. models low bias high variance tend perform better work fine complex relationships. q11 3 data preprocessing techniques handle outliers? 1. winsorize (cap threshold). 2. transform reduce skew (using box-cox similar). 3. remove outliers you're certain anomalies measurement errors. q12 data allocate training, validation, test sets? find balance, there's right answer problem. test set small, you'll unreliable estimation model performance (performance statistic high variance). training set small, actual model parameters high variance. good rule thumb use 80/20 train/test split. then, train set split train/validation partitions cross-validation. q13 false positive false negative significant? false positives cases wrongly classified true false. false negatives cases wrongly classified false true. term ‘false positive,’ word ‘positive’ refers ‘yes’ row predicted value confusion matrix. complete term indicates system predicted positive, actual value negative. q14 explain difference l1 l2 regularization. l2 regularization tends spread error terms, l1 binary/sparse, variables assigned 1 0 weighting. l1 corresponds setting laplacean prior terms, l2 corresponds gaussian prior. q15 what’s fourier transform? fourier transform generic method decompose generic functions superposition symmetric functions. intuitive tutorial puts it, given smoothie, it’s find recipe. fourier transform finds set cycle speeds, amplitudes, phases match time signal. fourier transform converts signal time frequency domain — it’s common way extract features audio signals time series sensor data. q16 deep learning, contrast machine learning algorithms? deep learning subset machine learning concerned neural networks use backpropagation certain principles neuroscience accurately model large sets unlabelled semi-structured data. sense, deep learning represents steve nouri ​ ​https//www.linkedin.com/in/stevenouri/\n",
      "\n",
      "unsupervised learning algorithm learns representations data use neural nets. q17 what’s difference generative discriminative model? generative model learn categories data discriminative model simply learn distinction different categories data. discriminative models generally outperform generative models classification tasks. q18 applications supervised machine learning modern businesses? applications supervised machine learning include ● email spam detection train model historical data consists emails categorized spam spam. labeled information fed input model. ● healthcare diagnosis providing images disease, model trained detect person suffering disease not. ● sentiment analysis refers process algorithms documents determine they’re positive, neutral, negative sentiment. ● fraud detection training model identify suspicious patterns, detect instances possible fraud. q19 semi-supervised machine learning? supervised learning uses data completely labeled, unsupervised learning uses training data. case semi-supervised learning, training data contains small labeled data large unlabeled data. q20. unsupervised machine learning techniques? techniques unsupervised learning clustering association. clustering ● clustering problems involve data divided subsets. subsets, called clusters, contain data similar other. different clusters reveal different details objects, unlike classification regression. association ● association problem, identify patterns associations different variables items. ● example, ecommerce website suggest items buy, based prior purchases made, spending habits, items wishlist, customers’ purchase habits, on. steve nouri ​ ​https//www.linkedin.com/in/stevenouri/\n",
      "\n",
      "q21 ‘naive’ naive bayes classifier? classifier called ‘naive’ makes assumptions turn correct. algorithm assumes presence feature class related presence feature (absolute independence features), given class variable. instance, fruit considered cherry red color round shape, regardless features. assumption right (as apple matches description). q22 explain latent dirichlet allocation (lda). latent dirichlet allocation (lda) common method topic modeling, classifying documents subject matter. lda generative model represents documents mixture topics probability distribution possible words. \"dirichlet\" distribution simply distribution distributions. lda, documents distributions topics distributions words. q23 explain principle component analysis (pca). pca method transforming features dataset combining uncorrelated linear combinations. new features, principal components, sequentially maximize variance represented (i.e. principal component variance, second principal component second most, on). result, pca useful dimensionality reduction set arbitrary variance cutoff. q24 what’s f1 score? use it? f1 score measure model’s performance. weighted average precision recall model, results tending 1 best, tending 0 worst. use classification tests true negatives don’t matter much. steve nouri ​ ​https//www.linkedin.com/in/stevenouri/\n",
      "\n",
      "q25 use classification regression? classification produces discrete values dataset strict categories, regression gives continuous results allow better distinguish differences individual points. use classification regression wanted results reflect belongingness data points dataset certain explicit categories (ex wanted know male female correlated male female names.) q26 ensure you’re overfitting model? simple restatement fundamental problem machine learning possibility overfitting training data carrying noise data test set, providing inaccurate generalizations. main methods avoid overfitting 1- model simpler reduce variance taking account fewer variables parameters, removing noise training data. 2- use cross-validation techniques k-folds cross-validation. 3- use regularization techniques lasso penalize certain model parameters they’re likely cause overfitting. q27 know machine learning algorithm choose classification problem? fixed rule choose algorithm classification problem, follow guidelines ● accuracy concern, test different algorithms cross-validate ● training dataset small, use models low variance high bias ● training dataset large, use models high variance little bias q28 design email spam filter? building spam filter involves following process ● email spam filter fed thousands emails ● emails label ‘spam’ ‘not spam.’ ● supervised machine learning algorithm determine type emails marked spam based spam words like lottery, free offer, money, refund, etc. ● time email hit inbox, spam filter use statistical analysis algorithms like decision trees svm determine likely email spam ● likelihood high, label spam, email won’t hit inbox steve nouri ​ ​https//www.linkedin.com/in/stevenouri/\n",
      "\n",
      "● based accuracy model, use algorithm highest accuracy testing models q29 evaluation approaches work gauge effectiveness machine learning model? split dataset training test sets, use cross-validation techniques segment dataset composite sets training test sets data. implement choice selection performance metrics fairly comprehensive list. use measures f1 score, accuracy, confusion matrix. what’s important demonstrate understand nuances model measured choose right performance measures right situations. q30 implement recommendation system company’s users? lot machine learning interview s type involve implementation machine learning models company’s problems. you’ll research company industry in-depth, especially revenue drivers company has, types users company takes context industry it’s in. q31 explain bagging. bagging, bootstrap aggregating, ensemble method dataset divided multiple subsets resampling. then, subset train model, final predictions voting averaging component models. bagging performed parallel. q32 roc curve auc (a.k.a. auroc)? roc (receiver operating characteristic) performance plot binary classifiers true positive rate (y-axis) vs. false positive rate (x- axis). auc area roc curve, it's common performance metric evaluating binary classification models. it's equivalent expected probability uniformly drawn random positive ranked uniformly drawn random negative. steve nouri ​ ​https//www.linkedin.com/in/stevenouri/\n",
      "\n",
      "q33 area roc curve (auroc) better raw accuracy out-of-sample evaluation metric? auroc robust class imbalance, unlike raw accuracy. example, want detect type cancer that's prevalent 1% population, build model achieves 99% accuracy simply classifying cancer-free. q34 advantages disadvantages neural networks? advantages​ neural networks (specifically deep nns) led performance breakthroughs unstructured datasets images, audio, video. incredible flexibility allows learn patterns ml algorithm learn. disadvantages​ however, require large training data converge. it's difficult pick right architecture, internal \"hidden\" layers incomprehensible. q35 define precision recall. precision ● precision ratio events correctly recall total number events recall (mix correct wrong recalls). ● precision = (true positive) / (true positive + false positive) recall ● recall ratio number events recall number total events. ● recall = (true positive) / (true positive + false negative) q36 decision tree classification? decision tree builds classification (or regression) models tree structure, datasets broken ever-smaller subsets developing decision tree, literally tree-like way branches nodes. decision trees handle categorical numerical data. q37 pruning decision trees, done? pruning technique machine learning reduces size decision trees. reduces complexity final classifier, improves predictive accuracy reduction overfitting. pruning occur in ● top-down fashion. traverse nodes trim subtrees starting root ● bottom-up fashion. begin leaf nodes popular pruning algorithm called reduced error pruning, which ● starting leaves, node replaced popular class ● prediction accuracy affected, change kept ● advantage simplicity speed steve nouri ​ ​https//www.linkedin.com/in/stevenouri/\n",
      "\n",
      "q38 recommendation system? spotify shopped amazon recognize recommendation system it’s information filtering system predicts user want hear based choice patterns provided user. q39 kernel svm? kernel svm abbreviated version kernel support vector machine. kernel methods class algorithms pattern analysis, common kernel svm. q40 methods reducing dimensionality? reduce dimensionality combining features feature engineering, removing collinear features, algorithmic dimensionality reduction. gone machine learning interview s, got idea strengths weaknesses domain. q41 stages building model machine learning? stages building machine learning model are ● model building choose suitable algorithm model train according requirement ● model testing​ check accuracy model test data ● applying mode ​make required changes testing use final model real-time projects. here, it’s important remember while, model needs checked sure it’s working correctly. modified sure up-to-date. q42 knn different k-means clustering? k-nearest neighbors supervised classification algorithm, k-means clustering unsupervised clustering algorithm. mechanisms similar first, means order k-nearest neighbors work, need labeled data want classify unlabeled point (thus nearest neighbor part). k-means clustering requires set unlabeled points threshold algorithm unlabeled points gradually learn cluster groups computing mean distance different points. q43 mention difference data mining machine learning? machine learning relates study, design, development algorithms computers capability learn explicitly programmed. data mining defined process unstructured data tries extract knowledge unknown interesting patterns. processing machine, learning algorithms used. steve nouri ​ ​https//www.linkedin.com/in/stevenouri/\n",
      "\n",
      "q44 different algorithm techniques machine learning? different types techniques machine learning ● supervised learning ● unsupervised learning ● semi-supervised learning ● reinforcement learning ● transduction ● learning learn q45 given data set. data set missing values spread 1 standard deviation median. percentage data remain unaffected? why? hints start thinking! data spread median, let’s assume it’s normal distribution. know, normal distribution, ~68% data lies 1 standard deviation mean (or mode, median), leaves ~32% data unaffected. therefore, ~32% data remain unaffected missing values. q46 pca, kpca, ica for? pca (principal components analysis), kpca ( kernel-based principal component analysis) ica ( independent component analysis) important feature extraction techniques dimensionality reduction. q47 support vector machines? support vector machines supervised learning algorithms classification regression analysis. q48 batch statistical learning? statistical learning techniques allow learning function predictor set observed data predictions unseen future data. techniques provide guarantees performance learned predictor future unseen data based statistical assumption data generating process. q49 bias-variance decomposition classification error ensemble method? expected error learning algorithm decomposed bias variance. bias term measures closely average classifier produced learning algorithm matches target function. variance term measures learning algorithm’s prediction fluctuates different training sets. steve nouri ​ ​https//www.linkedin.com/in/stevenouri/\n",
      "\n",
      "q50 ridge regression favorable lasso regression? quote islr’s authors hastie, tibshirani asserted that, presence variables medium / large sized effect, use lasso regression. presence variables small/medium-sized effects, use ridge regression. conceptually, say, lasso regression (l1) variable selection parameter shrinkage, ridge regression parameter shrinkage end including coefficients model. presence correlated variables, ridge regression preferred choice. also, ridge regression works best situations square estimates higher variance. therefore, depends model objective. q51 you’ve built random forest model 10000 trees. got delighted getting training error 0.00. but, validation error 34.23. going on? haven’t trained model perfectly? model overfitted. training error 0.00 means classifier mimicked training data patterns extent, available unseen data. hence, classifier run unseen sample, couldn’t find patterns returned predictions higher error. random forest, happens use larger number trees necessary. hence, avoid situation, tune number trees cross-validation. q50 convex hull? case linearly separable data, convex hull represents outer boundaries groups data points. convex hull created, maximum margin hyperplane (mmh) perpendicular bisector convex hulls. mmh line attempts create greatest separation groups. q51 understand type vs type ii error? type error committed null hypothesis true reject it, known ‘false positive’. type ii error committed null hypothesis false accept it, known ‘false negative’. context confusion matrix, type error occurs classify value positive (1) actually negative (0). type ii error occurs classify value negative (0) actually positive(1). q52. k-means knn, use euclidean distance calculate distance nearest neighbors. manhattan distance? don’t use manhattan distance calculates distance horizontally vertically only. dimension restrictions. hand, euclidean metric space calculate distance. data points present dimension, euclidean distance viable option. steve nouri ​ ​https//www.linkedin.com/in/stevenouri/\n",
      "\n",
      "example think chessboard, movement bishop rook calculated manhattan distance respective vertical & horizontal movements. q53 suggest treating categorical variable continuous variable result better predictive model? better predictions, categorical variable considered continuous variable variable ordinal nature. q54 ols linear regression. maximum likelihood logistic regression. explain statement. ols maximum likelihood methods respective regression methods approximate unknown parameter (coefficient) value. simple words, ordinary square(ols) method linear regression approximates parameters resulting minimum distance actual predicted values. maximum likelihood helps choosing values parameters maximizes likelihood parameters likely produce observed data. q55 regularization necessary machine learning? regularization necessary model begins overfit/underfit. technique introduces cost term bringing features objective function. hence, tries push coefficients variables zero reduce cost term. helps reduce model complexity model better predicting (generalizing). q56 linear regression? linear regression supervised machine learning algorithm. find linear relationship dependent independent variables predictive analysis. q57 variance inflation factor? variance inflation factor (vif) estimate volume multicollinearity collection regression variables. vif = variance model / variance model single independent variable calculate ratio independent variable. vif high, shows high collinearity independent variables. q58 know hot encoding increases dimensionality dataset, label encoding doesn’t. how? use ​one-hot encoding​, increase dimensionality dataset. reason increase dimensionality that, class categorical variables, forms different variable. steve nouri ​ ​https//www.linkedin.com/in/stevenouri/\n",
      "\n",
      "q59 decision tree? decision tree explain sequence actions performed desired output. hierarchical diagram shows actions. q60 binarizing data? binarize? machine learning interviews, apart theoretical s, interviewers focus implementation part. so, ml interview s focused implementation theoretical concepts. converting data binary values basis threshold values known binarizing data. values threshold set 0 values greater threshold set 1. process useful perform feature engineering, use adding unique features. q61 cross-validation? cross-validation essentially technique assess model performs new independent dataset. simplest example cross-validation split data groups training data testing data, use training data build model testing data test model. q62 use random forests vs svm why? couple reasons random forest better choice model support vector machine ● random forests allow determine feature importance. svm’s can’t this. ● random forests quicker simpler build svm. ● multi-class classification problems, svms require one-vs-rest method, scalable memory intensive. q63 drawbacks linear model? couple drawbacks linear model ● linear model holds strong assumptions true application. assumes linear relationship, multivariate normality, little multicollinearity, auto-correlation, homoscedasticity ● linear model can’t discrete binary outcomes. ● can’t vary model flexibility linear model. steve nouri ​ ​https//www.linkedin.com/in/stevenouri/\n",
      "\n",
      "q64 think 50 small decision trees better large one? why? way asking “is random forest better model decision tree?” answer yes random forest ensemble method takes weak decision trees strong learner. random forests accurate, robust, prone overfitting. q65 kernel? explain kernel trick kernel way computing dot product vectors 𝐱x 𝐲y (possibly high dimensional) feature space, kernel functions called “generalized dot product” kernel trick method linear classifier solve non-linear problem transforming linearly inseparable data linearly separable ones higher dimension. q66 state differences causality correlation? causality applies situations action, x, causes outcome, y, correlation relating action (x) action(y) x necessarily cause y. q67 exploding gradient problem backpropagation technique? large error gradients accumulate result large changes neural network weights training, called exploding gradient problem. values weights large overflow result nan values. makes model unstable learning model stall like vanishing gradient problem. q68 mean associative rule mining (arm)? associative rule mining techniques discover patterns data like features (dimensions) occur features (dimensions) correlated. q69 marginalisation? explain process. marginalizationarginalisation summing probability random variable x given joint probability distribution x variables. application law total probability. q70 rotation components important principle component analysis(pca)? rotation pca important maximizes separation variance obtained components interpretation components easier. components rotated, need extended components describe variance components. steve nouri ​ ​https//www.linkedin.com/in/stevenouri/\n",
      "\n",
      "q71 difference regularization normalisation? normalisation adjusts data; regularisation adjusts prediction function. data different scales (especially low high), want normalise data. alter column compatible basic statistics. helpful sure loss accuracy. goals model training identify signal ignore noise model given free rein minimize error, possibility suffering overfitting. regularization imposes control providing simpler fitting functions complex ones. q72 linear regression line stop rotating finds optimal spot fitted data? place highest rsquared value found, place line comes rest. rsquared represents variance captured virtual linear regression line respect total variance captured dataset. q73 svm algorithm deal self-learning? svm learning rate expansion rate takes care this. learning rate compensates penalises hyperplanes making wrong moves expansion rate deals finding maximum separation area classes. q74 handle outliers data? outlier observation data set far away observations data set. discover outliers tools functions like box plot, scatter plot, z-score, iqr score etc. handle based visualization got. handle outliers, cap threshold, use transformations reduce skewness data remove outliers anomalies errors. q75 define techniques find similarities recommendation system. pearson correlation cosine correlation techniques find similarities recommendation systems. q76 prune tree? context data science aiml, pruning refers process reducing redundant branches decision tree. decision trees prone overfitting, pruning tree helps reduce size minimizes chances overfitting. pruning involves turning branches decision tree leaf nodes removing leaf nodes original branch. serves tool perform tradeoff. steve nouri ​ ​https//www.linkedin.com/in/stevenouri/\n",
      "\n",
      "q77 mention eda techniques? exploratory data analysis (eda) helps analysts understand data better forms foundation better models. visualization ● univariate visualization ● bivariate visualization ● multivariate visualization missing value treatmen​t – replace missing values mean/median outlier detection – use boxplot identify distribution outliers, apply iqr set boundary iqr q78 data augmentation? examples? data augmentation technique synthesizing new data modifying existing data way target changed, changed known way. cv fields data augmentation useful. modifications images ● resize ● horizontal vertical flip ● rotate ● add noise ● deform ● modify colors problem needs customized data augmentation pipeline. example, ocr, flips change text won’t beneficial; however, resizes small rotations help. q79 inductive logic programming machine learning (ilp)? inductive logic programming (ilp) subfield machine learning uses logic programming representing background knowledge examples. q80 difference inductive machine learning deductive machine learning? difference inductive machine learning deductive machine learning follows machine-learning model learns examples set observed instances draw generalized conclusion deductive learning model draws conclusion conclusion drawn. steve nouri ​ ​https//www.linkedin.com/in/stevenouri/\n",
      "\n",
      "q81 difference machine learning deep learning machine learning branch computer science method implement artificial intelligence. technique provides ability automatically learn improve experiences explicitly programmed. deep learning said subset machine learning. mainly based artificial neural network data taken input technique makes intuitive decisions artificial neural network. q82 steps involved machine learning project? plan machine learning project. important steps follow achieve good working model data collection, data preparation, choosing machine learning model, training model, model evaluation, parameter tuning lastly prediction. q83 differences artificial intelligence machine learning? artificial intelligence broader prospect machine learning. artificial intelligence mimics cognitive functions human brain. purpose ai carry task intelligent manner based algorithms. hand, machine learning subclass artificial intelligence. develop autonomous machine way learn explicitly programmed goal machine learning. q84 steps needed choose appropriate machine learning algorithm classification problem. firstly, need clear picture data, constraints, problems heading different machine learning algorithms. secondly, understand type kind data plays primary role deciding algorithm use. following step data categorization step, two-step process – categorization input categorization output. step understand constraints; is, data storage capacity? fast prediction be? etc. finally, find available machine learning algorithms implement wisely. that, try optimize hyperparameters ways – grid search, random search, bayesian optimization. steve nouri ​ ​https//www.linkedin.com/in/stevenouri/\n",
      "\n",
      "q85 explain backpropagation machine learning. important machine learning interview. backpropagation ​is algorithm computing artificial neural networks (ann). gradient descent optimization exploits chain rule. calculating gradient loss function, weight neurons adjusted certain value. train multi-layered neural network prime motivation backpropagation learn appropriate internal demonstrations. help learn map input respective output arbitrarily. q86 convex function? asked machine learning interviews. convex function continuous function, value midpoint interval given domain numerical mean values ends interval. q87 what’s relationship true positive rate recall? true positive rate machine learning percentage positives properly acknowledged, recall count results correctly identified relevant. therefore, things, having different names. known sensitivity. q88 list tools parallelizing machine learning algorithms. easy, sure skip closely related artificial intelligence thereby, ai interview s. machine learning algorithms easy serialize. basic tools parallelizing matlab, weka, r, octave, python-based sci-kit learn. q89 mean genetic programming? genetic programming (gp) similar evolutionary algorithm, subset machine learning. genetic programming software systems implement algorithm uses random mutation, fitness function, crossover, multiple generations evolution resolve user-defined task. genetic programming model based testing choosing best option set results. q90 know bayesian networks? bayesian networks referred 'belief networks' 'casual networks', represent graphical model probability relationship set variables. example, bayesian network represent probabilistic relationships diseases symptoms. symptoms, network compute probabilities presence diseases. steve nouri ​ ​https//www.linkedin.com/in/stevenouri/\n",
      "\n",
      "efficient algorithms perform inference learning bayesian networks. bayesian networks relate variables (e.g., speech signals protein sequences) called dynamic bayesian networks. q91 components bayesian logic program? bayesian logic program consists components ● logical contains set bayesian clauses, capture qualitative structure domain. ● quantitative​ encode quantitative information domain. q92 machine learning day-to-day life? people machine learning everyday life. assume engaging internet, actually expressing preferences, likes, dislikes searches. things picked cookies coming computer, this, behavior user evaluated. helps increase progress user internet provide similar suggestions. navigation system considered examples machine learning calculate distance places optimization techniques. surely, people going engage machine learning near future q93 define sampling. need it? sampling process choosing subset target population serve representative. use data sample understand pattern community whole. sampling necessary often, gather process complete data reasonable time. q94 term decision boundary mean? decision boundary decision surface hypersurface divides underlying feature space subspaces, class. decision boundary hyperplane, classes linearly separable. q95 define entropy? entropy measure uncertainty associated random variable y. expected number bits required communicate value variable. q96 indicate intents machine learning? intents machine learning stated below, ● system gets information established computations well-founded decisions outputs. ● locates certain patterns data makes certain predictions provide answers matters. steve nouri ​ ​https//www.linkedin.com/in/stevenouri/\n",
      "\n",
      "q97 highlight differences generative model discriminative model? aim generative model generate new samples distribution new data instances, whereas, discriminative model highlights differences different kinds data instances. tries learn directly data classifies data. q98 identify important aptitudes machine learning engineer? machine learning allows computer learn decidedly programmed. helps system learn experience improve mistakes. intelligence system, based machine learning, learn recorded data past incidents. in-depth knowledge statistics, probability, data modelling, programming language, cs, application ml libraries algorithms, software design required successful machine learning engineer. q99 feature engineering? apply process modelling? feature engineering process transforming raw data features better represent underlying problem predictive models, resulting improved model accuracy unseen data. q100 learning curves help create better model? learning curves indication presence overfitting underfitting. learning curve, training error cross-validating error plotted number training data points. references 1 springboard.com 2 ​simplilearn.com 3 geeksforgeeks.org 4 ​elitedatascience.com 5 analyticsvidhya.com 6 ​guru99.com 7 ​intellipaat.com 8 towardsdatascience.com 9 mygreatlearning.com 10 ​mindmajix.com 11 toptal.com 12 ​glassdoor.co.in 13 ​udacity.com 14 educba.com 15 ​analyticsindiamag.com 16 ​ubuntupit.com 17 ​javatpoint.com 18 quora.com 19 hackr.io 20 kaggle.com steve nouri ​ ​https//www.linkedin.com/in/stevenouri/\n",
      "\n",
      "steve nouri https//www.linkedin.com/in/stevenouri/ 100 nlp s steve nouri q1. following techniques keyword normalization nlp, process converting keyword base form? a. lemmatization b. soundex c. cosine similarity d. n-grams answer a) lemmatization helps base form word, e.g. playing -> play, eating -> eat, etc.other options meant different purposes. q2. following techniques compute distance word vectors nlp? a. lemmatization b. euclidean distance c. cosine similarity d. n-grams answer b) c) distance word vectors computed cosine similarity euclidean distance. cosine similarity establishes cosine angle vector words. cosine angle close word vectors indicates words similar vice versa. e.g. cosine angle words “football” “cricket” closer 1 compared angle words “football” “new delhi” q3. possible features text corpus nlp? a. count word document b. vector notation word c. speech tag d. basic dependency grammar e. answer e)all features text corpus. q4. created document term matrix input data 20k documents machine learning model. following reduce dimensions data?\n",
      "\n",
      "steve nouri https//www.linkedin.com/in/stevenouri/ 1. keyword normalization 2. latent semantic indexing 3. latent dirichlet allocation a. 1 b. 2, 3 c. 1, 3 d. 1, 2, 3 answer d) q5. text parsing techniques noun phrase detection, verb phrase detection, subject detection, object detection nlp. a. speech tagging b. skip gram n-gram extraction c. continuous bag words d. dependency parsing constituency parsing answer d) q6. dissimilarity words expressed cosine similarity values significantly higher 0.5 a. true b. false answer a) q7. following keyword normalization techniques nlp a. stemming b. speech c. named entity recognition d. lemmatization answer a) d) speech (pos) named entity recognition(ner) keyword normalization techniques. named entity help extract organization, time, date, city, etc..type entities given sentence, speech helps extract noun, verb, pronoun, adjective, etc..from given sentence tokens. q8. nlp use cases? a. detecting objects image b. facial recognition c. speech biometric d. text summarization\n",
      "\n",
      "steve nouri https//www.linkedin.com/in/stevenouri/ answer (d) a) b) computer vision use cases, c) speech use case. d) text summarization nlp use case. q9. corpus n documents, randomly chosen document contains total t terms term “hello” appears k times. correct value product tf (term frequency) idf (inverse-document- frequency), term “hello” appears approximately one-third total documents? a. kt * log(3) b. t * log(3) / k c. k * log(3) / t d. log(3) / kt answer (c) formula tf k/t formula idf log(total docs / docs containing “data”) = log(1 / (⅓)) = log (3) correct choice klog(3)/t q10. nlp, algorithm decreases weight commonly words increases weight words collection documents a. term frequency (tf) b. inverse document frequency (idf) c. word2vec d. latent dirichlet allocation (lda) answer b) q11. nlp, process removing words like “and”, “is”, “a”, “an”, “the” sentence called a. stemming b. lemmatization c. stop word d. answer c) lemmatization, stop words a, an, the, etc.. removed. define custom stop words removal.\n",
      "\n",
      "steve nouri https//www.linkedin.com/in/stevenouri/ q12. nlp, process converting sentence paragraph tokens referred stemming a. true b. false answer b) statement describes process tokenization stemming, false. q13. nlp, tokens converted numbers giving neural network a. true b. false answer a) nlp, words converted number feeding neural network. q14 identify odd a. nltk b. scikit learn c. spacy d. bert answer d) ones mentioned nlp libraries bert, word embedding q15 tf-idf helps establish? a. frequently occurring word document b. important word document answer b) tf-idf helps establish important particular word context document corpus. tf-idf takes account number times word appears document offset number documents appear corpus. ● tf frequency term divided total number terms document. ● idf obtained dividing total number documents number documents containing term taking logarithm quotient.\n",
      "\n",
      "steve nouri https//www.linkedin.com/in/stevenouri/ ● tf.idf multiplication values tf idf. q16 nlp, process identifying people, organization given sentence, paragraph called a. stemming b. lemmatization c. stop word removal d. named entity recognition answer d) q17 following pre-processing technique nlp a. stemming lemmatization b. converting lowercase c. removing punctuations d. removal stop words e. sentiment analysis answer e) sentiment analysis pre-processing technique. pre-processing nlp use case. listed ones statement pre-processing. q18 text mining, converting text tokens converting integer floating-point vectors a. countvectorizer b. tf-idf c. bag words d. ners answer a) countvectorizer helps above, applicable. text =[“rahul avid writer, enjoys studying understanding presenting. loves play”] vectorizer = countvectorizer() vectorizer.fit(text) vector = vectorizer.transform(text)\n",
      "\n",
      "steve nouri https//www.linkedin.com/in/stevenouri/ print(vector.toarray()) output [[1 1 1 1 2 1 1 1 1 1 1 1 1 1]] second section interview s covers advanced nlp techniques word2vec, glove word embeddings, advanced models gpt, elmo, bert, xlnet based s, explanations. q19. nlp, words represented vectors called neural word embeddings a. true b. false answer a) word2vec, glove based models build word embedding vectors multidimensional. q20. nlp, context modeling supported following word embeddings 1. a. word2vec 2. b) glove 3. c) bert 4. d) answer c) bert (bidirectional encoder representations transformer) supports context modelling previous sentence context taken consideration. word2vec, glove word embeddings considered previous sentence context considered. q21. nlp, bidirectional context supported following embedding a. word2vec b. bert c. glove d. answer b) bert provides bidirectional context. bert model uses previous sentence arrive context.word2vec glove word embeddings, provide context. q22. following word embeddings custom trained specific subject nlp a. word2vec b. bert\n",
      "\n",
      "steve nouri https//www.linkedin.com/in/stevenouri/ c. glove d. answer b) bert allows transform learning existing pre-trained models custom trained given specific subject, unlike word2vec glove existing word embeddings used, transfer learning text possible. q23. word embeddings capture multiple dimensions data represented vectors a. true b. false answer a) q24. nlp, word embedding vectors help establish distance tokens a. true b. false answer a) use cosine similarity establish distance vectors represented word embeddings q25. language biases introduced historical data training word embeddings, example bias a. new delhi india, beijing china b. man computer, woman homemaker answer a) statement b) bias buckets woman homemaker, statement a) biased statement. q26. following better choice address nlp use cases semantic similarity, reading comprehension, common sense reasoning a. elmo b. open ai’s gpt c. ulmfit answer b) open ai’s gpt able learn complex pattern data transformer models attention mechanism suited complex use cases semantic similarity, reading comprehensions, common sense reasoning. q27. transformer architecture introduced with? a. glove b. bert\n",
      "\n",
      "steve nouri https//www.linkedin.com/in/stevenouri/ c. open ai’s gpt d. ulmfit answer c) ulmfit lstm based language modeling architecture. got replaced transformer architecture open ai’s gpt q28. following architecture trained faster needs training data a. lstm based language modelling b. transformer architecture answer b) transformer architectures supported gpt onwards faster train needed data training too. q29. word multiple word embeddings possible ____________? a. glove b. word2vec c. elmo d. nltk answer c) emlo word embeddings supports word multiple embeddings, helps word different context captures context meaning word unlike glove word2vec. nltk word embedding. q30 given token, input representation sum embedding token, segment position embedding a. elmo b. gpt c. bert d. ulmfit answer c) bert uses token, segment position embedding. q31. trains independent lstm language model left right right left shallowly concatenates a. gpt b. bert c. ulmfit d. elmo\n",
      "\n",
      "steve nouri https//www.linkedin.com/in/stevenouri/ answer d) elmo tries train independent lstm language models (left right right left) concatenates results produce word embedding. q32. uses unidirectional language model producing word embedding a. bert b. gpt c. elmo d. word2vec answer b) gpt unidirectional model word embedding produced training information flow left right. elmo bidirectional shallow. word2vec provides simple word embedding. q33. architecture, relationship words sentence modelled irrespective position. architecture this? a. openai gpt b. elmo c. bert d. ulmfit answer c)bert transformer architecture models relationship word words sentence generate attention scores. attention scores later weights weighted average words’ representations fed fully-connected network generate new representation. q34. list 10 use cases solved nlp techniques? ● sentiment analysis ● language translation (english german, chinese english, etc..) ● document summarization ● answering ● sentence completion ● attribute extraction (key information extraction documents) ● chatbot interactions ● topic classification ● intent extraction ● grammar sentence correction ● image captioning ● document ranking ● natural language inference q35. transformer model pays attention important word sentence a. true b. false\n",
      "\n",
      "steve nouri https//www.linkedin.com/in/stevenouri/ answer a) attention mechanisms transformer model model relationship words provide weights important word. q36. nlp model gives best accuracy following? a. bert b. xlnet c. gpt-2 d. elmo answer b) xlnet given best accuracy models. outperformed bert 20 tasks achieves state art results 18 tasks including sentiment analysis, answering, natural language inference, etc. q37. permutation language models feature a. bert b. emmo c. gpt d. xlnet answer d) xlnet provides permutation-based language modelling key difference bert. permutation language modeling, tokens predicted random manner sequential. order prediction necessarily left right right left. original order words changed prediction random. conceptual difference bert xlnet seen following diagram. q38. transformer xl uses relative positional embedding a. true b. false a) instead embedding having represent absolute position word, transformer xl uses embedding encode relative distance words. embedding compute attention score 2 words separated n words after. q39. naive bayes algorithm, use algorithm nlp? naive bayes algorithm collection classifiers works principles bayes’ theorem. series nlp model forms family algorithms wide range classification tasks including sentiment prediction, filtering spam, classifying documents more. naive bayes algorithm converges faster requires training data. compared discriminative models like logistic regression, naive bayes model takes lesser time train. algorithm perfect use working multiple classes text classification data dynamic changes frequently. q40. explain dependency parsing nlp?\n",
      "\n",
      "steve nouri https//www.linkedin.com/in/stevenouri/ dependency parsing, known syntactic parsing nlp process assigning syntactic structure sentence identifying dependency parses. process crucial understand correlations “head” words syntactic structure. process dependency parsing little complex considering sentence dependency parses. multiple parse trees known ambiguities. dependency parsing needs resolve ambiguities order effectively assign syntactic structure sentence. dependency parsing semantic analysis sentence apart syntactic structuring. q41. text summarization? text summarization process shortening long piece text meaning effect intact. text summarization intends create summary given piece text outlines main points document. technique improved recent times capable summarizing volumes text successfully. text summarization proved blessing machines summarise large volumes text time time-consuming. types text summarization ● extraction-based summarization ● abstraction-based summarization q42. nltk? different spacy? nltk natural language toolkit series libraries programs symbolic statistical natural language processing. toolkit contains powerful libraries work different ml techniques break understand human language. nltk lemmatization, punctuation, character count, tokenization, stemming. difference nltk spacey follows ● nltk collection programs choose from, spacey contains best- suited algorithm problem toolkit ● nltk supports wider range languages compared spacey (spacey supports 7 languages) ● spacey object-oriented library, nltk string processing library ● spacey support word vectors nltk q43. information extraction? information extraction context natural language processing refers technique extracting structured information automatically unstructured sources ascribe meaning it. include extracting information attributes entities, relationship different entities more. models information extraction includes ● tagger module ● relation extraction module ● fact extraction module ● entity extraction module\n",
      "\n",
      "steve nouri https//www.linkedin.com/in/stevenouri/ ● sentiment analysis module ● network graph module ● document classification & language modeling module q44. bag words? bag words commonly model depends word frequencies occurrences train classifier. model creates occurrence matrix documents sentences irrespective grammatical structure word order. q45. pragmatic ambiguity nlp? pragmatic ambiguity refers words meaning use sentence depend entirely context. pragmatic ambiguity result multiple interpretations sentence. not, come sentences words multiple meanings, making sentence open interpretation. multiple interpretation causes ambiguity known pragmatic ambiguity nlp. q46. masked language model? masked language models help learners understand deep representations downstream tasks taking output corrupt input. model predict words sentence. q48. best nlp tools? best nlp tools open sources are ● spacy ● textblob ● textacy ● natural language toolkit ● retext ● nlp.js ● stanford nlp ● cogcompnlp q49. pos tagging? parts speech tagging better known pos tagging refers process identifying specific words document group speech, based context. pos tagging known grammatical tagging involves understanding grammatical structures identifying respective component. pos tagging complicated process word different parts speech depending context. generic process word mapping ineffective pos tagging reason. q50. nes? entity recognition commonly known ner process identifying specific entities text document informative unique context. denote places, people, organisations, more. like entities\n",
      "\n",
      "steve nouri https//www.linkedin.com/in/stevenouri/ proper nouns, ner process far identifying nouns. fact, ner involves entity chunking extraction entities segmented categorise different predefined classes. step helps extracting information. q51 explain masked language model? masked language modelling process output taken corrupted input. model helps learners master deep representations downstream tasks. predict word words sentence model. q52 pragmatic analysis nlp? pragmatic analysis deals outside word knowledge, means knowledge external documents and/or queries. pragmatics analysis focuses described reinterpreted actually meant, deriving aspects language require real-world knowledge. q53 perplexity nlp? word \"perplexed\" means \"puzzled\" \"confused\", perplexity general means inability tackle complicated problem specified. therefore, perplexity nlp way determine extent uncertainty predicting text. nlp, perplexity way evaluating language models. perplexity high low; low perplexity ethical inability deal complicated problem high perplexity terrible failure deal complicated high. q54 ngram nlp? n-gram nlp simply sequence n words, conclude sentences appeared frequently, example, let consider progression words ● new york (2 gram) ● golden compass (3 gram) ● hotel (4 gram) sequence, easily conclude sentence (a) appeared frequently sentences, sentence(c) seen often. assign probability occurrence n-gram, advantageous. help making next-word predictions spelling error corrections. q55 explain differences ai, machine learning nlp\n",
      "\n",
      "steve nouri https//www.linkedin.com/in/stevenouri/ q56 self-attention awesome? “in terms computational complexity, self-attention layers faster recurrent layers sequence length n smaller representation dimensionality d, case sentence representations state-of-the-art models machine translations, word-piece byte-pair representations.” — attention need q57 stop words? stop words said useless data search engine. words articles, prepositions, etc. considered stop words. stop words was, were, is, am, the, a, an, how, why, more. natural language processing, eliminate stop words understand analyze meaning sentence. removal stop words important tasks search engines. engineers design algorithms search engines way ignore use stop words. helps relevant search result query. q58 latent semantic indexing (lsi)?\n",
      "\n",
      "steve nouri https//www.linkedin.com/in/stevenouri/ latent semantic indexing mathematical technique improve accuracy information retrieval process. design lsi algorithms allows machines detect hidden (latent) correlation semantics (words). enhance information understanding, machines generate concepts associate words sentence. technique information understanding called singular value decomposition. generally handle static unstructured data. matrix obtained singular value decomposition contains rows words columns documents. method best suits identify components group according types. main principle lsi words carry similar meaning similar context. computational lsi models slow comparison models. however, good contextual awareness helps improve analysis understanding text document. q60 regular expressions? regular expression match tag words. consists series characters matching strings. suppose, b regular expressions, following true them ● {ɛ} regular language, ɛ regular expression it. ● b regular expressions, + b regular expression language {a, b}. ● b regular expressions, concatenation b (a.b) regular expression. ● regular expression, a* (a occurring multiple times) regular expression. q61 unigrams, bigrams, trigrams, n-grams nlp? parse sentence word time, called unigram. sentence parsed words time bigram. sentence parsed words time, trigram. similarly, n-gram refers parsing n words time. example understand unigrams, bigrams, trigrams, refer diagram q62 steps involved solving nlp problem? steps involved solving nlp problem 1. gather text available dataset web scraping\n",
      "\n",
      "steve nouri https//www.linkedin.com/in/stevenouri/ 2. apply stemming lemmatization text cleaning 3. apply feature engineering techniques 4. embed word2vec 5. train built model neural networks machine learning techniques 6. evaluate model’s performance 7. appropriate changes model 8. deploy model q63. common elements natural language processing. elements important understanding nlp properly, explain details example? lot components normally natural language processing (nlp). major components explained below ● extraction entity actually identifying extracting critical data available information help segmentation provided sentence identifying entity. help identifying human it’s fictional real, kind reality identification organization, events geographic location etc. ● analysis syntactic way mainly helps maintaining ordering properly available words. q64 case processing natural language, normally mentioned common terminology nlp binding language terminology properly. explain details nlp terminology example? basic nlp interview s asked interview. factors available case explaining natural language processing. key factors given below ● vectors weights google word vectors, length tf-idf, varieties documents, word vectors, tf-idf. ● structure text named entities, tagging speech, identifying head sentence. ● analysis sentiment know features sentiment, entities available sentiment, sentiment common dictionary. ● classification text learning supervising, set train, set validation dev, set define test, feature individual text, lda. ● reading machine language extraction possible entity, linking individual entity, dbpedia, libraries like pikes fred. q65 explain briefly word2vec word2vec embeds words lower-dimensional vector space shallow neural network. result set word-vectors vectors close vector space similar meanings based context, word-vectors distant differing meanings. example, apple orange close apple gravity relatively far.\n",
      "\n",
      "steve nouri https//www.linkedin.com/in/stevenouri/ versions model based skip-grams (sg) continuous-bag-of-words (cbow). q66 metrics test nlp model? accuracy, precision, recall f1. accuracy usual ratio prediction desired output. going accuracy naive considering complexities involved. q67 ways preprocess text input? preprocessing steps commonly nlp tasks ● case normalization convert input case (lowercase uppercase) way reducing text canonical form ● punctuation/stop word/white space/special characters removal don’t think words characters relevant, remove reduce feature space ● lemmatizing/stemming reduce words inflectional forms (i.e. walks → walk) trim vocabulary ● generalizing irrelevant information replace numbers <number> token names <name> token q68 encoder-decoder structure work language modelling? encoder-decoder structure deep learning model architecture responsible state art solutions, including machine translation. input sequence passed encoder transformed fixed-dimensional vector representation neural network. transformed input decoded neural network. then, outputs undergo transformation softmax layer. final output vector probabilities vocabularies. meaningful information extracted based probabilities. q69 attention mechanisms use them? followup encoder-decoder . output time step passed decoder, resulting loss information learned previous time steps. information loss compounded longer text sequences time steps. attention mechanisms function hidden weights time step. use attention encoder-decoder networks, fixed-dimensional vector passed decoder function vectors outputted intermediary steps. commonly attention mechanisms additive attention multiplicative attention. names suggest, additive attention weighted sum multiplicative attention weighted multiplier hidden weights. training process, model learns weights attention mechanisms recognize relative importance time step. q70 implement nlp system service, pitfalls face production? nlp  productionizing machine learning models. certain intricacies nlp models.\n",
      "\n",
      "steve nouri https//www.linkedin.com/in/stevenouri/ diving productionization aspect, ideal machine learning service have ● endpoint(s) business systems use inference ● feedback mechanism validating model predictions ● database store predictions ground truths feedback ● workflow orchestrator (upon signal) re-train load new model serving based records database + prior training data ● form model version control facilitate rollbacks case bad deployments ● post-production accuracy error monitoring q71 handle misspellings text input? word embeddings trained large corpus (for instance, extensive web scrape billions words), model vocabulary include common misspellings design. model learn relationship misspelled correctly spelled words recognize semantic similarity. preprocess input prevent misspellings. terms found model vocabulary mapped “closest” vocabulary term using ● edit distance strings ● phonetic distance word pronunciations ● keyword distance catch common typos q72 following models perform tweet classification regards context mentioned above? a) naive bayes b) svm c) solution (c) since, given data tweets information, means target variable present. train supervised learning model, svm naive bayes supervised learning techniques. q73 created document term matrix data, treating tweet document. following correct, regards document term matrix? 1. removal stopwords data affect dimensionality data 2. normalization words data reduce dimensionality data\n",
      "\n",
      "steve nouri https//www.linkedin.com/in/stevenouri/ 3. converting words lowercase affect dimensionality data a) 1 b) 2 c) 3 d) 1 2 e) 2 3 f) 1, 2 3 solution (d) choices b correct stopword removal decrease number features matrix, normalization words reduce redundant features, and, converting words lowercase decrease dimensionality. q74 following features accuracy improvement classification model? a) frequency count terms b) vector notation sentence c) speech tag d) dependency grammar e) solution (e) techniques purpose engineering features model. q75 percentage total statements correct regards topic modeling? 1. supervised learning technique 2. lda (linear discriminant analysis) perform topic modeling 3. selection number topics model depend size data 4. number topic terms directly proportional size data a) 0 b) 25 c) 50 d) 75 e) 100 solution (a) lda unsupervised learning model, lda latent dirichlet allocation, linear discriminant analysis. selection number topics directly proportional size data, number topic terms directly proportional size data. statements correct.\n",
      "\n",
      "steve nouri https//www.linkedin.com/in/stevenouri/ q76 latent dirichlet allocation model text classification purposes, alpha beta hyperparameter represent- a) alpha number topics documents, beta number terms topics false b) alpha density terms generated topics, beta density topics generated terms false c) alpha number topics documents, beta number terms topics false d) alpha density topics generated documents, beta density terms generated topics true solution (d) option d correct q77 problem relu? ● exploding gradient(solved gradient clipping) ● dying relu — learning activation 0 (solved parametric relu) ● mean variance activations 0 1.(partially solved subtracting 0.5 activation. better explained fastai videos) q78 difference learning latent features svd getting embedding vectors deep network? svd uses linear combination inputs neural network uses nonlinear combination. q79 information hidden cell state lstm? hidden stores information till time step cell state stores particular information needed future time step. number parameters lstm model bias 4(𝑚h+h²+h) 𝑚 input vectors size h output vectors size a.k.a. hidden point mh dictates model size m>>h. it's important small vocab. time complexity lstm seq_length*hidden² time complexity transfomer seq_length²*hidden hidden size seq_length(which normally case), transfomer faster lstm. q80 self-attention faster recurrent layers? sequence length greater representation dimensions. rare. q81 benefit learning rate warm-up?\n",
      "\n",
      "steve nouri https//www.linkedin.com/in/stevenouri/ learning rate warm-up learning rate schedule low (or lower) learning rate beginning training avoid divergence unreliable gradients beginning. model stable, learning rate increase speed convergence. q82 what’s difference hard soft parameter sharing multi-task learning? hard sharing train task time update weights losses soft sharing train task time. q83 what’s difference batchnorm layernorm? batchnorm computes mean variance layer minibatch layernorm computes mean variance sample layer independently. batch normalisation allows set higher learning rates, increasing speed training reduces unstability initial starting weights. q84 difference batchnorm layernorm? batchnorm — compute mean var layer minibatch layernorm — compute mean var single sample layer independently q85 transformer block layernorm instead batchnorm? looking advantages layernorm, robust batch size works better works sample level batch level. q86 changes deep learning code knew errors training data? label smoothening smoothening value based % error. particular class known error, use class weights modify loss. q87 tricks ulmfit? (not great s checks awareness) ● lm tuning task text ● weight dropout ● discriminative learning rates layers ● gradual unfreezing layers ● slanted triangular learning rate schedule followed explaining help. q88 tell language model doesn’t use dropout albert v2 — throws light fact lot assumptions granted necessarily true. regularisation effect parameter sharing albert strong dropouts needed. (albert v1 dropouts.) q89 differences gpt gpt-2? (from lilian weng)\n",
      "\n",
      "steve nouri https//www.linkedin.com/in/stevenouri/ ● layer normalization moved input sub-block, similar residual unit type “building block” (differently original type “bottleneck”, batch normalization applied weight layers). ● additional layer normalization added final self-attention block. ● modified initialization constructed function model depth. ● weights residual layers initially scaled factor 1/√n n number residual layers. ● use larger vocabulary size context size. q90 differences gpt bert? ● gpt bidirectional concept masking ● bert adds sentence prediction task training segment embedding q91 differences bert albert v2? ● embedding matrix factorisation(helps reducing no. parameters) ● dropout ● parameter sharing(helps reducing no. parameters regularisation) q92 parameter sharing albert affect training inference time? effect. parameter sharing decreases number parameters. q93 reduce inference time trained nn model? ● serve gpu/tpu/fpga ● 16 bit quantisation served gpu fp16 support ● pruning reduce parameters ● knowledge distillation (to smaller transformer model simple neural network) ● hierarchical softmax/adaptive softmax ● cache results explained here. q94 use bpe classical models? course! bpe smart tokeniser help smaller vocabulary help find model parameters. q95 arxiv papers search engine? (i asked — plagiarism detector?) k results tf-idf similarity rank results ● semantic encoding + cosine similarity ● model trained ranking\n",
      "\n",
      "steve nouri https//www.linkedin.com/in/stevenouri/ q96 k results tf-idf similarity rank results ● semantic encoding + cosine similarity ● model trained ranking q97 sentiment classifier? trick . interviewee things transfer learning latest models need talk having neutral class good accuracy/f1 still, model classify positive negative. truth lot news neutral training needs class. interviewee talk create dataset training strategies like selection language model, language model fine-tuning datasets multi- task learning. q98 difference regular expression regular grammar? regular expression representation natural language form mathematical expressions containing character sequence. hand, regular grammar generator natural language, defining set defined rules syntax strings natural language follow. q99 use batch normalization? interviewer asked fundamentals deep learning architectures, key topic improving deep learning model’s performance. batch normalization techniques reducing training time deep learning algorithm. like normalizing input helps improve logistic regression model, normalize activations hidden layers deep learning model well q100 backpropagation different rnn compared ann? recurrent neural networks, additional loop node loop essentially includes time component network well. helps capturing sequential information data, possible generic artificial neural network. backpropagation rnn called backpropagation time, backpropagation time step.\n",
      "\n",
      "100 python interview s prepare 2019 updated aug 14,2019 379.7k views python certiõcation sought-after skill programming domain. python interview s blog, introduce frequently asked s python interviews. python interview s one-stop resource boost interview preparation. 100+ s python programming basics help diàerent expertise levels reap maximum beneõt blog. edureka 2019 tech career guide out! hottest job roles, precise learning paths, industry outlook & guide. download now. let start taking look frequently asked python interview s, q1. diàerence list tuples python? q2. key features python? q3. type language python? q4. python interpreted language? q5. pep 8? q6. memory managed python? q7. space python? q8. python path? q9. python modules? q10. local variables global variables python? compiled list python interview s classiõed 7 sections, namely basic interview s oops interview s basic python programs python libraries interview s web scraping interview s data analysis interview s multiple choice s (mcq) moving ahead, recording python interview s instructor shared experience expertise help crack python interview python interview s answers | python training | edureka aayushi johari technophile likes writing diàerent technologies spreading knowledge. mymock interview service real tech jobs  mock interview latest tech domains i.e java, ai, devops,etc interviewed leading tech experts real time assement report video recording python mock interview   follow linkedin more steve nouri https//www.linkedin.com/in/stevenouri/\n",
      "\n",
      "doubts python, feel free post qna forum. expert team earliest. basic python interview s q1. diàerence list tuples python? list vs tuples list tuples lists mutable i.e edited. tuples immutable (tuples lists can’t edited). lists slower tuples. tuples faster list. syntax list_1 = [10, ‘chelsea’, 20] syntax tup_1 = (10, ‘chelsea’ , 20) q2. key features python? python interpreted language. means that, unlike languages like c variants, python need compiled run. interpreted languages include php ruby. python dynamically typed, means don’t need state types variables declare like that. things like x=111 x=\"i'm string\" error python suited object orientated programming allows deõnition classes composition inheritance. python access speciõers (like c++’s public, private). python, functions õrst-class objects. means assigned variables, returned functions passed functions. classes õrst class objects writing python code quick running slower compiled languages. fortunately ，python allows inclusion c based extensions bottlenecks optimized away are. numpy package good example this, it’s quick lot number crunching isn’t actually python python õnds use spheres – web applications, automation, scientiõc modeling, big data applications more. it’s “glue” code languages components play nice. q3. type language python? programming scripting? ans python capable scripting, general sense, considered general-purpose programming language. know scripting, refer python scripting tutorial. q4.how python interpreted language? ans interpreted language programming language machine level code runtime. therefore, python interpreted language. q5.what pep 8? ans pep stands python enhancement proposal. set rules specify format python code maximum readability. q6. memory managed python? ans 1. memory management python managed python private heap space. python objects data structures located private heap. programmer access private heap. python interpreter takes care instead. 2. allocation heap space python objects python’s memory manager. core api gives access tools programmer code. 3. python inbuilt garbage collector, recycles unused memory available heap space. q7. namespace python? ans namespace naming system sure names unique avoid naming conöicts. q8. pythonpath? python interview s answers | python interview preparati python interview s answers | python interview preparati… \n",
      "\n",
      "ans environment variable module imported. module imported, pythonpath looked check presence imported modules directories. interpreter uses determine module load. q9. python modules? commonly built-in modules python? ans python modules õles containing python code. code functions classes variables. python module .py õle containing executable code. commonly built-in modules are os sys math random data time json q10.what local variables global variables python? global variables variables declared outside function global space called global variables. variables accessed function program. local variables variable declared inside function known local variable. variable present local space global space. example a=2 def add() b=3 c=a+b print(c) add() output 5 try access local variable outside function add(), throw error. q11. python case sensitive? ans yes. python case sensitive language. q12.what type conversion python? ans type conversion refers conversion data type iinto another. int() – converts data type integer type öoat() – converts data type öoat type ord() – converts characters integer hex() – converts integers hexadecimal oct() – converts integer octal tuple() – function convert tuple. set() – function returns type converting set. list() – function convert data type list type. dict() – function convert tuple order (key,value) dictionary. str() – convert integer string. \n",
      "\n",
      "complex(real,imag) – functionconverts real numbers complex(real,imag) number. q13. install python windows set path variable? ans install python windows, follow steps install python link https//www.python.org/downloads/ this, install pc. look location python installed pc following command command prompt cmd python. advanced system settings add new variable python_name paste copied path. look path variable, select value select ‘edit’. add semicolon end value it’s present type %python_home% q14. indentation required python? ans indentation necessary python. speciões block code. code loops, classes, functions, etc speciõed indented block. usually space characters. code indented necessarily, execute accurately throw errors well. q15. diàerence python arrays lists? ans arrays lists, python, way storing data. but, arrays hold single data type elements lists hold data type elements. example import array arr my_array=arr.array('i',[1,2,3,4]) my_list=[1,'abc',1.20] print(my_array) print(my_list) output array(‘i’, [1, 2, 3, 4]) [1, ‘abc’, 1.2] q16. functions python? ans function block code executed called. deõne python function, def keyword used. example def newfunc() print(\"hi, welcome edureka\") newfunc(); #calling function output hi, welcome edureka q17.what __init__? ans __init__ method constructor python. method automatically called allocate memory new object/ instance class created. classes __init__ method. example use it. class employee def __init__(self, name, age,salary) self.name = self.age = age self.salary = 20000 e1 = employee(\"xyz\", 23, 20000) # e1 instance class employee. #__init__ allocates memory e1. print(e1.name) print(e1.age) print(e1.salary) output xyz \n",
      "\n",
      "23 20000 q18.what lambda function? ans anonymous function known lambda function. function number parameters but, statement. example = lambda x,y x+y print(a(5, 6)) output 11 q19. self python? ans self instance object class. python, explicitly included õrst parameter. however, case java it’s optional. helps diàerentiate methods attributes class local variables. self variable init method refers newly created object methods, refers object method called. q20. break, continue pass work? break allows loop termination condition met control transferred statement. continue allows skipping loop speciõc condition met control transferred beginning loop pass need block code syntactically, want skip execution. basically null operation. happens executed. q21. [-1} do? ans [-1] reverse order array sequence. example import array arr my_array=arr.array('i',[1,2,3,4,5]) my_array[-1] output array(‘i’, [5, 4, 3, 2, 1]) [-1] reprints reversed copy ordered data structures array list. original array list remains unchanged. q22. randomize items list place python? ans consider example shown below random import shuffle x = ['keep', 'the', 'blue', 'flag', 'flying', 'high'] shuffle(x) print(x) output following code below. ['flying', 'keep', 'blue', 'high', 'the', 'flag'] q23. python iterators? ans iterators objects traversed iterated upon. q24. generate random numbers python? ans random module standard module generate random number. method deõned as import random random.random \n",
      "\n",
      "statement random.random() method return öoating point number range [0, 1). function generates random öoat numbers. methods random class bound methods hidden instances. instances random multi-threading programs creates diàerent instance individual threads. random generators are 1. randrange(a, b) chooses integer deõne range in-between [a, b). returns elements selecting randomly range speciõed. doesn’t build range object. 2. uniform(a, b) chooses öoating point number deõned range [a,b).iyt returns öoating point number 3. normalvariate(mean, sdev) normal distribution mu mean sdev sigma standard deviation. 4. random class instantiated creates independent multiple random number generators. q25. diàerence range & xrange? ans part, xrange range exact terms functionality. provide way generate list integers use, please. diàerence range returns python list object x range returns xrange object. means xrange doesn’t actually generate static list run-time like range does. creates values need special technique called yielding. technique type object known generators. means gigantic range you’d like generate list for, billion, xrange function use. especially true memory sensitive system cell phone working with, range use memory create array integers, result memory error crash program. it’s memory hungry beast. q26. write comments python? ans comments python start # character. however, alternatively times, commenting docstrings(strings enclosed triple quotes). example #comments python start like print(\"comments python start #\") output comments python start # q27. pickling unpickling? ans pickle module accepts python object converts string representation dumps õle dump function, process called pickling. process retrieving original python objects stored string representation called unpickling. q28. generators python? ans functions return iterable set items called generators. q29. capitalize õrst letter string? ans python, capitalize() method capitalizes õrst letter string. string consists capital letter beginning, then, returns original string. q30. convert string lowercase? ans convert string lowercase, lower() function used. example stg='abcd' print(stg.lower()) output abcd q31. comment multiple lines python? ans multi-line comments appear line. lines commented preõxed #. good shortcut method comment multiple lines. need hold ctrl key left click place want include # character type # once. comment lines introduced cursor. q32.what docstrings python? \n",
      "\n",
      "ans docstrings actually comments, but, documentation strings. docstrings triple quotes. assigned variable therefore, times, serve purpose comments well. example \"\"\" docstring comment. code divides 2 numbers \"\"\" x=8 y=4 z=x/y print(z) output 2.0 q33. purpose is, operators? ans operators special functions. values produce corresponding result. is returns true 2 operands true (example “a” ‘a’) not returns inverse boolean value in checks element present sequence q34. usage help() dir() function python? ans help() dir() functions accessible python interpreter viewing consolidated dump built-in functions. 1. help() function help() function display documentation string facilitates help related modules, keywords, attributes, etc. 2. dir() function dir() function display deõned symbols. q35. python exits, isn’t memory de-allocated? ans 1. python exits, especially python modules having circular references objects objects referenced global namespaces de-allocated freed. 2. impossible de-allocate portions memory reserved c library. 3. exit, having eþcient clean mechanism, python try de-allocate/destroy object. q36. dictionary python? ans built-in datatypes python called dictionary. deõnes one-to-one relationship keys values. dictionaries contain pair keys corresponding values. dictionaries indexed keys. let’s example following example contains keys. country, capital & pm. corresponding values india, delhi modi respectively. dict={'country''india','capital''delhi','pm''modi'} print dict[country] india print dict[capital] delhi print dict[pm] modi q37. ternary operators python? ans ternary operator operator conditional statements. consists true false values statement evaluated it. \n",
      "\n",
      "syntax ternary operator given as [on_true] [expression] [on_false]x, y = 25, 50big = x x < y y example expression gets evaluated like x<y y, case x<y true value returned big=x incorrect big=y sent result. q38. mean *args, **kwargs? use it? ans use *args aren’t sure arguments going passed function, want pass stored list tuple arguments function. **kwargs don’t know keyword arguments passed function, pass values dictionary keyword arguments. identiõers args kwargs convention, use *bob **billy wise. q39. len() do? ans determine length string, list, array, etc. example stg='abcd' len(stg) q40. explain split(), sub(), subn() methods “re” module python. ans modify strings, python’s “re” module providing 3 methods. are split() – uses regex pattern “split” given string list. sub() – õnds substrings regex pattern matches replace diàerent string subn() – similar sub() returns new string no. replacements. q41. negative indexes used? ans sequences python indexed consists positive negative numbers. numbers positive uses ‘0’ uses õrst index ‘1’ second index process goes like that. index negative number starts ‘-1’ represents index sequence ‘-2’ penultimate index sequence carries forward like positive number. negative index remove new-line spaces string allow string character given s[-1]. negative index index represent string correct order. q42. python packages? ans python packages namespaces containing multiple modules. q43.how õles deleted python? ans delete õle python, need import os module. that, need use os.remove() function. example import os os.remove(\"xyz.txt\") q44. built-in types python? ans built-in types python follows – integers floating-point complex numbers strings boolean built-in functions q45. advantages numpy arrays oàer (nested) python lists? ans 1. python’s lists eþcient general-purpose containers. support (fairly) eþcient insertion, deletion, appending, concatenation, python’s list comprehensions easy construct manipulate. \n",
      "\n",
      "2. certain limitations don’t support “vectorized” operations like elementwise addition multiplication, fact contain objects diàering types mean python store type information element, execute type dispatching code operating element. 3. numpy eþcient; convenient. lot vector matrix operations free, allow avoid unnecessary work. eþciently implemented. 4. numpy array faster lot built numpy, ffts, convolutions, fast searching, basic statistics, linear algebra, histograms, etc. q46. add values python array? ans elements added array append(), extend() insert (i,x) functions. example a=arr.array('d', [1.1 , 2.1 ,3.1] ) a.append(3.4) print(a) a.extend([4.5,6.3,6.8]) print(a) a.insert(2,3.8) print(a) output array(‘d’, [1.1, 2.1, 3.1, 3.4]) array(‘d’, [1.1, 2.1, 3.1, 3.4, 4.5, 6.3, 6.8]) array(‘d’, [1.1, 2.1, 3.8, 3.1, 3.4, 4.5, 6.3, 6.8]) q47. remove values python array? ans array elements removed pop() remove() method. diàerence functions returns deleted value not. example a=arr.array('d', [1.1, 2.2, 3.8, 3.1, 3.7, 1.2, 4.6]) print(a.pop()) print(a.pop(3)) a.remove(1.1) print(a) output 4.6 3.1 array(‘d’, [2.2, 3.8, 3.7, 1.2]) q48. python oops concepts? ans python object-oriented programming language. means program solved python creating object model. however, python treated procedural structural language. q49. diàerence deep shallow copy? ans shallow copy new instance type gets created keeps values copied new instance. shallow copy copy reference pointers like copies values. references point original objects changes member class aàect original copy it. shallow copy allows faster execution program depends size data used. deep copy store values copied. deep copy doesn’t copy reference pointers objects. makes reference object new object pointed object gets stored. changes original copy won’t aàect copy uses object. deep copy makes execution program slower making certain copies object called. q50. multithreading achieved python? \n",
      "\n",
      "ans 1. python multi-threading package want multi-thread speed code up, it’s usually good idea use it. 2. python construct called global interpreter lock (gil). gil makes sure ‘threads’ execute time. thread acquires gil, little work, passes gil thread. 3. happens quickly human eye like threads executing parallel, taking turns cpu core. 4. gil passing adds overhead execution. means want code run faster threading package isn’t good idea. q51. process compilation linking python? ans compiling linking allows new extensions compiled properly error linking passes compiled procedure. dynamic loading depends style provided system. python interpreter provide dynamic loading conõguration setup õles rebuild interpreter. steps required as 1. create õle language supported compiler system. example õle.c õle.cpp 2. place õle modules/ directory distribution getting used. 3. add line õle setup.local present modules/ directory. 4. run õle spam õle.o 5. successful run rebuild interpreter command top-level directory. 6. õle changed run rebuildmakeõle command ‘make makeõle’. q52. python libraries? them. python libraries collection python packages. majorly python libraries – numpy, pandas, matplotlib, scikit-learn more. q53. split for? split() method separate given string python. example a=\"edureka python\" print(a.split()) output [‘edureka’, ‘python’] q54. import modules python? modules imported import keyword. import modules ways- example import array #importing original module import array arr # importing alias array import * #imports present array module oops interview s q55. explain inheritance python example. ans inheritance allows class gain members(say attributes methods) class. inheritance provides code reusability, makes easier create maintain application. class inheriting called super- class class inherited called derived / child class. diàerent types inheritance supported python 1. single inheritance – derived class acquires members single super class. 2. multi-level inheritance – derived class d1 inherited base class base1, d2 inherited base2. 3. hierarchical inheritance – base class inherit number child classes 4. multiple inheritance – derived class inherited base class. q56. classes created python? \n",
      "\n",
      "ans class python created class keyword. example class employee def __init__(self, name) self.name = e1=employee(\"abc\") print(e1.name) output abc q57. monkey patching python? ans python, term monkey patch refers dynamic modiõcations class module run-time. consider example # m.py class myclass def f(self) print \"f()\" run monkey-patch testing like this import m def monkey_f(self) print \"monkey_f()\" m.myclass.f = monkey_f obj = m.myclass() obj.f() output below monkey_f() see, changes behavior f() myclass function deõned, monkey_f(), outside module m. q58. python support multiple inheritance? ans multiple inheritance means class derived parent classes. python support multiple inheritance, unlike java. q59. polymorphism python? ans polymorphism means ability multiple forms. so, instance, parent class method named abc child class method abc having parameters variables. python allows polymorphism. q60. deõne encapsulation python? ans encapsulation means binding code data together. python class example encapsulation. q61. data abstraction python? ans data abstraction providing required details hiding implementation world. achieved python interfaces abstract classes. q62.does python use access speciõers? ans python deprive access instance variable function. python lays concept preõxing variable, function method single double underscore imitate behavior protected private access speciõers. q63. create class python? ans class class code deõned block. created pass keyword. however, create objects class outside class itself. python pass command executed. it’s null statement. example- \n",
      "\n",
      "class a &nbsp; pass obj=a() obj.name=\"xyz\" print(\"name = \",obj.name) output = xyz q64. object() do? ans returns featureless object base classes. also, parameters. basic python programs q65. write program python execute bubble sort algorithm. def bs(a)&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# = list &nbsp; b=len(a)-1&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# minus 1 compare 2 adjacent values &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; x range(b) &nbsp; &nbsp; &nbsp; y range(b-x) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; a[y]>a[y+1] &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; a[y],a[y+1]=a[y+1],a[y] &nbsp; return a=[32,5,3,6,7,54,87] bs(a) output [3, 5, 6, 7, 32, 54, 87] q66. write program python produce star triangle. def pyfunc(r) x range(r) print(' '*(r-x-1)+'*'*(2*x+1)) pyfunc(9) output * *** ***** ******* ********* *********** ************* *************** ***************** q67. write program produce fibonacci series python. \n",
      "\n",
      "# enter number terms needed&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#0,1,1,2,3,5.... a=int(input(\"enter terms\")) f=0&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#first element series s=1&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#second element series a<=0 &nbsp; print(\"the requested series \",f) else &nbsp; print(f,s,end=\" \") &nbsp; x range(2,a) &nbsp; &nbsp; &nbsp; next=f+s&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; print(next,end=\" \") &nbsp; &nbsp; &nbsp; f=s &nbsp; &nbsp; &nbsp; s=next</pre> output enter terms 5 0 1 1 2 3 q68. write program python check number prime. a=int(input(\"enter number\"))&nbsp; &nbsp; &nbsp; a>1 &nbsp; x range(2,a) &nbsp; &nbsp; &nbsp; if(a%x)==0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; print(\"not prime\") &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; break &nbsp; else &nbsp; &nbsp; &nbsp; print(\"prime\") else &nbsp; print(\"not prime\") output enter number 3 prime q69. write program python check sequence palindrome. a=input(\"enter sequence\") b=a[-1] a==b &nbsp; print(\"palindrome\") else &nbsp; print(\"not palindrome\") output enter sequence 323 palindrome q70. write one-liner count number capital letters õle. code work õle big õt memory. ans let õrst write multiple line solution convert one-liner code. open(some_large_file) fh count = 0 text = fh.read() character text character.isupper() count += 1 try transform single line. \n",
      "\n",
      "count sum(1 line fh character line character.isupper()) q71. write sorting algorithm numerical dataset python. ans following code sort list python list = [\"1\", \"4\", \"0\", \"6\", \"9\"] list = [int(i) list] list.sort() print (list) q72. looking code, write õnal values a0, a1, …an. a0 = dict(zip(('a','b','c','d','e'),(1,2,3,4,5))) a1 = range(10)a2 = sorted([i a1 a0]) a3 = sorted([a0[s] s a0]) a4 = [i a1 a3] a5 = {ii*i a1} a6 = [[i,i*i] a1] print(a0,a1,a2,a3,a4,a5,a6) ans following õnal outputs a0, a1, … a6 a0 = {'a' 1, 'c' 3, 'b' 2, 'e' 5, 'd' 4} # order vary a1 = range(0, 10) a2 = [] a3 = [1, 2, 3, 4, 5] a4 = [1, 2, 3, 4, 5] a5 = {0 0, 1 1, 2 4, 3 9, 4 16, 5 25, 6 36, 7 49, 8 64, 9 81} a6 = [[0, 0], [1, 1], [2, 4], [3, 9], [4, 16], [5, 25], [6, 36], [7, 49], [8, 64], [9, 81]] python libraries interview s q73. explain flask beneõts? ans flask web microframework python based “werkzeug, jinja2 good intentions” bsd license. werkzeug jinja2 dependencies. means little dependencies external libraries. makes framework light little dependency update fewer security bugs. session basically allows remember information request another. öask, session uses signed cookie user look session contents modify. user modify session secret key flask.secret_key. q74. django better flask? ans django flask map url’s addresses typed web browsers functions python. flask simpler compared django but, flask lot meaning need specify details, django lot need work. django consists prewritten code, user need analyze flask gives users create code, therefore, making simpler understand code. technically equally good contain pros cons. q75. mention diàerences django, pyramid flask. ans flask “microframework” primarily build small application simpler requirements. öask, use external libraries. flask ready use. pyramid built larger applications. provides öexibility lets developer use right tools project. developer choose database, url structure, templating style more. pyramid heavy conõgurable. django larger applications like pyramid. includes orm. q76. discuss django architecture. ans django mvt pattern \n",
      "\n",
      "figure python interview s – django architecture developer provides model, view template maps url django magic serve user. q77. explain set database django. ans use command edit mysite/setting.py, normal python module module level representing django settings. django uses sqlite default; easy django users won’t require type installation. case database choice diàerent following keys database ‘default’ item match database connection settings. engines change database ‘django.db.backends.sqlite3’ , ‘django.db.backeneds.mysql’, ‘django.db.backends.postgresql_psycopg2’, ‘django.db.backends.oracle’ name database. case sqlite database, case, database õle computer, absolute path, including õle õle. choosing sqlite database settings like password, host, user, etc. added. django uses sqlite default database, stores data single õle õlesystem. database server— postgresql, mysql, oracle, mssql—and want use sqlite, use database’s administration tools create new database django project. way, (empty) database place, remains tell django use it. project’s settings.py õle comes in. add following lines code setting.py õle databases = { 'default' { 'engine' 'django.db.backends.sqlite3', 'name' os.path.join(base_dir, 'db.sqlite3'), } } q78. example write view django? ans use write view django django.http import httpresponse import datetime def current_datetime(request) = datetime.datetime.now() html = \"<html><body>it %s</body></html> % return httpresponse(html) returns current date time, html document q79. mention django templates consist of. ans template simple text õle. create text-based format like xml, csv, html, etc. template contains variables replaced values template evaluated tags (% tag %) control logic template. figure python interview s – django template \n",
      "\n",
      "q80. explain use session django framework? ans django provides session lets store retrieve data per-site-visitor basis. django abstracts process sending receiving cookies, placing session id cookie client side, storing related data server side. figure python interview s – django framework data stored client side. nice security perspective. q81. list inheritance styles django. ans django, possible inheritance styles 1. abstract base classes style want parent’s class hold information don’t want type child model. 2. multi-table inheritance style sub-classing existing model need model database table. 3. proxy models use model, want modify python level behavior model, changing model’s õelds. web scraping – python interview s q82. save image locally python url address know? ans use following code save image locally url address import urllib.request urllib.request.urlretrieve(\"url\", \"local-filename.jpg\") q83. google cache age url web page? ans use following url format http//webcache.googleusercontent.com/search?q=cacheurlgoeshere sure replace “urlgoeshere” proper web address page site cache want retrieve time for. example, check google webcache age edureka.co you’d use following url http//webcache.googleusercontent.com/search?q=cacheedureka.co q84. required scrap data imdb 250 movies page. õelds movie name, year, rating. ans use following lines code \n",
      "\n",
      "bs4 import beautifulsoup import requests import sys url = 'http//www.imdb.com/chart/top' response = requests.get(url) soup = beautifulsoup(response.text) tr = soup.findchildren(\"tr\") tr = iter(tr) next(tr) movie tr title = movie.find('td', {'class' 'titlecolumn'} ).find('a').contents[0] year = movie.find('td', {'class' 'titlecolumn'} ).find('span', {'class' 'secondaryinfo'}).contents[0] rating = movie.find('td', {'class' 'ratingcolumn imdbrating'} ).find('strong').contents[0] row = title + ' - ' + year + ' ' + ' ' + rating print(row) code help scrap data imdb’s 250 list data analysis – python interview s q85. map function python? ans map function executes function given õrst argument elements iterable given second argument. function given takes 1 arguments, iterables given. #follow link know similar functions. q86. python numpy better lists? ans use python numpy array instead list reasons 1. memory 2. fast 3. convenient information parameters, refer section – numpy vs list. q87. indices n maximum values numpy array? ans indices n maximum values numpy array code import numpy np arr = np.array([1, 3, 2, 4, 5]) print(arr.argsort()[-3][-1]) output [ 4 3 1 ] q88. calculate percentiles python/ numpy? ans calculate percentiles following code import numpy np = np.array([1,2,3,4,5]) p = np.percentile(a, 50) #returns 50th percentile, e.g. median print(p) output 3 q89. diàerence numpy scipy? ans 1. ideal world, numpy contain array data type basic operations indexing, sorting, reshaping, basic elementwise functions, et cetera. \n",
      "\n",
      "2. numerical code reside scipy. however, numpy’s important goals compatibility, numpy tries retain features supported predecessors. 3. numpy contains linear algebra functions, properly belong scipy. case, scipy contains fully-featured versions linear algebra modules, numerical algorithms. 4. scientiõc computing python, probably install numpy scipy. new features belong scipy numpy. q90. 3d plots/visualizations numpy/scipy? ans like 2d plotting, 3d graphics scope numpy scipy, 2d case, packages exist integrate numpy. matplotlib provides basic 3d plotting mplot3d subpackage, mayavi provides wide range high-quality 3d visualization features, utilizing powerful vtk engine. multiple choice s (mcq) q91. following statements create dictionary? (multiple correct answers possible) a) d = {} b) d = {“john”40, “peter”45} c) d = {40”john”, 45”peter”} d) d = (40”john”, 45”50”) b, c & d. dictionaries created specifying keys values. q92. öoor division? a) / b) // c) % d) mentioned b) // operands integer python chops fraction gives round oà value, accurate answer use öoor division. ex, 5/2 = 2.5 operands integer answer expression python 2. 2.5 answer, use öoor division //. so, 5//2 = 2.5 q93. maximum possible length identiõer? a) 31 characters b) 63 characters c) 79 characters d) d) identiõers length. q94. local variable names beginning underscore discouraged? a) indicate private variables class b) confuse interpreter c) indicate global variables d) slow execution a) indicate private variable class python concept private variables, leading underscores indicate variables accessed outside class. q95. following invalid statement? a) abc = 1,000,000 b) b c = 1000 2000 3000 c) a,b,c = 1000, 2000, 3000 d) a_b_c = 1,000,000 b) b c = 1000 2000 3000 spaces allowed variable names. q96. output following? \n",
      "\n",
      "try '1' != 1 raise \"someerror\" else print(\"someerror occured\") \"someerror\" print (\"someerror occured\") a) someerror occured b) someerror occured c) invalid code d) c) invalid code new exception class inherit baseexception. inheritance here. q97. suppose list1 [2, 33, 222, 14, 25], list1[-1] ? a) error b) c) 25 d) 2 c) 25 index -1 corresponds index list. q98. open õle cscores.txt writing, use a) outõle = open(“cscores.txt”, “r”) b) outõle = open(“cscores.txt”, “w”) c) outõle = open(õle = “cscores.txt”, “r”) d) outõle = open(õle = “cscores.txt”, “o”) b) location contains double slashes ( ) w indicate õle written to. q99. output following? f = range (5) open(\"data.txt\", \"w\") f > 2 break print f.closed a) true b) false c) d) error a) true statement open õle guarantees õle object closed block exits. q100. try-except-else executed? a) b) exception occurs c) exception occurs d) exception occurs block c) exception occurs executed exception occurs. hope set python interview s help preparing interviews. best! got us? mention comments section earliest. wish learn python gain expertise quantitative analysis, data mining, presentation data numbers transforming career data scientist role, check interactive, live-online python certiõcation training. use libraries like pandas, numpy, matplotlib, scipy, scikit, pyspark master concepts like python machine learning, scripts, sequence, web scraping big data analytics leveraging apache spark. training comes 24*7 support guide learning period. \n",
      "\n",
      "http//career.guru99.com/ 40 python interview s & answers 1) python? benefits python? python programming language objects, modules, threads, exceptions automatic memory management. benefits pythons simple easy, portable, extensible, build-in data structure open source. 2) pep 8? pep 8 coding convention, set recommendation, write python code readable. 3) pickling unpickling? pickle module accepts python object converts string representation dumps file dump function, process called pickling. process retrieving original python objects stored string representation called unpickling. 4) python interpreted? python language interpreted language. python program runs directly source code. converts source code written programmer intermediate language, translated machine language executed. 5) memory managed python? python memory managed python private heap space. python objects data structures located private heap. programmer access private heap interpreter takes care python private heap. allocation python heap space python objects python memory manager. core api gives access tools programmer code. python inbuilt garbage collector, recycle unused memory frees memory makes available heap space. 1 / 7 read click >>>>> ai related cheatsheets tutorials place https//www.linkedin.com/pulse/all-cheatsheets-one-place-vipul-patel/\n",
      "\n",
      "http//career.guru99.com/ 6) tools help find bugs perform static analysis? pychecker static analysis tool detects bugs python source code warns style complexity bug. pylint tool verifies module meets coding standard. 7) python decorators? python decorator specific change python syntax alter functions easily. 8) difference list tuple? difference list tuple list mutable tuple not. tuple hashed e.g key dictionaries. 9) arguments passed value reference? python object variables hold references objects. references values according functions; result change value references. however, change objects mutable. 10) dict list comprehensions are? syntax constructions ease creation dictionary list based existing iterable. 11) built-in type python provides? mutable immutable types pythons built types mutable built-in types list sets dictionaries immutable built-in types 2 / 7\n",
      "\n",
      "http//career.guru99.com/ strings tuples numbers 12) namespace python? python, introduced place lives hooked for. known namespace. like box variable mapped object placed. variable searched out, box searched, corresponding object. 13) lambda python? single expression anonymous function inline function. 14) lambda forms python statements? lambda form python statements new function object return runtime. 15) pass python? pass means, no-operation python statement, words place holder compound statement, blank left written there. 16) python iterators? python, iterators iterate group elements, containers like list. 17) unittest python? unit testing framework python known unittest. supports sharing setups, automation testing, shutdown code tests, aggregation tests collections etc. 18) python slicing? mechanism select range items sequence types like list, tuple, strings etc. known slicing. 19) generators python? way implementing iterators known generators. normal function yields expression function. 20) docstring python? python documentation string known docstring, way documenting python 3 / 7\n",
      "\n",
      "http//career.guru99.com/ functions, modules classes. 21) copy object python? copy object python, try copy.copy () copy.deepcopy() general case. copy objects them. 22) negative index python? python sequences index positive negative numbers. positive index, 0 index, 1 second index forth. negative index, (-1) index (-2) second index forth. 23) convert number string? order convert number string, use inbuilt function str(). want octal hexadecimal representation, use inbuilt function oct() hex(). 24) difference xrange range? xrange returns xrange object range returns list, uses memory matter range size is. 25) module package python? python, module way structure program. python program file module, imports modules like objects attributes. folder python program package modules. package modules subfolders. 26) mention rules local global variables python? local variables variable assigned new value function's body, it's assumed local. global variables variables referenced inside function implicitly global. 27) share global variables modules? share global variables modules single program, create special module. import config module modules application. module available global variable modules. 28) explain python script executable unix? 4 / 7\n",
      "\n",
      "http//career.guru99.com/ python script executable unix, need things, script file's mode executable line begin # ( #!/usr/local/bin/python) 29) explain delete file python? command os.remove (filename) os.unlink(filename) 30) explain generate random numbers python? generate random numbers python, need import command import random random.random() returns random floating point number range [0,1) 31) explain access module written python c? access module written python c following method, module = =pyimport_importmodule(\"\"); 32) mention use // operator python? floor divisionoperator , dividing operands result quotient showing digits decimal point. instance, 10//5 = 2 10.0//5.0 = 2.0. 33) mention benefits python? python comprises huge standard library internet platforms like email, html, etc. python require explicit memory management interpreter allocates memory new variables free automatically provide easy readability use square brackets easy-to-learn beginners having built-in data types saves programming time effort declaring variables 34) mention use split function python? use split function python breaks string shorter strings defined separator. gives list words present string. 5 / 7\n",
      "\n",
      "http//career.guru99.com/ 35) explain flask & benefits? flask web micro framework python based “werkzeug, jinja 2 good intentions” bsd licensed. werkzeug jingja dependencies. flask micro-framework. means little dependencies external libraries. makes framework light little dependency update security bugs. 36) mention difference django, pyramid, flask? flask “microframework” primarily build small application simpler requirements. flask, use external libraries. flask ready use. pyramid build larger applications. provides flexibility lets developer use right tools project. developer choose database, url structure, templating style more. pyramid heavy configurable. like pyramid, django larger applications. includes orm. 37) mention flask-wtf features? flask-wtf offers simple integration wtforms. features include flask wtf integration wtforms secure form csrf token global csrf protection internationalization integration recaptcha supporting file upload works flask uploads 38) explain common way flask script work? common way flask script work import path application path python file 39) explain access sessions flask? session basically allows remember information request another. flask, uses signed cookie user look session contents modify. user 6 / 7\n",
      "\n",
      "http//career.guru99.com/ modify session secret key flask.secret_key. 40) flask mvc model yes example showing mvc pattern application? basically, flask minimalistic framework behaves mvc framework. mvc perfect fit flask, pattern mvc consider following example powered tcpdf (www.tcpdf.org) 7 / 7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for page in tqdm(pages_and_metadata, total=len(pages_and_metadata)):\n",
    "    print(page[\"formatted_text\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d1c3bd529dadde",
   "metadata": {},
   "source": [
    "## 2.5 Converting the paragraphs to sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3247973ed0de93c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:42:03.889149Z",
     "start_time": "2025-02-10T16:42:03.168549Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b8c53edf6e645f58b8b6b3c817dea4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/641 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nlp = English()\n",
    "nlp.add_pipe('sentencizer')\n",
    "for page in tqdm(pages_and_metadata, total=len(pages_and_metadata)):\n",
    "    sentences = nlp(page[\"formatted_text\"]).sents\n",
    "    sentences = list(set([str(sentence).strip() for sentence in sentences if len(str(sentence).split())>10]))\n",
    "    pages_and_metadata[page[\"page_number\"]][\"sentences\"] = sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3d1e7e6e7b96e2e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:42:05.307900Z",
     "start_time": "2025-02-10T16:42:05.277901Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39c38f2d4b0b4c34bb44c7792d52ed35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/641 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['boltzmann developed simple learning algorithms allow find important information presented complex regularities data.', 'banking industry giving loans primary source making money time repayment rate good profit, risk huge losses.', 'says sample means, sample variance sample standard deviation converge trying estimate.', 'cluster sampling technique difficult study target population spread wide area simple random sampling applied.', 'systematic sampling, list progressed circular manner reach end list, progressed again.', 'banks don’t want lose good customers point time, don’t want acquire bad customers.', 'consider restricted boltzmann machines, single algorithm feature detectors faster compared others.']\n",
      "\n",
      "['predictor variables money spent election campaigning particular candidate, time spent campaigning, etc.', 'studying target population spread wide area difficult applying simple random sampling ineffective, technique cluster sampling used.', 'couple layers added input output size layer smaller size pertaining input layer.', 'logistic regression referred logit model technique predict binary outcome linear combination predictor variables.', 'addition giving insights effective efficient manner, data visualization way restricted bar, line stereotypic graphs.']\n",
      "\n",
      "['fine line simple insightful dashboard awesome looking 0 fruitful insight dashboards.', 'method comparing testing versions systems applications determine version application performs better.', 'businesses like e-commerce, retail, government organizations contain large customer transaction information outdated needs cleaned.', 'different types data existing data source dirty data, clean data, mixed clean dirty data sample clean data.', 'data cleaning important data science end results outcomes data analysis come existing data useless unimportant need cleaned periodically required.', 'area data science, a/b testing know variable existing variables order optimize increase outcome goal.', 'depending size data, suitable tools methods clean data database big data environment.', 'data cleaning reduces data redundancy gives good results data analysis large customer information exists cleaned periodically.', 'modern data science applications rely machine learning model learner learns existing data.', 'learning design principles help build effective efficient visualizations tableau prep tool drastically increase time focusing important part.']\n",
      "\n",
      "['resampling cases estimating accuracy sample statistics subsets accessible data drawing randomly replacement set data points', 'involved process determining sample size needed detecting effect given size cause certain degree assurance.', 'scenarios machine learning finds applications real world ecommerce understanding customer churn, deploying targeted advertising, remarketing.', 'multiple comparisons way comparing variables case customer interests causes false positives resulting requirement correction confidence level seller area e-commerce.', 'techniques statistical power analysis sample size estimation widely deployed making statistical judgment accurate evaluates size needed experimental effects practice.', 'search engine ranking pages depending personal preferences searcher finance evaluating investment opportunities & risks, detecting fraudulent transactions medicare designing drugs depending patient’s history needs robotics machine learning handling situations ordinary social media understanding relationships recommending connections extraction information framing s getting answers databases web.']\n",
      "\n",
      "['answer candidate’s need additional training basic programming languages platforms transferable skills.', 'vital understand cost time money train candidate knowledgeable languages applications required position.', 'statistics machine learning, common tasks fit model set training data, able reliable predictions general untrained data.', 'substituting labels data points performing significance tests validating models random subsets (bootstrapping, cross-validation) 21.', 'firm uses advanced technology address everyday problems consumers businesses alike, admire.', 'underfitting occurs statistical model machine learning algorithm capture underlying trend data.', 'answers look include interest data mining respect company’s innovative practices desire apply analytical skills solve real-world issues data “i passion working data-driven, innovative companies.', 'devise complex models algorithms lend prediction commercial use known predictive analytics.', 'candidate pursuing position passionate data believe company, elements determine candidate’s performance.']\n",
      "\n",
      "['coming square data science projects seen lot companies project kaggle problems.', 'advances handling huge data, best platform engage graphical capacities comes functional graphical capacities limited knowledge field.', 'unsupervised learning, hand, type machine learning inferences drawn datasets containing input data labeled responses.', 'unsupervised learning uses anomaly detection, clustering, latent variable models, neural networks.', 'useful customize plots better tool management benefits release updates regards controlled conditions.', 'following differences types machine learning algorithms – supervised learning makes use decision trees, k-nearest neighbor algorithm, neural networks, regression, support vector machines.', 'enables – supervised learning enables classification regression, unsupervised learning enables classification, dimension reduction, density estimation use – supervised learning prediction, unsupervised learning finds use analysis 26.', 'prediction rate provides low prediction training error test error leads high business problem, error rate training set high error rate test set high, conclude overfitting model.', 'problem faced hands-on analysis data science poor understanding problem hand concentrating tools, end results aspects project.']\n",
      "\n",
      "['method, implementation python programming technique important method machine learning technique area data science.', 'r seen data science area data analysis standalone servers computing separately.', 'linear regression called regression analysis comes area statistical sciences integrated data science.', 'recommender system today widely deployed multiple fields like movie recommendations, music preferences, social tags, research articles, search queries on.', 'dependent variable outcome variable response variable independent variable predictor variable explanatory variable.', 'python better cases repeated tasks jobs data manipulations r programming querying retrieving datasets customized data analysis.', 'process linear regression method predict variable called target variable making best relationship dependent variable independent variable.', 'type system works based person’s past behavior order build model future.', 'python preferred cases general-purpose programming language found applications data science too.', 'python preferred types data science applications time r programming preferred cases high complex data applications.', 'linear regression technique supervised machine learning algorithmic process area data science.', 'example real life, depending expenses occurred financial year monthly expenses, predictions happen calculating approximate upcoming months financial years expenses.', 'predictive analytics area statistical sciences existing information extracted processed predict trends outcomes pattern.']\n",
      "\n",
      "['statistics help data scientists look data patterns, hidden insights convert big data big insights.', 'overfitting comes light data associated complexity, means associated parameters relative number observations.', 'systematic sampling, list actually circular manner selection starts end reaches final, cycle goes on.', 'filtering process recommender systems find patterns information numerous data sources, agents, collaborating perspectives.', 'unnderfittinng happens machine learning algorithm statistical model unable focus underlying insights data.', 'overfitting, statistical model help letting know random noise errors instead underlying relationship.', 'model overfitted performed poor predictive performance acts overly minor fluctuations training data.', 'data scientists learn consumer behavior, interest, engagement, retention finally conversion power insightful statistics.', 'systematic sampling technique, resembles follows systematic way, samples selected ordered sampling frame.', 'machine learning statistics, common task undergo fit model set training data.', 'method helps sense data random creating order interpreting results bell-shaped graph.']\n",
      "\n",
      "['activation function helps introducing nonlinearity neural network enables neural network learn complex functions.', 'machine learning, feature vectors represent numeric symbolic characteristics, called features, object mathematical, easily analyzable way.', 'recommender systems widely areas like news, movies, social tags, music, products, etc.', 'movie recommenders netflix, imdb, & bookmyshow, product recommender e-commerce sites like ebay, amazon, flipcart, youtube video recommendations, game recommendations.', 'recommender systems treated information filtering systems work predict likeness user product.', 'cluster sampling technique difficult study target population spread wide area simple random sampling applied.', 'connection acts similar synapses human brain helps transmitting signals artificial neurons.', 'algorithm learns training data knowledge applied test data, referred supervised learning.', 'artificial neural networks work based nodes called artificial neurons connected another.']\n",
      "\n",
      "['outlier values, simply outliers, data points statistics don’t belong certain population.', 'outlier values assessed individually assessing large set outlier values require substitution 99th 1st percentile values.', 'pushing information readers brain, need figure easily help consume dashboard chart.', 'following steps involved analytics project understand business problem explore data familiar it.', 'prepare data modeling detecting outliers, treating missing values, transforming variables, etc.', 'popular ways treating outlier values change value brought range simply remove value note extreme values outlier values.', 'moment, start explaining simple things mission making complex simple goes away.']\n",
      "\n",
      "['cleaning data multiple sources transform format data analysts data scientists work cumbersome process – number data sources increases, time clean data increases exponentially number sources volume data generated sources.', 'data mining working unlimited data extract level unusual unknown patterns identified.', 'define key performance indicators product playing product, think this key metrics product want optimize?', 'data scientist’s role certain companies involves working closely product teams help define, measure, report metrics.', 'machine learning method study closely relates design, development concerning algorithms provide ability certain computers capacity learn.', 'data scientist’s time goes cleaning data – data gets bigger, time takes clean.', 'survey relied service unit, drawn telephone directories car registration lists. •', 'cleaning right foundation analysis, time takes clean data, alone, makes important.']\n",
      "\n",
      "['methods assess results logistic regression analysis- classification matrix look true negatives false positives.', 'satellite tables map id’s physical description connected central fact table id fields; tables known lookup tables, principally useful real- time applications, save lot memory.', 'focus makes path data scientist unique – mentor preferred method data extraction.', 'data science extraction knowledge large volumes data structured unstructured, continuation field data mining predictive analytics, known knowledge discovery data mining.', 'steps involved build decision trees bootstrapped training samples data tree, time split considered, random sample mm predictors chosen split candidates, pp predictors', 'data scientists are, all, numbers-based problem-solvers, so, it’s important determine example problem you’ve solved ahead time.', 'hash table (hash map) kind data structure implement associative array, structure map keys values.', 'ideally, hash function assign key unique bucket, possible keys generate identical hash causing keys point bucket.']\n",
      "\n",
      "['linear regression statistical technique score variable y predicted score second variable x. x referred predictor variable y criterion variable.', 'goal cross-validation term data set test model training phase (i.e. validation data set) order limit problems like overfitting insight model generalize independent data set.', 'model validation technique evaluating outcomes statistical analysis generalize independent data set.', 'explain utilize coding language know, r sql, language helps complete certain tasks.', 'best possible answer python pandas library provides easy use data structures high-performance data analysis tools.', 'simple terms, differences summarized as- training set fit parameters i.e. weights.', 'validation set considered training set parameter selection avoid overfitting model built.', 'focus data science – means extracting insights numbers – explain makes personal.']\n",
      "\n",
      "['method, error form end network inside it, brings efficient computation gradient.', 'consists below-mentioned steps forward data propagation data training derivatives computed help output target.', 'univariate analysis descriptive analysis differentiate number variables involved given point time.', 'easier assessed individually outlier values outlier values number values required substituted 1st 99th percentile values.', 'wide format method, subject, repeated responses recorded single row, recorded response separate column.', 'sampling bias bias arises select particular people non-random selection samples happened.']\n",
      "\n",
      "['unlike supervised learning, special teacher predefined data machine quickly learn from.', 'helpful areas backgrounds objective exactly forecasted, people want estimate accurately model work real-time.', 'main ambition cross-validation test model test model training phase limit problems like overfitting insights generalize independent data set.', 'supervised learning supervised learning process training machines labeled right kind data.', 'data data bias separate set data taken support conclusion eliminates terrible data based arbitrary grounds, instead generally relying generally stated criteria.', 'attrition bias attrition bias defined error occurs unequal loss participants randomized controlled trial (rct).', 'time interval trial terminated earlier actual time (probably ethical reasons) extreme value finally taken consideration significant value variables similar mean.', 'batch gradient descent backpropagation method, consider data calculating gradient executes update iteration.', 'mentioned different variants backpropagation stochastic gradient descent module, help single training example updating parameters calculation gradient.', 'easier assessed individually outlier values outlier values number values required substituted 1st 99th percentile values.', 'data science defined multidisciplinary subject extract meaningful insights different types data employing scientific methods scientific processes algorithms.']\n",
      "\n",
      "['boltzmann developed simple learning algorithms allow find important information presented complex regularities data.', 'time interval trial terminated early extreme value (often ethical reasons), extreme value likely reached variable largest variance, variables similar mean.', 'types selection bias include sampling bias systematic error non-random sample population causing members population likely included resulting biased sample.', 'cumbersome process number data sources increases, time taken clean data increases exponentially number sources volume data generated sources.', 'data specific subsets data chosen support conclusion rejection bad data arbitrary grounds, instead according previously stated generally agreed criteria.', 'consider restricted boltzmann machines, single algorithm feature detectors faster compared others.', 'attrition attrition bias kind selection bias caused attrition (loss participants) discounting trial subjects/tests run completion.', 'data cleaning help analysis because cleaning data multiple sources helps transform format data analysts data scientists work with.']\n",
      "\n",
      "['also, data cleaning solely 80% total time required carrying data analysis task.', 'stochastic gradient descent use single training example calculation gradient update parameters.', 'main reasons increase data generated sources growth hardware resources required run models gpus multiple times faster help build bigger deeper deep learning models comparatively time required previously.', 'it’s variant stochastic gradient descent instead single training example, mini-batch samples used.', 'important ones are cleaning data different sources helps transforming data format easy work data cleaning increases accuracy machine learning model', 'data cleaning daunting task fact increase number data sources, time required cleaning data increases exponential rate.', 'simple terms, differences summarized as; training set fit parameters i.e. weights test set assess performance model i.e. evaluating predictive power generalization.', 'validation set considered training set parameter selection avoid overfitting model built.']\n",
      "\n",
      "['matplotlib provides primary 3d plotting mplot3d subpackage, mayavi produces wide range high-quality 3d visualization features, utilizing powerful vtk engine.', 'prediction rate high inconsistency training error test error leads ta high business problem, error rate training set low error rate ithe n test set high, conclude overfitting model.', 'best statistical functions, graphical user interface, come price tag readily adopted smaller enterprises r best r open source tool generously academia research community.', 'tableau prep reduce lot time like parent software (tableau) creating impressive visualizations.', 'known logit model, logistic regression statistical technique predicting binary outcome linear combination predictor variables.', 'mainly to increase data generation sources growth hardware resources required running deep learning models caffe, chainer, keras, microsoft cognitive toolkit, pytorch, tensorflow popular deep learning frameworks today.', 'like 2d plotting, 3d graphics scope numpy scipy, 2d example, packages exist integrate numpy.', 'tool lot potentials taking professionals data cleaning, merging step creating final usable data linked tableau desktop getting visualization business insights.', 'deep learning wide array uses, ranging social network filtering medical image analysis speech recognition.', 'deep learning paradigm machine learning displays great degree analogy functioning human brain.', 'linear regression form statistical technique score variable y predicted basis score second variable x, referred predictor variable.']\n",
      "\n",
      "['especially useful data extremities certain region don’t data points specific point.', 'extrapolation determination estimation known set values facts extending taking area region unknown.', 'strictly speaking, database design includes detailed logical model database include physical design choices storage parameters.', 'python python powerful open source programming language easy learn, works tools technologies.', 'data modeling – data modeling (or modeling) software engineering process creating data model information system applying formal data modeling techniques.', 'interpolation, hand, method determining certain value falls certain set values sequence values.']\n",
      "\n",
      "['univariate analyses descriptive statistical analysis techniques differentiated based number variables involved given point time.', 'selection bias product inadequately improperly randomized data leading data sets representative whole.', 'example, pie charts sales based territory involve variable analysis referred univariate analysis.', '3 rejected marry good person based predictive model happen meet him/her years realize false negative?', '1 assume airport ‘a’ received high-security threats based certain characteristics identify particular passenger threat not.']\n",
      "\n",
      "['sampling bias systematic error resulting non-random sample populace causing certain members likely included results biased sample.', 'elbow curve graph contains point represents point post aren’t decrements wss.', 'data – results specific data subsets selected supporting conclusion rejection bad data arbitrarily.', 'defining number clusters clustering algorithm, wss plotted range pertaining number clusters.', 'recommender systems subclass information filtering systems, meant predicting preferences ratings awarded user product.', 'time interval – trial ended extreme value, usually ethical reasons, extreme value likely reached variable variance, variables similar mean.', 'primary objective clustering group similar identities way entities group similar other, groups remain different another.', 'attrition – caused attrition, i.e. loss participants, discounting trial subjects tests didn’t run completion.']\n",
      "\n",
      "['want update algorithm when want model evolve data streams infrastructure underlying data source changing case non-stationarity planning data science certification r – programming?', 'important understand data scientists able communicate findings, work team environment skills perform task.', 'unsupervised learning type machine learning algorithm draw inferences datasets consisting input data labeled responses.', 'p-value – 0.05 denotes weak evidence null hypothesis means null hypothesis rejected.', 'gaussian distribution exponential family distributions, lot them, sort ease use, cases, person machine learning solid grounding statistics, utilized appropriate 111.', 'p-value -0.05 denotes strong evidence null hypothesis means null hypothesis rejected.', 'here’re 100 data science foundations s. free practice test know stand.', 'algorithms clustering, anomaly detection, neural networks, latent variable models data science mock interviews interviews industry expertspersonalized detailed interview feedback access exclusive curated content e.g. example, fruit clustering categorize “fruits soft skin lots dimples”, “fruits shiny hard skin” “elongated yellow fruits”.', 'helps determine candidate’s experience holistic perspective reveals experience demonstrating interpersonal, communication technical skills.']\n",
      "\n",
      "['time series algorithms like arima, arimax, sarima, holts winters interesting learn use solve lot complex problems businesses.', 'validation set training set parameter selection avoiding overfitting machine learning model developed.', 'underfitting occurs, statistical model machine learning algorithm fails capturing underlying trend data.', 'boltzmann machine features simple learning algorithm enables discover fascinating features representing complex regularities present training data.', 'working visualization projects helps develop key skills data scientist possess i.e. thinking shoes end-user.', 'overfitted model overreacts minor fluctuations training data, underfit model under-reacts bigger fluctuations.', 'occurrence – statistical model machine learning algorithm excessively complex, result overfitting.', 'simple learning algorithm involved boltzmann machine slow networks layers feature detectors.', 'order reliable predictions general untrained data machine learning statistics, required fit model set training data.', 'poor predictive performance – overfitting underfitting yield poor predictive performance, way different.', 'contrary, test set meant evaluating testing performance trained machine learning model.', 'following differences overfitting underfitting definition – statistical model suffering overfitting describes random error noise place underlying relationship.']\n",
      "\n",
      "['highlights r programming environment include following extensive collection tools data analysis operators performing calculations matrix array data analysis technique graphical representation highly developed simple effective programming language extensively supports machine learning applications acts connecting link software, tools, datasets create high-quality reproducible analysis flexible powerful provides robust package ecosystem diverse needs useful solve data-oriented problem 120.', 'r programming language includes set software suite graphical representation, statistical computing, data manipulation, calculation.', 'data cleansing essential data science data prone error human negligence, corruption transmission storage things.', 'data cleansing takes huge chunk time effort data scientist multiple sources data emanates speed comes.', 'data cleansing extensively deals process detecting correcting data records, ensuring data complete accurate components data irrelevant deleted modified needs.']\n",
      "\n",
      "['arti+cial intelligence going create 2.3 million jobs 2020 crack job interview come set deep learning interview s. divided article sections basic deep learning interview s advance deep learning interview s basics deep learning interview s q1.', 'traditional ml algorithms solve lot cases, useful working high dimensional data, large number inputs outputs.', 'deep learning interview s know 1.3k views kurt updated 22,2019 deep learning hottest topics 2018-19 good reason.', 'machine learning subset ai technique uses statistical methods enable machines improve experience.', 'second major challenge tell computer features look play important role predicting outcome achieve better accuracy so.', 'dendrite receives signals neurons cell body sums inputs axon transmit signals cells similarly, perceptron receives multiple inputs, applies transformations functions provides output.', 'example, case handwriting recognition, large input different type inputs associated different type handwriting.']\n",
      "\n",
      "['cost function measure accuracy neural network respect given training sample expected output.', 'activation function decides neuron activated calculating weighted sum adding bias it.', 'activation functions like linear identity unit binary step sigmoid logistic tanh relu softmax q6.', 'gradient descent optimization algorithm minimize function iteratively moving direction steepest descent defined negative gradient.', 'stochastic gradient descent uses single training example calculate gradient update parameters.', 'repeat steps 2 3 wj (t+1) – updated weight wj (t) – old weight d – desired output y – actual output x – input q7.', 'weights determine slope classifier line, bias allows shift line left right.', 'mini-batch gradient descent mini-batch gradient variation stochastic gradient descent instead single training example, mini-batch samples used.', 'mini-batches allows help approximate gradient entire training set helps avoid local minima.']\n",
      "\n",
      "['1 2 3 4 5 6 7 8 9 10 11 12 13 params = [weights_hidden, weights_output, bias_hidden, bias_output] def sgd(cost, params, lr=0.05) grads = t.grad(cost=cost, wrt=params) updates = [] p, g zip(params, grads) updates.append([p, p - g * lr]) return updates updates = sgd(cost, params)', 'network single input layer single output layer, zero multiple hidden layers.', 'data normalization important preprocessing step, rescale values +t speci+c range assure better convergence backpropagation.', 'composed input layer receive signal, output layer makes decision prediction input, two, arbitrary number hidden layers true computational engine mlp.', 'output nodes output nodes collectively referred “output layer” responsible computations transferring information network outside world.', 'bad weight initialization prevent network learning good weight initialization helps giving quicker convergence better overall error.', 'input nodes input nodes provide information outside world network referred “input layer”.', 'hidden nodes hidden nodes perform computations transfer information input nodes output nodes.']\n",
      "\n",
      "['batch size mini batch size number sub-samples given network parameter update happens.', 'hyperparameters variables determine network structure(eg number hidden units) variables determine network trained(eg learning rate).', 'feed-forward neural network type neural network architecture connections “fed forward”, i.e. form cycles.', 'activation function activation functions introduce nonlinearity models, allows deep learning models learn nonlinear prediction boundaries.', 'network hyperparameters number hidden layers hidden units layer regularization techniques increase accuracy.', 'likely better performance dropout larger network, giving model opportunity learn independent representations.', 'number hidden layers network weight initialization activation function learning rate momentum number epochs batch size q20.', 'network weight initialization ideally, better use different weight initialization schemes according activation function layer.', 'number epochs number epochs number times training data shown network training.', 'generally, use small dropout value 20%-50% neurons 20% providing good starting point.', 'training hyperparameters learning rate learning rate de+nes quickly network updates parameters.', 'reasons be learning rate low regularization parameter high stuck local minima', 'term “feed-forward” input input layer travels input hidden hidden output layer.']\n",
      "\n",
      "['deep learning frameworks tensorflow caffe microsoft cognitive toolkit/cntk torch/pytorch mxnet chainer keras q24.', 'general, deep learning deal high dimensional data sets dimensions refer different features present data set.', 'operations assigned different nodes computational graph performed parallel, thus, providing better performance terms computations.', 'tensorflow auto differentiation capabilities advanced support threads, asynchronous computation, queue es.', 'convolutional neural network (cnn, convnet) class deep neural networks, commonly applied analyzing visual imagery.', 'layered concepts understand convolutional neural networks convolution convolution layer comprises set independent +lters.', 'basically, think computational graph alternative way conceptualizing mathematical calculations takes place tensorflow program.']\n",
      "\n",
      "['features instantiation parameters like position, size, orientation, deformation, velocity, hue, texture more.', 'connectedness neurons fully connected layer connections activations previous layer, seen regular neural networks.', 'obviously, improper inaccurate results, expect layers complete network perform nicely produce accurate results.', 'recurrent networks type arti+cial neural network designed recognize patterns sequences data, text, genomes, handwriting, spoken word, numerical times series data.', 'unlike standard feedforward neural networks, lstm feedback connections “general purpose computer”.', 'pooling function progressively reduce spatial size representation reduce number parameters computation network.', 'recurrent neural networks use backpropagation algorithm training internal memory, rnn’s able remember important things input received, enables precise predicting what’s coming next.', 'long short-term memory(lstm) arti+cial recurrent neural network architecture +eld deep learning.', 'exploding gradients problem large error gradients accumulate result large updates neural network model weights training.', 'magnitudes gradients accumulate, unstable network likely occur, cause poor prediction results model reports useful ever.', 'earlier layers network important responsible learn detecting simple patterns actually building blocks network.', 'means neurons earlier layers learn slowly compared neurons later layers hierarchy.']\n",
      "\n",
      "['feature variation extracts required features image generates output removing noise unnecessary interruption.', 'autoencoder neural network unsupervised machine learning algorithm applies backpropagation, setting target values equal inputs.']\n",
      "\n",
      "['algorithm useful dimensionality reduction, classification, regression, collaborative filtering, feature learning, topic modeling.', 'restricted boltzmann machine undirected graphical model plays major role deep learning framework recent times.', 'task training minimize error reconstruction, i.e. find efficient compact representation input data.', 'deep autoencoder composed two, symmetrical deep-belief networks shallow layers representing encoding half net.', 'rbm shares similar idea, uses stochastic units particular distribution instead deterministic distribution.', 'autoencoder simple 3-layer neural network output units directly connected input units.']\n",
      "\n",
      "['browse world’s leading job boards, you’ll find it’s heart in-demand tech careers today. “', 'everyone’s trying figure ways optimize businesses practices, automate day-to-day lives little bit easier, little bit productive functional,” www.springboard.com 20 mins read', 'o interview prep 40 artificial intelligence s ver decade, artificial intelligence (ai) grown pipe dream driving force fourth industrial revolution.']\n",
      "\n",
      "['artificial intelligence s introduction ai ai interview internship, there’s good chance interviewer try break ice feel com- fortable asking “simple” general interest s. types s usually cover basics, sound straightforward, sure don’t stumped (seemingly simple s require answer', 'artificial intelligence s categories it’s broad area computer science, ai s popping job interview scenarios.', 'easier navigate space, curated list s artificial intelligence divided multiple categories.', 'so, job interview related data science, machine learning (ml), deep learning (dl), bet artifi- cial intelligence s come up.', 'you’re hoping data science career ladder looking start machine learning internship, sure brush ai interview s answers walk interview oozing confidence.']\n",
      "\n",
      "['open-source modular programming language python leads ai industry simplicity predictable coding behavior.', 'it’s amazon’s alexa self-driving car, goal mimic human intelligence lightning speed (and reduced rate error).']\n",
      "\n",
      "['popularity attributed open-source libraries like mat- plotlib numpy, efficient frameworks scikit-learn, practical version libraries like tensorflow vtk.', 'happens, men- tion following • java • julia • haskell • lisp reading 5 best programming languages ai 4.']\n",
      "\n",
      "['• human-level intelligence • processes data clustering association weak ai • great performing simple tasks • uses supervised unsupervised learning • scope minimal reading what’s difference weak strong ai?', 'current application ml ai based idea enable access data machines observe learn themselves.']\n",
      "\n",
      "['compelling examples ai applications are • chatbots • facial recognition • image tagging • natural language processing • sales prediction • self-driving cars • sentiment analysis reading ask ai experts applications ai?', 'interviewer prods provide real-world examples, list following • amazon product recommendations • fraud detection • search ranking • spam detection • spell correction reading explain machine learning data mining non-computer science people?']\n",
      "\n",
      "['example, human-versus- machine scenario, judge tasked identifying ter- minal occupied human occupied computer based individual performance.', 'reading tensorflow tutorial scratch building deep learning model fashion mnist dataset (part 1) 11.', 'game theory, developed american mathematician josh nash, essential ai plays underlying role smart algorithms improve time.', 'it’s com- prehensive highly adaptable ecosystem libraries, tools, community resources help developers build deploy ml-pow- ered applications.']\n",
      "\n",
      "['you’re naturally passionate ai every- thing related it, knowledge current industry trends.', 'devops replaced calling aiops allows developers engage accurate root cause analysis combining big data, ml, visualization.', 'data scientists space aware following games • symmetric vs. asymmetric • perfect vs. imperfect information • cooperative vs. non-cooperative • simultaneous vs. sequential • zero-sum vs. non-zero-sum reading connection game theory ai?']\n",
      "\n",
      "['common ones are • ai replace humans • ai systems aren’t safe • ai lead significant unemployment']\n",
      "\n",
      "['perspective systems theory, good knowledge represen- tation system following • acquisition efficiency acquire incorporate new data • inferential adequacy derive knowledge representation structures like symbols new knowledge learned old knowledge • inferential efficiency enable addition data existing knowledge structures help inference process • representation adequacy represent knowledge required specific domain reading knowledge representation ai 15.', 'variety keys relational database, including • alternate keys candidate keys exclude primary keys.', 'ai-based technology able complete tasks—for example, analyzing zettabytes data second—it needs humans gather data define pat- terns identification.']\n",
      "\n",
      "['• artificial keys created assigning unique number occurrence record aren’t compound standalone keys. •', 'compound keys combining multiple elements develop unique identifier construct isn’t single data element uniquely identifies occurrences construct.', 'foreign keys groups fields database record point key field group fields create key database record that’s usually different table.']\n",
      "\n",
      "['however, recent years grown go-to programming language following • machine learning • predictive analytics • simple data analytics • statistics data science projects, following packages python stand- ard library life easier accelerate deliveries • numpy (to process large multidimensional arrays, extensive collections high-level mathematical functions, matrices) • pandas (to leverage built-in methods rapidly combining, filtering, grouping data) • scipy (to extend numpy’s capabilities solve tasks related integral calculus, linear algebra, probability theory)', 'relative, ai produces actions, ml produces predictions, data sci- ence produces insights.', 'artificial intelligence s statistics ai, ml, data science great deal overlap, it’s crucial cover bases ai interview.']\n",
      "\n",
      "['disadvantages linear models, main ones are • errors linearity assumptions • lacks autocorrelation • can’t solve overfitting problems • can’t use calculate outcomes binary outcomes reading limitations linear regression modeling data analysis?', 'known social filtering, approach essentially makes sug- gestions based recommendations preferences peo- ple share similar interests.', 'collaborative filtering described process finding pat- terns available information build personalized recommenda- tions.', 'reading 20 python interview s answers—start pre- paring ideal job 17.', 'exam- ple, object’s numerical features list numbers taken output neural network layer.']\n",
      "\n",
      "['means like matrix, row multiple columns (or single column multiple rows) like [1,2,3,5,6,3,2,0].', 'it’s difficult predict ai interview unfold, fol- low asking list keys dictionary, respond following', 'ai data science, feature vectors represent numeric symbolic characteristics object mathematical terms seamless analysis.']\n",
      "\n",
      "['random forest data construct that’s applied ml projects develop large number random decision trees analyzing var- iables.', 'obtain list keys dictionary, you’ll use following function keys() mydict={‘a’1,’b’2,’c’3,’e’5} mydict.keys() dict_keys([‘a’, ‘b’, ‘c’, ‘e’]) reading sort python dictionaries key value 21.', 'it’s important consider reduce biases you’ll want smart algorithms accurate predic- tions based data.']\n",
      "\n",
      "['excellent tool ai ml projects work large labeled unlabeled data sets large number attributes.']\n",
      "\n",
      "['example, better sense covariance covariance matrix, eigenvector help identify direction covariances going.', 'artificial intelligence s programming ai interview s bound enter sphere programming sooner later.', 'popular known principal component analysis dimensionality reduction (e.g., eigenfaces face recognition).', 'example, normalize features 0 1 1 100, helps accelerate learning cycle.']\n",
      "\n",
      "['it’s data structure implements associative array abstract data type map key values.', 'algorithm techniques leveraged are • learning learn • reinforcement learning (deep adversarial networks, q-learning, temporal difference) • semi-supervised learning', 'array, actual table data stored, mapping function that’s known hash function.']\n",
      "\n",
      "['• supervised learning (decision trees, linear regression, naive bayes, nearest neighbor, neural networks, support vector machines) • transduction • unsupervised learning (association rules k-means clustering) reading types machine learning algorithms know 27.', 'defined problem statement, identify appropriate algorithm following • classification algorithm • clustering algorithm • regression algorithm • recommendation algorithm algorithm use depend specific problem you’re trying solve.', 'step essential it’ll help ensure fully understand type problem input output problem want solve.']\n",
      "\n",
      "['interviewer follows methods avoid overfitting, mention cross-valida- tion techniques k-folds cross-validation.', 'reading choose ml algorithm machine learning ques- tions & answers – iii 28.', 'ing algorithm choose k-means algorithm achieve goal filtering spam email system.', 'regular- ization techniques like lasso help penalize model parameters likely lead overfitting.', 'examples aren’t necessary answering s artificial intelligence, help easier point across.']\n",
      "\n",
      "['option cross-validation technique seg- ment data set composite training test sets data.', 'statistical ml, k-nearest neighbor support vector machine good examples inductive learning.', 'literals (top-down) inductive learning • arithmetic literals • equality inequality • predicates deductive learning, smart algorithms draw conclusions fol- lowing truth-generating structure (major premise, minor premise, conclusion) improve based previous decisions.']\n",
      "\n",
      "['• confusion matrix • accuracy • precision • recall sensitivity • specificity • f1 score part, use measures accuracy, confusion matrix, f1 score.', 'pandas, isnull() dropna() handy tools find missing cor- rupted data drop values.', 'however, it’ll critical demonstrate understand nuances model measured choosing right performance measure match problem.']\n",
      "\n",
      "['(source) reading build simple neural network 9 lines python code 34.']\n",
      "\n",
      "['source) reading drawing flower python turtle artificial intelligence s general ai interest tech talent shortage created fierce demand skills, land “dream job” it’ll help demonstrate pas- sion field.', 'scheduled ai interview startup established tech giant, ready wide-ranging ques- tions like ones listed below.']\n",
      "\n",
      "['excellent place start following sciencedirect track published research papers what’s pipeline.', 'example, talk ai jour- ney started weekend hobby grew space years.', 'fact, step summary research experience research papers ready share interviewing panel.']\n",
      "\n",
      "['dyann daley (founder ceo predict align prevent), siddha ganju (solutions architect nvidia), dr.', 'example, you’re interested use ai medical images, health analyt- ics interesting use cases • detecting fractures musculoskeletal injuries • aiding diagnosis neurological diseases • flagging thoracic complications conditions • screening common cancers 38.']\n",
      "\n",
      "['you’re genuinely passionate field, worked projects know find free data sets.', 'company training data collected sebastian thrun, ceo kitty hawk corporation co-founder (and ceo) udacity.', 'example, freely available public data sets know (without conducting google search) • celebfaces (with 200,000 celebrity images 40 attribute annotations) • cifar (with 60,000 images map 10 different classes) • youtube-8m (with 4,000 annotated entities taken enormous data set youtube videos) researchers released hundreds free resources like actual network architecture weights exam- ples.', 'talk ai projects you’ve worked free time, interviewer probably ask sourced data sets.', 'google recaptcha source labeled data store- fronts traffic signs years now.']\n",
      "\n",
      "[]\n",
      "\n",
      "['machine learning interview s categories we’ve traditionally seen machine learning interview s pop categories.', 'm achine learning interview s integral data science interview path data scientist, machine learning engineer, data engi- neer.', 'order help resolve that, curated created list key s machine learning interview.', 'springboard created free guide data science interviews, know exactly trip candidates!', 'general interest machine learning you’ll asked what’s going industry latest machine learning trends.']\n",
      "\n",
      "['essentially, model complex add variables, you’ll lose bias gain variance — order optimally reduced error, you’ll tradeoff bias variance.', 'leads algorithm highly sensitive high degrees variation training data, lead model overfit data.', 'lead model underfit- ting data, making hard high predictive accuracy generalize knowledge training set test set.', 'we’ve divided guide machine learning interview s categories mentioned easily information need comes machine learning inter- view s. machine learning interview s algorithms/theory algorithms s test grasp theory machine learning.', 'bias-variance decomposition essentially decomposes learning error algorithm adding bias, variance bit irreducible error noise underlying dataset.', 'reading bias-variance tradeoff (wikipedia) bias error erroneous overly simplistic assumptions learning algorithm you’re using.']\n",
      "\n",
      "['reading receiver operating characteristic (wikipedia) roc curve graphical representation contrast true positive rates false positive rate thresholds.', 'quora) k-nearest neighbors supervised classification algorithm, k-means clustering unsupervised clustering algorithm.', 'mechanisms similar first, means order k-nearest neighbors work, need labeled data want classify unlabeled point (thus nearest neighbor part).', 'k-means clustering requires set unlabeled points threshold algorithm unlabeled points gradually learn cluster groups computing mean distance different points.', 'example, order classification (a supervised learning task), you’ll need label data you’ll use train model classify data labeled groups.', 'critical difference knn needs labeled points supervised learning, k-means doesn’t — unsu- pervised learning.']\n",
      "\n",
      "['model (true positives) vs fall-out probability trig- ger false alarm (false positives).', 'reading precision recall (wikipedia) recall known true positive rate positives model claims compared actual number positives data.', 'you’d perfect recall (there actually 10 apples, predicted 10) 66.7% precision 15 events predicted, 10 (the apples) correct.', 'precision known positive pre- dictive value, measure accurate positives model claims compared number positives actually claims.', 'easier think recall precision context case you’ve predicted 10 apples 5 oranges case 10 apples.']\n",
      "\n",
      "['mathematically, it’s expressed true positive rate condition sample divided sum false positive rate population true positive rate condition.', 'says (.6 * 0.05) (true pos- itive rate condition sample) / (.6*0.05)(true positive rate condition sample) + (.5*0.95) (false positive rate population) = 0.0594 5.94% chance getting flu.', '60% chance actually having flu flu test, people flu, test false 50% time, overall population 5% chance having flu.', 'reading intuitive (and short) explanation bayes’ theorem (betterexplained) bayes’ theorem gives posterior probability event given known prior knowledge.', 'bayes’ theorem basis branch machine learning notably includes naive bayes classifier.']\n",
      "\n",
      "['quora) despite practical applications, especially text mining, naive bayes considered “naive” makes assumption virtually impossible real-life data conditional probabil- ity calculated pure product individual probabilities components.', 'important consider you’re faced machine learning inter- view s. q7- “naive” bayes naive?', 'quora commenter whimsically, naive bayes classifier figured liked pickles ice cream probably naively recommend pickle ice cream.', 'quora) l2 regularization tends spread error terms, l1 binary/sparse, variables assigned 1 0 weighting.']\n",
      "\n",
      "['sure choice sure explain different algorithms simply effectively five-year-old grasp basics!', 'type tests understanding communi- cate complex technical nuances poise ability sum- marize quickly efficiently.', 'briefly stated, type error means claiming happened hasn’t, type ii error means claim happening fact is.', 'machine learning inter- view s attempt lob basic s sure you’re game you’ve prepared bases.', 'clever way think think type error telling man pregnant, type ii error means tell pregnant woman isn’t carrying baby.']\n",
      "\n",
      "['fourier transform converts signal time frequency domain — it’s common way extract features audio signals time series sen- sor data.', 'fourier transform finds set cycle speeds, amplitudes phases match time signal.', 'fourier transform generic method decompose generic func- tions superposition symmetric functions.', 'reading deep learning (wikipedia) deep learning subset machine learning concerned neural networks use backpropagation certain principles']\n",
      "\n",
      "['sense, deep learning represents unsupervised learning algorithm learns representations data use neural nets.', 'reading k-fold cross-validation time-series model selection (crossvalidated) instead standard k-folds cross-validation, pay attention fact time series randomly distributed data — inherently ordered chronological order.', 'you’ll want like forward chaining you’ll able model past data look forward-facing data. •', 'stack overflow) generative model learn categories data discrimina- tive model simply learn distinction different catego- ries data.', 'pattern emerges later time periods example, model pick effect doesn’t hold earlier years!', 'fold 1 training [1], test [2] • fold 2 training [1 2], test [3] • fold 3 training [1 2 3], test [4] • fold 4 training [1 2 3 4], test [5] • fold 5 training [1 2 3 4 5], test [6] q16- decision tree pruned?']\n",
      "\n",
      "['reading accuracy paradox (wikipedia) tests grasp nuances machine learning model performance!', 'pruning happen bottom-up top-down, approaches reduced error pruning cost complexity prun- ing.', 'however, useless predictive model — model designed find fraud asserted fraud all!', 'reading pruning (decision trees) pruning happens decision trees branches weak predictive power removed order reduce complexity model increase predictive accuracy decision tree model.', 's like help demonstrate understand model accuracy isn’t be-all end-all model performance.', 'example, wanted detect fraud massive dataset sample millions, accurate model likely predict fraud vast minority cases fraud.']\n",
      "\n",
      "['use classification regression wanted results reflect belongingness data points dataset certain explicit categories (ex wanted know male female correlated male female names.)', 'reading regression vs classification (math stackexchange) classification produces discrete values dataset strict catego- ries, regression gives continuous results allow better distinguish differences individual points.', 'reading 8 tactics combat imbalanced classes machine learning dataset (machine learning mastery) imbalanced dataset have, example, classification test 90% data class.', 'weighted average precision recall model, results tending 1 best, tending 0 worst.']\n",
      "\n",
      "['list examples ensemble methods, bagging boosting “bucket models” method demonstrate increase predictive power.', 'main methods avoid overfitting 1- model simpler reduce variance taking account fewer variables parameters, removing noise training data.', 'quora) simple restatement fundamental problem machine learning possibility overfitting training data carrying noise data test set, providing inaccu- rate generalizations.', '3- use regularization techniques lasso penalize certain model parameters they’re likely cause overfitting.', 'reading ensemble learning (wikipedia) ensemble techniques use combination learning algorithms optimize better predictive performance.', 'typically reduce overfit- ting models model robust (unlikely influ- enced small changes training data).']\n",
      "\n",
      "['reading evaluating logistic regression (crossvalidated), lo- gistic regression plain english subsection above.', 'learn springboard’s ai / machine learning bootcamp, kind come job guaran- tee.)', 'allows useful attribute calculating coordinates higher dimensions computationally cheaper explicit calculation said coordinates.', 'what’s important demonstrate understand nuances model measured choose right performance measures right situations.', 'reading kernel method (wikipedia) kernel trick involves kernel functions enable higher- dimension spaces explicitly calculating coordinates points dimension instead, kernel functions compute inner products images pairs data feature space.', 'split dataset training test sets, per- haps use cross-validation techniques segment dataset composite sets training test sets data.', 'kernel trick enables effectively run algorithms high-dimensional space lower-dimensional data. (']\n",
      "\n",
      "['reading 50 open source tools big data (datamation) you’ll want familiar meaning big data different companies different tools they’ll want.', 'spark big data tool demand now, able handle immense datasets speed.', 'machine learning interview s programming machine learning interview s test knowledge programming principles need implement machine learning principles practice.', 'reading handling missing data (o’reilly) find missing/corrupted data dataset drop rows columns, decide replace value.', 'machine learning interview s tend technical s test logic programming skills section focuses latter.', 'pandas, useful methods isnull() dropna() help find columns data missing corrupted data drop values.', 'want fill invalid values placeholder value (for example, 0), use fillna() method.']\n",
      "\n",
      "['look pseudocode frameworks peril-l visualization tools web sequence diagrams help demonstrate ability write code reflects parallelism.', 'demanded, look job descriptions tools pop up you’ll want invest familiarizing them.', 'shuffling linked list involves changing points direct — meanwhile, shuffling array complex takes memory.', 'reading array versus linked list (stack overflow) array ordered collection objects.', 'reading writing pseudocode parallel programming (stack overflow) kind demonstrates ability think parallel- ism handle concurrency programming imple- mentations dealing big data.', 'reading hash table (wikipedia) hash table data structure produces associative array.']\n",
      "\n",
      "['stack overflow) lot machine learning interview s type involve implementation machine learning models company’s problems.', 'you’ll asked create case studies extend knowledge company industry you’re applying machine learning skills.', 'related 20 python interview s machine learning interview s company/industry specific machine learning interview s deal imple- ment general machine learning knowledge specific compa- ny’s requirements.', 'reading 31 free data visualization tools (springboard) what’s important define views properly vis- ualize data personal preferences comes tools.', 'popular tools include r’s ggplot, python’s seaborn matplotlib, tools plot.ly tableau.']\n",
      "\n",
      "['cially revenue drivers company has, types users company takes context industry it’s in.', 'example, interviewing music-streaming startup spotify, remark skills developing bet- ter recommendation model increase user retention, increase revenue long run.', 'startup metrics slideshare linked help understand exactly performance indicators important startups tech companies think revenue growth.']\n",
      "\n",
      "['interviewer trying gauge you’d valuable member team grasp nuances certain things set way company’s data process based company- indus- try-specific conditions.', 'overview deep learning nature scions deep learning (from hinton bengio lecun) good reference paper overview what’s happening deep learning — kind paper want cite.', 'machine learning interview s general machine learning interest series machine learning interview s attempts gauge passion interest machine learning.', 'keeping latest scientific literature machine learning want demonstrate interest machine learning position.']\n",
      "\n",
      "['reading 19 free public data sets data science project (springboard) machine learning interview s like try heart machine learning interest.', 'familiarity case solution help demonstrate you’ve paid attention machine learning while.', 'quora) quora thread contains examples, decision trees categorize people different tiers intelligence based iq scores.', 'team won called bellkor 10% improvement ensem- ble different methods win.', 'sure summary research experience papers ready — explanation background lack formal research experience don’t.', 'related point, organizations hiring machine learn- ing positions look formal experience field.', 'reading netflix prize (wikipedia) netflix prize famed competition netflix offered $1,000,000 better collaborative filtering algorithm.']\n",
      "\n",
      "['related 40 artificial intelligence interview s looking land role machine learning engineer?', 'you’re missing any, check quandl economic financial data, kaggle’s datasets collection great list.', 'find springboard’s machine learning engineer- ing career track, kind come job guarantee.', 'building training data collected sebastian thrun googlex — obtained grad students driving buggies desert dunes!', 'reading waymo tech machine learning interview s like test knowledge different machine learning methods, inven- tiveness don’t know answer.', 'nature paper describes accomplished “monte-carlo tree search deep neural networks trained supervised learning, human expert games, reinforcement learning games self- play.”', 'reading mastering game deep neural networks tree search (nature) alphago beating lee sidol, best human player go, best-of- series truly seminal event history machine learn- ing deep learning.']\n",
      "\n",
      "['browse world’s leading job boards, you’ll find it’s heart in-demand tech careers today. “', 'everyone’s trying figure ways optimize businesses practices, automate day-to-day lives little bit easier, little bit productive functional,” www.springboard.com 20 mins read', 'o interview prep 40 artificial intelligence s ver decade, artificial intelligence (ai) grown pipe dream driving force fourth industrial revolution.']\n",
      "\n",
      "['artificial intelligence s introduction ai ai interview internship, there’s good chance interviewer try break ice feel com- fortable asking “simple” general interest s. types s usually cover basics, sound straightforward, sure don’t stumped (seemingly simple s require answer', 'artificial intelligence s categories it’s broad area computer science, ai s popping job interview scenarios.', 'easier navigate space, curated list s artificial intelligence divided multiple categories.', 'so, job interview related data science, machine learning (ml), deep learning (dl), bet artifi- cial intelligence s come up.', 'you’re hoping data science career ladder looking start machine learning internship, sure brush ai interview s answers walk interview oozing confidence.']\n",
      "\n",
      "['open-source modular programming language python leads ai industry simplicity predictable coding behavior.', 'it’s amazon’s alexa self-driving car, goal mimic human intelligence lightning speed (and reduced rate error).']\n",
      "\n",
      "['popularity attributed open-source libraries like mat- plotlib numpy, efficient frameworks scikit-learn, practical version libraries like tensorflow vtk.', 'happens, men- tion following • java • julia • haskell • lisp reading 5 best programming languages ai 4.']\n",
      "\n",
      "['• human-level intelligence • processes data clustering association weak ai • great performing simple tasks • uses supervised unsupervised learning • scope minimal reading what’s difference weak strong ai?', 'current application ml ai based idea enable access data machines observe learn themselves.']\n",
      "\n",
      "['compelling examples ai applications are • chatbots • facial recognition • image tagging • natural language processing • sales prediction • self-driving cars • sentiment analysis reading ask ai experts applications ai?', 'interviewer prods provide real-world examples, list following • amazon product recommendations • fraud detection • search ranking • spam detection • spell correction reading explain machine learning data mining non-computer science people?']\n",
      "\n",
      "['example, human-versus- machine scenario, judge tasked identifying ter- minal occupied human occupied computer based individual performance.', 'reading tensorflow tutorial scratch building deep learning model fashion mnist dataset (part 1) 11.', 'game theory, developed american mathematician josh nash, essential ai plays underlying role smart algorithms improve time.', 'it’s com- prehensive highly adaptable ecosystem libraries, tools, community resources help developers build deploy ml-pow- ered applications.']\n",
      "\n",
      "['you’re naturally passionate ai every- thing related it, knowledge current industry trends.', 'devops replaced calling aiops allows developers engage accurate root cause analysis combining big data, ml, visualization.', 'data scientists space aware following games • symmetric vs. asymmetric • perfect vs. imperfect information • cooperative vs. non-cooperative • simultaneous vs. sequential • zero-sum vs. non-zero-sum reading connection game theory ai?']\n",
      "\n",
      "['common ones are • ai replace humans • ai systems aren’t safe • ai lead significant unemployment']\n",
      "\n",
      "['perspective systems theory, good knowledge represen- tation system following • acquisition efficiency acquire incorporate new data • inferential adequacy derive knowledge representation structures like symbols new knowledge learned old knowledge • inferential efficiency enable addition data existing knowledge structures help inference process • representation adequacy represent knowledge required specific domain reading knowledge representation ai 15.', 'variety keys relational database, including • alternate keys candidate keys exclude primary keys.', 'ai-based technology able complete tasks—for example, analyzing zettabytes data second—it needs humans gather data define pat- terns identification.']\n",
      "\n",
      "['• artificial keys created assigning unique number occurrence record aren’t compound standalone keys. •', 'compound keys combining multiple elements develop unique identifier construct isn’t single data element uniquely identifies occurrences construct.', 'foreign keys groups fields database record point key field group fields create key database record that’s usually different table.']\n",
      "\n",
      "['however, recent years grown go-to programming language following • machine learning • predictive analytics • simple data analytics • statistics data science projects, following packages python stand- ard library life easier accelerate deliveries • numpy (to process large multidimensional arrays, extensive collections high-level mathematical functions, matrices) • pandas (to leverage built-in methods rapidly combining, filtering, grouping data) • scipy (to extend numpy’s capabilities solve tasks related integral calculus, linear algebra, probability theory)', 'relative, ai produces actions, ml produces predictions, data sci- ence produces insights.', 'artificial intelligence s statistics ai, ml, data science great deal overlap, it’s crucial cover bases ai interview.']\n",
      "\n",
      "['disadvantages linear models, main ones are • errors linearity assumptions • lacks autocorrelation • can’t solve overfitting problems • can’t use calculate outcomes binary outcomes reading limitations linear regression modeling data analysis?', 'known social filtering, approach essentially makes sug- gestions based recommendations preferences peo- ple share similar interests.', 'collaborative filtering described process finding pat- terns available information build personalized recommenda- tions.', 'reading 20 python interview s answers—start pre- paring ideal job 17.', 'exam- ple, object’s numerical features list numbers taken output neural network layer.']\n",
      "\n",
      "['means like matrix, row multiple columns (or single column multiple rows) like [1,2,3,5,6,3,2,0].', 'it’s difficult predict ai interview unfold, fol- low asking list keys dictionary, respond following', 'ai data science, feature vectors represent numeric symbolic characteristics object mathematical terms seamless analysis.']\n",
      "\n",
      "['random forest data construct that’s applied ml projects develop large number random decision trees analyzing var- iables.', 'obtain list keys dictionary, you’ll use following function keys() mydict={‘a’1,’b’2,’c’3,’e’5} mydict.keys() dict_keys([‘a’, ‘b’, ‘c’, ‘e’]) reading sort python dictionaries key value 21.', 'it’s important consider reduce biases you’ll want smart algorithms accurate predic- tions based data.']\n",
      "\n",
      "['excellent tool ai ml projects work large labeled unlabeled data sets large number attributes.']\n",
      "\n",
      "['example, better sense covariance covariance matrix, eigenvector help identify direction covariances going.', 'artificial intelligence s programming ai interview s bound enter sphere programming sooner later.', 'popular known principal component analysis dimensionality reduction (e.g., eigenfaces face recognition).', 'example, normalize features 0 1 1 100, helps accelerate learning cycle.']\n",
      "\n",
      "['it’s data structure implements associative array abstract data type map key values.', 'algorithm techniques leveraged are • learning learn • reinforcement learning (deep adversarial networks, q-learning, temporal difference) • semi-supervised learning', 'array, actual table data stored, mapping function that’s known hash function.']\n",
      "\n",
      "['• supervised learning (decision trees, linear regression, naive bayes, nearest neighbor, neural networks, support vector machines) • transduction • unsupervised learning (association rules k-means clustering) reading types machine learning algorithms know 27.', 'defined problem statement, identify appropriate algorithm following • classification algorithm • clustering algorithm • regression algorithm • recommendation algorithm algorithm use depend specific problem you’re trying solve.', 'step essential it’ll help ensure fully understand type problem input output problem want solve.']\n",
      "\n",
      "['interviewer follows methods avoid overfitting, mention cross-valida- tion techniques k-folds cross-validation.', 'reading choose ml algorithm machine learning ques- tions & answers – iii 28.', 'ing algorithm choose k-means algorithm achieve goal filtering spam email system.', 'regular- ization techniques like lasso help penalize model parameters likely lead overfitting.', 'examples aren’t necessary answering s artificial intelligence, help easier point across.']\n",
      "\n",
      "['option cross-validation technique seg- ment data set composite training test sets data.', 'statistical ml, k-nearest neighbor support vector machine good examples inductive learning.', 'literals (top-down) inductive learning • arithmetic literals • equality inequality • predicates deductive learning, smart algorithms draw conclusions fol- lowing truth-generating structure (major premise, minor premise, conclusion) improve based previous decisions.']\n",
      "\n",
      "['• confusion matrix • accuracy • precision • recall sensitivity • specificity • f1 score part, use measures accuracy, confusion matrix, f1 score.', 'pandas, isnull() dropna() handy tools find missing cor- rupted data drop values.', 'however, it’ll critical demonstrate understand nuances model measured choosing right performance measure match problem.']\n",
      "\n",
      "['(source) reading build simple neural network 9 lines python code 34.']\n",
      "\n",
      "['source) reading drawing flower python turtle artificial intelligence s general ai interest tech talent shortage created fierce demand skills, land “dream job” it’ll help demonstrate pas- sion field.', 'scheduled ai interview startup established tech giant, ready wide-ranging ques- tions like ones listed below.']\n",
      "\n",
      "['excellent place start following sciencedirect track published research papers what’s pipeline.', 'example, talk ai jour- ney started weekend hobby grew space years.', 'fact, step summary research experience research papers ready share interviewing panel.']\n",
      "\n",
      "['dyann daley (founder ceo predict align prevent), siddha ganju (solutions architect nvidia), dr.', 'example, you’re interested use ai medical images, health analyt- ics interesting use cases • detecting fractures musculoskeletal injuries • aiding diagnosis neurological diseases • flagging thoracic complications conditions • screening common cancers 38.']\n",
      "\n",
      "['you’re genuinely passionate field, worked projects know find free data sets.', 'company training data collected sebastian thrun, ceo kitty hawk corporation co-founder (and ceo) udacity.', 'example, freely available public data sets know (without conducting google search) • celebfaces (with 200,000 celebrity images 40 attribute annotations) • cifar (with 60,000 images map 10 different classes) • youtube-8m (with 4,000 annotated entities taken enormous data set youtube videos) researchers released hundreds free resources like actual network architecture weights exam- ples.', 'talk ai projects you’ve worked free time, interviewer probably ask sourced data sets.', 'google recaptcha source labeled data store- fronts traffic signs years now.']\n",
      "\n",
      "[]\n",
      "\n",
      "['machine learning interview s categories we’ve traditionally seen machine learning interview s pop categories.', 'm achine learning interview s integral data science interview path data scientist, machine learning engineer, data engi- neer.', 'order help resolve that, curated created list key s machine learning interview.', 'springboard created free guide data science interviews, know exactly trip candidates!', 'general interest machine learning you’ll asked what’s going industry latest machine learning trends.']\n",
      "\n",
      "['essentially, model complex add variables, you’ll lose bias gain variance — order optimally reduced error, you’ll tradeoff bias variance.', 'leads algorithm highly sensitive high degrees variation training data, lead model overfit data.', 'lead model underfit- ting data, making hard high predictive accuracy generalize knowledge training set test set.', 'we’ve divided guide machine learning interview s categories mentioned easily information need comes machine learning inter- view s. machine learning interview s algorithms/theory algorithms s test grasp theory machine learning.', 'bias-variance decomposition essentially decomposes learning error algorithm adding bias, variance bit irreducible error noise underlying dataset.', 'reading bias-variance tradeoff (wikipedia) bias error erroneous overly simplistic assumptions learning algorithm you’re using.']\n",
      "\n",
      "['reading receiver operating characteristic (wikipedia) roc curve graphical representation contrast true positive rates false positive rate thresholds.', 'quora) k-nearest neighbors supervised classification algorithm, k-means clustering unsupervised clustering algorithm.', 'mechanisms similar first, means order k-nearest neighbors work, need labeled data want classify unlabeled point (thus nearest neighbor part).', 'k-means clustering requires set unlabeled points threshold algorithm unlabeled points gradually learn cluster groups computing mean distance different points.', 'example, order classification (a supervised learning task), you’ll need label data you’ll use train model classify data labeled groups.', 'critical difference knn needs labeled points supervised learning, k-means doesn’t — unsu- pervised learning.']\n",
      "\n",
      "['model (true positives) vs fall-out probability trig- ger false alarm (false positives).', 'reading precision recall (wikipedia) recall known true positive rate positives model claims compared actual number positives data.', 'you’d perfect recall (there actually 10 apples, predicted 10) 66.7% precision 15 events predicted, 10 (the apples) correct.', 'precision known positive pre- dictive value, measure accurate positives model claims compared number positives actually claims.', 'easier think recall precision context case you’ve predicted 10 apples 5 oranges case 10 apples.']\n",
      "\n",
      "['mathematically, it’s expressed true positive rate condition sample divided sum false positive rate population true positive rate condition.', 'says (.6 * 0.05) (true pos- itive rate condition sample) / (.6*0.05)(true positive rate condition sample) + (.5*0.95) (false positive rate population) = 0.0594 5.94% chance getting flu.', '60% chance actually having flu flu test, people flu, test false 50% time, overall population 5% chance having flu.', 'reading intuitive (and short) explanation bayes’ theorem (betterexplained) bayes’ theorem gives posterior probability event given known prior knowledge.', 'bayes’ theorem basis branch machine learning notably includes naive bayes classifier.']\n",
      "\n",
      "['quora) despite practical applications, especially text mining, naive bayes considered “naive” makes assumption virtually impossible real-life data conditional probabil- ity calculated pure product individual probabilities components.', 'important consider you’re faced machine learning inter- view s. q7- “naive” bayes naive?', 'quora commenter whimsically, naive bayes classifier figured liked pickles ice cream probably naively recommend pickle ice cream.', 'quora) l2 regularization tends spread error terms, l1 binary/sparse, variables assigned 1 0 weighting.']\n",
      "\n",
      "['sure choice sure explain different algorithms simply effectively five-year-old grasp basics!', 'type tests understanding communi- cate complex technical nuances poise ability sum- marize quickly efficiently.', 'briefly stated, type error means claiming happened hasn’t, type ii error means claim happening fact is.', 'machine learning inter- view s attempt lob basic s sure you’re game you’ve prepared bases.', 'clever way think think type error telling man pregnant, type ii error means tell pregnant woman isn’t carrying baby.']\n",
      "\n",
      "['fourier transform converts signal time frequency domain — it’s common way extract features audio signals time series sen- sor data.', 'fourier transform finds set cycle speeds, amplitudes phases match time signal.', 'fourier transform generic method decompose generic func- tions superposition symmetric functions.', 'reading deep learning (wikipedia) deep learning subset machine learning concerned neural networks use backpropagation certain principles']\n",
      "\n",
      "['sense, deep learning represents unsupervised learning algorithm learns representations data use neural nets.', 'reading k-fold cross-validation time-series model selection (crossvalidated) instead standard k-folds cross-validation, pay attention fact time series randomly distributed data — inherently ordered chronological order.', 'you’ll want like forward chaining you’ll able model past data look forward-facing data. •', 'stack overflow) generative model learn categories data discrimina- tive model simply learn distinction different catego- ries data.', 'pattern emerges later time periods example, model pick effect doesn’t hold earlier years!', 'fold 1 training [1], test [2] • fold 2 training [1 2], test [3] • fold 3 training [1 2 3], test [4] • fold 4 training [1 2 3 4], test [5] • fold 5 training [1 2 3 4 5], test [6] q16- decision tree pruned?']\n",
      "\n",
      "['reading accuracy paradox (wikipedia) tests grasp nuances machine learning model performance!', 'pruning happen bottom-up top-down, approaches reduced error pruning cost complexity prun- ing.', 'however, useless predictive model — model designed find fraud asserted fraud all!', 'reading pruning (decision trees) pruning happens decision trees branches weak predictive power removed order reduce complexity model increase predictive accuracy decision tree model.', 's like help demonstrate understand model accuracy isn’t be-all end-all model performance.', 'example, wanted detect fraud massive dataset sample millions, accurate model likely predict fraud vast minority cases fraud.']\n",
      "\n",
      "['use classification regression wanted results reflect belongingness data points dataset certain explicit categories (ex wanted know male female correlated male female names.)', 'reading regression vs classification (math stackexchange) classification produces discrete values dataset strict catego- ries, regression gives continuous results allow better distinguish differences individual points.', 'reading 8 tactics combat imbalanced classes machine learning dataset (machine learning mastery) imbalanced dataset have, example, classification test 90% data class.', 'weighted average precision recall model, results tending 1 best, tending 0 worst.']\n",
      "\n",
      "['list examples ensemble methods, bagging boosting “bucket models” method demonstrate increase predictive power.', 'main methods avoid overfitting 1- model simpler reduce variance taking account fewer variables parameters, removing noise training data.', 'quora) simple restatement fundamental problem machine learning possibility overfitting training data carrying noise data test set, providing inaccu- rate generalizations.', '3- use regularization techniques lasso penalize certain model parameters they’re likely cause overfitting.', 'reading ensemble learning (wikipedia) ensemble techniques use combination learning algorithms optimize better predictive performance.', 'typically reduce overfit- ting models model robust (unlikely influ- enced small changes training data).']\n",
      "\n",
      "['reading evaluating logistic regression (crossvalidated), lo- gistic regression plain english subsection above.', 'learn springboard’s ai / machine learning bootcamp, kind come job guaran- tee.)', 'allows useful attribute calculating coordinates higher dimensions computationally cheaper explicit calculation said coordinates.', 'what’s important demonstrate understand nuances model measured choose right performance measures right situations.', 'reading kernel method (wikipedia) kernel trick involves kernel functions enable higher- dimension spaces explicitly calculating coordinates points dimension instead, kernel functions compute inner products images pairs data feature space.', 'split dataset training test sets, per- haps use cross-validation techniques segment dataset composite sets training test sets data.', 'kernel trick enables effectively run algorithms high-dimensional space lower-dimensional data. (']\n",
      "\n",
      "['reading 50 open source tools big data (datamation) you’ll want familiar meaning big data different companies different tools they’ll want.', 'spark big data tool demand now, able handle immense datasets speed.', 'machine learning interview s programming machine learning interview s test knowledge programming principles need implement machine learning principles practice.', 'reading handling missing data (o’reilly) find missing/corrupted data dataset drop rows columns, decide replace value.', 'machine learning interview s tend technical s test logic programming skills section focuses latter.', 'pandas, useful methods isnull() dropna() help find columns data missing corrupted data drop values.', 'want fill invalid values placeholder value (for example, 0), use fillna() method.']\n",
      "\n",
      "['look pseudocode frameworks peril-l visualization tools web sequence diagrams help demonstrate ability write code reflects parallelism.', 'demanded, look job descriptions tools pop up you’ll want invest familiarizing them.', 'shuffling linked list involves changing points direct — meanwhile, shuffling array complex takes memory.', 'reading array versus linked list (stack overflow) array ordered collection objects.', 'reading writing pseudocode parallel programming (stack overflow) kind demonstrates ability think parallel- ism handle concurrency programming imple- mentations dealing big data.', 'reading hash table (wikipedia) hash table data structure produces associative array.']\n",
      "\n",
      "['stack overflow) lot machine learning interview s type involve implementation machine learning models company’s problems.', 'you’ll asked create case studies extend knowledge company industry you’re applying machine learning skills.', 'related 20 python interview s machine learning interview s company/industry specific machine learning interview s deal imple- ment general machine learning knowledge specific compa- ny’s requirements.', 'reading 31 free data visualization tools (springboard) what’s important define views properly vis- ualize data personal preferences comes tools.', 'popular tools include r’s ggplot, python’s seaborn matplotlib, tools plot.ly tableau.']\n",
      "\n",
      "['cially revenue drivers company has, types users company takes context industry it’s in.', 'example, interviewing music-streaming startup spotify, remark skills developing bet- ter recommendation model increase user retention, increase revenue long run.', 'startup metrics slideshare linked help understand exactly performance indicators important startups tech companies think revenue growth.']\n",
      "\n",
      "['interviewer trying gauge you’d valuable member team grasp nuances certain things set way company’s data process based company- indus- try-specific conditions.', 'overview deep learning nature scions deep learning (from hinton bengio lecun) good reference paper overview what’s happening deep learning — kind paper want cite.', 'machine learning interview s general machine learning interest series machine learning interview s attempts gauge passion interest machine learning.', 'keeping latest scientific literature machine learning want demonstrate interest machine learning position.']\n",
      "\n",
      "['reading 19 free public data sets data science project (springboard) machine learning interview s like try heart machine learning interest.', 'familiarity case solution help demonstrate you’ve paid attention machine learning while.', 'quora) quora thread contains examples, decision trees categorize people different tiers intelligence based iq scores.', 'team won called bellkor 10% improvement ensem- ble different methods win.', 'sure summary research experience papers ready — explanation background lack formal research experience don’t.', 'related point, organizations hiring machine learn- ing positions look formal experience field.', 'reading netflix prize (wikipedia) netflix prize famed competition netflix offered $1,000,000 better collaborative filtering algorithm.']\n",
      "\n",
      "['related 40 artificial intelligence interview s looking land role machine learning engineer?', 'you’re missing any, check quandl economic financial data, kaggle’s datasets collection great list.', 'find springboard’s machine learning engineer- ing career track, kind come job guarantee.', 'building training data collected sebastian thrun googlex — obtained grad students driving buggies desert dunes!', 'reading waymo tech machine learning interview s like test knowledge different machine learning methods, inven- tiveness don’t know answer.', 'nature paper describes accomplished “monte-carlo tree search deep neural networks trained supervised learning, human expert games, reinforcement learning games self- play.”', 'reading mastering game deep neural networks tree search (nature) alphago beating lee sidol, best human player go, best-of- series truly seminal event history machine learn- ing deep learning.']\n",
      "\n",
      "['neural networks extract features fed algorithms clustering classification; think deep neural networks components larger machine-learning applications involving algorithms reinforcement learning, classification regression.)', \"tips & stories inbox \\ue602 directory beginner's guide neural networks deep learning contents neural network definition concrete examples neural network elements key concepts deep neural networks example feedforward networks & backprop multiple linear regression updaters custom layers, activation functions loss functions logistic regression & classifiers loss functions deeplearning4j neural networks & artificial intelligence neural network definition neural networks set algorithms, modeled loosely human brain, designed recognize patterns.\", 'help group unlabeled data according similarities example inputs, classify data labeled dataset train on. (', 'patterns recognize numerical, contained vectors, real-world data, images, sound, text time series, translated.']\n",
      "\n",
      "['known “universal approximator”, learn approximate unknown function f(x) = y input x output y, assuming related (by correlation causation, example).', 'outcomes labels applied data example, spam not_spam email filter, good_guy bad_guy fraud detection, angry_customer happy_customer customer relationship management.', 'process learning, neural network finds right f, correct manner transforming x y, f(x) = 3x + 12 f(x) = 9x - 0.1.', 'classification classification tasks depend labeled datasets; is, humans transfer knowledge dataset order neural network learn correlation labels data.', 'is, find labeled data, create labeled dataset (with service like aws mechanical turk figure mighty.ai) spam labeled spam, order teach algorithm correlation labels inputs?', 'detect faces, identify people images, recognize facial expressions (angry, joyful) identify objects images (stop signs, pedestrians, lane markers…) recognize gestures video detect voices, identify speakers, transcribe speech text, recognize sentiment voices classify text spam (in emails), fraudulent (in insurance claims); recognize sentiment text (customer feedback) labels humans generate, outcomes care correlate data, train neural network.']\n",
      "\n",
      "['token, exposed right data, deep learning able establish correlations present events future events.', 'node place computation happens, loosely patterned neuron human brain, fires encounters sufficient stimuli.', 'brief overview deep learning use cases, let’s look neural nets of.', 'given time series, deep learning read string number predict number likely occur next.', 'neural network elements deep learning use “stacked neural networks”; is, networks composed layers.', 'we’re moving world smarter agents combine neural networks algorithms like reinforcement learning attain goals.', 'hardware breakdowns (data centers, manufacturing, transport) health breakdowns (strokes, heart attacks based vital stats data wearables) customer churn (predicting likelihood customer leave, based web activity metadata) employee turnover (ditto, employees) better predict, better prevent pre-empt.', 'predictive analytics regressions classification, deep learning able establish correlations between, say, pixels image person.', 'node combines input data set coefficients, weights, amplify dampen input, assigning significance inputs regard task algorithm trying learn; e.g. input helpful classifying data error?']\n",
      "\n",
      "['advance neural net, complex features nodes recognize, aggregate recombine features previous layer.', 'pairing model’s adjustable weights input features assign significance features regard neural network classifies clusters input.', 'deep-learning networks, layer nodes trains distinct set features based previous layer’s output.', 'earlier versions neural networks perceptrons shallow, composed input output layer, hidden layer between.', 'key concepts deep neural networks deep-learning networks distinguished commonplace single- hidden-layer neural networks depth; is, number node layers data pass multistep process pattern recognition.', 'deep buzzword algorithms like read sartre listen bands haven’t heard yet.', 'layer’s output simultaneously subsequent layer’s input, starting initial input layer receiving data.', 'products summed sum passed node’s so-called activation function, determine extent signal progress network affect ultimate outcome, say, act classification.']\n",
      "\n",
      "['word unstructured data raw media; i.e. pictures, texts, video audio recordings.', 'all, neural nets capable discovering latent structures unlabeled, unstructured data, vast majority data world.', 'apply idea data types deep learning cluster raw text emails news articles.', 'time series data generated smart phone, provide insight users’ health habits; generated autopart, prevent catastrophic breakdowns.', 'example, deep learning million images, cluster according similarities cats corner, ice breakers another, photos grandmother.', 'emails angry complaints cluster corner vector space, satisfied customers, spambot messages, cluster others.', 'therefore, problems deep learning solves best processing clustering world’s raw, unlabeled media, discerning similarities anomalies data human organized relational database to.', 'deep-learning networks perform automatic feature extraction human intervention, unlike traditional machine-learning algorithms.', 'makes deep-learning networks capable handling large, high-dimensional data sets billions parameters pass nonlinear functions.', 'given feature extraction task teams data scientists years accomplish, deep learning way circumvent chokepoint limited experts.']\n",
      "\n",
      "['step neural network involves guess, error measurement slight update weights, incremental adjustment coefficients, slowly learns pay attention important features.', 'given raw data form image, deep-learning network decide, example, input data 90 percent likely represent person.', 'collection weights, start end state, called model, attempt model data’s relationship ground-truth labels, grasp data’s structure.', 'deep learning’s ability process learn huge quantities unlabeled data distinct advantage previous algorithms.', 'process, neural networks learn recognize correlations certain relevant features optimal results – draw connections feature signals features represent, reconstruction, labeled data.', 'training unlabeled data, node layer deep network learns features automatically repeatedly trying reconstruct input draws samples, attempting minimize difference network’s guesses probability distribution input data itself.', 'starting line race state weights initialized, finish line state parameters capable producing sufficiently accurate classifications predictions.', 'models normally start bad end bad, changing time neural network updates parameters.', 'recipe higher performance data net train on, accurate likely be. (', 'example feedforward networks goal neural net arrive point error fast possible.', 'deep-learning networks end output layer logistic, softmax, classifier assigns likelihood particular outcome label.', 'deep-learning network trained labeled data applied unstructured data, giving access input machine-learning nets.']\n",
      "\n",
      "['like child born knowing much, exposure life experience, slowly learn solve problems world.', 'you think neural network miniature enactment scientific method, testing hypotheses trying – scientific method blindfold on.', 'neural takes guess compares ground-truth data, effectively asking expert “did right?”', 'input * weight = guess weighted input results guess input is.', \"error * weight's contribution error = adjustment pseudo-mathematical formulas account key functions neural networks scoring input, calculating loss applying update model – begin three-step process again.\", 'multiple linear regression despite biologically inspired name, artificial neural networks math code, like machine-learning algorithm.', 'network measures error, walks error model, adjusting weights extent contributed error.', 'fact, understands linear regression, methods learn statistics, understand neural net works.', 'imagine time add unit x, dependent variable y_hat increases proportionally, matter far x axis.', 'simplest form, linear regression expressed y_hat = bx + y_hat estimated output, x input, b slope intercept line vertical axis two-dimensional graph. (', 'to concrete x radiation exposure y cancer risk; x daily pushups y_hat total weight benchpress; x fertilizer y_hat size crop.)', 'ground truth - guess = error difference network’s guess ground truth error.', 'neural network corrective feedback loop, rewarding weights support correct guesses, punishing weights lead err.']\n",
      "\n",
      "['gradient descent commonly optimization function adjusts weights according error caused called “gradient descent.”', 'here’s why node merely performed multiple linear regression, y_hat increase linearly limit x’s increase, doesn’t suit purposes.', 'gradient word slope, slope, typical form x-y graph, represents variables relate other rise run, change money change time, etc.', 'it’s typically expressed like this y_hat = b_1*x_1 + b_2*x_2 + b_3*x_3 + (to extend crop example above, add sunlight rainfall growing season fertilizer variable, affecting y_hat.)', 'output nodes, squashed s-shaped space 0 1, passed input layer feed forward neural network, signal reaches final layer net, decisions made.', 'is, inputs mixed different proportions, according coefficients, different leading node subsequent layer.', 'trying build node switch (like neuron…) turns off, depending let signal input pass affect ultimate decisions network.', 'particular case, slope care describes relationship network’s error single weight; i.e. is, error vary weight adjusted.', 'binary decision expressed 1 0, logistic regression non-linear function squashes input translate space 0 1.']\n",
      "\n",
      "['chain rule calculus states feedforward network, relationship net’s error single weight look like this is, given variables, error weight, mediated variable, activation, weight passed, calculate change weight affects change error calculating change activation affects change error, change weight affects change activation.', 'relationship network error weights derivative, de/dw, measures degree slight change weight causes slight change error.', 'essence learning deep learning that adjusting model’s weights response error produces, can’t reduce error more.', 'weight factor deep network involves transforms; signal weight passes activations sums layers, use chain rule calculus march networks activations outputs finally arrive weight , relationship overall error.', 'optimization algorithms examples optimization algorithms include adadelta adagrad adam nesterovs rmsprop sgd conjugate gradient hessian free lbfgs line gradient descent activation functions activation function determines output node generate, based input.']\n",
      "\n",
      "['output layer condense signals $67.59 spent diapers, 15 visits website, range 0 1; i.e. probability given input labeled not.', 'neural networks working labeled data produce binary output, input receive continuous.', 'is, signals network receives input span range values include number metrics, depending problem seeks solve.', 'examples include cube elu hardsigmoid hardtanh identity leakyrelu rationaltanh relu rrelu sigmoid softmax softplus softsign tanh custom layers, activation functions loss functions deeplearning4j, major ai frameworks skymind supports alongside keras, includes custom layers, activations loss functions.', 'output node produces possible outcomes, binary output values 0 1, input variable deserves label .', 'input bases decision include customer spent amazon week, customer visits site.', 'node output layer represents label, node turns according strength signal receives previous layer’s input parameters.']\n",
      "\n",
      "['mse mean squared error linear regression expll exponential log likelihood poisson regression xent cross entropy binary classification mcxent multiclass cross entropy rmse_xent rmse cross entropy squared_loss squared loss negativeloglikelihood negative log likelihood neural networks & artificial intelligence circles, neural networks thought “brute force” ai,', 'input correlates negatively output value flipped negative sign e’s exponent, negative signal grows, quantity e x larger, pushing entire fraction closer zero.', 'input x triggers label grows, expression e x shrinks zero, leaving fraction 1/1, 100%, means approach (without reaching) absolute certainty label applies.', 'that’s you’re feeding logistic regression layer output layer neural network classifier.', 'imagine that, having x exponent, sum products weights corresponding inputs – total signal passing net.', 'set different thresholds prefer – low threshold increase number false positives, higher increase number false negatives – depending like err.', 'that’s input exponent e denominator – exponents force results greater zero.']\n",
      "\n",
      "['effective, eyes inefficient approach modeling, can’t assumptions functional dependencies output input.', 'reading recipe training neural networks, andrej karpathy interactive demo learn build ai applications interactive learning portal.', 'try company press kit contact press privacy platform skil subscriptions documentation community support international english japanese follow facebook twitter linkedin', 'moreover, algorithms hinton’s capsule networks require far fewer instances data converge accurate model; is, present research potential resolve brute force nature deep learning.', 'said, gradient descent recombining weight find best match – method pathfinding shrinks relevant weight space, number updates required computation, orders magnitude.']\n",
      "\n",
      "[]\n",
      "\n",
      "[]\n",
      "\n",
      "['that’s data science interview s cover bunch different topics (data science interdisciplinary field, all) cheeky interviewers love throw odd curveball.']\n",
      "\n",
      "[]\n",
      "\n",
      "[]\n",
      "\n",
      "['step hitting curveballs park coming, coming you’ve got confident rest game.', 'that’s data science interview s cover bunch different topics (data science interdisciplinary field, all) cheeky interviewers love throw odd curveball.']\n",
      "\n",
      "['remember hundred-odd different examples serve confuse more, plus comes didn’t study for?']\n",
      "\n",
      "['technical s 1.1 mathematics 1.2 statistics 1.3 coding 1.4 machine learning 2.']\n",
      "\n",
      "['technical s strong grasp mathematics, statistics, coding, machine learning data scientist.']\n",
      "\n",
      "[]\n",
      "\n",
      "[]\n",
      "\n",
      "['prepared answer quick (mental) maths s, as • sum numbers 1 100? •']\n",
      "\n",
      "[]\n",
      "\n",
      "[]\n",
      "\n",
      "['unlikely you’ll given equation solve, you’ll asked simply worded requires conceptual preparation answer.']\n",
      "\n",
      "['examples are • consider extension rock, paper, scissors n options instead 3 options.', 'values n possible construct fair game, ‘fair’ mean player plays equal number moves beat lose it? •']\n",
      "\n",
      "[]\n",
      "\n",
      "[]\n",
      "\n",
      "[]\n",
      "\n",
      "[]\n",
      "\n",
      "[]\n",
      "\n",
      "[]\n",
      "\n",
      "[]\n",
      "\n",
      "[]\n",
      "\n",
      "[]\n",
      "\n",
      "[]\n",
      "\n",
      "[]\n",
      "\n",
      "[]\n",
      "\n",
      "[]\n",
      "\n",
      "['check following machine learning s we’ve picked you • difference supervised unsupervised machine learning? •']\n",
      "\n",
      "['additionally, stumble way specific way vague s as • explain difference gaussian mixture model k- means. •']\n",
      "\n",
      "[]\n",
      "\n",
      "['countless data science s interviewer going waste time asking dozens s gauge candidate them.']\n",
      "\n",
      "['practical experience qs practical experience s, designed shed light pace work, experiences, habits.', 'avoid having sift catalogue experiences spot, mind experiences versatile – ones exemplify different skills based .']\n",
      "\n",
      "[]\n",
      "\n",
      "['practical experience qs course, vice-versa • so, r preferred programming language.']\n",
      "\n",
      "[]\n",
      "\n",
      "['like job interview, employers interested handle workplace situations, work team good fit company.', 'behavioural s asked indirectly, example, interviewer pose broad s motivation tasks enjoy.']\n",
      "\n",
      "['instead asking hypothetical s (“how deal with…”), interviewer hoping elicit meaningful response pushing chat real-life past event.', 'devote 10% answer time) known star technique, steps help present answers clear succinct fashion.']\n",
      "\n",
      "[]\n",
      "\n",
      "[]\n",
      "\n",
      "[]\n",
      "\n",
      "[]\n",
      "\n",
      "['given market sizing s, called guestimates some, term sounds like need stab dark, case.', 'reaching conclusion require degree guesswork estimation, process use difficult requires rigid logic.']\n",
      "\n",
      "['single correct answer s like chances interviewer doesn’t know exact answer, either.']\n",
      "\n",
      "['industry booming such, companies constantly adapting interview sessions (what common today hardly asked 2 years).', 'data science interview s vary peculiarities, types s remain same, having base knowledge types good preparation allow logically tackle interviewer sleeve.']\n",
      "\n",
      "['authors 365 data science online educational career website offers incredible opportunity find way data science world matter previous knowledge experience.', 'we, authors, committed educators believe curio- sity hindered inability access good learning resources.', 'courses cover necessary topics build data science skills ground up, including mathematics statistics, python, r, sql, data visualization, machine deep learning.', 'comprehensive programs suit needs aspiring bi analysts, data analysts, data scientists.']\n",
      "\n",
      "['comprehensive data science curriculum grow data science skillset training 365 data science program comprehensive set courses, work help student learn need expert data scientist months.', 'training includes sought-after skills, including • fundamentals mathematics • probability • intro data & data science • tableau • sql • r • python • machine learning program consists 45 hours on-demand video, split 12 courses, real-life business examples, 300 exercises.']\n",
      "\n",
      "[]\n",
      "\n",
      "['machine learning, statistical model describes random error noise instead underlying relationship ‘overﬁtting’ occurs.', 'technique, model usually given dataset known data training (training data set) run dataset unknown data model tested.', 'machine learning branch computer science deals system programming order automatically learn improve experience.', 'lot data overﬁtting avoided, overﬁtting happens relatively small dataset, try learn it.', 'model excessively complex, overﬁtting normally observed, having parameters respect number training data types.', 'machine learning relates study, design development algorithms computers capability learn explicitly programmed.', 'method dataset splits section, testing training datasets, testing dataset test model while, training dataset, datapoints come model.', 'inductive machine learning involves process learning examples, system, set observed instances tries induce general rule.', 'https//career.guru99.com/ 50 machine learning interview s & answers 1) machine learning?', 'while, data mining deﬁned process unstructured data tries extract knowledge unknown interesting patterns.']\n",
      "\n",
      "['a) model building b) model testing c) applying model 10) standard approach supervised learning?', 'areas information science like machine learning, set data discover potentially predictive relationship known ‘training set’.', 'a) decision trees b) neural networks (back propagation) c) probabilistic networks d) nearest neighbor e) support vector machines 8) diﬀerent algorithm techniques machine learning?', 'diﬀerent approaches machine learning a) concept vs classiﬁcation learning b) symbolic vs statistical learning', 'training set examples given learner, test set test accuracy hypotheses generated learner, set example held learner.', 'diﬀerent types techniques machine learning a) supervised learning b) unsupervised learning c) semi-supervised learning d) reinforcement learning e) transduction f) learning learn 9) stages build hypotheses model machine learning?']\n",
      "\n",
      "['classiﬁer machine learning system inputs vector discrete continuous feature values outputs single discrete value, class.', 'a) classiﬁcations b) speech recognition c) regression d) predict time series e) annotate strings 16) algorithm independent machine learning?', 'naïve bayes classiﬁer converge quicker discriminative models like logistic regression, need training data.', 'machine learning mathematical foundations independent particular classiﬁer learning algorithm referred algorithm independent machine learning?', 'artiﬁcial intelligence addition machine learning, covers aspects like knowledge representation, natural language processing, planning, robotics etc.', 'designing developing algorithms according behaviours based empirical data known machine learning.', 'a) artiﬁcial intelligence b) rule based inference 14) explain function ‘unsupervised learning’?', 'a) find clusters data b) find low-dimensional representations data c) find interesting directions data d) interesting coordinates correlations e) find novel observations/ database cleaning 15) explain function ‘supervised learning’?']\n",
      "\n",
      "['methods predicting good probabilities supervised learning a) platt calibration b) isotonic regression methods designed binary classiﬁcation, trivial.', 'diﬀerence heuristics decision trees evaluate average quality number disjointed sets rule learners evaluate quality set instances covered candidate rule.', 'a) computer vision b) speech recognition c) data mining d) statistics e) informal retrieval f) bio-informatics 21) genetic programming?', 'inductive logic programming (ilp) subﬁeld machine learning uses logical programming representing background knowledge examples.', 'process selecting models diﬀerent mathematical models, describe data set known model selection.']\n",
      "\n",
      "['paradigms ensemble methods a) sequential ensemble methods b) parallel ensemble methods 36) general principle ensemble method bagging boosting ensemble method?', 'a) combining binary classiﬁers b) modifying binary incorporate multiclass learning 32) ensemble learning?', 'ﬁrst component logical ; consists set bayesian clauses, captures qualitative structure domain.', 'general principle ensemble method combine predictions models built given learning algorithm order improve robustness single model.', 'solve particular computational program, multiple models classiﬁers experts strategically generated combined.', 'instance based learning algorithm referred lazy learning algorithm delay induction generalization process classiﬁcation performed.']\n",
      "\n",
      "['important components relational evaluation techniques a) data acquisition b) ground truth acquisition c) cross validation technique d) query type e) scoring metric f) signiﬁcance test 43) diﬀerent methods sequential supervised learning?', 'pca (principal components analysis), kpca ( kernel based principal component analysis) ica ( independent component analysis) important feature extraction techniques dimensionality reduction.', 'machine learning statistics, dimension reduction process reducing number random variables considerations divided feature selection feature extraction 41) support vector machines?', 'incremental learning method ability algorithm learn new data available classiﬁer generated available dataset.', 'bias term measures closely average classiﬁer produced learning algorithm matches target function.', 'diﬀerent methods solve sequential supervised learning problems a) sliding-window methods b) recurrent sliding windows c) hidden markow models d) maximum entropy markow models e) conditional random ﬁelds']\n",
      "\n",
      "['areas robotics information processing sequential prediction problem arises a) imitation learning b) structured prediction c) model based reinforcement learning 45) batch statistical learning?', 'pac (probably approximately correct) learning learning framework introduced analyze learning algorithms statistical eﬃciency.', 'f) graph transformer networks 44) areas robotics information processing sequential prediction problem arises?', 'statistical learning techniques allow learning function predictor set observed data predictions unseen future data.', 'techniques machine learning a) genetic programming b) inductive learning 50) popular application machine learning day day basis?', 'recommendation engine implemented major ecommerce websites uses machine learning guru99 provides free online tutorial courses like', 'a) sequence prediction b) sequence generation c) sequence recognition d) sequential decision 48) sequence learning?', 'techniques provide guarantees performance learned predictor future unseen data based statistical assumption data generating process.']\n",
      "\n",
      "['java mis mongodb bigdata cassandra web services sqlite jsp informatica accounting sap training python excel asp net hbase project management test management business analyst ethical hacking pmp live project soapui photoshop manual testing mobile testing data warehouse r tutorial tableau devops aws jenkins agile testing rpa junit software engineering selenium ccna angularjs nodejs plsql']\n",
      "\n",
      "['select model family train different training samples resulting different models ml family. •', 'new data comes, prediction 10 models combine predictions finally joint prediction.', 'ensemble learning says, build multiple models select best 2, 3 10.', 'bagging ensemble learning general, machine learning problems try find best possible optimal model given problem.', 'building/training different models means below • select different model families knn, decision trees, linear regression etc. •', 'means finding best possible model given model family, example, finding best possible decision tree finding best possible knn model.', 'time try model families available, come best possible regression model, best possible knn model, best possible svm model etc.']\n",
      "\n",
      "['o weighted mean build models different set features different samples features high importance generate optimal model need assign weight models combining.', 'combine predictions models means • regression (weighted) mean, median, max, min o regression problem mean, median mode models outcomes final outcome.']\n",
      "\n",
      "['suppose build 5 models outcome y1, y2, y3, y4 y5 respectively.', 'create new sample replacement strategy means mix original dataset sample created step 1 create new fresh sample.', 'standard deviation square root variance hance variance reduced factor 1/n. combiner bagging reduces model variance.', 'this hyperparameter, mathematical formula determine this) important thought, different k-fold cross-validation technique.', 'hance bagging high variance machine learning algorithms like decision trees, knn neural networks.', 'recall central limit theorem, says large population size good number (greater 30 optimal) samples means samples mean follow normal distribution population mean.', 'however preferable use high variance algorithms) • suited high variance algorithms. •', 'variance reduction averaging set observations reduces variance – central limit theorem. (', 'important standard deviation samples reduced square root n n sample size.', 'important points bagging • algorithm independent general-purpose technique, work machine learning algorithms. (', 'remember outcomes y1, y2, y3, y4, y5 predicted models trained different samples big dataset.', 'note taking mean explain concept, median, mode etc based problem set.']\n",
      "\n",
      "['random feature subspaces build different models using • different subset training data (create samples replacement) • random subset features! (', 'limitation loss interpretability, example, decision tree single model, easy interpret build multiple decision trees different sample forest decide outcome combining outputs models lose interpretability. •', 'k-fold cross-validation, use training data different iteration choosing different hyperparameter values.', \"feature dominates build multiple decision trees exactly (think if-else rule decision trees) bagging won't work properly.\", 'in bagging time features) • ml algorithm training things gets change.', 'different feature selection different sampling assign different weight models depending dominating features.']\n",
      "\n",
      "['means include features sample training data set) advantages • de-correlates models ensemble • improve accuracy prediction • of-course reduces model variance.', 'bagged trees help reduce variance; random forests so… random forests • sample replacement (shift 1 training set multiple training sets) • train model training set • tree uses random subset feature random forest • dt predicts • mean / majority vote prediction final prediction • faster bagging (fewer splits evaluate tree)', 'recommended heuristics select m p m = sqrt(p) • m = p approach reduces bagged trees. (', 'random feature spaces decision trees known random forests algorithm • decision trees high variance. •', 'choose # models build # features (m) sample p available features. •']\n",
      "\n",
      "['thank like posts machine learning, connect follow blog https//ashutoshtripathi.com/ linkedin https//www.linkedin.com/in/ashutoshtripathi1/ instagram https//www.instagram.com/ashutosh_ai/ medium articles https//medium.com/@ashutosh.optimistic']\n",
      "\n",
      "['linear relationship dependent variables regressors, meaning model creating actually fits data, 2.', 'linear regression good tool quick predictive analysis example, price house depends myriad factors, size location.', 'basically, interaction effect factor (input variable) dependent variable (output variable) differs levels factor.”', 'selection (or ‘sampling’) bias occurs ‘active,’ sense sample data gathered prepared modeling characteristics representative true, future population cases', 'order relationship variables, need build linear regression, predicts line best fit help conclude factors positive negative relationship.', 'data sampling statistical analysis technique select, manipulate analyze representative subset data points identify patterns trends larger data set examined.”', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x data science interview s statistics 1.']\n",
      "\n",
      "['the binomial distribution consists probabilities possible numbers successes n trials independent events probability π (the greek letter pi) occurring.”', 'differences supervised unsupervised learning follows; supervised learning unsupervised learning input data labelled.', 'is, active selection bias occurs subset data systematically (i.e., non-randomly) excluded analysis.”', 'data science blend tools, algorithms, machine learning principles goal discover hidden patterns raw data.', 'the gaussian distribution exponential family distributions, lot them, sort ease use, cases, person machine learning solid grounding statistics, utilized appropriate.”']\n",
      "\n",
      "['train model time model makes simplified assumptions target function easier understand.', 'low bias machine learning algorithms — decision trees, k-nn svm high bias machine learning algorithms — linear regression, logistic regression variance variance error introduced model complex machine learning algorithm, model learns noise training data set performs badly test data set.', 'time interval trial terminated early extreme value (often ethical reasons), extreme value likely reached variable largest variance, variables similar mean.', 'bias-variance trade-off goal supervised machine learning algorithm low bias low variance achieve good prediction performance.', 'data specific subsets data chosen support conclusion rejection bad data arbitrary grounds, instead according previously stated generally agreed criteria.', 'sampling bias systematic error non-random sample population causing members population likely included resulting biased sample.', 'attrition attrition bias kind selection bias caused attrition (loss participants) discounting trial subjects/tests run completion.', 'continue model complex, end over-fitting model model start suffering high variance.']\n",
      "\n",
      "['k-nearest neighbour algorithm low bias high variance, trade-off changed increasing value k increases number neighbours contribute prediction turn increases bias model.', 'support vector machine algorithm low bias high variance, trade-off changed increasing c parameter influences number violations margin allowed training data increases bias decreases variance.']\n",
      "\n",
      "['false-negative(fn) — incorrect negative prediction basic measures derived confusion matrix 1.', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x predicted labels usually match observed labels real-world scenarios.', 'f-score(harmonic mean precision recall) = (1+b)(prec.rec)/(b²prec+rec) b commonly 0.5, 1, 2.']\n",
      "\n",
      "['figure normal distribution bell curve random variables distributed form symmetrical, bell-shaped curve.', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x statistics interview s q5.', 'however, chances data distributed central value bias left right reaches normal distribution form bell-shaped curve.']\n",
      "\n",
      "['likeliness probability called confidence level confidence coefficient represented 1 — alpha, alpha level significance.', 'covariance covariance items vary it’s measure indicates extent random variables change cycle.', 'statistical term; explains systematic relation pair random variables, changes variable reciprocal corresponding change variable.', 'method moments maximum likelihood estimator methods derive point estimators population parameters.', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x covariance correlation mathematical concepts; approaches widely statistics.', 'correlation correlation considered described best technique measuring estimating quantitative relationship variables.', 'a/b testing fantastic method figuring best online promotional marketing strategies business.', 'hypothesis testing randomized experiment variables b. goal a/b testing identify changes web page maximize increase outcome interest.']\n",
      "\n",
      "['follow steve nouri ai data science posts https//lnkd.in/gzu463x example identifying click-through rate banner ad.', 'case children, 4 equally likely possibilities bb, bg, gb gg; b = boy g = girl letter denotes child. ,', 'low p-value (≤ 0.05) indicates strength null hypothesis means reject null hypothesis.', 'remaining 3 possibilities bg, gb & bb, find probability case girls.', 'high p-value (≥ 0.05) indicates strength null hypothesis means accept null hypothesis p-value 0.05 indicates hypothesis way.', 'simple scenario exclude combination (6,6), i.e., roll die 6 appears twice. •', 'probability seeing shooting star 15 minutes = 1 – p( seeing shooting star ) = 1 – 0.2 = 0.8 probability seeing shooting star period hour = (0.8) ^ 4 = 0.4096 probability seeing shooting star hour = 1 – p( seeing star ) = 1 – 0.4096 = 0.5904 q12.']\n",
      "\n",
      "['statistics machine learning, common tasks fit model set training data, able reliable predictions general untrained data.', 'resampling cases • estimating accuracy sample statistics subsets accessible data drawing randomly replacement set data points • substituting labels data points performing significance tests • validating models random subsets (bootstrapping, cross-validation) q17.', 'probability selecting fair coin = 999/1000 = 0.999 probability selecting unfair coin = 1/1000 = 0.001 selecting 10 heads row = selecting fair coin * getting 10 heads + selecting unfair coin p (a) = 0.999 * (1/2)^5 = 0.999 * (1/1024) = 0.000976 p (b) = 0.001 * 1 = 0.001 p( / + b ) = 0.000976 / (0.000976 + 0.001) = 0.4939 p( b / + b ) = 0.001 / 0.001976 = 0.5061 probability selecting head = p(a/a+b) * 0.5 + p(b/a+b) * 1 = 0.4939 * 0.5 + 0.5061 = 0.7531 q15.', 'seasonality = ( true positives ) / ( positives actual dependent variable ) q16.']\n",
      "\n",
      "['combat overfitting underfitting, resample data estimate model accuracy (k-fold cross-validation) having validation dataset evaluate model.', 'data scientist masters program explore curriculum regularisation process adding tuning parameter model induce smoothness order prevent overfitting.', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x overfitting, statistical model describes random error noise instead underlying relationship.', 'underfitting occurs statistical model machine learning algorithm capture underlying trend data.']\n",
      "\n",
      "['example, researching lack exercise leads weight gain, lack exercise = independent variable weight gain = dependent variable.', 'logical error focusing aspects support surviving process casually overlooking work lack prominence.', 'says sample means, sample variance sample standard deviation converge trying estimate.', 'roc curve graphical representation contrast true positive rates false-positive rates thresholds.', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x theorem describes result performing experiment large number times.']\n",
      "\n",
      "['tf–idf value increases proportionally number times word appears document offset frequency word corpus, helps adjust fact words appear frequently general.', 'tf–idf short term frequency-inverse document frequency, numerical statistic intended reflect important word document collection corpus.']\n",
      "\n",
      "['univariate analyses descriptive statistical analysis techniques differentiated based number variables involved given point time.', 'prefer python following reasons • python best option pandas library provides easy use data structures high-performance data analysis tools. •', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x clear output probability distribution element non-negative sum components 1.', 'example, pie charts sales based territory involve variable analysis referred univariate analysis.', 'cumbersome process number data sources increases, time taken clean data increases exponentially number sources volume data generated sources. •', 'data cleaning help analysis because • cleaning data multiple sources helps transform format data analysts data scientists work with. •']\n",
      "\n",
      "['send free voucher mail directly 100 customers minimum purchase condition assume 20% profit sold items $10,000.', 'satellite tables map ids physical names descriptions connected central fact table id fields; tables known lookup tables principally useful real-time applications, save lot memory.', 'absence cancerous cell, chemotherapy certain damage normal healthy cells lead severe diseases, cancer.', 'false negatives cases wrongly classify events non-events, a.k.a type ii error.', 'false positives cases wrongly classified non-event event a.k.a type error. •', 'cluster sampling technique difficult study target population spread wide area simple random sampling applied.', 'issue send $1000 gift vouchers customers actually purchased marked having $10,000 worth purchase.', 'systematic sampling, list progressed circular manner reach end list, progressed again.', 'example 2 let’s e-commerce company decided $1000 gift voucher customers assume purchase $10,000 worth items.', 'assume patient comes hospital tested positive cancer, based lab prediction actually doesn’t cancer.']\n",
      "\n",
      "['goal cross-validation term data set test model training phase (i.e. validation data set) order limit problems like overfitting insight model generalize independent data set.', 'example 1 assume airport ‘a’ received high-security threats based certain characteristics identify particular passenger threat not.', 'banking industry giving loans primary source making money time repayment rate good profit, risk huge losses.', 'devise complex models algorithms lend prediction commercial use known predictive analytics.', 'banks don’t want lose good customers point time, don’t want acquire bad customers.', 'simple terms, differences summarized as; training set fit parameters i.e. weights test set assess performance model i.e. evaluating predictive power generalization.', 'validation set considered training set parameter selection avoid overfitting model built.', 'example 3 rejected marry good person based predictive model happen meet him/her years realize false negative?', 'cross-validation model validation technique evaluating outcomes statistical analysis generalize independent dataset.']\n",
      "\n",
      "['svm stands support vector machine, supervised machine learning algorithm regression classification.', 'algorithms clustering, anomaly detection, neural networks latent variable models e.g. example, fruit clustering categorize “fruits soft skin lots dimples”, “fruits shiny hard skin” “elongated yellow fruits”.', 'bayes’ theorem describes probability event, based prior knowledge conditions related event.', 'unsupervised learning type machine learning algorithm draw inferences datasets consisting input data labelled responses.', 'algorithms support vector machines, regression, naive bayes, decision trees, k-nearest neighbor algorithm neural networks e.g. built fruit classifier, labels “this orange, apple banana”, based showing classifier examples apples, oranges bananas.']\n",
      "\n",
      "['diagram, thinner lines mark distance classifier closest data points called support vectors (darkened data points).', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x n-dimensional space value feature value particular coordinate.']\n",
      "\n",
      "['id3 uses entropy information gain entropy decision tree built top-down root node involve partitioning data homogenious subsets.', 'breaks data set smaller smaller subsets time associated decision tree incrementally developed.', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x decision tree supervised machine learning algorithm mainly regression classification.']\n",
      "\n",
      "['pruning technique machine learning search algorithms reduces size decision trees removing sections tree provide little power classify instances.', 'predictor variables money spent election campaigning particular candidate, time spent campaigning, etc.', 'linear regression statistical technique score variable y predicted score second variable x. x referred predictor variable y criterion variable.', 'so, remove sub-nodes decision node, process called pruning opposite process splitting.', 'logistic regression referred logit model technique predict binary outcome linear combination predictor variables.']\n",
      "\n",
      "['supervised machine learning algorithm, train model labelled data set, training explicitly provide correct labels algorithm tries learn pattern input output.', 'recommender systems subclass information filtering systems meant predict preferences ratings user product.', 'recommender systems widely movies, news, research articles, products, social tags, music, etc.', 'examples include movie recommenders imdb, netflix & bookmyshow, product recommenders e- commerce sites like amazon, ebay & flipkart, youtube video recommendations game recommendations xbox.', 'process filtering recommender systems find patterns information collaborating viewpoints, data sources multiple agents.', 'can’t count outcomes binary outcomes • overfitting problems can’t solve q54.', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x follow ai data science posts https//lnkd.in/gzu463x q53.']\n",
      "\n",
      "['follow steve nouri ai data science posts https//lnkd.in/gzu463x example collaborative filtering predict rating particular user based his/her ratings movies others’ ratings movies.', 'number outlier values assessed individually large number outliers, values substituted 99th 1st percentile values.', 'concept widely recommending movies imdb, netflix & bookmyshow, product recommenders e-commerce sites like amazon, ebay & flipkart, youtube video recommendations game recommendations xbox.', 'patterns identified, missing values substituted mean median values (imputation) simply ignored.', '80% values variable missing answer dropping variable instead treating missing values.', 'prepare data modelling detecting outliers, treating missing values, transforming variables, etc.']\n",
      "\n",
      "['widely approach data scientists use hierarchical clustering create dendrograms identify distinct groups there.', 'bagging bagging tries implement similar learners small sample populations takes mean predictions.', 'objective clustering group similar entities way entities group similar groups different other.', 'red circled point graph i.e. number cluster =6 point don’t decrement wss. •', 'ensemble learning basically combining diverse set learners(individual models) improvise stability predictive power model.']\n",
      "\n",
      "['random forest versatile machine learning method capable performing regression classification tasks.', 'type ensemble learning method, group weak models combine form powerful model.', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x boosting boosting iterative technique adjusts weight observation based classification.']\n",
      "\n",
      "['follow steve nouri ai data science posts https//lnkd.in/gzu463x random forest, grow multiple trees opposed single tree.', 'fold 1 training[1], test[2] fold 1 training[1 2], test[3] fold 1 training[1 2 3], test[4] fold 1 training[1 2 3 4], test[5] q66.', 'forest chooses classification having votes(overall trees forest) case regression, takes average outputs different trees.', 'case time series data, use techniques like forward=chaining — model past data look forward-facing data.', 'steps involved • build decision trees bootstrapped training samples data • tree, time split considered, random sample mm predictors chosen split candidates, pp predictors • rule thumb split m=p√m=p • predictions majority rule q65.', 'box cox transformation statistical technique transform non-normal dependent variables normal shape.', 'instead k-fold cross-validation, aware fact time series randomly distributed data — inherently ordered chronological order.']\n",
      "\n",
      "['follow steve nouri ai data science posts https//lnkd.in/gzu463x techniques assume normality.', 'want update algorithm when • want model evolve data streams infrastructure • underlying data source changing • case non-stationarity • algorithm underperforms/ results lack accuracy q68.', 'numpy array property create mapping complete data set, doesn’t load complete data set memory.', 'normality important assumption statistical techniques, data isn’t normal, applying box-cox means able run broader number tests.', 'box-cox transformation named statisticians george box sir david roxbee cox collaborated 1964 paper developed technique.']\n",
      "\n",
      "['machine learning field computer science gives computers ability learn explicitly programmed.', 'reinforcement learning deep learning subfield machine learning concerned algorithms inspired structure function brain called artificial neural networks.']\n",
      "\n",
      "['follow steve nouri ai data science posts https//lnkd.in/gzu463x deep learning years, major breakthroughs techniques came recent years.', 'neural networks adapt changing input network generates best possible result needing redesign output criteria.', 'main reasons • increase data generated sources • growth hardware resources required run models gpus multiple times faster help build bigger deeper deep learning models comparatively time required previously.']\n",
      "\n",
      "['batch – refers pass entire dataset neural network once, divide dataset batches.', 'determines network trained structure network (such number hidden units, learning rate, epochs, etc.).', 'learning rate set high, causes undesirable divergent behaviour loss function drastic updates weights.', 'referred “loss” “error,” cost function measure evaluate good model’s performance is.', 'learning rate low, training model progress slowly making minimal updates weights.']\n",
      "\n",
      "['rnns type artificial neural networks designed recognise pattern sequence data time series, stock market government agencies etc.', 'convolutional layer – layer performs convolutional operation, creating smaller picture windows data.', 'networks rnn feed-forward named way channel information series mathematical orations performed nodes network.', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x • iteration – 10,000 images data batch size 200.', 'performs down-sampling operations reduce dimensionality creates pooled feature map sliding filter matrix input matrix.']\n",
      "\n",
      "['decision recurrent neural network reached time t-1 affects decision reach moment later time t. recurrent networks sources input, present recent past, combine determine respond new data, life.', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x recurrent networks, hand, input, current input example see, perceived previously time.', 'long-short-term memory (lstm) special kind recurrent neural network capable learning long- term dependencies, remembering information long periods default behaviour.', 'steps lstm network • step 1 network decides forget remember. •']\n",
      "\n",
      "['follow steve nouri ai data science posts https//lnkd.in/gzu463x neural networks, mlps input layer, hidden layer, output layer.', 'means input layers, data coming in, activation function based nodes weights added together, producing output.', 'single layer perceptron classify linear separable classes binary output (0,1), mlp classify nonlinear classes.']\n",
      "\n",
      "['method, error end network weights inside network allowing efficient computation gradient.', 'stochastic gradient descent use single training example calculation gradient update parameters. •', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x training rnn, exponentially growing (very large) error gradients accumulate result large updates neural network model weights training, they’re known exploding gradients.', 'it’s variant stochastic gradient descent instead single training example, mini-batch samples used.', 'following steps data scientist masters program weekday / weekend batchessee batch details • forward propagation training data • derivatives computed output target • propagate computing derivative error wrt output activation • previously calculated derivatives output • update weights q90.']\n",
      "\n",
      "['purpose libraries scientific computation numpy tabular data pandas data modelling & preprocessing scikit learn time-series analysis statsmodels text processing regular expressions, nltk deep learning tensorflow, pytorch q94.', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x • keras • caffe • chainer q92.', 'restricted boltzmann machines” algorithm single layer feature detectors makes faster rest.', 'auto-encoders simple learning networks aim transform inputs outputs minimum possible error.', 'boltzmann machines simple learning algorithm allows discover interesting features represent complex regularities training data.']\n",
      "\n",
      "['batch normalization technique improve performance stability neural networks normalizing inputs layer mean output activation zero standard deviation one.', 'tensorflow provides c++ python apis, making easier work faster compilation time compared deep learning libraries like keras torch.', 'arrays data different dimensions ranks fed input neural network called “tensors.”', 'batch gradient descent stochastic gradient descent batch gradient computes gradient entire dataset.', 'dropout technique dropping hidden visible units network randomly prevent overfitting data (typically dropping 20 cent nodes).']\n",
      "\n",
      "['logistic regression measures relationship dependent variable (our label want predict) independent variables (our features) estimating probability underlying logistic function (sigmoid).', 'network nodes node operates, nodes represent mathematical operations, edges represent tensors.', 'supervised learning unsupervised learning • uses known labeled data input • supervised learning feedback mechanism • commonly supervised learning algorithms decision trees, logistic regression, support vector machine • uses unlabeled data input • unsupervised learning feedback mechanism • commonly unsupervised learning algorithms k-means clustering, hierarchical clustering, apriori algorithm 102.']\n",
      "\n",
      "['follow steve nouri ai data science posts https//lnkd.in/gzu463x formula graph sigmoid function shown 103.', 'calculate information gain attributes (we gain information sorting different objects other) 4.', \"repeat procedure branch decision node branch finalized example, let's want build decision tree decide accept decline job offer.\"]\n",
      "\n",
      "[\"use regularization techniques, lasso, penalize certain model parameters they're likely cause overfitting 106.\", 'follow steve nouri ai data science posts https//lnkd.in/gzu463x clear decision tree offer accepted if • salary greater $50,000 • commute hour • incentives offered 104.', \"randomly select 'k' features total of'm' features k << m 2. '\", \"build forest repeating steps 'n' times create 'n' number trees 105.\", 'split data different packages decision tree different groups data, random forest brings trees together.']\n",
      "\n",
      "['follow steve nouri ai data science posts https//lnkd.in/gzu463x univariate univariate data contains variable.', 'example height students height (in cm) 164 167.3 170 174.2 178 180 patterns studied drawing conclusions mean, median, mode, dispersion range, minimum, maximum, etc.']\n",
      "\n",
      "['follow steve nouri ai data science posts https//lnkd.in/gzu463x temperature (in celcius) sales 20 2,000 25 2,100 26 2,300 28 2,400 30 2,600 36 3,100 here, relationship visible table temperature sales directly proportional other.']\n",
      "\n",
      "['wrapper methods involves • forward selection test feature time adding good fit • backward selection test features start removing works better • recursive feature elimination recursively looks different features pair wrapper methods labor-intensive, high-end computers needed lot data analysis performed wrapper method.', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x 3 2 1,100 $600,000 3.5 5 1,500 $900,000 4 3 2,100 $1,200,000 patterns studied drawing conclusions mean, median, mode, dispersion range, minimum, maximum, etc.', 'main methods feature selection filter methods involves • linear discrimination analysis • anova • chi-square best analogy selecting features \"bad data in, bad answer out.\"']\n",
      "\n",
      "['following ways handle missing data values data set large, simply remove rows missing data values.', 'numbers multiples five, print \"fizzbuzz\" code shown below note range mentioned 51, means zero 50.', 'smaller data sets, substitute missing values mean average rest data pandas data frame python.', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x multiples three, print \"fizz\" instead number multiples five, print \"buzz.\"']\n",
      "\n",
      "['plot1 = [1,3] plot2 = [2,5] euclidean distance calculated follows euclidean_distance = sqrt( (plot1[0]-plot2[0])**2 + (plot1[1]-plot2[1])**2 ) 111.', 'dimensionality reduction refers process converting data set vast dimensions data fewer dimensions (fields) convey similar information concisely.', '-2 -4 2 -2 1 2 4 2 5 characteristic equation shown expanding determinant (-2 – λ) [(1-λ) (5-λ)-2x2] + 4[(-2) x (5-λ) -4x2] + 2[(-2) x 2-4(1-λ)] =0 - λ3 + 4λ2 + 27λ – 90 = 0, λ3 - 4 λ2 -27 λ + 90 = 0 algebraic equation built eigenvectors.', \"removes redundant features; example, there's point storing value different units (meters inches).\"]\n",
      "\n",
      "['steps maintain deployed model are monitor constant monitoring models needed determine performance accuracy.', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x 33 – 4 x 32 - 27 x 3 +90 = 0 hence, (λ - 3) factor λ3 - 4 λ2 - 27 λ +90 = (λ – 3) (λ2 – λ – 30) eigenvalues 3,-5,6 (λ – 3) (λ2 – λ – 30) = (λ – 3) (λ+5) (λ-6), calculate eigenvector λ = 3 x = 1, -5 - 4y + 2z =0, -2 - 2y + 2z =0 subtracting equations 3 + 2y = 0, subtracting second equation y = -(3/2) z = -(1/2) similarly, calculate eigenvectors -5 6.']\n",
      "\n",
      "['split different areas collaborative filtering example, last.fm recommends tracks users similar interests play often.', 'commonly seen amazon making purchase; customers notice following message accompanied product recommendations \"users bought bought…\" content-based filtering example pandora uses properties song recommend music similar properties.', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x new models compared determine model performs best.', \"idea elbow method run k- means clustering data set 'k' number clusters.\"]\n",
      "\n",
      "['p-value typically ≤ 0.05 indicates strong evidence null hypothesis; reject null hypothesis.', 'example, data points clustered zero 10, point lies 100, remove point.', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x sum squares (wss), defined sum squared distance member cluster centroid.', 'p-value typically > 0.05 indicates weak evidence null hypothesis, accept null hypothesis.']\n",
      "\n",
      "['follow steve nouri ai data science posts https//lnkd.in/gzu463x graph, variance constant time.', 'formula accuracy is accuracy = (true positive + true negative) / total observations = (262 + 347) / 650 = 609 / 650 = 0.93 result, accuracy 93 percent.']\n",
      "\n",
      "['collaborative filtering explains behavior users purchase history terms ratings, selection, etc.', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x consider confusion matrix previous .', \"precision = (true positive) / (true positive + false positive) = 262 / 277 = 0.94 recall rate = (true positive) / (total positive + false negative) = 262 / 288 = 0.90 122. '\", 'example, sales page shows certain number people buy new phone buy tempered glass time.']\n",
      "\n",
      "['forger try different techniques sell fake wine sure specific techniques past shop owner’s check.', 'forger’s goal create wines indistinguishable authentic ones shop owner intends tell wine real accurately let understand example help image.', 'apart technical s, interviewer hit simple ones check overall confidence, likes following.', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x suppose wine shop purchasing wine dealers, resell later.', 'discriminator generator cnn keeps keys producing images closer appearance real images discriminator tries determine difference real fake images ultimate aim discriminator learn identify real fake images.']\n",
      "\n",
      "['want predict probability death heart disease based risk factors age, gender, blood cholesterol level.', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x hence, evaluate model performance, use sensitivity (true positive rate), specificity (true negative rate), f measure determine class wise performance classifier.', '0, 0, 0, 1, 1, 1, 1, 1] choose correct answer.', \"k-means clustering • linear regression • k-nn (k-nearest neighbor) • decision trees k nearest neighbor algorithm compute nearest neighbor doesn't value, computes nearest neighbor based features.\", 'following machine learning algorithms inputting missing values categorical continuous variables? •', 'formula calculating entropy is putting p=5 n=8, entropy = = -(5/8 log(5/8) + 3/8 log(3/8)) 127.', \"you're dealing k-means clustering linear regression, need pre- processing, otherwise, they'll crash.\"]\n",
      "\n",
      "['decision trees looking grouping people specifically different similarities, indicates value k. therefore, k-means clustering (answer a) appropriate algorithm study.', 'grape, apple} frequent itemset answer a {grape, apple} frequent itemset 130.', 'run association rules algorithm dataset, rules {banana, apple} => {grape} {apple, orange} => {grape} found relevant.']\n",
      "\n",
      "['recommender systems subclass information filtering systems meant predict preferences ratings user product.', 'factor called root cause deduction problem-fault-sequence averts final undesirable event recurring.', \"student's t-test answer a one-way anova additional data science interview s basic concepts 131.\", \"machine learning, feature vectors represent numeric symbolic characteristics (called features) object mathematical way that's easy analyze.\"]\n",
      "\n",
      "['statistical hypothesis testing randomized experiments variables, b. objective a/b testing detect changes web page maximize increase outcome strategy.', 'recommender systems use filtering process find patterns information collaborating perspectives, numerous data sources, agents.', 'satellite tables map ids physical names descriptions connected central fact table id fields; tables known', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x cross-validation model validation technique evaluating outcomes statistical analysis generalize independent data set.', 'states sample mean, sample variance sample standard deviation converge trying estimate.', 'goal cross-validation term data set test model training phase (i.e. validation data set) limit problems like overfitting gain insight model generalize independent data set.', \"assumption linearity errors • can't count outcomes binary outcomes • overfitting problems can't solve 141.\"]\n",
      "\n",
      "['follow steve nouri ai data science posts https//lnkd.in/gzu463x lookup tables principally useful real-time applications, save lot memory.', 'want update algorithm when • want model evolve data streams infrastructure • underlying data source changing • case non-stationarity 145.', 'resampling cases • estimating accuracy sample statistics subsets accessible data, drawing randomly replacement set data points • substituting labels data points performing significance tests • validating models random subsets (bootstrapping, cross-validation) 147.']\n",
      "\n",
      "['ability write small, clean functions (important developer), preferably pure functions don’t alter objects. •', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x survivorship bias logical error focusing aspects support surviving process casually overlooking lack prominence.', 'tree, time split considered, random sample mm predictors chosen split candidates pp predictors 3.', 'good understanding built-in data types especially lists, dictionaries, tuples, sets. •', 'credit kdnuggets, simplilearn, edureka, guru99, hackernoon, datacamp, nitin panwar, michael rundell', 'following important skills possess come handy performing data analysis python. •', 'scikit-learn cheat sheet** • ability write efficient list comprehensions instead traditional loops. •']\n",
      "\n",
      "['arti+cial intelligence going create 2.3 million jobs 2020 crack job interview come set deep learning interview s. divided article sections basic deep learning interview s advance deep learning interview s basics deep learning interview s q1.', 'traditional ml algorithms solve lot cases, useful working high dimensional data, large number inputs outputs.', 'deep learning interview s know 1.3k views kurt updated 22,2019 deep learning hottest topics 2018-19 good reason.', 'machine learning subset ai technique uses statistical methods enable machines improve experience.', 'second major challenge tell computer features look play important role predicting outcome achieve better accuracy so.', 'dendrite receives signals neurons cell body sums inputs axon transmit signals cells similarly, perceptron receives multiple inputs, applies transformations functions provides output.', 'example, case handwriting recognition, large input different type inputs associated different type handwriting.']\n",
      "\n",
      "['cost function measure accuracy neural network respect given training sample expected output.', 'activation function decides neuron activated calculating weighted sum adding bias it.', 'activation functions like linear identity unit binary step sigmoid logistic tanh relu softmax q6.', 'gradient descent optimization algorithm minimize function iteratively moving direction steepest descent defined negative gradient.', 'stochastic gradient descent uses single training example calculate gradient update parameters.', 'repeat steps 2 3 wj (t+1) – updated weight wj (t) – old weight d – desired output y – actual output x – input q7.', 'weights determine slope classifier line, bias allows shift line left right.', 'mini-batch gradient descent mini-batch gradient variation stochastic gradient descent instead single training example, mini-batch samples used.', 'mini-batches allows help approximate gradient entire training set helps avoid local minima.']\n",
      "\n",
      "['1 2 3 4 5 6 7 8 9 10 11 12 13 params = [weights_hidden, weights_output, bias_hidden, bias_output] def sgd(cost, params, lr=0.05) grads = t.grad(cost=cost, wrt=params) updates = [] p, g zip(params, grads) updates.append([p, p - g * lr]) return updates updates = sgd(cost, params)', 'network single input layer single output layer, zero multiple hidden layers.', 'data normalization important preprocessing step, rescale values +t speci+c range assure better convergence backpropagation.', 'composed input layer receive signal, output layer makes decision prediction input, two, arbitrary number hidden layers true computational engine mlp.', 'output nodes output nodes collectively referred “output layer” responsible computations transferring information network outside world.', 'bad weight initialization prevent network learning good weight initialization helps giving quicker convergence better overall error.', 'input nodes input nodes provide information outside world network referred “input layer”.', 'hidden nodes hidden nodes perform computations transfer information input nodes output nodes.']\n",
      "\n",
      "['batch size mini batch size number sub-samples given network parameter update happens.', 'hyperparameters variables determine network structure(eg number hidden units) variables determine network trained(eg learning rate).', 'feed-forward neural network type neural network architecture connections “fed forward”, i.e. form cycles.', 'activation function activation functions introduce nonlinearity models, allows deep learning models learn nonlinear prediction boundaries.', 'network hyperparameters number hidden layers hidden units layer regularization techniques increase accuracy.', 'likely better performance dropout larger network, giving model opportunity learn independent representations.', 'number hidden layers network weight initialization activation function learning rate momentum number epochs batch size q20.', 'network weight initialization ideally, better use different weight initialization schemes according activation function layer.', 'number epochs number epochs number times training data shown network training.', 'generally, use small dropout value 20%-50% neurons 20% providing good starting point.', 'training hyperparameters learning rate learning rate de+nes quickly network updates parameters.', 'reasons be learning rate low regularization parameter high stuck local minima', 'term “feed-forward” input input layer travels input hidden hidden output layer.']\n",
      "\n",
      "['deep learning frameworks tensorflow caffe microsoft cognitive toolkit/cntk torch/pytorch mxnet chainer keras q24.', 'general, deep learning deal high dimensional data sets dimensions refer different features present data set.', 'operations assigned different nodes computational graph performed parallel, thus, providing better performance terms computations.', 'tensorflow auto differentiation capabilities advanced support threads, asynchronous computation, queue es.', 'convolutional neural network (cnn, convnet) class deep neural networks, commonly applied analyzing visual imagery.', 'layered concepts understand convolutional neural networks convolution convolution layer comprises set independent +lters.', 'basically, think computational graph alternative way conceptualizing mathematical calculations takes place tensorflow program.']\n",
      "\n",
      "['features instantiation parameters like position, size, orientation, deformation, velocity, hue, texture more.', 'connectedness neurons fully connected layer connections activations previous layer, seen regular neural networks.', 'obviously, improper inaccurate results, expect layers complete network perform nicely produce accurate results.', 'recurrent networks type arti+cial neural network designed recognize patterns sequences data, text, genomes, handwriting, spoken word, numerical times series data.', 'unlike standard feedforward neural networks, lstm feedback connections “general purpose computer”.', 'pooling function progressively reduce spatial size representation reduce number parameters computation network.', 'recurrent neural networks use backpropagation algorithm training internal memory, rnn’s able remember important things input received, enables precise predicting what’s coming next.', 'long short-term memory(lstm) arti+cial recurrent neural network architecture +eld deep learning.', 'exploding gradients problem large error gradients accumulate result large updates neural network model weights training.', 'magnitudes gradients accumulate, unstable network likely occur, cause poor prediction results model reports useful ever.', 'earlier layers network important responsible learn detecting simple patterns actually building blocks network.', 'means neurons earlier layers learn slowly compared neurons later layers hierarchy.']\n",
      "\n",
      "['feature variation extracts required features image generates output removing noise unnecessary interruption.', 'autoencoder neural network unsupervised machine learning algorithm applies backpropagation, setting target values equal inputs.']\n",
      "\n",
      "['algorithm useful dimensionality reduction, classification, regression, collaborative filtering, feature learning, topic modeling.', 'restricted boltzmann machine undirected graphical model plays major role deep learning framework recent times.', 'task training minimize error reconstruction, i.e. find efficient compact representation input data.', 'deep autoencoder composed two, symmetrical deep-belief networks shallow layers representing encoding half net.', 'rbm shares similar idea, uses stochastic units particular distribution instead deterministic distribution.', 'autoencoder simple 3-layer neural network output units directly connected input units.']\n",
      "\n",
      "['map function executes function given argument elements iterable given second argument.', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x interview series #2 python programming numpy 1.', 'use numpy.bincount() >>> arr = numpy.array([0, 5, 5, 0, 2, 4, 3, 0, 0, 5, 4, 1, 9, 9]) >>> numpy.bincount(arr) argument bincount() consist booleans positive integers.', 'generate array ‘100’ random numbers sampled standard normal distribution numpy np.random.rand(100) create 100 random numbers generated standard normal distribution mean 0 standard deviation 1.', 'axis = 0 meant reading rows, axis = 1 meant reading columns', 'python numpy arrays considered instead list fast, consume memory convenient lots functionality.', \"python numpy supports nan definition nan system dependent systems don't round support like older cray vax computers.\", 'nan, short “not number”, special floating point value defined ieee-754 specification.']\n",
      "\n",
      "['>>> import numpy np >>> arr = np.array([11, 22, 33, 44 ,55 ,66, 77]) >>> perc = np.percentile(arr, 40) #returns 40th percentile >>> print(perc) 13.', 'numpy contain array data type basic operations indexing, sorting, reshaping, basic element wise functions, et cetera.', 'choose column 2 example >>> import numpy np >>> arr = np.array([[1, 2, 3], [4, 5, 6], [0,0,1]]) >>> arr[arr[,1].argsort()] # output >>> array([[0, 0, 1], [1, 2, 3], [4, 5, 6]]) 11.', \">>> = np.array([5, 4, 3, 2, 1]) >>> b = np.array([4, 8, 9, 10, 1]) # 'a' remove 'b' >>> np.setdiff1d(a,b) # output >>> array([5, 3, 2]) 10.\", 'numpy package library python, adding support large, multi-dimensional arrays matrices, large collection high level mathematical functions.', 'simple words, numpy optimized version python lists like financial functions, linear algebra, statistics, polynomials, sorting searching etc.', '>>> import numpy np >>> arr = np.array([9, 10, 1, 2, 0]) >>> reverse_arr = arr[-1] 12.']\n",
      "\n",
      "['given array a, condition arr > 3 returns boolean array false interpreted 0 python numpy.', '>>> import numpy np >>> arr = np.array([[9,8,7],[6,5,4],[3,2,1]]) >>> arr > 3 >>> array([[true, true, true], [ true, true, true], [false, false, false]], dtype=bool) 17.', 'write numpy program calculate difference maximum minimum values given array second axis.', '>>> import numpy np >>> = np.arange(4).reshape((2,2)) >>> max_val = np.amax(a) >>> min_val = np.amin(a) 18.', 'size attribute helpful determining length numpy array >>> arr = numpy.zeros((1,0)) >>> arr.size 15.', 'find median numpy flattened array >>> import numpy np >>> arr = np.arange(16).reshape((4, 5)) >>> res = np.median(arr)', '>>> import numpy np >>> arr = np.arange(16).reshape((4, 7)) >>> res = np.ptp(arr, 1) 19.']\n",
      "\n",
      "['develop numpy program compute histogram nums bins >>> import numpy np >>> nums = np.array([0.5, 0.7, 1.0, 1.2, 1.3, 2.1]) >>> bins = np.array([0, 1, 2, 3]) >>> np.histogram(nums, bins) 24.', 'write numpy program true division element-wise array inputs >>> import numpy np >>> x = np.arange(10) >>> np.true_divide(x, 3)', 'compute compute pearson product-moment correlation coefficients given numpy arrays >>> import numpy np >>> x = np.array([0, 1, 3]) >>> y = np.array([2, 4, 5]) >>> cross_corr = np.corrcoef(x, y) 23.', 'write numpy program compute mean, standard deviation, variance given array second axis import numpy np >>> import numpy np >>> x = np.arange(16) >>> mean = np.mean(x) >>> std = np.std(x) >>> var= np.var(x) 21.', 'calculate covariance matrix numpy arrays >>> import numpy np >>> x = np.array([2, 1, 0]) >>> y = np.array([2, 3, 3]) >>> cov_arr = np.cov(x, y) 22.', 'powers array values element-wise >>> import numpy np >>> x = np.arange(7) >>> np.power(x, 3) 25.']\n",
      "\n",
      "['returns new object new index produced equivalent current one, value copy false.']\n",
      "\n",
      "['follow steve nouri ai data science posts https//lnkd.in/gzu463x vectorization process running operations entire array.', '>>> import pandas pd >>> ds = pd.series([2, 4, 6, 8, 10])', 'mention different types data structures pandas pandas provide data structures, supported pandas library, series, dataframes.', 'time series pandas time series ordered sequence data basically represents quantity changes time.', \"write pandas program 5 rows given dataframe >>> import pandas pd >>> exam_data = {'name' ['anastasia', 'dima', 'katherine', 'james', 'emily', 'michael', 'matthew', 'laura', 'kevin', 'jonas'],} labels = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j'] >>> df = pd.dataframe(exam_data , index=labels) >>> df.iloc[5] 37.\", 'dataframe.to_numpy(self, dtype=none, copy=false) dtype parameter defines data type pass array copy ensures returned value view array.', '>>> import pandas pd >>> pd.series([2, 4, 6, 8, 10]) 38.', 'develop pandas program create display one-dimensional array- like object containing array data.', 'pandas number vectorized functions like aggregations, string functions optimized operate specifically series dataframes.', \"write python program convert panda module series python list it's type.\"]\n",
      "\n",
      "[\">>> import pandas pd >>> s1 = pd.series(['100', '200', 'python', '300.12', '400']) >>> s2 = pd.to_numeric(s1, errors='coerce') >>> s2 42.\", '>>> import pandas pd >>> ds1 = pd.series([2, 4, 6, 8, 10]) >>> ds2 = pd.series([1, 3, 5, 7, 9]) >>> sum = ds1 + ds2 >>> sub = ds1 - ds2 >>> mul = ds1 * ds2 >>> div = ds1 / ds2 40.', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x >>> type(ds) >>> ds.tolist() >>> type(ds.tolist()) 39.', '>>> import pandas pd >>> ds1 = pd.series([2, 4, 6, 8, 10]) >>> ds2 = pd.series([1, 3, 5, 7, 10]) >>> ds1 == ds2 >>> ds1 > ds2 >>> ds1 < ds2 41.', \"write pandas program convert series lists series >>> import pandas pd >>> s = pd.series([ ['red', 'black'], ['red', 'green', 'white'] , ['yellow']]) >>> s = s.apply(pd.series).stack().reset_index(drop=true) 43.\", 'write pandas program create subset given series based value condition >>> import pandas pd >>> s = pd.series([0, 1,2,3,4,5,6,7,8,9,10]) >>> n = 6']\n",
      "\n",
      "['write pandas program display frequent value given series replace “replaced” series.', '>>> import pandas pd >>> sr1 = pd.series([1, 2, 3, 4, 5]) >>> sr2 = pd.series([2, 4, 6, 8, 10]) >>> result = sr1[~sr1.isin(sr2)] >>> result 46.', \">>> import pandas pd >>> import numpy np >>> np.random.randomstate(100) >>> num_series = pd.series(np.random.randint(1, 5, [15])) >>> result = num_series[~num_series.isin(num_series.value_counts().index[1])] = 'replaced' 48.\", \"develop pandas code alter order index given series >>> import pandas pd >>> s = pd.series(data = [1,2,3,4,5], index = ['a', 'b', 'c','d','e']) >>> s.reindex(index = ['b','a','c','d','e']) 45.\", '>>> import pandas pd >>> import numpy np >>> num_series = pd.series(np.random.randint(1, 10, 9)) >>> result = np.argwhere(num_series % 5==0) 49.', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x >>> new_s = s[s < n] >>> new_s 44.']\n",
      "\n",
      "[\"follow steve nouri ai data science posts https//lnkd.in/gzu463x # importing pandas library >>> import pandas pd >>> info = {'one' pd.series([1, 2, 3, 4, 5], index=['a', 'b', 'c', 'd', 'e']), 'two' pd.series([1, 2, 3, 4, 5, 6], index=['a', 'b', 'c', 'd', 'e', 'f'])} >>> info = pd.dataframe(info) # add new column existing dataframe object >>> info['three']=pd.series([20,40,60],index=['a','b','c']) 50.\", 'xrange range exact terms functionality.the difference range returns python list object x range returns xrange object.', 'python exits, especially python modules having circular references objects objects referenced global namespaces']\n",
      "\n",
      "['python lays concept prefixing variable, function method single double underscore imitate behavior protected private access specifiers.', 'doesn’t allow copies provides good functions perform set operations like union, difference etc.', 'kwargs don’t know keyword arguments passed function, pass values dictionary keyword arguments.', 'generator function normal function contains yield expression function definition making generator function.', 'use *args aren’t sure arguments going passed function, want pass stored list tuple arguments function. **', 'shallow copy new instance type gets created keeps values copied new instance.']\n",
      "\n",
      "['apparently contributing python open-source community requires follow style guidelines sincerely strictly.', 'slicing python mechanism select range items sequence types like strings, list, tuple, etc.', 'decorators python essentially functions add functionality existing function python changing structure function itself.', 'pythonpath environment variable set add additional directories python look modules packages.', 'pep official design document providing information python community, describing new feature python processes.', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x arguments pass value python supports pass reference.']\n",
      "\n",
      "['access module written python c following method, module = =pyimport_importmodule(\"<modulename>\"); 73.', 'provides environment, document code, run it, look outcome, visualize data results leaving environment.', 'converts source code written programmer intermediate language, translated machine language executed.', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x .py files contain source code program.', 'pickle module accepts python object converts string representation dumps file dump function, process called pickling.']\n",
      "\n",
      "['ipywidgets package provides common user interface controls exploring code data interactively.', 'third-party extensions magic commands exist, example, %%cython magic allows write cython code directly notebook.']\n",
      "\n",
      "[\">>> data = 'this string want pass different notebook' >>> %store data # stored 'data' (str) # new notebook >>> %store -r data >>> print(data) 95.\", 'export contents cell/show contents external script %%writefile magic saves contents cell external file. %']\n",
      "\n",
      "[]\n",
      "\n",
      "['follow steve nouri ai data science posts https//lnkd.in/gzu463x references [1] https//www.edureka.co [2] https//www.kausalvikash.in [3] https//www.wisdomjobs.com [4] https//blog.edugrad.com [5]https//stackoverflow.com [6]http//www.ezdev.org [7]https//www.techbeamers.com [8]https//www.w3resource.com [9]https//www.javatpoint.com [10]https//analyticsindiamag.com [11]https//www.onlineinterviews.com [12]https//www.geeksforgeeks.org [13]https//www.springpeople.com [14]https//atraininghub.com [15]https//www.interviewcake.com [16]https//www.techbeamers.com [17]https//www.tutorialspoint.com [18]https//programmingwithmosh.com [19]https//www.interviewbit.com [20]https//www.guru99.com [21]https//hub.packtpub.com [22]https//analyticsindiamag.com [23]https//www.dataquest.io [24]https//www.infoworld.com']\n",
      "\n",
      "['machine learning, statistical model describes random error noise instead underlying relationship ‘overﬁtting’ occurs.', 'technique, model usually given dataset known data training (training data set) run dataset unknown data model tested.', 'machine learning branch computer science deals system programming order automatically learn improve experience.', 'lot data overﬁtting avoided, overﬁtting happens relatively small dataset, try learn it.', 'model excessively complex, overﬁtting normally observed, having parameters respect number training data types.', 'machine learning relates study, design development algorithms computers capability learn explicitly programmed.', 'method dataset splits section, testing training datasets, testing dataset test model while, training dataset, datapoints come model.', 'inductive machine learning involves process learning examples, system, set observed instances tries induce general rule.', 'https//career.guru99.com/ 50 machine learning interview s & answers 1) machine learning?', 'while, data mining deﬁned process unstructured data tries extract knowledge unknown interesting patterns.']\n",
      "\n",
      "['a) model building b) model testing c) applying model 10) standard approach supervised learning?', 'areas information science like machine learning, set data discover potentially predictive relationship known ‘training set’.', 'a) decision trees b) neural networks (back propagation) c) probabilistic networks d) nearest neighbor e) support vector machines 8) diﬀerent algorithm techniques machine learning?', 'diﬀerent approaches machine learning a) concept vs classiﬁcation learning b) symbolic vs statistical learning', 'training set examples given learner, test set test accuracy hypotheses generated learner, set example held learner.', 'diﬀerent types techniques machine learning a) supervised learning b) unsupervised learning c) semi-supervised learning d) reinforcement learning e) transduction f) learning learn 9) stages build hypotheses model machine learning?']\n",
      "\n",
      "['classiﬁer machine learning system inputs vector discrete continuous feature values outputs single discrete value, class.', 'a) classiﬁcations b) speech recognition c) regression d) predict time series e) annotate strings 16) algorithm independent machine learning?', 'naïve bayes classiﬁer converge quicker discriminative models like logistic regression, need training data.', 'machine learning mathematical foundations independent particular classiﬁer learning algorithm referred algorithm independent machine learning?', 'artiﬁcial intelligence addition machine learning, covers aspects like knowledge representation, natural language processing, planning, robotics etc.', 'designing developing algorithms according behaviours based empirical data known machine learning.', 'a) artiﬁcial intelligence b) rule based inference 14) explain function ‘unsupervised learning’?', 'a) find clusters data b) find low-dimensional representations data c) find interesting directions data d) interesting coordinates correlations e) find novel observations/ database cleaning 15) explain function ‘supervised learning’?']\n",
      "\n",
      "['methods predicting good probabilities supervised learning a) platt calibration b) isotonic regression methods designed binary classiﬁcation, trivial.', 'diﬀerence heuristics decision trees evaluate average quality number disjointed sets rule learners evaluate quality set instances covered candidate rule.', 'a) computer vision b) speech recognition c) data mining d) statistics e) informal retrieval f) bio-informatics 21) genetic programming?', 'inductive logic programming (ilp) subﬁeld machine learning uses logical programming representing background knowledge examples.', 'process selecting models diﬀerent mathematical models, describe data set known model selection.']\n",
      "\n",
      "['paradigms ensemble methods a) sequential ensemble methods b) parallel ensemble methods 36) general principle ensemble method bagging boosting ensemble method?', 'a) combining binary classiﬁers b) modifying binary incorporate multiclass learning 32) ensemble learning?', 'ﬁrst component logical ; consists set bayesian clauses, captures qualitative structure domain.', 'general principle ensemble method combine predictions models built given learning algorithm order improve robustness single model.', 'solve particular computational program, multiple models classiﬁers experts strategically generated combined.', 'instance based learning algorithm referred lazy learning algorithm delay induction generalization process classiﬁcation performed.']\n",
      "\n",
      "['important components relational evaluation techniques a) data acquisition b) ground truth acquisition c) cross validation technique d) query type e) scoring metric f) signiﬁcance test 43) diﬀerent methods sequential supervised learning?', 'pca (principal components analysis), kpca ( kernel based principal component analysis) ica ( independent component analysis) important feature extraction techniques dimensionality reduction.', 'machine learning statistics, dimension reduction process reducing number random variables considerations divided feature selection feature extraction 41) support vector machines?', 'incremental learning method ability algorithm learn new data available classiﬁer generated available dataset.', 'bias term measures closely average classiﬁer produced learning algorithm matches target function.', 'diﬀerent methods solve sequential supervised learning problems a) sliding-window methods b) recurrent sliding windows c) hidden markow models d) maximum entropy markow models e) conditional random ﬁelds']\n",
      "\n",
      "['areas robotics information processing sequential prediction problem arises a) imitation learning b) structured prediction c) model based reinforcement learning 45) batch statistical learning?', 'pac (probably approximately correct) learning learning framework introduced analyze learning algorithms statistical eﬃciency.', 'f) graph transformer networks 44) areas robotics information processing sequential prediction problem arises?', 'statistical learning techniques allow learning function predictor set observed data predictions unseen future data.', 'techniques machine learning a) genetic programming b) inductive learning 50) popular application machine learning day day basis?', 'recommendation engine implemented major ecommerce websites uses machine learning guru99 provides free online tutorial courses like', 'a) sequence prediction b) sequence generation c) sequence recognition d) sequential decision 48) sequence learning?', 'techniques provide guarantees performance learned predictor future unseen data based statistical assumption data generating process.']\n",
      "\n",
      "['java mis mongodb bigdata cassandra web services sqlite jsp informatica accounting sap training python excel asp net hbase project management test management business analyst ethical hacking pmp live project soapui photoshop manual testing mobile testing data warehouse r tutorial tableau devops aws jenkins agile testing rpa junit software engineering selenium ccna angularjs nodejs plsql']\n",
      "\n",
      "['ii year – sem (2021-2022) department computer science engineering malla reddy college engineering & technology (autonomous institution – ugc, govt.', 'india) (affiliated jntuh, hyderabad, approved aicte - accredited nba & naac – ‘a’ grade - iso 90012015 certified) maisammaguda, dhulapally (post via.']\n",
      "\n",
      "['2 malla reddy college engineering & technology department computer science engineering syllabus ii year m. tech.', 'unit - ii artificial neural networks -introduction, neural network representation, appropriate problems neural network learning, perceptions, multilayer networks propagation algorithm.', 'dimensionality reduction feature selection, principal component analysis, linear discriminate analysis, factor analysis, independent component analysis, multidimensional scaling, manifold learning.', 'unit - v genetic algorithms different search methods induction - explanation-based learning prior knowledge reduce sample complexity.', 'unit -iv pattern comparison techniques-temporal patterns, dynamic time warping methods,clustering, introduction clustering, k-means clustering, k-mode clustering.', 'cse – sem l/t/p/ c 3 / - / - 3 (r20d5803) machine learning objectives 1.', 'course explains machine learning techniques decision tree learning, bayesian learning etc.', 'decision tree learning-introduction, decision tree representation, appropriate problems decision tree learning, basic decision tree learning algorithm hypothesis space search decision tree learning, inductive bias decision tree learning, issues decision tree learning.', 'instance-based learning-introduction, k-nearest neighbor learning, locally weighted regression, radial basis functions, case-based reasoning, remarks lazy eager learning.', 'discussion propagation algorithm, illustrative example face recognition unit - iii bayesian learning-introduction, byes theorem, bayes theorem concept learning maximum likelihood squared error hypotheses, maximum likelihood hypotheses predicting probabilities, minimum description length principle, bayes optimal classifier, gibs algorithm, naïve bayes classifier, example learning classify text, bayesian belief networks, em algorithm.', 'unit - introduction well-posed learning problems, designing learning system perspectives issues machine learning concept learning general specific ordering introduction,a concept learning task, concept learning search, find-s finding maximally specific hypothesis, version spaces candidate elimination algorithm, remarks version spaces candidate elimination, inductive bias.']\n",
      "\n",
      "['mehryar mohri, afshin rostamizadeh, ameet talwalkar ” foundations machine learning”,mit press,2012 references 1.', 'fundamentals speech recognition lawrence rabiner biing – hwang juang .ethem alpaydin, ”introduction machine learning”, mit press, prentice hall india, 3 rd edition2014.']\n",
      "\n",
      "['25 s. unit topic page 1 ii artificial neural networks -introduction, neural network representation 26 2 ii appropriate problems neural network learning 28 3 ii perceptions, multilayer networks & propagation algorithm.', '29 4 ii discussion propagation algorithm 34 malla reddy college engineering & technology department computer science engineering', '4 index s. unit topic page 1 introduction well-posed learning problems 1 2 concept learning task, concept learning search 6 3 find-s finding maximally specific hypothesis 15 4 version spaces candidate elimination algorithm 17 5 remarks version spaces candidate elimination, inductive bias 21 6 decision tree learning-introduction, decision tree representation 22 7 appropriate problems decision tree learning 23 8 decision tree learning algorithm, issues decision tree learning.']\n",
      "\n",
      "['57 malla reddy college engineering & technology department computer science engineering', 'instance-based learning-introduction 51 7 iii k-nearest neighbor learning, locally weighted regression 55 8 iii radial basis functions, case-based reasoning 56 9 iii remarks lazy eager learning.', '5 s. unit topic page 1 iii bayesian learning-introduction ,bayes theorem & concept learning maximum 36 2 iii maximum likelihood hypotheses predicting probabilities(map) 42 3 iii gibs algorithm, naïve bayes classifier 46 4 iii minimum description length principle , bayes optimal classifier 47 5 iii example learning classify text, bayesian belief networks 50 6 iii em algorithm.']\n",
      "\n",
      "['76 s. unit topic page 1 v genetic algorithms different search methods induction 78 2 v explanation-based learning prior knowledge reduce sample complexity.', '79 3 v dimensionality reduction 82 4 v principal component analysis 84 5 v linear discriminate analysis, factor analysis, 85 6 v independent component analysis multidimensional scaling, manifold learning.', '6 s. unit topic page 1 iv pattern comparison techniques-temporal patterns, 58 2 iv dynamic time warping methods 61 3 iv clustering 67 5 iv k-means clustering 69 6 iv k-mode clustering.', '86 malla reddy college engineering & technology department computer science engineering']\n",
      "\n",
      "['scientists discovered good idea reward machine job expected way came reinforcement learning. •', 'machine learning broadly categorized following headings machine learning evolved left right shown diagram. •', 'department cse mrcet 1 unit-i machine learning field study gives computers capability learn explicitly programmed.', 'soon, data available days humongous conventional techniques developed far failed analyse big data provide predictions.']\n",
      "\n",
      "['given feature value x1 output y1, x2 y2, x3 y3, on.', 'technique advanced giving incentives deep learning networks awards finally comes deep reinforcement learning.', 'hold child’s hand, foot forward, walk demonstration on, child learns walk own.', 'satisfied machine able predictions desired level accuracy (say 80 90%) stop training machine.', 'machine learns high computing power huge memory resources available today. •', 'based data, let computer figure empirical relationship x y. machine trained way sufficient number data points, ask machine predict y given x. assuming know real value y given x, able deduce machine’s prediction correct.', 'department cse mrcet 2 • thus, came deep learning human brain simulated artificial neural networks (ann) created binary computers. •', 'now, safely use machine predictions unknown data points, ask machine predict y given x know real value y. training comes regression talked earlier.', 'let study categories details supervised learning supervised learning analogous training child walk.']\n",
      "\n",
      "['specifically, ask s given huge data set x, “what best groups x?” “', 'arrive answers s, understand number data points machine require deduce strategy large.', 'following principles regression training, train machine classify student based feature – height.', 'however, case unsupervised learning, number data points reasonably accepted learning starts millions.', 'example, set 100 students say, like group groups based heights - short, medium long.', 'department cse mrcet 3 classification use machine learning techniques classification problems.', 'again, use test data verify machine learned technique classification putting developed model production.', 'unsupervised learning unsupervised learning, specify target variable machine, ask machine “what tell x?”.', 'following figure shows boundary yellow red dots determined unsupervised machine learning.', 'however, data continuously flowing social area network, cases data curation impossible task.']\n",
      "\n",
      "['department cse mrcet 4 able determine class black dots fairly good accuracy.', 'reinforcement learning consider training pet dog, train pet bring ball us.', 'slowly, dog learns job rightly gives reward dog starts job right way time future.', 'slowly, machine start differentiating right wrong moves iterations learn solve game puzzle better accuracy.']\n",
      "\n",
      "['networks successfully applied solving problems computer vision, speech recognition, natural language processing, bioinformatics, drug design, medical image analysis, games.', 'reinforcement learning algorithms like q learning combined deep learning create powerful drl model.', 'deep reinforcement learning deep reinforcement learning (drl) combines techniques deep reinforcement learning.', 'architectures deep learning deep neural networks, deep belief networks, recurrent neural networks, convolutional neural networks.', 'department cse mrcet 5 deep learning deep learning model based artificial neural networks (ann), specifically convolutional neural networks (cnn)s.', 'deep learning requires huge processing power humongous data, generally easily available days.']\n",
      "\n",
      "['handwriting recognition problem • task – acknowledging handwritten words portrayal • performance measure – percent words accurately classified • experience – directory handwritten words given classifications 4.', 'better filter emails spam • task – classifying emails spam • performance measure – fraction emails accurately classified spam spam • experience – observing label emails spam spam 2.', 'robot driving problem • task – driving public four-lane highways sight scanners • performance measure – average distance progressed fallacy • experience – order images steering instructions noted observing human driver 5.', 'posed learning problems computer program said learn experience e context task t performance measure p, performance t, measured p, upgrades experience e. problem segregated well-posed learning problem traits – • task • performance measure • experience certain example efficiently defines well-posed learning problems are 1.', 'checkers learning problem • task – playing checkers game • performance measure – percent games won opposer • experience – playing implementation games 3.', 'department cse mrcet 6 got brief introduction machine learning models, let explore slightly deeper algorithms available models.']\n",
      "\n",
      "['automatic translation documents • task – translating type language document language • performance measure – able convert language efficiently • experience – training machine large dataset different types languages design learning system looked learning process understood goal learning.', 'final design look game - checkers learning problem apply design choices.', 'face recognition problem • task – predicting different types faces • performance measure – able predict maximum types faces • experience – training machine maximum datasets different face images 7.', 'want design learning system follows learning process, need consider design choices.', 'checkers learning problem, elements be, • task t play checkers • performance measure p total present game won tournament. •', \"type training experience design checker's learning system, type training experience available learning system significant effect success failure learning.\", 'department cse mrcet 7 • task – forecasting different fruits recognition • performance measure – able predict maximum variety fruits • experience – training machine largest datasets fruits images 6.']\n",
      "\n",
      "['training experience good \\uf0a2 training examples represent distribution examples final system performance measured?', '\\uf0a2 semi-supervised learner generates game states asks teacher help finding correct board state confusing.', 'department cse mrcet 8 direct indirect training experience case direct training experience, individual board states correct board state given.', 'choosing target function playing checkers game, moment time, decision choosing best different possibilities.', 'teacher not \\uf0a2 supervised training experience labelled, means, board states labelled correct move.', 'case indirect training experience, sequences game final result (win, lose draw) given number games.']\n",
      "\n",
      "['function assigns higher scores better board states system successfully learn target function v, easily use select best board position.', 'direct experience checkers learning system, needs learn choose best large search space.', 'let define target value v(b) arbitrary board state b b, follows', 'function v b →r indicating accepts input board set legal board states b produces output real score.', 'let function choose use notation choose move b →m indicate function accepts input board set legal board states b produces output set legal moves m. • indirect experience difficult learn function.']\n",
      "\n",
      "['hand, wish pick expressive representation allow representing close approximation possible ideal target function v. hand, expressive representation, training data program require order choose alternative hypotheses represent.', 'b final state game, v (b) = v (b’), b’ best final board state achieved starting b playing optimally end game. (', 'discussion brief, let choose simple representation given board state, function ^v calculated linear combination following board features • x1(b) — number black pieces board b • x2(b) — number red pieces b • x3(b) — number black kings b', 'choosing representation target function specified ideal target function v, choose representation learning program use describe function ^v learn.', '4) recursive definition determine value v(b) particular board state, performs search ahead optimal line play, way end game.', 'allow represent collection rules match features board state, quadratic polynomial function predefined board features, artificial neural network.', 'could, example, allow program represent large table distinct entry specifying value distinct board state.']\n",
      "\n",
      "['department cse mrcet 11 • x4(b) — number red kings b • x5(b) — number red pieces threatened black • x6(b) — number black pieces threatened red ^v = w0 + w1 · x1(b) + w2 · x2(b) + w3 · x3(b) + w4 · x4(b) +w5 · x5(b) + w6 · x6(b) w0 w6 numerical coefficients weights obtained learning algorithm.', 'specification machine learning problem time till worked choosing type training experience, choosing target function representation.', 'task t play checkers • performance measure % games won world tournament • training experience e opportunity play • target function v board → r • target function representation ^v = w0 + w1 · x1(b) + w2 · x2(b) + w3 · x3(b) + w4 · x4(b) +w5 · x5(b) + w6 · x6(b) items correspond specification learning task, final items constitute design choices implementation learning program.', 'choosing approximation algorithm target function generating training data — train learning program, need set training data, describing specific board state b training value v_train (b) b. training example ordered pair <b,v_train(b)>.']\n",
      "\n",
      "['experiment generator takes current hypothesis (currently learned function) input outputs new problem (an initial board state) performance system explore.', 'department cse mrcet 12 temporal difference (td) learning concept central reinforcement learning, learning happens iterative correction estimated returns accurate target return.', '\\uf056 v_train(b) ← ^v(successor(b)) final design checkers learning system final design checkers learning system naturally described distinct program modules represent central components learning systems.', 'critic takes trace game input outputs set training examples target function.', 'performance system takes new board input outputs trace game played itself.']\n",
      "\n",
      "[\"general bounds found relate confidence learned hypotheses training experience character learner's hypothesis space? •\", 'concept learning • inducing general functions specific training examples main issue machine learning. •', 'hypothesis space general-to-specific ordering hypotheses, search efficiently organized taking advantage naturally occurring structure hypothesis space.', 'concept learning problem searching predefined space potential hypotheses hypothesis best fits training examples. •', 'department cse mrcet 13 checkers example raises number generic s machine learning.', 'concept learning acquiring definition general category given sample positive negative training examples category. •', 'field machine learning, book, concerned answering s following • algorithms exist learning general target functions specific training examples?', 'best strategy choosing useful training experience, choice strategy alter complexity learning problem? •']\n",
      "\n",
      "['department cse mrcet 14 inferring boolean-valued function training examples input output. •', 'selecting hypothesis representation, designer learning algorithm implicitly defines space hypotheses program represent learn.', 'task learn predict value enjoy sport arbitrary day, based values attribute values.', 'concept learning search • concept learning viewed task searching large space hypotheses implicitly defined hypothesis representation. •', 'example concept-learning learning bird-concept given examples birds (positive examples) non-birds (negative examples). •', 'concept learning task enjoy sport training examples set example days, described attributes.']\n",
      "\n",
      "['find-s algorithm finds specific hypothesis h consistent positive training examples. –', 'department cse mrcet 15 find-s • find-s algorithm starts specific hypothesis generalize considering positive examples. •', 'find-s algorithm ignores negative example long hypothesis space contains hypothesis describes true target concept, training data contains errors, ignoring negative examples cause problem. •', 'positive training instance x attribute constraint a, h constraint a, satisfied x 3.', 'specific hypothesis represented by {ϕ, ϕ, ϕ, ϕ, ϕ, ϕ} steps involved find-s 1.', 'final hypothesis consistent negative examples correct target concept h, training examples correct.']\n",
      "\n",
      "['example positive find initial hypothesis specific update current hypothesis general condition.', 'hence, hypothesis be h = {ϕ, ϕ, ϕ, ϕ, ϕ, ϕ} consider example 1 data example 1 {green, hard, no, wrinkled}.', 'hence, hypothesis becomes h = {green, hard, no, wrinkled} consider example 2']\n",
      "\n",
      "['reached point attributes hypothesis general condition, example 6 example 7 result hypothesizes general attributes.', 'hence, given data final hypothesis be final hypothesis h = { ?, ?, ?, ? }.', 'h = {green, hard, no, wrinkled} consider example 3 example negative outcome.', 'hard, no, wrinkled } consider example 5 data present example 5 {green, soft, yes, smooth}.', 'h = {green, hard, no, wrinkled} consider example 4 data present example 4 {orange, hard, no, wrinkled}.', 'compare single attribute initial data mismatch found replace particular attribute general case (“ ?”).', 'compare single attribute initial data mismatch found replace particular attribute general case ( “?” ).']\n",
      "\n",
      "['initialize g set maximally general hypotheses h initialize s set maximally specific hypotheses h training example d, • d positive example • remove g hypothesis inconsistent d • hypothesis s s consistent d • remove s s • add s minimal generalizations h s h consistent d, member g general h • remove s hypothesis general hypothesis s • d negative example • remove s hypothesis inconsistent d • hypothesis g g consistent d • remove g g 18\\\\ • add g minimal specializations h g • h consistent d, member s specific h • remove g hypothesis general hypothesis g. candidate- elimintion algorithm version spaces illustrative example', 'department cse mrcet 18 candidate-elimintion algorithm computes version space containing hypotheses h consistent observed sequence training examples.']\n",
      "\n",
      "['second training example observed, similar effect generalizing s s2, leaving g unchanged i.e., g2 = g1 =g0', 'update g boundary needed response training example correctly covers example. •', 'department cse mrcet 19 candidate-elimintion algorithm begins initializing version space set hypotheses h; boundary set contain general hypothesis h, g0 ?, ?, ?, ?, ?,', 'training example presented, candidateelimintion algorithm checks s boundary finds overly specific fails cover positive example. •']\n",
      "\n",
      "['minimal specialization g2 correctly labels new example negative example, included g3.', 'negative example reveals boundary version space overly general, is, hypothesis g incorrectly predicts new example positive example. •']\n",
      "\n",
      "['department cse mrcet 21 • positive example generalizes s boundary version space.', 'results removing member g boundary, member fails cover new positive example processing examples, boundary sets s4 g4 delimit version space hypotheses consistent set incrementally observed training examples. •', 'processing examples, boundary sets s4 g4 delimit version space hypotheses consistent set incrementally observed training examples.']\n",
      "\n",
      "['classification trees (yes/no types) seen example classification tree, outcome variable like ‘fit’ ‘unfit’.', 'let’s want predict person fit given information like age, eating habit, physical activity, etc.', 'department cse mrcet decision tree decision trees type supervised machine learning (that explain input corresponding output training data) th e data continuously split according certain parameter.']\n",
      "\n",
      "['appropriate problems decision tree learning • instances represented attribute-value pair • target function discrete output values • disjunctive descriptions required • training data contain errors • training data contain missing attribute values. •', 'capability • hypothesis space decision trees complete space finite discrete valued functions. •', 'entropy, called shannon entropy denoted h(s) finite set s, measure uncertainty randomness data.', 'department cse mrcet 23 decision outcome variable continuous, e.g. number like 123.', 'hypothesis space search set possible decision tree, simple complex, hill climbing search.']\n",
      "\n",
      "['department cse mrcet 24 • id3 uses training example step statistically based decisions refine current hypothesis. •', 'inductive bias decision tree learning note h power set instances x • inductive bias id3 – approximate inductive bias id3 \\uf0a2 shorter trees preferred larger tress \\uf0a2 bfs-id3 difference (id3 & c-e) && restriction bias preference bias id3 candidate-elimination searches complete hypothesis space incompletely searches incomplete hypothesis space completely inductive bias solely consequence ordering hypotheses search strategy inductive bias solely consequence expressive power hypothesis representation sss restriction bias preference bias candidate-elimination id3 categorical restriction set hypotheses considered preference certain hypotheses']\n",
      "\n",
      "['department cse mrcet 25 possibility excluding unknown target function work complete hypothesis space issues decision tree learning • determine deeply grow decision tree • handling continuous attributes • choosing appropriate attribute selection measure • handling training data missing attribute values • handling attributes differing costs • improving computational efficiency']\n",
      "\n",
      "['ann applications classification, aim predict class input vector • pattern matching, aim produce pattern best associated given input vector. •', 'control, appropriate action suggested based given input vectors • function approximation/times series modelling, aim learn functional relationships input desired output vectors. •', 'ann architectures • neural networks known universal function approximators • architectures available approximate nonlinear function • different architectures allow generation functions different complexity power \\uf0a2 feed forward networks \\uf0a2 feedback networks \\uf0a2 lateral networks', 'department cse mrcet 26 unit-ii artificial neural networks introduction artificial neural networks (ann) algorithms based brain function model complicated patterns forecast issues.', 'artificial neural network (ann) deep learning method arose concept human brain biological neural networks.']\n",
      "\n",
      "['output anns discrete-valued, real-valued, vector multiple real discrete-valued characteristics, target function discrete-valued, real-valued, vector numerous real discrete- valued attributes.', 'number training instances evaluated, settings different learning algorithm parameters contribute extended training periods anns.', 'department cse mrcet 27 advantages artificial neural networks attribute-value pairs represent problems ann.', 'hardware dependence • construction artificial neural networks necessitates use parallel processors. •']\n",
      "\n",
      "['1943 mcculloch pitts proposed model neuron perceptron (read [mitchell, section 4.4]) 2.', 'department cse mrcet 28 • precise rule determine structure artificial neural networks. •', '1960s widrow hoff explored perceptron networks (which called “adelines”) delta rule.', 'network’s lifetime unknown • network’s error sample decreased specific amount, training complete. •', 'difficulty presenting issue network • anns capable working numerical data. •']\n",
      "\n",
      "['1986 invention backpropagation rumelhart mcclelland, parker earlier on werbos learn nonlinearly-separable data sets.', '1969 minsky papert showed perceptron deal nonlinearly-separable data sets---even represent simple function x-or.', 'multilayer neural network • multiplayer perceptron feed forward neural network hidden layers • network consists input layer source neurons, hidden layer computational neurons, output layer computational neurons. •']\n",
      "\n",
      "[]\n",
      "\n",
      "[]\n",
      "\n",
      "['algorithm composed parts repeated pre-set maximal number epochs, ep max. •', 'ii, propagation pass weights network updated- starting hidden output weights followed input hidden weights--with respect sum squares error series weight update rules called delta rule.', 'definition propagation algorithm neural network computes gradient loss function single weight chain rule.', 'i, feed forward pass activation values hidden output units computed. •', 'department cse mrcet 32 propagation overview • propagation works applying gradient descent rule feed forward network. •']\n",
      "\n",
      "['department cse mrcet 33 • inputs x, arrive preconnected path • input modelled real weights w. weights usually randomly selected. •', 'calculate error outputs errorb= actual output – desired output • travel output layer hidden layer adjust weights error decreased. •', 'prominent advantages propagation are • propagation fast, simple easy program • parameters tune apart numbers input • flexible method require prior knowledge network • standard method generally works • need special mention features function learned.']\n",
      "\n",
      "['propagation algorithm data mining sensitive noisy data • need use matrix-based approach propagation instead mini-batch.', 'propagation algorithm • initialize weights small random values; create random pool training patterns; set ep, number epochs training 0. •', 'department cse mrcet 34 types propagation networks types propagation networks are • static back-propagation • recurrent propagation static back-propagation kind propagation network produces mapping static input static output.', 'disadvantages propagation • actual performance propagation specific problem dependent input data. •', 'recurrent propagation recurrent propagation data mining fed forward fixed value achieved.', 'hidden layer propagating error • update connections • w newji = wjiold + wji w newkj = wkjold + wkj j']\n",
      "\n",
      "['new delta rule wpq(t+1) = - e/ wpq + wpq(t) • p q input hidden, or, hidden output units; t time step epoch; momentum parameter regulates inertia weights.', 'propagation momentum • point, propagation disadvantage slow small oscillate widely large. •', 'solve problem, add momentum connection inertia, forcing change direction downhill “force”. •', 'training patterns pool used, set ep = ep+1, ep epmax, create random pool patterns step 2.']\n",
      "\n",
      "['case 1 observed, certain coin fair coin, decide probability observing heads 0.50.5 confidence.', 'adjust belief accordingly value hh observed, decide probability observing heads recent observations.', 'thinking model uses recent observations beliefs inclination critical thinking known bayesian thinking.', 'neglect prior beliefs new data, decide probability observing heads h/10h/10 solely depending recent observations.', 'observations experiment fall following cases • case 1 observing 55 heads 55 tails. •', 'department cse mrcet 36 unit - iii introduction bayesian learning imagine situation friend gives new coin asks fairness coin (or probability observing heads) flipping coin once.']\n",
      "\n",
      "['department cse mrcet 37 moreover, assume friend allows conduct 1010 coin flips.', 'consequently, quantity pp deviates 0.50.5 indicates biased coin is, pp considered degree-of-fairness coin.', 'bayesian learning comes play occasions, unable use frequentist statistics drawbacks discussed above.', 'famous coin flip experiment flip coin, possible outcomes - heads tails.', 'course, rare possibility coin balances edge falling side, assume possible outcome coin flip discussion.', 'conduct series coin flips record observations i.e. number heads (or tails) observed certain number coin flips.', 'observed heads tails equal frequencies probability observing heads (or tails) 0.50.5, established coin fair coin.', 'conducted sufficient number coin flip trials, determine frequency probability observing heads (or tails).', 'use bayesian learning address drawbacks additional capabilities (such incremental updates posterior) testing hypothesis estimate unknown parameters machine learning models.', 'bayesian learning uses bayes’ theorem determine conditional probability hypotheses given evidence observations.']\n",
      "\n",
      "['therefore, pp 0.60.6 (note pp number heads observed number total coin flips).', 'department cse mrcet 38 testing hypothesis true false calculating probability event prolonged experiment known frequentist statistics.', 'hence, according frequencies statistics, coin biased coin — opposes assumption fair coin.', 'such, determining fairness coin probability observing heads example frequentist statistics (a.k.a.', 'new value pp change previous conclusion (i.e. coin biased), observation raises s • confident pp 0.60.6? •']\n",
      "\n",
      "['assume true value pp closer 0.550.55 0.60.6 computed observations considerable number trials compared compute latter.', '39 department cse mrcet pp continue change increase number coin flip trails?', 'determine confidence estimated pp value inferred conclusion, situation number trials limited, allow', 'yet, practical conduct experiment infinite number trials stop experiment sufficiently large number trials.', 'confidence estimated pp increase increasing number coin-flips, frequentist statistic facilitate indication confidence estimated pp value.', 'however, increase number trials, different probability values observing heads eventually, discover coin fair coin.', 'attempt understand importance confident measure studying following cases • experiment infinite number trials guarantees pp absolute accuracy (100% confidence).', 'number coin number heads probability observing heads flips 10 6 0.6 50 29 0.58 100 55 0.55 200 94 0.47 500 245 0.49 table 1 - coin flip experiment results increasing number trials table 1 presents possible outcomes hypothetical coin flip experiment increasing number trials.']\n",
      "\n",
      "['beliefs play significant role shaping outcome hypothesis test especially limited data.', 'however, frequentist statistics, possible incorporate beliefs past experience increase accuracy hypothesis test.', 'conditional probability - measure probability p(a|b)p(a|b) event given event b occurred. •', 'continuous probability distributions described probability density functions discrete probability distributions represented probability mass functions.', 'joint probability distribution bayes’ theorem bayes’ theorem describes conditional probability event hypothesis computed evidence prior knowledge.', 'department cse mrcet 40 decide accept conclusion extend experiment trials achieves sufficient confidence.', 'moreover, valuable insights prior beliefs (for example, coins usually fair coin biased intentionally, p≈0.5p≈0.5) describes value pp.', 'random variable (stochastic variable) - statistics, random variable variable possible values result random event.', 'provide lengthy explanations mathematical definition lot widely available content use understand concepts. •', 'therefore, possible value random variable probability attached represent likelihood values. •']\n",
      "\n",
      "['department cse mrcet 41 test cases, including prior belief rarely observed bugs code.', 'let assume unlikely find bugs code rarely observed bugs code past.', 'however, time applying bayes’ theorem, decide priors means (otherwise use previous posterior new prior).', 'however, intuition goes simple hypothesis test multiple events hypotheses involved (let worry moment).', 'prior represents beliefs gained past experience, refers common sense outcome bayes’ theorem past observations.', 'however, now, let assume p(θ)=pp(θ) term depends test coverage test cases.', 'know value term proper measurements, order continue discussion let assume p(x|¬θ)=0.5p(x|¬θ)=0.5.', 'p(θ)p(θ) - prior probability probability hypothesis θθ true applying bayes’ theorem.', 'past experience observing fewer bugs code, assign prior p(θ)p(θ) higher probability.', 'accordingly, p(x)=1×p+0.5×(1−p)=0.5(1+p)p(x)=1×p+0.5×(1−p)=0.5(1+p) • p(θ|x)p(θ|x) - posteriori probability denotes conditional probability hypothesis θθ observing evidence xx.', 'θθ xx denote code bug free passes test cases respectively. •']\n",
      "\n",
      "['therefore, express hypothesis θmapθmap concluded map follows θmap=argmaxθp(θi|x)=argmaxθ(p(x|θi)p(θi)p(x))θmap=argmaxθp(θi|x) =argmaxθ(p(x|θ i)p(θi)p(x)) argmaxθargmaxθ operator estimates event hypothesis θiθi maximizes posterior probability p(θi|x)p(θi|x).', 'p(¬θ|x)=p(x|¬θ).p(¬θ)p(x)=0.5×(1−p)0.5×(1+p)=(1−p)(1+p)p(¬θ|x)=p(x|¬ θ).p(¬θ) p(x)=0.5×(1−p)0.5×(1+p)=(1−p)(1+p) know conditional probabilities observing bug code observing bug code.', 'department cse mrcet 42 know values terms bayes’ theorem, calculate posterior probability following formula p(θ|x)=1×p0.5(1+p)p(θ|x)=1×p0.5(1+p) calculate probability observing bug, given code passes test cases p(¬θ|x)p(¬θ|x) .']\n",
      "\n",
      "['department cse mrcet 43 figure 1 - p(θ|x)p(θ|x) p(¬θ|x)p(¬θ|x) changing p(θ)=pp(θ)=p figure 1 illustrates posterior probabilities possible hypotheses change value prior probability.', 'unlike frequentist statistics belief past experience influence concluded hypothesis, bayesian learning capable incorporating belief improve accuracy predictions.', 'assuming fairly good programmers probability observing bug p(θ)=0.4p(θ)=0.4 , find θmapθmap map=argmaxθ{θp(|x)=0.40.5(1+0.4),¬θp(¬θ|x)=0.5(1−0.4)0.5(1+0.4)}=ar gmaxθ{θp(θ|x)=0.57,¬θp(¬θ|x)=0.43}=θ⟹no bugs present codemap=argmaxθ{θp(|x)=0.40.5(1+0.4),¬θp(¬θ|x)=0.5(1−0.4)0.5(1+0.4 )}=argmaxθ{θp(θ|x)=0.57,¬θp(¬θ|x)=0.43}=θ⟹no bugs present code']\n",
      "\n",
      "['however, problem deciding sufficiently large number trials attaching confidence concluded hypothesis.', 'therefore, simplify θmapθmap estimation, denominator posterior computation shown below θmap=argmaxθ(p(x|θi)p(θi))θmap=argmaxθ(p(x|θi)p(θi)) notice map estimation algorithms compute posterior probability hypothesis decide probable hypothesis.', 'binomial likelihood likelihood coin flip experiment given probability observing heads coin flips given fairness coin.', 'therefore, practical implementation map estimation algorithms use approximation techniques, capable finding probable hypothesis computing posteriors computing them.', 'assuming hypothesis space continuous (i.e. fairness coin encoded probability observing heads, coefficient regression model, etc.),', 'department cse mrcet 44 however, p(x)p(x) independent θθ, p(x)p(x) events hypotheses.', 'defined fairness coins (θθ) probability observing heads coin flip, define probability observing heads', 'endless possible hypotheses present smallest range human mind think of, discrete hypothesis space large number possible outcomes event, need find posterior hypothesis order decide probable hypothesis.']\n",
      "\n",
      "['rewrite expression single expression follows p(y=y|θ)=θy×(1−θ)1−yp(y=y|θ)=θy×(1−θ)1−y equation represents likelihood single test coin flip experiment.', 'interestingly, likelihood function single coin flip experiment similar bernoulli probability distribution.', 'accordingly p(y=1|θ)=θp(y=0|θ)=(1−θ)p(y=1|θ)=θp(y=0|θ)=(1−θ) defined conditional probabilities outcome above, let try find p(y=y|θ)p(y=y|θ) joint probability observing heads tails p(y=y|θ)={θ, y=11−θ, p(y=y|θ)={θ, y=11−θ, note yy 00 11, θθ lie range [0,1][0,1].', 'bernoulli probability distribution simplification binomial probability distribution single trail, represent likelihood coin flip experiment observe kk number heads nn number trials binomial probability distribution shown below p(k,n|θ)=(nk)θk(1−θ)n−k', 'department cse mrcet 45 tails given fairness coin p(y|θ)p(y|θ) y=1y=1 observing heads y=0y=0 observing tails.']\n",
      "\n",
      "['model referred bayes optimal learner, bayes classifier, bayes optimal decision boundary, bayes optimal discriminant function.', 'department cse mrcet 46 maximum likelihood estimation method (mle) likelihood function indicates likely observed sample function possible parameter values.', 'statistical point view, mle usually recommended large samples versatile, applicable models different types data, produces precise estimates.', 'repeat procedure additional n - 1 iterations, alternating drawing new sample conditional probability distribution x conditional probability distribution y, given current value random variable.', 'squares estimation method (lse) squares estimates calculated fitting regression line points data set minimal sum deviations squared (least square error).', 'bayes optimal classifier bayes optimal classifier probabilistic model makes probable prediction new example, given training dataset.', 'gibbs sampling algorithm start selecting initial value random variables x & y. then, sample conditional probability distribution x given y = y⁰ denoted p(x|y⁰).']\n",
      "\n",
      "['naive bayes classifier algorithm • naïve bayes algorithm supervised learning algorithm, based bayes theorem solving classification problems.']\n",
      "\n",
      "['popular examples naïve bayes algorithm spam filtration, sentimental analysis, classifying articles.', 'naïve bayes classifier simple effective classification algorithms helps building fast machine learning models quick predictions. •', 'solution solve this, consider dataset outlook play 0 rainy yes 1 sunny yes 2 overcast yes 3 overcast yes 4 sunny 5 rainy yes 6 sunny yes', 'department cse mrcet 48 • mainly text classification includes high-dimensional training dataset. •']\n",
      "\n",
      "[\"department cse mrcet frequency table weather conditions likelihood table weather condition weather yes overcast 0 5 5/14= 0.35 rainy 2 2 4/14=0.29 sunny 2 3 5/14=0.35 4/14=0.29 10/14=0.71 applying bayes'theorem p(yes|sunny)= p(sunny|yes)*p(yes)/p(sunny) 49 7 overcast yes 8 rainy 9 sunny 10 sunny yes 11 rainy 12 overcast yes 13 overcast yes weather yes overcast 5 0 rainy 2 2 sunny 3 2 total 10 5\"]\n",
      "\n",
      "['consider example • figure, alarm ‘a’ – node, installed house person ‘gfg’, rings probabilities i.e burglary ‘b’ fire', 'feature joint probability, probability bayesian belief network derived, based condition — p(attribute/parent) i.e probability attribute, true parent attribute.', 'department cse mrcet 50 p(sunny|yes)= 3/10= 0.3 p(sunny)= 0.35 p(yes)=0.71 p(yes|sunny) = 0.3*0.71/0.35= 0.60 p(no|sunny)= p(sunny|no)*p(no)/p(sunny) p(sunny|no)= 2/4=0.5 p(no)= 0.29 p(sunny)= 0.35 p(no|sunny)= 0.5*0.29/0.35 = 0.41 bayesian belief network graphical representation different probabilistic relationships random variables particular set.']\n",
      "\n",
      "['find local maximum likelihood parameters statistical model cases latent variables involved data missing incomplete.', 'expectation-maximization algorithm real-world applications machine learning, common relevant features available learning small subset observable.', 'explained, proposed given paper published 1977 arthur dempster, nan laird, donald rubin.', 'hand, expectation-maximization algorithm latent variables (variables directly observable actually inferred values observed variables) order predict values condition general form probability distribution governing latent variables known us.', 'expectation step (e – step) observed available data dataset, estimate (guess) values missing data.', 'but, drawbacks case, ‘p1’ forget person ‘gfg’, hearing alarm, tendency forget things, quick.', 'so, variables observable not, use instances variable visible observed purpose learning predict value instances observable.', 'alarm parent node probabilities p1 calls ‘p1’ & p2 calls ‘p2’ person nodes. •', 'maximization step (m – step) complete data generated expectation (e) step order update parameters.']\n",
      "\n",
      "['step, use observed data order estimate guess values missing incomplete data.', 'step, use complete data generated preceding “expectation” – step order update values parameters.', 'set incomplete observed data given system assumption observed data comes specific model. •', 'department cse mrcet 52 essence expectation-maximization algorithm use available observed data dataset estimate missing data data update values parameters.', 'now, fourth step, checked values converging not, yes, stop repeat step-2 step-3 i.e. “expectation” – step “maximization” – step convergence occurs.']\n",
      "\n",
      "['department cse mrcet 53 usage em algorithm • fill missing data sample. •']\n",
      "\n",
      "['department cse mrcet 54 instance-based learning machine learning systems categorized instance-based learning systems learn training examples heart generalizes new instances based similarity measure.', 'large memory required store data, query involves starting identification local model scratch.', 'example, create spam filter instance-based learning algorithm, instead flagging emails marked spam emails, spam filter programmed flag emails similar them.']\n",
      "\n",
      "['k-nn algorithm stores available data classifies new data point based similarity.', 'working knn algorithm k-nearest neighbours (knn) algorithm uses ‘feature similarity’ predict values new data points means new data point assigned value based closely matches points training set.', 'k-nn algorithm assumes similarity new case/data available cases new case category similar available categories. •', 'knn algorithm training phase stores dataset gets new data, classifies data category similar new data.', 'called lazy learner algorithm learn training set immediately instead stores dataset time classification, performs action dataset. •', 'department cse mrcet 55 k-nearest neighbor(knn) algorithm • k-nearest neighbour simplest machine learning algorithms based supervised learning technique. •', 'understand working help following steps − step 1 − implementing algorithm, need dataset.', 'means new data appears easily classified suite category k- nn algorithm. •', 'step 2 − next, need choose value k i.e. nearest data points.']\n",
      "\n",
      "['case-based reasoning classifiers (cbr) use database problem solutions solve new problems.', 'cbr tries combine solutions neighbouring training cases propose solution new case.', 'identical case found, cbr search training cases having components similar new case.', 'new case arrises classify, case-based reasoner(cbr) check identical training case exists.', 'step 4 – end example case based reasoning know nearest neighbour classifiers stores training tuples points euclidean space.', '3.4 − now, assign class test point based frequent class rows.', 'department cse mrcet 56 • 3.1 − calculate distance test data row training data help method namely euclidean, manhattan hamming distance.']\n",
      "\n",
      "['cbr intelligent number trade-off accuracy efficiency evolves number stored cases large.', 'problem resolution customer service help desks, cases describe product-related diagnostic problems.', 'medical educations, patient case histories treatments help diagnose treat new patients.', 'lazy learning methods construct different approximation target function encountered query instance.', 'certain point, system’s efficiency suffer time required search process relevant cases increases.', 'lazy learning suitable complex incomplete problem domains, complex target function represented collection complex local approximations.', 'differences eager lazy learning • eager learning methods construct general, explicit description target function based provided training examples. •', 'challenges cbr • finding good similarity metric (eg matching subgraphs) suitable methods combining solutions. •', 'eager learning methods use approximation target function, learned based training examples input queries observed.', 'lazy learning methods simply store data generalizing data postponed explicit request made. •']\n",
      "\n",
      "['examples speech recognition, speaker identification, multimedia document recognition (mdr), automatic medical diagnosis.', 'regression algorithms try find relationship variables predict unknown dependent variables based known data.', 'person keeps watching videos related cricket, youtube wouldn’t recommend chess tutorials videos.', 'now, similarities found based statistical analysis, historical data, gained knowledge machine itself.', 'based type data system choose appropriate algorithm classification, regression, regression recognize pattern. •', 'department cse mrcet 58 unit - iv pattern comparison techniques pattern recognition process finding regularities similarities data machine learning data.']\n",
      "\n",
      "['clinical settings, having ability know hidden relationships patient data unfold help save life aiding detection conditions obvious clinicians healthcare workers.', 'live ever-changing environment, intelligent system, human robot, encode patterns time, recognize generate temporal patterns.', 'temporal abstraction data mining research fields tried synthesis time oriented data bring understanding hidden relationships exist time oriented events.', 'example, temporal signal sequences movements head, hand, body, piece music, on.', 'temporal patterns temporal patterns pattern comparison techniques defined segment signals recurs frequently temporal signal sequence.', 'paper, propose temporal pattern recognition model based dimension reduction similarity measures maintaining temporal nature raw data introduction temporal pattern processing important intelligent behaviours, including hearing, vision, speech, music motor control.', 'understanding hidden patterns huge challenge exponential search space unique time-series data.']\n",
      "\n",
      "['recognition layer typically includes recurrent connections selecting winner self-organization (e.g. winner-take-all) training recognition.', 'speech recognition, example, want rate invariance distinguishing relative durations vowel /i/ (as beet) /i/ (as bit) temporal pattern recognition shared goal stm models input history available simultaneously recognition takes place.', 'department cse mrcet 60 refer syntactic structure, subject-verb-object, component category possible symbols • time duration.', 'recognition scheme essentially template matching, templates formed following hebbian learning wij(t) = wij(t–1) + c si (t)[xj (t) – wij(t–1)] wij connection weight unit xj input layer sequence recognizer si recognition layer.', 'template matching hebbian learning architecture type recognition simply two-layer network input layer incorporates stm, sequence recognition layer unit encodes individual sequence.', 'associative memory approach dynamics hopfield associative memory model characterized evolving memory state similar current input pattern.']\n",
      "\n",
      "['input layer, tdnn uses 2 hidden layers output layer unit encodes phoneme.', 'feed forward connections converge input layer successive layer unit specific layer receives inputs limited time window previous layer.', '1989) reported architecture called time delay neural networks (tdnn) spoken phoneme recognition.', 'recognizer encodes different template sequence unique weight vector acting inputs stm.', 'normalized exponential kernel stm, tank hopfield (1987) described recognition network based associative memory dynamics.', 'process dynamic evolution viewed optimization process, minimizes cost function equilibrium reached.', 'dynamic time warping sounds like time traveling kind future technic, however, not.', 'recognition process uses current input sequence (evidence) bias minimization process similar template wins competition, activating corresponding recognizer.', 'department cse mrcet 61 views memory state category, hopfield net performs pattern recognition recalled category recognized pattern.', 'demonstrated good recognition performance stop consonants /b/, /d/, /g/, accuracy speaker dependent recognition reached 98.5%.']\n",
      "\n",
      "['however, people speak word different ways, speaks hello slower pace like heeeeeeelloooooo , need algorithm match sound track different lengths able identify come person.', 'suppose want recognise voice person analysing sound track, able collect sound track saying hello scenario.', 'suppose want calculate distance equal-length arrays = [1, 2, 3] b = [3, 2, 2] that?', '= [1, 2, 3] b = [2, 2, 2, 3, 4] match up?']\n",
      "\n",
      "['department cse mrcet 63 stock market stock market, people hope able predict future, general machine learning algorithms exhaustive, prediction task requires test training set dimension features.', 'however, speculate stock market, know pattern stock different length reflection klines indicators.']\n",
      "\n",
      "['department cse mrcet 64 time series analysis, dynamic time warping (dtw) algorithms measuring similarity temporal sequences, vary speed.', 'idea compare arrays different length build one-to-many many-to-one matches total distance minimised two.', 'dtw applied temporal sequences video, audio, graphics data — indeed, data turned linear sequence analysed dtw.']\n",
      "\n",
      "['apply one-to-one match, shown top, mapping perfectly synced tail blue curve left out.', 'department cse mrcet 65 clearly series follow pattern, blue curve longer red.', 'dtw overcomes issue developing one-to-many match troughs peaks pattern perfectly matched, left curves(shown top).']\n",
      "\n",
      "['department cse mrcet 66 l > k rules general, dtw method calculates optimal match given sequences (e.g. time series) certain restriction rules(comes wiki) • index sequence matched indices sequence vice versa • index sequence matched index sequence (but match) • index sequence matched index sequence (but match) • mapping indices sequence indices sequence monotonically increasing, vice versa, i.e. sequence, indices indices sequence, index matched index l index j matched index k , vice versa.', 'optimal match denoted match satisfies restrictions rules minimal cost, cost computed sum absolute differences, matched pair indices, values.']\n",
      "\n",
      "['unsupervised learning method method draw references datasets consisting input data labelled responses.', 'department cse mrcet 67 introduction clustering basically type unsupervised learning method.', 'clustering task dividing population data points number groups data points groups similar data points group dissimilar data points groups.', 'generally, process find meaningful structure, explanatory underlying processes, generative features, groupings inherent set examples.']\n",
      "\n",
      "['department cse mrcet 68 dbscan density-based spatial clustering applications noise data points clustered basic concept data point lies given constraint cluster center.', 'example dbscan (density-based spatial clustering applications noise), optics (ordering points identify clustering structure), etc. •', 'clustering methods • density-based methods methods consider clusters dense region having similarities differences lower dense region space.', 'instance, interested finding representatives homogeneous groups (data reduction), finding “natural clusters” describe unknown properties (“natural” data types), finding useful suitable groupings (“useful” data classes) finding unusual data objects (outlier detection).', 'hierarchical based methods clusters formed method form treetype structure based hierarchy.']\n",
      "\n",
      "['clustering operations grids fast independent number data objects example sting (statistical information grid), wave cluster, clique (clustering quest), etc.', 'k means clustering simplest unsupervised learning algorithm solves clustering problem.k-means algorithm partitions n observations k clusters observation belongs cluster nearest mean serving prototype cluster.', 'method optimize objective criterion similarity function distance major parameter example k-means, clarans (clustering large applications based randomized search), etc. •', 'applications clustering different fields • marketing characterize & discover customer segments marketing purposes. •', 'city planning groups houses study values based geographical locations factors present.', 'department cse mrcet 69 examples cure (clustering representatives), birch (balanced iterative reducing clustering hierarchies), etc. •', 'grid-based methods method, data space formulated finite number cells form grid-like structure.']\n",
      "\n",
      "['method initialize means random values boundaries data set (if feature x items values [0,3], initialize means values x [0,3]).', 'department cse mrcet 70 • earthquake studies learning earthquake-affected areas determine dangerous zones.', 'categorize item closest mean update mean’s coordinates, averages items categorized mean far.', 'algorithm pseudocode k-mode clustering kmodes clustering unsupervised machine learning algorithms cluster categorical variables.']\n",
      "\n",
      "['repeat 2–3 steps re-assignment required example imagine dataset information hair color, eye color, skin color persons.', 'aim group based available information(maybe want suggest styling ideas) hair color, eye color, skin color categorical variables.', 'mismatches) assign observation closest cluster iteratively compare cluster data points observations.', 'let proceed defining number clusters(k)=3 step 1 pick k observations random use leaders/clusters choosing p1, p7, p8 leaders/clusters step 2 calculate dissimilarities(no.']\n",
      "\n",
      "['department cse mrcet 72 comparing leader/cluster p1 observation p1 gives 0 dissimilarities comparing leader/cluster p1 observation p2 gives 3(1+1+1) dissimilarities.', 'likewise, calculate dissimilarities matrix shown assign observations closest cluster (cluster dissimilarity)']\n",
      "\n",
      "['explanation cluster 1 observations(p1, p2, p5) brunette observed hair color, amber observed eye color, fair observed skin color.', 'repeat steps 2–4 obtaining new leaders, calculate dissimilarities observations newly obtained leaders.', 'department cse mrcet 73 step 2, observations p1, p2, p5 assigned cluster 1; p3, p7 assigned cluster 2; p4, p6, p8 assigned cluster 3.', 'observations cluster 1 marked yellow, cluster 2 marked brick red, cluster 3 marked purple.']\n",
      "\n",
      "['department cse mrcet 74 comparing cluster 1 observation p1 gives 1 dissimilarity.']\n",
      "\n",
      "['department cse mrcet 75 observations p1, p2, p5 assigned cluster 1; p3, p7 assigned cluster 2; p4, p6, p8 assigned cluster 3.']\n",
      "\n",
      "['department cse mrcet 76 vector quantization learning vector quantization ( lvq ) type artificial neural network inspired biological models neural systems.', 'based prototype supervised learning classification algorithm trained network competitive learning algorithm similar self organizing map.', 'architecture learning vector quantization number classes input data n number input features sample given below']\n",
      "\n",
      "['first, initializes weights size ( n, c ) c number training samples different labels discarded training samples.', 'department cse mrcet 77 let input data size ( m, n ) m number training example n number features example label vector size ( m, 1 ).', 'weight updation rule given wij = wij(old) - alpha(t) * (x k - wij(old)) alpha learning rate time t, j denotes winning vector, denotes ith feature training example k denotes kth training example input data.', 'algorithm steps involved • weight initialization • 1 n number epochs • select training example • compute winning vector • update winning vector • repeat steps 3, 4, 5 training example. •', 'iterate remaining input data, training example, updates winning vector ( weight vector shortest distance ( e.g euclidean distance ) training example ).']\n",
      "\n",
      "['words, algorithm place, set “knowledge data” end users improved, that’s quantity data, filtering noise undesirable results, refinement data points.', 'machine learning systems simple “rote input/output” function, evolve results supply continued use.', 'intelligent exploitation random search provided historical data direct search region better performance solution space.', 'fundamental ways engineers use induction algorithm enhance knowledge acquisition given system.', 'generation consist population individuals individual represents point search space possible solution.', 'different search methods induction field machine learning, induction algorithm represents example mathematical principles development sophisticated computing systems.', 'genetic algorithms simulate process natural selection means species adapt changes environment able survive reproduce generation.', 'department cse mrcet genetic algorithms unit- v genetic algorithms(gas) adaptive heuristic search algorithms belong larger evolutionary algorithms.']\n",
      "\n",
      "['department cse mrcet 79 technical descriptions induction algorithms largely territory mathematical scientific journals, basic ideas induction algorithm organize “classification rules” according induction principle separate corollary results different kinds system noise exceptions.', \"general, machine learning use visual dashboards generating new tools users rapidly develop in-depth knowledge given system, it's related marine research, medical diagnosis, e-commerce, kind data-rich system.\", 'idea real-world data filtering, induction algorithms compose different sets rules legitimate results system noise, order distinguish other.', 'like kinds machine learning software, induction algorithms thought form “decision support.” “', 'mind, induction algorithms kinds software products seek refine data produce evolving results human users.', 'sense, use induction algorithm uses induction principle “prove” certain results aid knowledge, provide marked delineations data set (or multiple data sets) – distinctions drive sorts end user capabilities.', 'we consider principal task real-world induction system assisting expert expressing expertise,” write authors turing institute paper induction machine learning 1980s. “', 'setting induction algorithms according certain training examples, stakeholders looking ability systems identify assess consistent rules data represents exceptions rules.']\n",
      "\n",
      "['terms machine learning, algorithm aims understand example particular concept generalizations form concepts training examples.', 'ebl architecture • ebl model training • training, model generalizes training example way scenarios lead goal concept, specific cases. (', 'department cse mrcet 80 simple terms, ability gain basic problem-solving techniques observing analysing solutions specific problems.', 'explanation — domain theory eliminate unimportant training example retaining important ones best describe goal concept.']\n",
      "\n",
      "['department cse mrcet 81 • ebl model training • post training, ebl model tends directly reach hypothesis space involving goal concept. (']\n",
      "\n",
      "['condition, classification problem relies humidity rainfall collapsed underlying feature, aforementioned correlated high degree.', '3d classification problem hard visualize, 2-d mapped simple 2 dimensional space, 1-d problem simple line.', 'department cse mrcet 82 dimensionality reduction intuitive example dimensionality reduction discussed simple e-mail classification problem, need classify e-mail spam not.', 'figure illustrates concept, 3-d feature space split 1-d feature spaces, later, found correlated, number features reduced further.', 'involve large number features, e-mail generic title, content e- mail, e-mail uses template, etc.']\n",
      "\n",
      "['department cse mrcet 83 components dimensionality reduction components dimensionality reduction • feature selection this, try find subset original set variables, features, smaller subset model problem.', 'methods dimensionality reduction methods dimensionality reduction include • principal component analysis (pca) • linear discriminant analysis (lda) • generalized discriminant analysis (gda) dimensionality reduction linear non-linear, depending method used.', 'embedded • feature extraction reduces data high dimensional space lower dimension space, i.e. space lesser no.']\n",
      "\n",
      "['department cse mrcet 84 principal component analysis method introduced karl pearson.', 'works condition data higher dimensional space mapped data lower dimension space, variance data lower dimensional space maximum.', 'advantages dimensionality reduction • helps data compression, reduced storage space. •']\n",
      "\n",
      "['example, possible variations observed variables mainly reflect variations unobserved (underlying) variables.', 'department cse mrcet 85 • know principal components keep- practice, thumb rules applied.', 'observed variables modelled linear combinations potential factors plus \"error\" terms, factor analysis thought special case errors-invariables models.', 'factor analysis statistical method describe variability observed, correlated variables terms potentially lower number observed variables called factors.']\n",
      "\n",
      "['multidimensional scaling multidimensional scaling visual representation distances dissimilarities sets objects. “', 'department cse mrcet 86 distances speakers recording ‘n’ speakers’ voice signals.', 'decomposing mixed signal microphone’s recording independent source’s speech signal machine learning technique, independent component analysis. [', 'objects” colors, faces, map coordinates, political persuasion, kind real conceptual stimuli (kruskal wish, 1978).', 'objects similar (or shorter distances) closer graph objects similar (or longer distances).', 'x1, x2, ….., xn ] => [ y1, y2, ….., yn ] where, x1, x2, …, xn original signals present mixed signal y1, y2, …, yn new features independent components independent other.', 'now, microphones’ recordings, want separate ‘n’ speakers’ voice signals room given microphone recorded voice signals coming speaker different intensity difference distances them.']\n",
      "\n",
      "['two-dimensional manifold 2-d shape fit higher dimensional space twisting bending it, loosely speaking.', 'term scaling comes psychometrics, abstract concepts (“objects”) assigned numbers according rule (trochim, 2006).', 'department cse mrcet 87 graph, mds serve dimension reduction technique high- dimensional data (buja et.', 'it’s use isn’t limited specific matrix set data; fact, matrix analyzed technique long matrix contains type relational data (young, 2013).', 'think “scaling” fact you’re essentially scaling data (i.e. making simpler creating lower-dimensional data).', 'assign “1” “doesn’t believe global warming”, 10 “firmly believes global warming” scale 2 9 attitudes between.', 'example, data points close high-dimensional space close low- dimensional space (martinez, 2005). “']\n",
      "\n",
      "['unsupervised learning algorithm produces low-dimensional embeddings high-dimensional inputs, relating training instance closest neighbor.', 'general, m training instances total, tries find set weights w minimizes squared distance x(i) linear representation.', 'training instance x(i), algorithm finds k nearest neighbors tries express x(i) linear function them.', 'the manifold hypothesis states real-world high-dimensional data lie low dimensional manifolds embedded high-dimensional space.”', 'so, cost function given wi,j =0, j included k closest neighbors i. also, normalizes weights training instance x(i),', 'locally linear embedding (lle) locally linear embedding (lle) manifold learning technique non-linear dimensionality reduction.']\n",
      "\n",
      "['choosing d-dimensional coordinates minimize cost function, weights wi,j kept fixed try find optimum coordinates y(i)', 'department cse mrcet 89 finally, high-dimensional training instance x(i) mapped low- dimensional (say, d dimensions) vector y(i) preserving neighborhood relationships.']\n",
      "\n",
      "['introduction machine learning early draft proposed textbook nils j. nilsson robotics laboratory department computer science stanford university stanford, 94305 e-mail nilsson@cs.stanford.edu november 3, 1998 copyright c⃝2005 nils j. nilsson material copied, reproduced, distributed written permission copyright holder.']\n",
      "\n",
      "[]\n",
      "\n",
      "['23 2.3 summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', '16 2.2 classes boolean functions . . . . . . . . . . . . . . . . . . . .', '9 1.4 sample applications . . . . . . . . . . . . . . . . . . . . . . . . .', '5 1.2.2 input vectors . . . . . . . . . . . . . . . . . . . . . . . . .', '8 1.2.5 noise . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', '15 2.1.2 diagrammatic representations . . . . . . . . . . . . . . .', '17 2.2.1 terms clauses . . . . . . . . . . . . . . . . . . . . . .', '17 2.2.2 dnf functions . . . . . . . . . . . . . . . . . . . . . . . .', '9 1.3 learning requires bias . . . . . . . . . . . . . . . . . . . . . . . .', '13 1.6 bibliographical historical remarks . . . . . . . . . . . . . .', '18 2.2.3 cnf functions . . . . . . . . . . . . . . . . . . . . . . . .', '7 1.2.3 outputs . . . . . . . . . . . . . . . . . . . . . . . . . . . .', '3 1.1.3 varieties machine learning . . . . . . . . . . . . . . . .', '5 1.2.1 types learning . . . . . . . . . . . . . . . . . . . . . .', 'contents 1 preliminaries 1 1.1 introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', '23 2.2.6 linearly separable functions . . . . . . . . . . . . . . . .', '1 1.1.2 wellsprings machine learning . . . . . . . . . . . . . .', '4 1.2 learning input-output functions . . . . . . . . . . . . . . . . . .', '8 1.2.4 training regimes . . . . . . . . . . . . . . . . . . . . . . .', '11 1.5 sources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', '21 2.2.4 decision lists . . . . . . . . . . . . . . . . . . . . . . . . .', '22 2.2.5 symmetric voting functions . . . . . . . . . . . . . .', '13 2 boolean functions 15 2.1 representation . . . . . . . . . . . . . . . . . . . . . . . . . . . .', '9 1.2.6 performance evaluation . . . . . . . . . . . . . . . . . . .', '1 1.1.1 machine learning? . . . . . . . . . . . . . . . . .', '24 2.4 bibliographical historical remarks . . . . . . . . . . . . . .', '15 2.1.1 boolean algebra . . . . . . . . . . . . . . . . . . . . . . .']\n",
      "\n",
      "['42 4.1.6 training tlu non-linearly-separable training sets 44 4.2 linear machines . . . . . . . . . . . . . . . . . . . . . . . . . . .', '46 4.3.1 motivation examples . . . . . . . . . . . . . . . . . .', '68 5.2 learning belief networks . . . . . . . . . . . . . . . . . . . . . .', '51 4.4 training feedforward networks backpropagation . . . . . . .', '52 4.4.1 notation . . . . . . . . . . . . . . . . . . . . . . . . . . . .', '63 5.1.2 gaussian (or normal) distributions . . . . . . . . . . . .', '70 5.4 bibliographical historical remarks . . . . . . . . . . . . . .', '56 4.4.4 computing changes weights intermediate layers 58 4.4.5 variations backprop . . . . . . . . . . . . . . . . . . .', '70 5.3 nearest-neighbor methods . . . . . . . . . . . . . . . . . . . . . .', '37 4.1.3 error-correction training tlu . . . . . . . . . . . .', '52 4.4.2 backpropagation method . . . . . . . . . . . . . . . .', '44 4.3 networks tlus . . . . . . . . . . . . . . . . . . . . . . . . . .', '53 4.4.3 computing weight changes final layer . . . . . .', '32 3.5 bibliographical historical remarks . . . . . . . . . . . . . .', '49 4.3.3 piecewise linear machines . . . . . . . . . . . . . . . . . .', '27 3.2 version graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . .', '34 4 neural networks 35 4.1 threshold logic units . . . . . . . . . . . . . . . . . . . . . . . .', '38 4.1.4 weight space . . . . . . . . . . . . . . . . . . . . . . . . .', '32 3.4 candidate elimination method . . . . . . . . . . . . . . . .', '35 4.1.2 special cases linearly separable functions . . . . . . .', '63 5.1.1 background general method . . . . . . . . . . . . . .', '60 4.5 synergies neural network knowledge-based methods 61 4.6 bibliographical historical remarks . . . . . . . . . . . . . .', '40 4.1.5 widrow-hoﬀprocedure . . . . . . . . . . . . . . . . .', '65 5.1.3 conditionally independent binary components . . . . . .', '61 5 statistical learning 63 5.1 statistical decision theory . . . . . . . . . . . . . . . . . .', '35 4.1.1 deﬁnitions geometry . . . . . . . . . . . . . . . . . .', '59 4.4.6 application steering van . . . . . . . . . . . . . . .', '50 4.3.4 cascade networks . . . . . . . . . . . . . . . . . . . . . .', '3 version spaces learning 27 3.1 version spaces mistake bounds . . . . . . . . . . . . . . . .', '46 4.3.2 madalines . . . . . . . . . . . . . . . . . . . . . . . . . . .', '29 3.3 learning search version space . . . . . . . . . . . . . . .']\n",
      "\n",
      "['101 7.7 bibliographical historical remarks . . . . . . . . . . . . . .', '73 6.2 supervised learning univariate decision trees . . . . . . . . .', '104 8 computational learning theory 107 8.1 notation assumptions pac learning theory . . . . . . .', '81 6.4.3 avoiding overﬁtting decision trees . . . . . . . . . . .', '82 6.4.4 minimum-description length methods . . . . . . . . . . .', '109 8.2.1 fundamental theorem . . . . . . . . . . . . . . . . .', '80 6.4.2 validation methods . . . . . . . . . . . . . . . . . . . . .', '115 8.3.3 general capacity result . . . . . . . . . . . . . .', '94 7.4 inducing recursive programs . . . . . . . . . . . . . . . . . . . .', '75 6.2.3 non-binary attributes . . . . . . . . . . . . . . . . . . . .', '84 6.6 problem missing attributes . . . . . . . . . . . . . . . . .', '86 6.8 bibliographical historical remarks . . . . . . . . . . . . . .', '113 8.3.1 linear dichotomies . . . . . . . . . . . . . . . . . . . . . .', '84 6.5 problem replicated subtrees . . . . . . . . . . . . . . . .', '90 7.2 generic ilp algorithm . . . . . . . . . . . . . . . . . . . . . .', '113 8.3.2 capacity . . . . . . . . . . . . . . . . . . . . . . . . . . .', '111 8.2.3 properly pac-learnable classes . . . . . . . . . . .', '6 decision trees 73 6.1 deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', '79 6.4 overﬁtting evaluation . . . . . . . . . . . . . . . . . . . . .', '87 7 inductive logic programming 89 7.1 notation deﬁnitions . . . . . . . . . . . . . . . . . . . . . . .', '107 8.2 pac learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', '91 7.3 example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', '75 6.2.2 uncertainty reduction select tests . . . . . . .', '109 8.2.2 examples . . . . . . . . . . . . . . . . . . . . . . . . . . .', '98 7.5 choosing literals add . . . . . . . . . . . . . . . . . . . . . .', '112 8.3 vapnik-chervonenkis dimension . . . . . . . . . . . . . . . .', '117 8.4 vc dimension pac learning . . . . . . . . . . . . . . . . .', '79 6.3 networks equivalent decision trees . . . . . . . . . . . . . . .', '83 6.4.5 noise data . . . . . . . . . . . . . . . . . . . . . . . . .', '118 8.5 bibliographical historical remarks . . . . . . . . . . . . . .', '86 6.7 comparisons . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', '80 6.4.1 overﬁtting . . . . . . . . . . . . . . . . . . . . . . . . . .', '100 7.6 relationships ilp decision tree induction . . . . .', '74 6.2.1 selecting type test . . . . . . . . . . . . . . . . . .']\n",
      "\n",
      "['125 9.3.2 method based probabilities . . . . . . . . . . . . . .', '153 11.5.4 partially observable states . . . . . . . . . . . . . . . . .', '140 10.8 bibliographical historical remarks . . . . . . . . . . . . . .', '154 11.5.5 scaling problems . . . . . . . . . . . . . . . . . . . . . . .', '131 10.3 incremental computation (∆w)i . . . . . . . . . . . . . .', '120 9.2.2 method based probabilities . . . . . . . . . . . . . .', '130 10 temporal-diﬀerence learning 131 10.1 temporal patterns prediction problems . . . . . . . . . . . .', '150 11.5.1 illustrative example . . . . . . . . . . . . . . . . . . .', '126 9.4 bibliographical historical remarks . . . . . . . . . . . . . .', '138 10.6 intra-sequence weight updating . . . . . . . . . . . . . . . . . .', '152 11.5.3 generalizing inputs . . . . . . . . . . . . . . . . . .', '124 9.3 hierarchical clustering methods . . . . . . . . . . . . . . . . . .', '150 11.5.2 random actions . . . . . . . . . . . . . . . . . . .', '138 10.7 example application td-gammon . . . . . . . . . . . . . . .', '131 10.2 supervised temporal-diﬀerence methods . . . . . . . . . . .', '141 11 delayed-reinforcement learning 143 11.1 general problem . . . . . . . . . . . . . . . . . . . . . . . .', '134 10.4 experiment td methods . . . . . . . . . . . . . . . . .', '135 10.5 theoretical results . . . . . . . . . . . . . . . . . . . . . . . . . .', '144 11.3 temporal discounting optimal policies . . . . . . . . . . . .', '9 unsupervised learning 119 9.1 unsupervised learning? . . . . . . . . . . . . . . . . . .', '119 9.2 clustering methods . . . . . . . . . . . . . . . . . . . . . . . . . .', '145 11.4 q-learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', '154 11.6 bibliographical historical remarks . . . . . . . . . . . . . .', '147 11.5 discussion, limitations, extensions q-learning . . . . . .', '120 9.2.1 method based euclidean distance . . . . . . . . . .', '125 9.3.1 method based euclidean distance . . . . . . . . . .', '143 11.2 example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .']\n",
      "\n",
      "['164 12.7.1 macro-operators planning . . . . . . . . . . . . . . . .', '164 12.7.2 learning search control knowledge . . . . . . . . . . . .', '167 12.8 bibliographical historical remarks . . . . . . . . . . . . . .', '12 explanation-based learning 157 12.1 deductive learning . . . . . . . . . . . . . . . . . . . . . . . . . .', '164 12.6 utility ebl . . . . . . . . . . . . . . . . . . . . . . . . . . . .', '158 12.3 example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', '157 12.2 domain theories . . . . . . . . . . . . . . . . . . . . . . . . . . .', '159 12.4 evaluable predicates . . . . . . . . . . . . . . . . . . . . . . . . .', '162 12.5 general proofs . . . . . . . . . . . . . . . . . . . . . . . . .', '164 12.7 applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .']\n",
      "\n",
      "[]\n",
      "\n",
      "['and, treat matters practical importance applications; book handbook machine learning practice.', 'students stanford courses machine learning useful suggestions, colleague, pat langley, teaching assistants, ron kohavi, karl pﬂeger, robert allen, lise getoor.', 'hope future versions cover hopﬁeld nets, elman nets re- current nets, radial basis functions, grammar automata learning, genetic algorithms, bayes networks . . ..']\n",
      "\n",
      "['dictionary deﬁnition includes phrases “to gain knowledge, understanding of, skill in, study, instruction, expe- rience,” “modiﬁcation behavioral tendency experience.”', 'regards machines, say, broadly, machine learns changes structure, program, data (based inputs response external information) manner expected future performance improves.', 'tasks involve recognition, diag- nosis, planning, robot control, prediction, etc. “', 'but, example, performance speech-recognition machine improves hearing samples person’s speech, feel justiﬁed case machine learned.', 'likely concepts techniques explored researchers machine learning illuminate certain aspects biological learning.', 'certainly, techniques machine learning derive eﬀorts psychologists precise theories animal human learning computational models.', 'changes, addition record data base, fall comfortably province disciplines necessarily better understood called learning.', 'learning, like intelligence, covers broad range processes dif- ﬁcult deﬁne precisely.', 'machine learning usually refers changes systems perform tasks associated artiﬁcial intelligence (ai).']\n",
      "\n",
      "['sensory signals perception actions action computation model planning reasoning goals figure 1.1 ai system ask “why machines learn?', 'like machines able adjust internal structure produce correct outputs large number sample inputs suitably constrain input/output function approximate relationship implicit examples. •', 'are • tasks deﬁned example; is, able specify input/output pairs concise relationship inputs desired outputs.']\n",
      "\n",
      "['details statistical the- ory underlying methods found statistical textbooks [anderson, 1958]. •', 'brain models non-linear elements weighted inputs suggested simple models biological neu- rons.', 'dif- ferent traditions bring diﬀerent methods diﬀerent vocabulary assimilated uniﬁed discipline.', 'introduction 3 • human designers produce machines work desired environments used.', 'machines learn knowledge gradually able capture humans want write down. •', 'statistical methods dealing problems considered instances machine learning decision estimation rules depend corpus samples drawn problem environment.', 'continuing redesign ai systems conform new knowledge impractical, machine learning methods able track it.', 'brief listing separate disciplines contributed machine learning; details follow appropriate chapters • statistics long-standing problem statistics best use sam- ples drawn unknown probability distributions help decide distribution new sample drawn.', 'related problem estimate value unknown function new point given values function set sample points.', 'networks elements studied sev- eral researchers including [mcculloch & pitts, 1943, hebb, 1949, rosenblatt, 1958] and, recently [gluck & rumelhart, 1989, sejnowski, koch, & churchland, 1988].']\n",
      "\n",
      "['recent work directed discovering rules expert systems decision-tree methods [quinlan, 1990] in- ductive logic programming [muggleton, 1991, lavraˇc & dˇzeroski, 1994].', 'related work led number early decision tree [hunt, marin, & stone, 1966] semantic network [anderson & bower, 1973] methods.', 'samuel developed prominent early pro- gram learned parameters function evaluating board posi- tions game checkers [samuel, 1959].', 'work reinforcement learning traced eﬀorts model reward stimuli inﬂuence learning goal-seeking behavior animals [sutton & barto, 1987].', 'adaptive control theory control theorists study problem con- trolling process having unknown parameters estimated operation.', 'evolutionary models nature, individual animals learn perform better, species evolve better ﬁt individual niches.', 'distinc- tion evolving learning blurred computer systems, techniques model certain aspects biological evolution proposed learning methods improve performance computer programs.', 'early example epam net- work storing retrieving member pair words given [feigenbaum, 1961].', 'ai researchers explored role analogies learning [carbonell, 1983] fu- ture actions decisions based previous exemplary cases [kolodner, 1993].', 'theme saving generalizing results prob- lem solving explanation-based learning [dejong & mooney, 1986, laird, et al.,', 'genetic algorithms [holland, 1975] genetic programming [koza, 1992, koza, 1994] prominent computational tech- niques evolution.', 'shall important machine learning techniques based networks nonlinear elements—often called neural networks.']\n",
      "\n",
      "['consider variety diﬀerent computational structures • functions • logic programs rule sets • finite-state machines • grammars • problem solving systems present methods synthesis structures examples changing existing structures.', 'assume ﬁnd hypothesis, h, closely agrees f members ξ, hypothesis good guess f—especially ξ large.', 'assume priori hypothesized function, h, selected class functions h. know f belongs class subset class.', 'learning input-output functions 5 1.1.3 varieties machine learning orthogonal historical source learning technique important learned.', 'hypothesis function learned denoted h. f h functions vector-valued input x = (x1, x2, . . . ,', 'terminology shall book best introduced discussing problem learning functions, turn matter ﬁrst.', 'one, called supervised learning, know (sometimes approximately) values f m samples training set, ξ.', 'case, change existing structure simply computationally eﬃcient increase coverage situations handle.']\n",
      "\n",
      "['sup- pose given values two-dimensional function, f, sample points shown solid circles fig.', 'setting, termed unsupervised learning, simply train- ing set vectors function values them.', 'unsupervised learning methods application taxonomic problems desired invent ways classify data meaningful categories.', 'we regard problem learning function; value function subset input vector be- longs.)', 'problem case, typically, partition training set subsets, ξ1, . . . ,', 'formulas ⊃b b ⊃c, deduce c given a. deductive process, create formula ⊃c—a new formula sanction con-', 'xn h d h figure 1.2 input-output function curve-ﬁtting simple example supervised learning function.']\n",
      "\n",
      "['example illustrating categorical values, information student represented values attributes class, major, sex, adviser.', 'components, xi, input vector variously called features, attributes, input variables, components.', 'contrast speed-up learning methods create genuinely new functions—ones diﬀerent results learning before.', '1.2.2 input vectors machine learning methods derive diﬀerent traditions, terminology rife synonyms, book.', 'additionally, categorical values ordered (as {small, medium, large}) unordered (as example given).', 'learning input-output functions 7 -10 -5 0 5 10-10 -5 0 5 10 0 500 1000 1500 -10 -5 0 5 10-10 -5 0 5 10 0 00 00 0 x1 x2 h sample f-value figure 1.3 surface fits points clusions derived formulas previously had.']\n",
      "\n",
      "['case, training pattern having value 1 called positive instance, training sample having value 0 called negative instance.', 'input case suitable representation printed character, classiﬁer maps input of, say, 64 categories.', 'alternatively, output categorical value, case pro- cess embodying h variously called classiﬁer, recognizer, categorizer, output called label, class, category, decision.', 'batch method, entire training set available compute function, h. variation method uses entire training set modify current hypothesis iteratively acceptable hypothesis obtained.', '1.2.3 outputs output real number, case process embodying function, h, called function estimator, output called output value estimate.', 'entire training set available member time, use incremental method—selecting training set members arrive. (', 'contrast, incremental method, select member time training set use instance modify current hypothesis.', 'important specialization uses boolean values, regarded special case discrete numbers (1,0) categorical variables (true, false).']\n",
      "\n",
      "['present members training set, graph number hypotheses ruled function number diﬀerent patterns presented shown fig.', 'class noise randomly alters value function; attribute noise randomly alters values components input vector.', 'remaining functions constitute called “version space;” we’ll explore concept detail later.', '1.2.6 performance evaluation correct answer inductive learning, important methods evaluate result learning.', 'certainly, example, uncountable number diﬀerent functions having values agree samples shown fig.', 'learning requires bias 9 training instance function current hypothesis previ- ous instance—as classiﬁer decide robot’s action given current set sensory inputs.', 'case, inappropriate insist hypothesized function agree precisely values samples training set.', 'gain insight role bias considering special case learning boolean function n dimensions.', 'case, presented member training set value rule precisely one-half members h—those boolean functions misclassify labeled sample.', 'discuss matter detail later, but, brieﬂy, supervised learning induced function usually evaluated separate set inputs function values called testing set .', 'order selection limit priori set hypotheses quadratic functions insist chose passed sample points.', '1.3 learning requires bias long reader undoubtedly asked learning function possible all?', 'suppose bias; h set 22n boolean functions, preference ﬁt samples training set.']\n",
      "\n",
      "['labeled patterns seen 0 0 2n < j (generalization possible) |hv| = no.', 'depending subset order presentation training patterns, curve hypotheses ruled look like shown fig.', 'functions ruled figure 1.4 hypotheses remaining function labeled patterns presented suppose limited h subset, hc, boolean functions.', 'hypothesis set consists linearly separable functions—those positive negative instances separated linear surface, function remaining hypothsis set consistent training set.', 'so, case, training set contain possible patterns, pin function be—given bias.', 'preliminaries half remaining boolean functions value 1 half value 0 training pattern seen.', 'there, training set sample patterns marked having value 1 small square having value 0 small circle.', 'case possible seeing fewer 2n labeled samples, hypothesis agrees training set.']\n",
      "\n",
      "['example, way measuring complex- ity hypothesis, select simplest performed satisfactorily training set.', 'principle occam’s razor, science prefer simple explanations complex ones, type preference bias. (', 'langley, 1992] cites following applications others a. rule discovery variant id3 printing industry problem', 'absolute bias (also called restricted hypothesis-space bias), restricts h deﬁnite subset functions.', 'william occam, 1285-?1349, english philosopher said “non sunt multiplicanda entia praeter necessitatem,” means “entities multiplied unnecessarily.”)', 'functions ruled depends order presentation log2|hc| figure 1.5 hypotheses remaining restricted subset machine learning researchers identiﬁed main varieties bias, ab- solute preference.']\n",
      "\n",
      "['b. electric power load forecasting k-nearest-neighbor rule system [jabbour, k., et al.,', 'd. planning scheduling steel mill expertease, marketed (id3-like) system [michie, 1992].', 'c. automatic “help desk” assistant nearest-neighbor system [acorn & walden, 1992].', 'papers on speech recognition, dolphin echo recognition, image processing, bio-engineering, diag- nosis, commodity trading, face recognition, music composition, optical character recognition, control applications [various editors, 1989-1994].', 'b. neuroforecasting centre’s (london business school university col- lege london) trading strategy selection network earned average annual proﬁt 18% conventional system’s 12.3%.', 'additional examples, [hammerstrom, 1993] mentions a. sharp’s japanese kanji character recognition system processes 200 char- acters second 99+% accuracy.', 'preliminaries x1 x2 x3 figure 1.6 training set completely determines linearly separable function [evans & fisher, 1992].']\n",
      "\n",
      "['fact come surprise inasmuch machine learning techniques viewed extensions known statistical meth- ods successfully applied years.', 'sources 13 c. fujitsu’s (plus partner’s) neural network monitoring continuous steel casting operation successful operation early 1990.', '1.5 sources rich literature machine learning (a small referenced bibliography), text- books worth mentioning [hertz, krogh, & palmer, 1991, weiss & kulikowski, 1991, natarjan, 1991, fu, 1994, langley, 1996]. [', 'shavlik & dietterich, 1990, buchanan & wilkins, 1993] edited vol- umes containing important papers.', 'established conferences publications papers given appear including • annual conferences advances neural information processing systems • annual workshops computational learning theory • annual international workshops machine learning • annual international conferences genetic algorithms (the proceedings above-listed conferences published morgan kaufmann.) •']\n",
      "\n",
      "[]\n",
      "\n",
      "['deﬁnitions compactly given following rules boolean algebra 1 + 1 = 1, 1 + 0 = 1, 0 + 0 = 0, 1 · 1 = 1, 1 · 0 = 0, 0 · 0 = 0, 1 = 0, 0 = 1.', 'chapter 2 boolean functions 2.1 representation 2.1.1 boolean algebra important ideas learning functions easily presented special case boolean functions.', 'arguments values boolean functions expressed terms constants t (true) f (false) instead 1 0, respectively.', 'x1x2 value 1 x1 x2 value 1; x1 x2 value 0, x1x2 value 0. (', 'x1 + x2 value 1 x1 x2 value 1; x1 x2 value 0, x1 + x2 value 0.', 'complement negation variable, x, written x. x value 1 x value 0; x value 1, x value 0.']\n",
      "\n",
      "['2.1.2 diagrammatic representations saw chapter boolean function represented labeling vertices cube.', 'vertices having value 1 labeled small square, vertices having value 0 labeled small circle.', 'instead, demorgan’s laws (which veriﬁed deﬁnitions) x1x2 = x1 + x2, x1 + x2 = x1 x2.', '3-dimensional cube 23 = 8 vertices, labeled diﬀerent ways; 2(23) = 256', 'x1 x2 x1 x2 x1 x2 xor (exclusive or) x1x2 x1 + x2 x1x2 + x1x2 parity function x1 x2 x3 x1x2x3 + x1x2x3 + x1x2x3 + x1x2x3 figure 2.1 representing boolean functions cubes hypercube representations, easy boolean functions n dimensions are.']\n",
      "\n",
      "['an parity function boolean function value 1 number arguments value 1; value 0.)', 'term function written form l1l2 · · · lk, li literals.', 'rows columns arranged way entries adjacent map correspond vertices adjacent hypercube representation.', '00 01 10 11 00 01 10 11 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 x1,x2 x3,x4 figure 2.2 karnaugh map 2.2 classes boolean functions 2.2.1 terms clauses use absolute bias machine learning, limit class hypotheses.', 'course, visualize hypercubes (for n > 3), surprising properties higher dimensional spaces, careful intuitions gained low dimensions.', 'karnaugh map array values boolean function horizontal rows indexed values variables vertical columns indexed rest.']\n",
      "\n",
      "['consider, example, function f = x2x3 + x1 x3 + x2x1x3.', 'number terms size k bounded pk i=0 c(2n, i) = o(nk), c(i, j) = i! (', 'term, t, prime implicant f term, t′, formed taking literal implicant t longer implicant f. (the implicant “divided” term remain implicant.)', 'note planes ﬁgure “cuts oﬀ” group vertices having value 1, cuts oﬀany vertices having value 0.', 'group vertices subface corresponds implicants function, f, implicant corresponds subface dimension.', 'relationship implicants prime implicants geometri- cally illustrated cube representation boolean functions.', 'term dnf expression function called implicant “implies” function (if term value 1, function).', '3n possible clauses fewer pk i=0 c(2n, i) clauses size k less.', 'general, term, t, implicant function, f, f value 1 t does.', 'dnf expression called k-term dnf expression disjunction k terms; class k-dnf size largest term k. examples 2-term 3-term expressions, respectively.', '2.2.2 dnf functions boolean function said disjunctive normal form (dnf) written disjunction terms.', 'clause function written form l1 +l2 +· · ·+lk, li literals.', 'thus, x2x3 x1 x3 prime implicants f = x2x3+x1 x3+x2x1x3, x2x1x3 not.', 'examples dnf are f = x1x2+x2x3x4 f = x1x3 + x2 x3 + x1x2x3.']\n",
      "\n",
      "['consensus method relies results replace section describing quine-mccluskey method instead. •', 'note term x2x1x3 prime implicant f. (in case, don’t include term function vertex cut oﬀby plane corresponding x2x1x3 cut oﬀby plane corresponding x2x3.)', 'x2 x1 x3 1, 0, 0 1, 0, 1 1, 1, 1 0, 0, 1 f = x2x3 + x1x3 + x2x1x3 = x2x3 + x1x3 x2x3 x1x3 prime implicants figure 2.3 function implicants note boolean functions represented dnf—trivially disjunctions terms size n term corresponds vertices value 1.', 'boolean functions represented dnf term prime implicant, representation unique, shown fig.', '22n functions n dimensions dnf (since boolean function written dnf), 2o(nk) functions k-dnf.', 'express function dnf form, use consensus method ﬁnd expression function term prime implicant.']\n",
      "\n",
      "['boolean functions x2 x1 x3 1, 0, 0 1, 0, 1 1, 1, 1 0, 0, 1 f = x2x3 + x1x3 + x1x2 = x1x2 + x1x3 terms prime implicants, unique representation figure 2.4 non-uniqueness representation prime implicants xi · f1 + xi · f2 = xi · f1 + xi · f2 + f1 · f2 f1 f2 terms literal appearing f1 appears complemented f2.', 'f1 · f2 called consensus xi · f1 xi · f2.']\n",
      "\n",
      "['ﬁnal form function terms prime implicants is f = x1x2 +x1x3 +x1 x4x5.', 'x1x2 x1x2x3 x1x2x3x4x5 x1x3 x1x2x4x5 x1x4x5 f = x1x2 + + x1x3 x1x4x5 1 2 6 4 5 3 figure 2.5 consensus tree 2.2.3 cnf functions disjunctive normal form dual conjunctive normal form (cnf).', 'process halts, terms remaining t prime implicants f. example let f = x1x2 + x1 x2x3 + x1 x2 x3 x4x5.', 'classes boolean functions 21 consensus method ﬁnding set prime implicants function, f, iterates following operations terms dnf expression f operations applied a. initialize process set, t , terms dnf expression f, b. compute consensus pair terms t add result t , c. eliminate terms t subsumed terms t .']\n",
      "\n",
      "['boolean functions example cnf is f = (x1 +x2)(x2 +x3 +x4).', 'f written dnf, application de morgan’s law renders f cnf, vice versa.', 'value 1 x1 = 1, x2 = 0, x3 = 1.', 'decision list size k, size largest term k. class decision lists size k called k-dl.', 'at ti value 1, does; v1 regarded default value decision list.)', 'decision list written ordered list pairs (tq, vq) (tq−1, vq−1) · · · (ti, vi) · · · (t2, v2) (t, v1) vi 0 1, ti terms (x1, . . . ,', 'value decision list value vi ﬁrst ti list value 1. (', 'example decision list is f = (x1x2, 1) (x1 x2x3, 0) x2x3, 1) (1, 0) f value 0 x1 = 0, x2 = 0, x3 = 1.', 'example use linearly separable functions place ti (see [marchand & golea, 1993]).', '2.2.4 decision lists rivest proposed class boolean functions called decision lists [rivest, 1987].', 'cnf expression called k-clause cnf expression conjunction k clauses; class k-cnf size largest clause k. example 2-clause expression 3-cnf.']\n",
      "\n",
      "['classes boolean functions 23 2.2.5 symmetric voting functions boolean function called symmetric invariant permutations input variables.', 'wn) n-dimensional vector weight values, x · w dot (or inner) product vectors.', 'input vectors f value 1 lie half-space (and on) hyperplane orientation normal w position (with respect origin) determined θ.', 'closed-form expression number linearly separable func- tions n dimensions, following table gives numbers n 6.', 'important subclass symmetric functions class voting func- tions (also called m-of-n functions).', 'n, real-valued numbers called weights, θ real-valued number called threshold, thresh(σ, θ) 1 σ ≥θ 0 otherwise. (', 'xn) n-dimensional vector input variables, w = (w1, . . . ,', 'k-voting functions members class linearly separable functions weights unit value threshold depends k. thus, terms clauses special cases linearly separable functions.', '2.2.6 linearly separable functions linearly separable functions expressed follows f = thresh( n x i=1 wixi, θ) wi, = 1, . . . ,', 'k = 1, voting function n-sized clause; k = n, voting function n-sized term; k = (n + 1)/2 n odd k = 1 + n/2 n even, majority function.', 'parity functions, value 1 depending number input variables value 1 odd symmetric function. (', 'convenient way write linearly separable functions uses vector notation f = thresh(x · w, θ) x = (x1, . . . ,']\n",
      "\n",
      "['boolean functions n boolean linearly separable functions functions 1 4 4 2 16 14 3 256 104 4 65,536 1,882 5 ≈4.3 × 109 94,572 6 ≈1.8 × 1019 15,028,134 [muroga, 1971] shown (for n > 1) 2n2 linearly separable functions n dimensions. (', 'dnf (all) k-dl k-dnf k-size- terms terms lin sep figure 2.6 classes boolean functions sizes classes given following table (adapted [dietterich, 1990, page 262])']\n",
      "\n",
      "['bibliographical historical remarks 25 class size class terms 3n clauses 3n k-term dnf 2o(kn) k-clause cnf 2o(kn) k-dnf 2o(nk) k-cnf 2o(nk) k-dl 2o[nkk log(n)] lin sep 2o(n2) dnf 22n 2.4 bibliographical historical remarks added.']\n",
      "\n",
      "[]\n",
      "\n",
      "['consider following procedure classifying arbitrary input pattern, x pattern class (0 1) majority outputs functions version space.', 'stage process left subset functions consistent patterns presented far; subset version space patterns presented.', 'hypothesis, h, consistent values x ξ h(x) = f(x) x ξ.', 'devices implement- ing function h. incremental training procedure deﬁned presented pattern ξ functions eliminated functions values pattern agree given value.', 'learning procedure, majority equal value pattern presented, mistake made, revise version space accordingly—eliminating (majority the) functions voting incorrectly.', 'chapter 3 version spaces learning 3.1 version spaces mistake bounds ﬁrst learning methods present based concepts version spaces version graphs.', 'given initial hypothesis set h (a subset boolean functions) values f(x) x training set, ξ, version space subset hypotheses, hv, consistent values.']\n",
      "\n",
      "['special case, bias limit h terms, log2(3n) = n log2(3) = 1.585n mistakes exhausting version space.', 'version spaces learning h1 h2 hi hk x subset, h, boolean functions rule hypotheses consistent training patterns hj hypotheses ruled constitute version space k = |h| 1 0 figure 3.1 implementing version space original hypothesis set, h. (note, though, number training patterns seen maximum number mistakes greater.)', 'suﬃcient training patterns reduce version space single function, training patterns reduce version space set functions assign values patterns henceforth.', 'result means f term, 1.585n mistakes learning f, number mistakes able decide f term.', 'result (due [littlestone, 1988]) example mistake bound—an important concept machine learning theory.']\n",
      "\n",
      "['3.2, example version graph 3-dimensional input space hypotheses restricted terms (with ruled out).', '1” row nodes corresponding terms having literal, row nodes corresponding terms having literals,', 'boolean function, f1, general function, f2, (and f2 speciﬁc f1), f1 value 1 arguments f2 value 1, f1 ̸= f2.', '0 x1 x2 x3 x2 x3 1 x1x2 x3 x1x2 x1 version graph terms x1 x2 x3 (for simplicity, arcs graph shown) (none ruled out) (k = 1) (k = 2) (k = 3) x1 x3 figure 3.2 version graph terms function, denoted “1,” value 1 inputs, corre- sponds node graph. (']\n",
      "\n",
      "['3.4, version graph exists learning (1,0,1) value 0 (1, 0, 0) value 1.', 'portrayal graph cluttered arcs shown; node actual graph arc directed nodes general.', '0 x1 x2 x3 x2 x3 1 x1x2 x3 x1x2 x1 new version graph 1, 0, 1 value 0 x1x3 x1x2 x2x3 x1x2x3 x1 x2 x3 x1x3 (only arcs graph shown) ruled nodes figure 3.3 version graph seeing (1, 0, 1) version graph, set hypotheses maximally general set hypotheses maximally speciﬁc.', 'use example version graph changes consider set labeled samples training set, ξ.', '33 = 27 functions altogether (the function “0,” included graph, technically term).']\n",
      "\n",
      "['maximally speciﬁc corresponds subface minimal dimension contains members training set labelled 1 members labelled 0.', 'determination possible fact member version space (that member boundary sets) speciﬁc member general boundary set general member speciﬁc boundary set.', 'version graphs 31 0 x1 x2 x3 x2 x3 1 x1x2 x3 x1 x2x3 x1x3 general boundary set (gbs) specific boundary set (sbs) x1x2 specific gbs, general sbs 1, 0, 1 value 0 x1 x2 x3 1, 0, 0 value 1 figure 3.4 version graph seeing (1, 0, 1) (1, 0, 0) boundary sets important provide alternative repre- senting entire version space explicitly, impractical.', '3.4, subface minimal dimension contains (1, 0, 0) contain (1, 0, 1) vertex (1, 0, 0) itself—corresponding function x1x2 x3.', 'maximally general corresponds subface maximal dimension contains members training set labelled 1 members labelled 0.', 'given boundary sets, possible determine hypoth- esis (in prescribed class boolean functions using) member version space.', 'limit boolean functions version space terms, simple matter determine maximally general maximally speciﬁc functions (assuming term version space).']\n",
      "\n",
      "['negative instance algorithm specializes elements [gbs] longer cover new instance remain consis- tent past data, removes [sbs] elements mistakenly cover new, negative instance.”', 'version spaces learning maximal dimension contains (1, 0, 0) contain (1, 0, 1) face cube—corresponding function x3.', '3.4 candidate elimination method candidate elimination method, incremental method computing boundary sets.', 'quoting [hirsh, 1994, page 6] “the candidate-elimination algorithm manipulates boundary-set representation version space create boundary sets rep- resent new version space consistent previous instances plus new one.', 'method uses following deﬁnitions (adapted [genesereth & nilsson, 1987]) • hypothesis called suﬃcient value 1 training samples labeled 1, • hypothesis called necessary value 0 training samples labeled 0.', 'compare view top-down versus bottom-up divide-and-conquer covering (or aq) methods decision-tree induction.', 'start general function specialize specialization operators ﬁnds function consistent (or adequately so) set training patterns.', 'positive exmple algorithm generalizes elements [sbs] little possible cover new instance remain consistent past data, removes elements [gbs] cover new instance.']\n",
      "\n",
      "['new general boundary set obtained previous re- placing element, hi, specializations.', 'candidate elimination method 33 think deﬁnitions hypothesis implements suﬃ- cient condition training sample value 1 hypothesis value 1 positive instances; hypothesis implements necessary condition training sample value 1 hypothesis value 0 negative instances.', 'hypothesis hg generalization h if a) h speciﬁc hg, b) hg suﬃcient, c) function (including h) speciﬁc hg suﬃcient, d) hg speciﬁc member new general boundary set.', 'b. new vector labelled 0 new speciﬁc boundary set obtained previous ex- cluding elements necessary. (', 'hypothesis hs specialization h if a) h general hs, b) hs necessary, c) function (including h) general hs necessary, d) hs general member new speciﬁc boundary set.', 'again, hs = h, specializations diﬀerent functions general boundary set identical.', 'example, suppose present vectors following order vector label (1, 0, 1) 0 (1, 0, 0) 1 (1, 1, 1) 0 (0, 0, 1) 0', 'hg = h. also, generalizations diﬀerent functions speciﬁc boundary set identical.', 'start (before receiving members training set) function “0” singleton element speciﬁc boundary set function “1” singleton element general boundary set.', 'new speciﬁc boundary set obtained previous re- placing element, hi, generalizations.', 'receiving new labeled input vector, boundary sets changed follows a. new vector labelled 1 new general boundary set obtained previous ex- cluding elements suﬃcient. (']\n",
      "\n",
      "['version spaces learning start general boundary set, “1”, speciﬁc boundary set, “0.”', 'functions, x1, x2, x3, specializations “1” (they necessary, “1” not, general “0”, functions general necessary).', 'order accomodate noisy data, version spaces generalized [hirsh, 1994] allow hypotheses necessarily consistent training set.', '1, 1, 1), labeled 0, change speciﬁc boundary set function necessary.', 'single function generalization “0” (it suﬃcient, “0” speciﬁc it, function (including “0”) speciﬁc suﬃcient, speciﬁc member general boundary set. (', 'seeing ﬁrst sample, (1, 0, 1), labeled 0, speciﬁc boundary set stays “0” (it necessary), change general boundary set {x1, x2, x3}.', 'finally, (0, 0, 1), labeled 0, change speciﬁc boundary set function necessary.', '3.5 bibliographical historical remarks concept version spaces role learning ﬁrst investigated tom mitchell [mitchell, 1982].', 'ideas prac- tical machine learning procedures, provide insight nature hypothesis selection.', 'then, seeing (1, 0, 0), labeled 1, general boundary set changes {x3} (because x1 x2 suﬃcient), speciﬁc boundary set changed {x1x2 x3}.']\n",
      "\n",
      "['networks commonly use threshold element encountered chapter study linearly separable boolean functions.', 'called neural networks be- cause non-linear elements inputs weighted sum outputs elements—much like networks biological neurons do.', 'begin treatment neural nets studying threshold element simplest networks, ones composed single threshold element.', 'called adaline (for adaptive linear element) [widrow, 1962, widrow & lehr, 1990], ltu (linear threshold unit), perceptron, neuron. (', '4.1 threshold logic units 4.1.1 deﬁnitions geometry linearly separable (threshold) functions implemented straightforward way summing weighted inputs comparing sum threshold value shown fig.', 'although word “per- ceptron” nowadays refer single tlu, rosenblatt originally deﬁned class networks threshold elements [rosenblatt, 1958].)', 'networks non-linear elements, interconnected adjustable weights, play prominent role machine learning.', 'best implement function device gives outputs prescribed function arbi- trary inputs.', 'chapter describe networks non-linear elements implement input-output functions trained supervised learning methods.', 'output 1 0 depending weighted sum inputs greater equal threshold value, θ.']\n",
      "\n",
      "['n + 1)-st component, xn+1, augmented feature vector, y, value 1; (n + 1)-st component, wn+1, augmented weight vector, v, set equal negative desired threshold value. (', 'weighted sum calculated tlu simply represented vector dot product, x•w. (', 'x1 x2 xn+1 = 1 xi w1 w2 wn+1 wi wn x threshold weight xn w threshold \" = 0 f f = thresh( !', 'if pattern weight vectors thought “column” vectors, dot product written xtw, “row” vector xt transpose x.) often, threshold, θ, tlu ﬁxed 0; case, arbitrary thresholds achieved (n + 1)- dimensional “augmented” vectors, y, v, ﬁrst n components x w, respectively. (', 'weights tlu represented n-dimensional weight vector, w = (w1, . . . ,', 'when want emphasize use augmented vectors, we’ll use y,v notation; context discussion makes clear sort vectors talking about, we’ll lapse familiar x,w notation.)', 'unit vector normal hyperplane n = w |w|, |w| = p (w2 1 + . . .', 'hyperplane boundary patterns x•w + wn+1 > 0 patterns x•w + wn+1 < 0.', 'wi xi, 0) = 1 n+1 figure 4.1 threshold logic unit (tlu) n-dimensional feature input vector denoted x = (x1, . . . ,']\n",
      "\n",
      "['threshold logic units 37 form hyperplane equation x•n + w |w| = 0.)', 'distance hyperplane origin negative (that is, wn+1 < 0), origin negative hyperplane (that is, x•w + wn+1 < 0).', 'weight +1 input corresponding positive literal, weight −1 input corresponding negative literal. (', 'distance hyperplane origin wn+1 |w| , distance arbitrary point, x, hyperplane x•w+wn+1 |w| .', '4.1.2 special cases linearly separable functions terms term size k implemented tlu weight inputs corresponding variables occurring term.', 'x.w + wn+1 > 0 w x w n = w |w| origin unit vector normal hyperplane w + wn+1 = 0 x n + = 0 x equations hyperplane wn+1 |w| wn+1 w + wn+1 x x.w + wn+1 < 0 figure 4.2 tlu geometry adjusting weight vector, w, changes orientation hyperplane; adjusting wn+1 changes position hyperplane (relative origin).', 'threshold, θ, set equal kp −1/2, kp number positive literals term.']\n",
      "\n",
      "['present family incremental training procedures parameter c. methods adjustments weight vector tlu trained makes error training pattern; called error-correction procedures.', 'example, negation clause f = x1 + x2 + x3 term f = x1 x2 x3.', '1,1,1) (1,1,0) x2 x1 x3 f = x1x2 x1 + x2 - 3/2 = 0 equation plane is figure 4.3 implementing term clauses negation clause term.', 'a. start ﬁnite training set, ξ, vectors, yi , binary labels.']\n",
      "\n",
      "['note adjustment new dot product (v − ciyi)•yi = v•yi −ciyi•yi, smaller weight adjustment. (', 'threshold logic units 39 f = x1 + x2 + x3 x1 x1 + x2 + x3 < 1/2 = 0 f = x1x2x3 equation plane is x2 x3 figure 4.4 implementing clause b. compose inﬁnite training sequence, σ, vectors ξ labels member ξ occurs inﬁnitely σ.', 'b) yi supposed produce output 0 produces output 1 instead, modify weight vector follows v ←−v −ciyi ci positive real number called learning rate parame- ter (whose value diﬀererent diﬀerent instances family procedures depend i).', 'c) yi supposed produce output 1 produces output 0 instead, modify weight vector follows v ←−v + ciyi case, new dot product (v + ciyi)•yi = v•yi + ciyi•yi, larger weight adjustment.', 'c. repeat forever present vector, yi, σ tlu note response. (']\n",
      "\n",
      "['weight points half-spaces deﬁned hyperplane cause corresponding pattern yield dot product 0, weight points half- space cause corresponding pattern yield dot product greater 0.', 'note weight vector v includes wn+1 thresh- old component, threshold tlu changed adjust- ments.', '2) fractional-correction procedure, parameter ci set λ yi•v yi•yi , v weight vector changed.', 'use augmented vectors discussion threshold function compares dot product, yi•v, threshold 0.', '4.1.4 weight space intuitive idea procedures work considering happens augmented weight vector “weight space” corrections made.', 'neural networks v ←−v + ci(di −fi)yi di desired response (1 0) yi , fi actual response (1 0) yi.]', 'identify versions procedure 1) ﬁxed-increment procedure, learning rate parameter, ci, ﬁxed, positive constant i. depending value constant, weight adjustment correct response erroneously classiﬁed feature vector.', 'particular weight vector, v, corresponds point (n + 1)-dimensional weight space.', 'maass & tur´an, 1994] hyperplane-ﬁnding procedure makes o(n2 log n) mistakes.', 'now, pattern vector, yi, consider locus points weight space corresponding weight vectors yielding yi•v = 0.', 'proved weight vector, v, produces correct output feature vectors ξ, ﬁnite number feature vector presentations, ﬁxed-increment procedure ﬁnd weight vector weight changes.']\n",
      "\n",
      "['2 3 4 1 v 0 1 1 2 3 2 3 4 figure 4.6 solution region weight space', 'suppose wanted weight values positive responses patterns y1, y3, y4, negative response pattern y2.', 'threshold logic units 41 y2, y3, y4, respectively, indicate arrow half-space weight vectors dot products greater 0.', '2 3 4 1 v figure 4.5 weight space exists weight vector gives desired responses given set patterns given geometric interpretation.']\n",
      "\n",
      "['the boxed numbers show, later purposes, number errors weight vectors regions.)', 'subsequent corrections overshoot solution region, eventually work way far solution region corrections (for ﬁxed increment size) it.', 'starting v1, gives incorrect response pattern y1, v1 v2 direction normal plane 1. (', 'solution region “hyper-wedge” region vertex origin weight space cross-section increases increasing distance origin.', 'neural networks weight vector exists correctly classiﬁes set patterns, half-spaces deﬁned correct responses patterns non- intersection, called solution region.', '2 3 4 1 v v1 v2 v3 v4 v5 v6 figure 4.7 moving solution region 4.1.5 widrow-hoﬀprocedure widrow-hoﬀprocedure (also called lms delta procedure) at- tempts ﬁnd weights minimize squared-error function pat- tern labels dot product computed tlu.', 'ﬁxed-increment error-correction procedure changes weight vector moving normal pattern hyperplane weight vector gives incorrect response.', 'suppose example present patterns sequence y1, y2, y3, y4, start process weight point v1, shown fig.']\n",
      "\n",
      "['total squared error (over patterns training set, ξ, containing m patterns) then ε = m x i=1 (di − n+1 x j=1 xijwj)2 want choose weights wj minimize squared error.', 'entire weight vector (in augmented, v, notation) adjusted according following rule v ←−v + ci(di −fi)yi where, before, yi i-th augmented pattern vector.', 'way ﬁnd set weights start arbitrary weight vector negative gradient ε function weights.', 'often, preferable use incremental procedure try tlu element, xi, ξ time, compute gradient single- pattern squared error, εi, appropriate adjustment weights, try member ξ.', 'ε quadratic wj, know global minimum, steepest descent procedure guaranteed ﬁnd minimum.', 'widrow-hoﬀprocedure makes adjustments weight vector when- dot product itself, yi•v, equal speciﬁed desired target', 'threshold logic units 43 squared error pattern, xi, label di (for desired output) is εi = (di − n+1 x j=1 xijwj)2 xij j-th component xi.', 'j-th component gradient single-pattern error is ∂εi ∂wj = −2(di − n+1 x j=1 xijwj)xij adjustment direction negative gradient change weight follows wj ←−wj + ci(di −fi)xij fi = pn+1 j=1 xijwj, ci governs size adjustment.']\n",
      "\n",
      "['finding weight values desired dot products corresponds solv- ing set linear equalities, widrow-hoﬀprocedure interpreted descent procedure attempts minimize mean-squared-error be- tween actual desired values dot product. (', 'described [hertz, krogh, & palmer, 1991, p. 160] . . .', '4.1.6 training tlu non-linearly-separable training sets training set linearly separable (perhaps noise inherently), desired ﬁnd “best” separating hy- perplane.', 'mean-squared-error criterion gives un- satisfactory results, however, prefers small errors large ones.', 'examples training curves tlu’s; performance training set; performance test set; cumulative number corrections.', 'typically, error-correction procedures non- linearly-separable training sets continue attempt correct inevitable errors, hyperplane settle acceptable place.', '4.2 linear machines natural generalization (two-category) tlu r-category classiﬁer structure, shown fig.', 'alternative, error correction continuous decrease zero value learning rate constant, c, result decreasing changes hyperplane.', 'consists simply storing (or “putting pocket”) set weights longest un- modiﬁed run successes far.', 'first, use widrow-hoﬀprocedure, (although converge zero error non-linearly separable problems) weight vector min- imizes mean-squared-error.', 'duda [duda, 1966] suggested keeping track average value weight vector error correction average separating hyperplane performs reasonably non-linearly-separable problems.']\n",
      "\n",
      "['linear machines 45 familiar notation, ws x meant augmented vectors (with (n+1)-st component).', '4.9 shows character regions 2-dimensional space created linear machine r = 5.', 'r1 r3 r4 r5 x.w4 * x.wi & 4 r2 region figure 4.9 regions linear machine train linear machine, straightforward generalization 2-category error-correction rule.', 'y y argmax w1.x wr.x figure 4.8 linear machine diagram fig.', 'note r = 2, linear machine reduces tlu weight vector w = (w1 −w2).']\n",
      "\n",
      "['4.3 networks tlus 4.3.1 motivation examples layered networks classify correctly patterns non-linearly-separable training sets re- quires separating surfaces complex hyperplanes.', 'correction increases value u-th dot product decreases value v-th dot product.', 'tlus feedforward network arranged layers, elements layer j receiving inputs tlus layer j −1, network layered, feedforward', 'ﬁgure, weight values input lines tlu threshold value inside circle representing tlu.', 'single line 2-dimensional square separate vertices (1,1) (0,0) vertices (1,0) (0,1)—the function linearly separable im- plemented single tlu.', '2-category ﬁxed increment proce- dure, procedure guaranteed terminate, constant ci, exists weight vectors correct separations training set.', 'feedforward networks cycles; feedforward network tlu’s input depends (through zero intermediate tlus) tlu’s output. (', 'b. machine mistakenly classiﬁes category u pattern, xi, category v (u ̸= v), then wu ←−wu + cixi wv ←−wv −cixi weight vectors changed.', 'consider, example, 2- dimensional, parity function, f = x1x2 + x1 x2.']\n",
      "\n",
      "['networks tlus 47 f x1 x2 1.5 -0.5 0.5 1 1 -1 -1 1 1 figure 4.10 network parity function network.', 'x hidden units output units figure 4.11 layered, feedforward network implementing dnf functions two-layer networks deﬁned k-term dnf functions—they dnf functions having k terms.', 'some people count layers tlus include inputs layer also; network three-layer network.)', 'k-term dnf function implemented two-layer network k units hidden layer—to implement k terms—and output unit implement disjunction terms.']\n",
      "\n",
      "['x conjuncts disjunct feedforward, 2-layer network tlus disjunction terms conjunctions literals (terms) figure 4.12 two-layer network x2 x1 x3 f = x1x2 + x2x3 + x1x3 figure 4.13 planes implemented hidden units train two-layer network implements k-term dnf function, ﬁrst note output unit implements disjunction, weights ﬁnal layer ﬁxed.', 'discuss half-space intersections, half-space unions, np-hardness optimal versions, single-side-error-hypeplane methods, relation “aq” methods.']\n",
      "\n",
      "['leave reader think training procedure modiﬁed output tlu implemented function (or function).', '4.3.2 madalines two-category networks interesting example layered, feedforward network two-layer odd number hidden units, “vote-taking” tlu output unit.', 'ﬁrst layer partition feature space way, regardless subse- quent layers do, ﬁnal outputs consistent labeled training set.', 'hidden units voting incorrectly, change weight vectors ki dot products closest 0 error correction rule w ←−w + ci(di −fi)xi di desired response hidden unit (0 1) fi actual response (0 1). (', 'networks tlus 49 important comment layered networks adding additional layers compensate inadequate ﬁrst layer tlus.', 'is, perform error-correction hidden units correct vote majority voting correctly, change easiest change.', 'typically, response vote taking unit deﬁned response majority hidden units, output logics possible.', 'ridgway [ridgway, 1962] proposed following error-correction rule adjusting weights hidden units madaline • madaline correctly classiﬁes pattern, xi, corrections hidden units’ weight vectors, • madaline incorrectly classiﬁes pattern, xi, determine minimum number hidden units responses need changed (from 0 1 1 0—depending type error) order madaline correctly classify xi.', 'example problems set weight values exists given madaline structure classify members training set correctly, procedure fail ﬁnd them.', 'ﬁrst layer tlus partitions feature space dif- ferently labeled vectors region (that is, vectors yield set outputs ﬁrst-layer units).']\n",
      "\n",
      "['similarly, r-category training set linearly separable exists linear machine correctly classiﬁes members training set.', 'neural networks r-category madalines error-correcting output codes k hidden units (k > 1) two-layer network, responses correspond vertices k-dimensional hypercube.', 'wr wr arg max 1 r 1 n1 1 nr figure 4.14 piecewise linear machine', 'ordinary two-category madaline identiﬁes special points space, vertex consisting k 1’s vertex consisting k 0’s.', 'design r-category madaline identifying r vertices hidden-unit space classifying pattern according vertices hidden-unit response closest to.', '4.3.3 piecewise linear machines two-category training set linearly separable exists threshold func- tion correctly classiﬁes members training set.']\n",
      "\n",
      "['tlu network implements set 2k parallel hyperplanes, k number tlus receives inputs. (', 'reader consider n-dimensional parity function implemented cascade network having log2 n tlus.', 'unfortunately, example training sets separable given pwl machine structure error-correction training method fails ﬁnd solution.', 'method appear work situations [duda & fossum, 1966], al- [nilsson, 1965, page 89] observed “it probably eﬀective method training pwl machines having [weight vectors] bank.”', 'each k preceding tlus output 1 0; that’s 2k diﬀerent combinations—resulting 2k diﬀerent positions parallel hyperplanes.)', '4.3.4 cascade networks interesting class feedforward networks tlus ordered tlu receives inputs pattern components tlus lower ordering.', 'pattern classiﬁed incorrectly, subtract (a constant times) pattern vec- tor weight vector producing largest dot product (it incorrectly largest) add (a constant times) pattern vector weight vector correct bank weight vectors dot product locally largest bank. (', 'networks tlus 51 pwl machine groups weighted summing units r banks corre- sponding r categories.']\n",
      "\n",
      "['intuitively, looks weight-adjusting procedures network correct direction (relative error) making minimal changes.', 'network output unit, but, course, possible tlus output layer—each implementing diﬀerent function.', 'we assume “threshold weight” component associated weight vector; v notation instead include', 'neural networks l1 l2 l2 figure 4.16 planes implemented cascade network tlus cascade networks trained ﬁrst training l1 good job possible separating training patterns (perhaps pocket algorithm, example), training l2 (including weight l1 l2) good job possible separating training patterns, resulting network classiﬁes patterns training set satisfactorily.', '4.4 training feedforward networks back- propagation 4.4.1 notation general problem training network tlus diﬃcult.', 'input feature vector denoted x(0), ﬁnal output (of k-th layer tlu) f. tlu layer weight vector (connecting inputs) threshold; i-th tlu j-th layer weight vector denoted w(j) . (']\n",
      "\n",
      "['x(0) . . . . . . . . . . . .', 'desired response, di, i-th input vector, xi, training set, ξ, compute squared error entire training set be ε = x xi ϵ ξ (di −fi)2 fi actual response network input xi.', 'training feedforward networks backpropagation53 threshold component, chosen use familiar x,w notation, assuming vectors “augmented” appropriate.)', 'squared error single input vector, x, evoking output f desired output d is', 'wi(k-1) x(k-1) mj tlus m(k-1) tlus wli(j) wl(k) layer j-th layer (k-1)-th layer k-th layer . . .', 'vector w(j) components w(j) l,i l = 1, . . . ,', 'gradient descent squared error, adjust weight network proportional negative partial derivative ε respect weight.', 'denote weighted sum input i-th threshold unit j-th layer s(j) . (', 'f si(1) si(j) si(k-1) s(k) figure 4.17 k-layer network 4.4.2 backpropagation method gradient descent method, similar widrow hoﬀmethod, proposed authors training multi-layer, feedforward network.', 'before, deﬁne error function ﬁnal output network adjust weight network minimize error.']\n",
      "\n",
      "['vector partial derivative φ called gradient φ respect w denoted ∇wφ.', 'δ(j) tells sensitive squared error network output changes input threshold function.', 'ε’s dependence w(j) entirely s(j) , use chain rule write ∂ε ∂w(j) = ∂ε ∂s(j) ∂s(j) ∂w(j) s(j) = x(j−1)•w(j) , ∂s(j) ∂w(j) = x(j−1).', 'changing weight vectors directions negative gradient, fundamental rule weight changes network be w(j) ←w(j) + c(j) δ(j) x(j−1) c(j) learning rate constant weight vector. (', 'neural networks ε = (d −f)2 convenient partial derivatives ε respect weights groups corresponding weight vectors.', 'thus, ∂ε ∂w(j) = −2(d −f) ∂f ∂s(j) x(j−1) quantity (d−f) ∂f ∂s(j) plays important role calculations; shall denote δ(j) .', 'substituting yields ∂ε ∂w(j) = ∂ε ∂s(j) x(j−1) note ∂ε ∂s(j) = −2(d −f) ∂f ∂s(j) .', 'deﬁne partial derivative quantity φ, say, respect weight vector, w(j) , thus ∂φ ∂w(j) def = \" ∂φ ∂w(j) 1i , . . . ,']\n",
      "\n",
      "['sigmoid threshold function f (s) s f (s) = 1/[1 + e<s] figure 4.18 sigmoid function 1[russell & norvig 1995, page 595] attributes use idea [bryson & ho 1969].', 'small changes sums change f all, f change, changes abruptly 1 0 vice versa.', 'now, turn attention calculation δ(j) ’s. deﬁnition, have δ(j) = (d −f) ∂f ∂s(j) problem, however, attempting carry partial deriva- tives f respect s’s.', 'trick involves replacing threshold functions diﬀerentiable functions called sigmoids.1 output sigmoid function, superimposed threshold function, shown fig.', 'way diﬃculty proposed werbos [werbos, 1974] (perhaps independently) pursued researchers, example [rumelhart, hinton, & williams, 1986].', 'usually, sigmoid function f(s) = 1 1+e−s , s input f output.']\n",
      "\n",
      "['x(0) . . . . . . . . . . . .', 'wi(k-1) fi(k-1) si(k-1) f(k) s(k) x(k-1) mj sigmoids m(k-1) sigmoids wli(j) wl(k) bi(j) bi(1) bi(k-1) b(k) layer j-th layer (k-1)-th layer k-th layer . . .', 'output i-th sigmoid unit j-th layer denoted f (j) . (', 'figure 4.19 network sigmoid units 4.4.3 computing weight changes final layer ﬁrst calculate δ(k) order compute weight change ﬁnal sigmoid unit']\n",
      "\n",
      "['backpropagation weight adjustment single element ﬁnal layer written as w ←−w + c(d −f)f(1 −f)x written format, error-correction rule is w ←−w + c(d −f)x widrow-hoﬀrule is w ←−w + c(d −f)x diﬀerence (except fact f thresholded widrow- hoﬀ) f(1 −f) term presence sigmoid function.', 'f 0, f(1 −f) 0; f 1, f(1 −f) 0; f(1 −f) obtains maximum value 1/4 f 1/2 (that is, input sigmoid 0).', 'weight changes region “fuzz” surrounding hyperplane, changes direction correcting error, error-correction widrow-hoﬀrules.', 'pattern far away fuzzy hyperplane, f(1 −f) value close 0, backpropagation rule makes little change weight values regardless desired output. (', 'substituting gives us δ(k) = (d −f (k))f (k)(1 −f (k)) rewriting general rule weight vector changes, weight vector ﬁnal layer changed according rule w(k) ←w(k) + c(k)δ(k)x(k−1) δ(k) = (d −f (k))f (k)(1 −f (k)) interesting compare backpropagation error-correction rule widrow-hoﬀrule.', 'training feedforward networks backpropagation57 δ(k) = (d −f (k))∂f (k) ∂s(k) given sigmoid function using, f(s) = 1 1+e−s , ∂f ∂s = f(1 −f).']\n",
      "\n",
      "['so δ(j) = (d −f) ∂f ∂s(j) = (d −f) \" ∂f ∂s(j+1) 1 ∂s(j+1) 1 ∂s(j) + · · · + ∂f ∂s(j+1) l ∂s(j+1) l ∂s(j) + · · · + ∂f ∂s(j+1) mj+1 ∂s(j+1) mj+1 ∂s(j) # = mj+1 x l=1 (d −f) ∂f ∂s(j+1) l ∂s(j+1) l ∂s(j) = mj+1 x l=1 δ(j+1) l ∂s(j+1) l ∂s(j) remains compute ∂s(j+1) l ∂s(j) ’s. ﬁrst write s(j+1) l = x(j)•w(j+1) l = mj+1 x ν=1 f (j) ν w(j+1) νl then, weights depend s’s ∂s(j+1) l ∂s(j) = ∂ hpmj+1 ν=1 f (j) ν w(j+1) νl ∂s(j) = mj+1 x ν=1 w(j+1) νl ∂f (j) ν ∂s(j) now, note ∂f (j) ν ∂s(j) = 0 ν = i, case ∂f (j) ν ∂s(j) ν = f (j) ν (1 −f (j) ν ).', 'therefore ∂s(j+1) l ∂s(j) = w(j+1) il f (j) (1 −f (j) )', 'ﬁnal output, f, depends s(j) summed inputs sigmoids (j + 1)-th layer.', 'neural networks 4.4.4 computing changes weights intermediate layers expression δ’s, similarly compute change weight vectors network.']\n",
      "\n",
      "['calculations simply implemented “backpropagating” δ’s weights reverse direction (thus, backprop algorithm).', 'quantity δ(k) = (d −f)f(1 −f) controls overall sign weight adjustments network. (', 'fall early error-function valley deep (a local minimum), typically broad, soon', 'average eﬀect depends weights going sigmoid unit j-th layer (small weights produce little downstream eﬀect) eﬀects changes outputs (j + 1)-th layer sigmoid units ﬁnal output (as measured δ(j+1)’s).', 'having computed δ(j+1) layer j + 1, use equation compute δ(j) ’s. base case δ(k), computed δ(k) = (d −f (k))f (k)(1 −f (k)) use expression δ’s generic weight changing rule, namely w(j) ←w(j) + c(j) δ(j) x(j−1) rule appears complex, intuitively reasonable explanation.', 'it interesting note expression independent error function; error function explicitly aﬀects computation δ(k).)', 'adjustments diminish ﬁnal output, f, approaches 0 1, vanishing eﬀect f then.)', 'recursion equation δ’s shows, adjustments weights going sigmoid unit j-th layer proportional eﬀect adjustments sigmoid unit’s output (its f (j)(1−f (j)) factor).', 'training feedforward networks backpropagation59 use result expression δ(j) give δ(j) = f (j) (1 −f (j) ) mj+1 x l=1 δ(j+1) l w(j+1) il equation recursive δ’s. (', '1986, [hertz, krogh, & palmer, 1991]), quickprop, regulariza- tion methods] simulated annealing apply simulated annealing, value learning rate constant gradually decreased time.', '4.4.5 variations backprop [to written problem local minima, simulated annealing, momemtum (plaut, et al.,']\n",
      "\n",
      "['sharp left sharp right straight ahead centroid outputs steers vehicle figure 4.20 alvinn network network ﬁve hidden units ﬁrst layer 30 output units second layer; sigmoid units.', 'unit near array output units higher output units, van steered left; unit near array high output, van steered right. “', '4.4.6 application steering van neural network system called alvinn (autonomous land vehicle neural network) trained steer chevy van successfully ordinary roads highways speeds 55 mph [pomerleau, 1991, pomerleau, 1993].', 'process gets analogy annealing metallurgy, material’s temperature gradually decreased allowing crystalline structure reach minimal energy state.', '5 hidden units connected 960 inputs 30 output units connected hidden units . . .', 'likely deep valleys, end process (with small values learning rate constant), descend deepest point.']\n",
      "\n",
      "['4.5 synergies neural network knowledge-based methods written; discuss rule-generating procedures (such [towell & shavlik, 1992]) expert-provided rules aid neural net training vice-versa [towell, shavlik, & noordweier, 1990].', 'also, long, straight stretches road, network trained long time produce straight-ahead steering angles; training swamp earlier training follow curved road.', 'network trained incrementally backprop produce driver-speciﬁed steering angles response visual pattern occurs real time driving.', 'driver drives van, actual steering angles taken correct labels corresponding inputs.', 'synergies neural network knowledge-based methods61 units computed, van’s steering angle set corresponding value hard left hard right.', 'wouldn’t want try avoid problems instructing driver drive erratically occasionally, system learn mimic erratic behavior.', 'instead, original image shifted rotated software create 14 additional images vehicle appears situated diﬀerently relative road.', 'model tells system steering angle ought shifted images, given driver-speciﬁed steering angle original image, system constructs additional 14 labeled training patterns add encountered ordinary driver training.', 'first, driver usually driving well, network experience far-from-center vehicle positions and/or incorrect vehicle orien- tations.']\n",
      "\n",
      "[]\n",
      "\n",
      "['given pattern, x, want use statistical tech- niques determine category—that is, determine distribution drawn.', 'chapter 5 statistical learning 5.1 statistical decision theory 5.1.1 background general method suppose pattern vector, x, random variable probability distri- bution category 1 diﬀerent category 2. (', 'describe information loss function, λ(i | j), i, j = 1, 2.', 'given pattern, x, want decide category way minimizes expected value loss.', 'λ(i | j) represents loss incurred decide pattern category category j. assume λ(1 | 1) λ(2 | 2) 0.', 'speciﬁcally, suppose probability distributions (perhaps probability density functions), p(x | 1) p(x | 2).', 'developing decision method, necessary know relative serious- ness kinds mistakes made. (', 'techniques based idea minimizing ex- pected value quantity similar error function deriving weight-changing rules backprop.', 'given pattern, x, decide category i, expected value loss be lx(i) = λ(i | 1)p(1 | x) + λ(i | 2)p(2 | x) p(j | x) probability given pattern x, category j. decision rule decide x belongs category 1 lx(1) ≤lx(2), decide category 2 otherwise.']\n",
      "\n",
      "['generally, deﬁne k(i | j) λ(i | j)p(j), decision rule simply, decide category1 iﬀ k(1 | 2)p(x | 2) ≤k(2 | 1)p(x | 1) case, need compare (perhaps weighted) quantities p(x | i) = 1 2.', 'statistical learning use bayes’ rule expressions p(j | x) terms p(x | j), assume known (or estimatible) p(j | x) = p(x | j)p(j) p(x) p(j) (a priori) probability category j (one category probable other); p(x) (a priori) probability pattern x pattern asked classify.', 'performing substitutions given bayes’ rule, decision rule becomes decide category 1 iﬀ λ(1 | 1)p(x | 1)p(1) p(x) + λ(1 | 2)p(x | 2)p(2) p(x) ≤λ(2 | 1)p(x | 1)p(1) p(x) + λ(2 | 2)p(x | 2)p(2) p(x) fact λ(i | i) = 0, noticing p(x) common expressions, obtain, decide category 1 iﬀ λ(1 | 2)p(x | 2)p(2) ≤λ(2 | 1)p(x | 1)p(1) λ(1 | 2) = λ(2 | 1) p(1) = p(2), decision particu- larly simple decide category 1 iﬀ p(x | 2) ≤p(x | 1) p(x | j) called likelihood j respect x, simple decision rule implements called maximum-likelihood decision.']\n",
      "\n",
      "['statistical decision theory 65 5.1.2 gaussian (or normal) distributions multivariate (n-dimensional) gaussian distribution given proba- bility density function p(x) = 1 (2π)n/2|σ|1/2 e −(x−m)tς −1 (x−m) 2 n dimension column vector x, column vector m called mean vector, (x −m)t transpose vector (x −m), σ covariance matrix distribution (an n × n symmetric, positive deﬁnite matrix), σ−1 inverse covariance matrix, |σ| determinant covariance matrix.', 'general, formula exponent gaussian distribution positive deﬁnite quadratic form (that is, value positive); equi-probability contours hyper-ellipsoids n-dimensional space.', 'components covariance matrix given by σ2 ij = e[(xi −mi)(xj −mj)] particular, σ2 ii called variance xi.', 'covariance matrix diagonal, oﬀ-diagonal terms 0, major axes elliptical contours aligned coordinate axes.', 'formula appears complex, intuitive idea gaussian dis- tributions given n = 2.', 'suppose assume classes pattern vectors want distinguish distributed according gaussian distribution diﬀerent means covariance matrices.', 'is, class tends patterns clustered point n-dimensional space, class tends patterns clustered point.', 'three-dimensional plot distribution shown ﬁgure, contours equal probability shown bot- tom.']\n",
      "\n",
      "['statistical learning -5 0 5 -5 0 5 0 0.25 0.5 0.75 1 -5 0 5 -5 0 5 0 25 .5 75 1 -6 -4 -2 0 2 4 6 -6 -4 -2 0 2 4 6 x1 x2 p(x1,x2) 2 4 6 2 4 6 x1 x2 figure 5.1 two-dimensional gaussian distribution decide category 1 iﬀ 1 (2π)n/2|σ2|1/2 e−1/2(x−m2)tς −1 2 (x−m2) equal 1 (2π)n/2|σ1|1/2 e−1/2(x−m1)tς −1 1 (x−m1) category 1 patterns distributed mean covariance m1 σ1, respectively, category 2 patterns distributed mean covariance m2 σ2.']\n",
      "\n",
      "['statistical decision theory 67 -5 0 5 10 -5 0 5 10 0 0.25 0.5 0.75 1 -5 0 5 10 -5 0 5 10 0 25 .5 75 1 x1 x2 p(x1,x2) -5 -2.5 0 2.5 5 7.5 10 -5 -2.5 0 2.5 5 7.5 10 figure 5.2 sum gaussian distributions decide category 1 iﬀ (x −m1)tς−1 1 (x −m1) < (x −m2)tς−1 2 (x −m2) + b b, constant bias term, incorporates logarithms fractions preceding exponential, etc.', 'surface separates space parts, contains points assigned category 1 contains points assigned category 2.', 'covariance matrices category identical diagonal, σii equal other, contours equal probability distributions', 'quadratic forms multiplied represented terms components xi, decision rule involves quadric surface (a hyperquadric) n-dimensional space.']\n",
      "\n",
      "['caution sample covariance matrix singular training patterns happen lie subspace n-dimensional space—as certainly will, example, number training patterns n.) 5.1.3 conditionally independent binary components suppose vector x random variable having binary (0,1) components.', 'quadric forms (1/|σ|)(x−mi)t(x−mi), decision rule is decide category 1 iﬀ (x −m1)t(x −m1) < (x −m2)t(x −m2) multiplying yields x•x −2x•m1 + m1•m1 < x•x −2x•m2 + m2•m2 ﬁnally, decide category 1 iﬀ x•m1 ≥x•m2 + constant x•(m1 −m2) ≥constant constant depends lengths mean vectors.', 'parameters (mi, σi) probability distributions categories known, techniques estimating them, estimates decision rule.', 'example, suﬃcient training patterns, use sample means sample covariance matrices. (']\n",
      "\n",
      "['p(xn | 1) p(x1 | 2)p(x2 | 2) . . .', 'p(xn | 2) ≥p(2) p(1) iﬀ log p(x1 | 1) p(x1 | 2) + log p(x2 | 1) p(x2 | 2) + · · · + log p(xn | 1) p(xn | 2) + log p(1) p(2) ≥0 let deﬁne values components distribution speciﬁc values arguments, xi p(xi = 1 | 1) = pi p(xi = 0 | 1) = 1 −pi p(xi = 1 | 2) = qi p(xi = 0 | 2) = 1 −qi now, note xi assume values 1 0 log p(xi | 1) p(xi | 2) = xi log pi qi + (1 −xi) log (1 −pi) (1 −qi)', 'statistical decision theory 69 p(x | i) = p(x1 | i)p(x2 | i) · · · p(xn | i) = 1, 2 recall minimum-average-loss decision rule, decide category 1 iﬀ λ(1 | 2)p(x | 2)p(2) ≤λ(2 | 1)p(x | 1)p(1) assuming conditional independence components λ(1 | 2) = λ(2 | 1), obtain, decide category 1 iﬀ p(1)p(x1 | 1)p(x2 | 1) · · · p(xn | 1) ≥p(x1 | 2)p(x2 | 2) · · · p(xn | 2)p(2) iﬀ p(x1 | 1)p(x2 | 1) . . .']\n",
      "\n",
      "['relatively large values k decreases chance decision unduly inﬂuenced noisy training pattern close x. large values k reduce acuity method.', 'precisely, k-nearest-neighbor method assigns new pattern, x, category plurality k closest neighbors belong.', 'statistical learning = xi log pi(1 −qi) qi(1 −pi) + log (1 −pi) (1 −qi) substituting expressions decision rule yields decide category 1 iﬀ n x i=1 xi log pi(1 −qi) qi(1 −pi) + n x i=1 log (1 −pi) (1 −qi) + log p(1) p(2) ≥0 achieve decision tlu weight values follows wi = log pi(1 −qi) qi(1 −pi) = 1, . . . ,', 'n, wn+1 = log p(1) 1 −p(1) + n x i=1 log (1 −pi) (1 −qi) know pi, qi p(1), use sample labeled training patterns estimate parameters.', 'k-nearest-neighbor method thought estimating values probabilities classes given x. course denser points x, larger value k, better estimate.', 'given training set ξ m labeled patterns, nearest-neighbor procedure decides new pattern, x, belongs category closest neighbors ξ.']\n",
      "\n",
      "['also, distance calculations required ﬁnd nearest neighbors eﬃciently computed kd-tree methods [friedman, et al.,', 'theorem cover hart [cover & hart, 1967] relates performance 1-nearest-neighbor method performance minimum-probability- of-error classiﬁer.', 'mentioned earlier, minimum-probability-of-error clas- siﬁer assign new pattern x category maximized p(i)p(x | i), p(i) priori probability category i, p(x | i) probability (or probability density function) x given x belongs category i, categories = 1, . . . ,', 'distance measure modiﬁed scaling features spread attribute values dimension approximately same.', 'case, distance vectors qpn j=1 a2 j(x1j −x2j)2, aj scale factor dimension j. example nearest-neighbor decision problem shown fig.', 'k = 8 x (a pattern classified) 1 1 1 1 1 1 1 1 2 1 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 training pattern class training pattern patterns category 1 patterns category 2 patterns category 3 plurality category 1, decide x category 1 figure 5.3 8-nearest-neighbor decision [baum, 1994] theoretical analysis error rate function number training patterns case points randomly distributed surface unit sphere underlying function linearly separable.', 'nearest-neighbor methods 71 distance metric nearest-neighbor methods (for numerical at- tributes) simple euclidean distance.', 'nearest-neighbor methods memory intensive large number training patterns stored achieve good generalization.']\n",
      "\n",
      "['statistical learning probability density functions) probability error, εnn, 1-nearest- neighbor classiﬁer bounded by ε ≤εnn ≤ε \\x12 2 −ε r r −1 \\x13 ≤2ε r number categories. [']\n",
      "\n",
      "['chapter 6 decision trees 6.1 deﬁnitions decision tree (generally deﬁned) tree internal nodes tests (on input patterns) leaf nodes categories (of patterns).', 'dimensions decision trees diﬀer a. tests multivariate (testing features input once) univariate (testing features).', 'decision tree assigns class number (or output) input pattern ﬁltering pattern tests tree.', '6.1 outcomes; left-most assigns input pattern class 3, middle sends input pattern test t4, right-most assigns pattern class 1.', 'calls subsets patterns ﬁlter tip categories subsets patterns having label classes.', 'distinction, however, use words “category” “class” interchangeably refer quinlan calls “class.”', 'follow usual convention depicting leaf nodes class number.1 note discussing decision trees limited implementing boolean functions—they useful general, categorically valued functions.']\n",
      "\n",
      "['prominent id3 new version, c4.5 [quinlan, 1986, quinlan, 1993], cart [breiman, et al.,', 'classes binary inputs, tree implements boolean function, called boolean decision tree.', 'k-dl class boolean functions implemented multivariate decision tree having (highly unbalanced) form shown fig.', 'decision trees t1 t2 t3 t4 t4 t4 3 1 3 2 1 2 3 2 1 figure 6.1 decision tree d. classes two.', 'attribute value 0 input pattern, branch left; value 1, branch right.', 'dnf form implemented tree obtained tracing path leading tip node corresponding output value 1, forming conjunction tests path, taking disjunction conjunctions.', '6.2 supervised learning univariate decision trees systems learning decision trees proposed.']\n",
      "\n",
      "['entropy uncertainty remaining class pattern— knowing set, ξ, patterns deﬁned as h(ξ) = − x p(i|ξ) log2 p(i|ξ)', 'extension multiple-outcome tests straightforward computation- ally gives poor results entropy decreased having outcomes.', 'supervised learning univariate decision trees 75 x3 x2 x4 x1 1 0 1 1 0 0 0 1 x3x2 x3x2 x3x4 x3x4x1 x3x4x1 f = x3x2 + x3x4x1 1 0 0 1 0 figure 6.2 decision tree implementing dnf function 6.2.1 selecting type test usual, n features attributes.', '6.2.2 uncertainty reduction select tests main problem learning decision trees binary-attribute case selecting order tests.', 'attributes categorical, non-binary, tests formed dividing attribute values mutually exclusive exhaustive subsets.']\n",
      "\n",
      "['want select tests node travel decision tree, uncertainty class pattern less.', 'suppose ni patterns ξ ξi = 1, ..., k. (some ni 0.)', 'perform test, t, having k possible outcomes patterns ξ, create k subsets, ξ1, ξ2, . . . ,', 'estimate uncertainty is ˆh(ξ) = − x ˆp(i|ξ) log2 ˆp(i|ξ) simplicity, we’ll drop “hats” use sample statistics real probabilities.', 'let ˆp(i|ξ) number patterns ξ belonging class divided total number patterns ξ.', 'decision trees cq cq-1 ci 1 vn vn-1 vi v1 figure 6.3 decision tree implementing decision list p(i|ξ) probability pattern drawn random ξ belongs class i, summation classes.', 'knew t applied pattern ξ resulted j-th outcome (that is, knew pattern ξj), uncertainty class be h(ξj) = − x p(i|ξj) log2 p(i|ξj) reduction uncertainty (beyond knowing pattern ξ) be']\n",
      "\n",
      "['estimate ˆp(ξj) p(ξj) number patterns ξ outcome j divided total number patterns ξ.', 'supervised learning univariate decision trees 77 x3 = a, b, c, d {a, c} {b} x1 = e, b, d {e,b} {d} x4 = a, e, f, g {a, g} {e, f} x2 = a, g {a} {g} 1 2 1 1 2 {d} 2 figure 6.4 decision tree categorical attributes h(ξ) −h(ξj) course test t guaranteed produce reduction uncertainty don’t know result test j-th outcome.', 'average reduction uncertainty achieved test t (applied patterns ξ) then rt (ξ) = h(ξ) −e[ht (ξ)] important family decision tree learning algorithms selects root tree test gives maximum reduction uncertainty, applies criterion recursively termination condition met (which shall discuss detail later).', 'estimate average uncertainty ξj, by e[ht (ξ)] = x j p(ξj)h(ξj) ht (ξ) mean average uncertainty performing test t patterns ξ, p(ξj) probability test outcome j, sum taken 1 k. again, don’t know probabilities p(ξj), use sample values.']\n",
      "\n",
      "['x1 x2 x3 test x1 figure 6.5 patterns classiﬁed decision tree initial uncertainty set, ξ, containing points is h(ξ) = −(6/8) log2(6/8) −(2/8) log2(2/8) = 0.81 next, calculate uncertainty reduction perform x1 ﬁrst.', 'left- hand branch patterns belonging class 0 (we set ξl), right-hand-branch (ξr) patterns class.', 'suppose want use uncertainty-reduction method build decision tree classify following patterns pattern class (0, 0, 0) 0 (0, 0, 1) 0 (0, 1, 0) 0 (0, 1, 1) 0 (1, 0, 0) 0 (1, 0, 1) 1 (1, 1, 0) 0 (1, 1, 1) 1 single test, x1, x2, x3, performed ﬁrst?']\n",
      "\n",
      "['principle, given set labeled patterns, measure uncertainty reduc- tion test achieved possible threshold (there ﬁnite number thresholds diﬀerent test results ﬁnite number training patterns).', 'networks equivalent decision trees 79 hx1(ξl) = −(4/4) log2(4/4) −(0/4) log2(0/4) = 0 uncertainty right-hand branch is hx1(ξr) = −(2/4) log2(2/4) −(2/4) log2(2/4) = 1 half patterns “go left” half “go right” test x1.', 'thus, average uncertainty performing x1 test is 1/2hx1(ξl) + 1/2hx1(ξr) = 0.5 uncertainty reduction ξ achieved x1 is rx1(ξ) = 0.81 −0.5 = 0.31 similar calculations, test x3 achieves exactly uncertainty reduction, x2 achieves reduction whatsoever.', '6.2.3 non-binary attributes attributes non-binary, use uncertainty-reduction tech- nique select tests.', 'similarly, attribute categorical (with ﬁnite number categories), ﬁnite number mutually exclusive exhaustive subsets values attribute split.', 'suppose example value at- tribute real number test performed set threshold test number greater threshold.', '6.3 networks equivalent decision trees univariate boolean decision trees implementations dnf functions, equivalent two-layer, feedforward neural networks.', 'decision tree procedure creates implements boolean function f = x1x3. [']\n",
      "\n",
      "['6.7 linearly separable functions, im- plemented tlu, indicated l1, l2, l3, l4.', 'dif- ferent approaches training procedures discussed [brent, 1990], [john, 1995], (for special case) [marchand & golea, 1993].', 'decision-tree induction methods discussed chapter thought particular ways establish structure weight values networks.', 'x x1 x2 x3 x4 terms -1 +1 disjunction x3x2 x3x4x1 +1 -1 +1 f 1.5 0.5 x3 x2 x4 x1 1 0 1 1 0 0 0 1 x3x2 x3x2 x3x4 x3x4x1 x3x4x1 f = x3x2 + x3x4x1 1 0 0 1 0 figure 6.6 univariate decision tree equivalent network multivariate decision trees linearly separable functions node implemented feedforward networks—in case three-layer ones.', '6.4 overﬁtting evaluation 6.4.1 overﬁtting supervised learning, choose function ﬁt training set set hypotheses.', 'know priori function trying guess belongs small subset possible functions, then, incomplete set training samples, possible reduce subset functions consistent training set suﬃciently useful guesses value function inputs training set.', 'course, implemented network, features evaluated parallel input pattern, implemented decision tree features branch traveled input pattern need evaluated.']\n",
      "\n",
      "['however, bias, training set suﬃciently large compared size hypothesis space, consistent functions useful guesses, generalization performance poor.', 'but, comparing learning systems (for example, comparing diﬀerent decision trees) select performs best test set, comparison amounts “training test data.”', 'true, training test data enlarges training set, consequent ex- pected improvement generalization, danger overﬁtting comparing diﬀerent learning systems.', 'is, decision tree synthesized classify members training set correctly, perform poorly new patterns build decision tree.', '6.4.2 validation methods straightforward way estimate hypothesized function (such decision tree) performs test set test test set!', 'overfitting evaluation 81 l1 l2 l3 l4 1 0 1 1 0 0 0 1 1 0 0 1 0 x l1 l2 l3 l4 conjunctions l1l2 l1 l3 l4 < + + + disjunction < f figure 6.7 multivariate decision tree equivalent network larger training set, likely randomly selected consistent function appropriate outputs patterns seen.', 'decision tree suﬃcient size implement boolean function danger overﬁtting—especially training set small.']\n",
      "\n",
      "['procedure result errors accepting small number errors training set results fewer errors testing set.', 'cross-validation cross-validation, divide training set ξ k mutually exclusive exhaustive equal-sized subsets ξ1, . . . ,', 'subset, ξi, train union subsets, empirically determine error rate, εi, ξi. (', 'the error rate number classiﬁcation errors ξi divided number patterns ξi.)', 'decision trees split training set—using (say) two-thirds training estimating generalization performance.', 'estimate error rate expected new patterns classiﬁer trained patterns ξ average εi.', 'leave-one-out validation leave-one-out validation cross validation special case k equals number patterns ξ, ξi consists single pattern.', '6.4.3 avoiding overﬁtting decision trees near tips decision tree patterns node.', 'general rule lowest error-rate attainable sub-tree fully expanded tree 1/2 error rate fully expanded tree [weiss & kulikowski, 1991, page 126].', 'type validation is, course, expensive computationally, useful accurate estimate error rate classiﬁer needed.']\n",
      "\n",
      "['consider problem transmitting labels training set patterns, assuming receiver information ordered set patterns.', 'number bits transmission require depends technique encoding decision trees size tree.', 'tree small accurately classiﬁes patterns, economical transmit tree transmit labels directly.', '6.4.4 minimum-description length methods important tree-growing pruning technique based minimum- description-length (mdl) principle. (', 'general, number bits (or description length binary encoded message) t + d, t length message required transmit tree, d length message required transmit labels', 'overfitting evaluation 83 (from weiss, s., kulikowski, c., computer systems learn, morgan kaufmann, 1991) training errors validation errors 1 2 3 4 5 6 7 8 9 0.2 0.4 0.6 0.8 1.0 0 0 error rate number terminal nodes iris data decision tree figure 6.8 determining overﬁtting begins stopping growth decision tree, grow size prune away leaf nodes ancestors cross- validation accuracy longer increases.']\n",
      "\n",
      "['sense, tree associated smallest value t + d best economical tree.', '6.4.5 noise data noise data means inevitably accept number errors—depending noise level.', 'dealing noise, then, requires accepting errors leaf nodes fact small number patterns leaf nodes.', 'use description length measure quality tree ways a. growing tree, use reduction description length select tests (instead reduction uncertainty).', 'b. pruning tree grown zero error, prune away nodes (starting tips) achieve decrease description length.', 'dnf-form equivalent function implemented decision tree f = x1x2 + x1x2x3x4 + x1x3x4.', '6.5 problem replicated subtrees decision trees economical means implementing boolean functions.', 'quinlan rivest [quinlan & rivest, 1989] proposed techniques encoding decision trees lists exception labels calculating description length (t+d) trees labels.', 'dnf form non-minimal (in number disjunctions) equivalent f = x1x2 + x3x4.', 'need replication means takes longer learn tree subtrees replicated tree learned smaller training subset.', 'attempt build decision graph instead tree [oliver, dowe, & wallace, 1992, kohavi, 1994].']\n",
      "\n",
      "['problem missing attributes 85 x1 x3 x2 1 0 x4 0 1 x3 0 x4 0 1 figure 6.9 decision tree subtree replication test x3x4, decision tree simpliﬁed, shown fig.', 'rule set processed, case rule “active” given pattern, care taken active rules conﬂict decision class pattern.', 'john, 1995] gives nice overview (with citations) learning linear discriminant trees presents method based “soft entropy.”', 'researchers proposed techniques learning decision trees tests node linearly separable functions. [', 'conjunct rule determined unnecessary elimination little eﬀect classiﬁcation accuracy—as determined chi-square test, example.', 'quinlan [quinlan, 1987] discusses methods reducing set rules sim- pler set 1) eliminating antecedent rule “unnecessary” conjuncts, 2) eliminating “unnecessary” rules.', 'method dealing replicated subtree problem involves ex- tracting propositional “rules” decision tree.', 'rules an- tecedents conjunctions lead leaf nodes, consequents class corresponding leaf node.', 'example rule tree repeating subtree example be x1 ∧¬x2 ∧x3 ∧x4 ⊃1.']\n",
      "\n",
      "['statlog project, [taylor, michie, & spiegalhalter, 1994] thorough comparisons machine learning algorithms diﬀerent types problems.', '6.7 comparisons experimenters compared decision-tree, neural-net, nearest- neighbor classiﬁers wide variety problems.', 'decision trees x1 x3 x2 1 0 x4 0 1 figure 6.10 decision graph 6.6 problem missing attributes added.']\n",
      "\n",
      "['and, general conclusions enable classiﬁer method best sorts classiﬁcation problems, [quinlan, 1994] provide intuition properties problems render ill suited decision trees, hand, backpropa- gation, other.']\n",
      "\n",
      "[]\n",
      "\n",
      "['far, seen (boolean) algebraic expressions, decision trees, neural networks, plus computational mechanisms techniques computing nearest neighbors.', 'chapter 7 inductive logic programming diﬀerent representational forms functions input vari- ables.', 'chapter, consider matter learning logic programs given set variable values logic program return t (the positive instances) set variable values return f (the negative instances).', 'unary function “true” returns t value argument t. (we think boolean functions arguments having values t f instead 0 1.)', 'example, boolean exclusive-or (odd parity) function variables computed following logic program parity(x,y) - true(x), ¬ true(y) - true(y), ¬ true(x) follow prolog syntax (see, example, [mueller & page, 1988]), convention write variables strings beginning lower-case letters predicates strings beginning upper-case letters.', 'similarly, logic program (whose ordinary application compute bindings variables) simply decide predicate value true (t) false (f).', 'subspecialty machine learning deals learning logic programs called inductive logic programming (ilp) [lavraˇc & dˇzeroski, 1994].']\n",
      "\n",
      "['following terminology introduced connection version spaces, program suﬃcient covers positive instances necessary cover neg- ative instances. (', 'logic program, π, returns t set arguments x, program covers arguments write covers(π, x).', 'example ilp problem, suppose trying induce func- tion nonstop(x,y), value t pairs cities connected non-stop air ﬂight f pairs cities.', 'depending exact set examples, induce program nonstop(x,y) - hub(x), hub(y) - satellite(x,y) - satellite(y,x) value t cities hub cities satellite other.', 'positive examples, (a,b), (a, a1), pairs; negative examples, (a1, a2), pairs.', 'learning problems, want induced program generalize well; is, presented arguments represented training set (but needed background knowledge), like function guess well.', 'that is, program implements suﬃcient condition training instance positive covers positive training instances;', '7.1 notation deﬁnitions evaluating logic programs ilp, implicitly append background facts program adopt usual convention program value t set inputs program interpreter returns t actually running program (with background facts appended) inputs; oth- erwise value f. given background facts, program return t input (a, a1), example.', 'hub(a) intended mean city denoted hub city, satellite(a1,a) intended mean city denoted a1 satellite city denoted a.) train- ing facts, want induce program nonstop(x,y), written terms background relations hub satellite, value t positive instances value f negative instances.', 'air-ﬂight problem, background information ground facts hub(a), hub(b), satellite(a1,a), plus others. (']\n",
      "\n",
      "['diﬀerent ways search consistent logic program are 1) start [ρ - ] specialize program consistent, 2) start [ρ - f ] generalize program consistent.', '< < < < < < < /1 necessary program /2 sufficient program /3 consistent program + + + + + + + + + + < < positive instance covered /2 /3 figure 7.1 suﬃcient, necessary, consistent programs version spaces, program suﬃcient necessary cover fewer examples specializing it.', 'discussing method starts [ρ - ], specializes program necessary (but longer suﬃcient), reachieves suﬃciency stages generalizing—ensuring stage program remains necessary (by specializing).', 'special logic program, certainly necessary, value f inputs, [ρ - f ].', 'general logic program, certainly suﬃcient, value t inputs, single clause body, [ρ - ], called fact prolog.', '7.2 generic ilp algorithm primary operators search consistent program special- ization generalization, discuss operations.', 'imperfect (noisy) training sets, relax criterion settle program covers fraction positive instances allowing cover fraction negative instances.']\n",
      "\n",
      "['c. add clause program analogously, ways logic program specialized a. replace variables program clause terms (a substitution).', 'special case, use here, clause c1 special clause c2 set literals body c2 subset c1.', 'ordering relation structure partially ordered clauses, called reﬁnement graph, similar version space.', 'inductive logic programming major ways logic program generalized a. replace terms program clause variables. (', 'add clause, add clause [ρ - ] specialize adding literals body.', 'clause c1 immediate successor clause c2 graph clause c1 obtained clause c2 adding literal body c2.', 'd. literal equates variable head clause variable term mentioned background knowledge. (', 'c. remove clause program presenting ilp learning method adds clauses program generalizing adds literals body clause special- izing.']\n",
      "\n",
      "['given training set, ξ argument sets known relation ρ ρ; ξ+ positive instances, ξ−are negative instances.', 'algorithm written follows generic ilp algorithm (adapted [lavraˇc & dˇzeroski, 1994, p. 60].)', 'ilp programs follow approach discussing (of specializing clauses adding literal) deﬁned methods computing possible literals add clause.', 'restrictions additional literals imposed, syntactic ones successors reﬁne- ment graph easily computed.', 'ready write simple generic algorithm inducing logic program, π inducing relation ρ.', 'the positive instances ξcur denoted ξ+ cur, negative ones ξ− cur.)', 'literals consider adding are hub(x) hub(y) hub(z) satellite(x,y) satellite(y,x) satellite(x,z) satellite(z,y) (x = y) (if recursive programs allowed, add literals nonstop(x,z) nonstop(z,y).)', 'generic ilp algorithm 93 e. literal (except arguments) head clause. (', 'inner loop constructing clause, c, necessary refers subset, ξcur, training instances. (']\n",
      "\n",
      "['inductive logic programming nonstop(x,y) - nonstop(x,y) - hub(x) nonstop(x,y) - satellite(x,y) nonstop(x,y) - (x = y) . . . . . . . . . . . .', 'initialize c = ρ −. repeat [the inner loop makes c necessary.]', 'cities a, b, c “hub” cities, know nonstop ﬂights hub cities (even shown portion route map).', 'the termination tests inner outer loops relaxed appro- priate case noisy instances.)', 'nonstop(x,y) - hub(x), hub(y) . . . . . . . . .']\n",
      "\n",
      "['ξ+ contains pairs {< a, b >, < a, c >, < b, c >, < b, >, < c, >, < c, b >, < a, a1 >, < a, a2 >, < a1, >, < a2, >, < b, b1 >, < b, b2 >, < b1, b >, < b2, b >, < c, c1 >, < c, c2 >, < c1, c >, < c2, c >} example, assume ξ−contains pairs cities shown fig.', 'are {< a, b1 >, < a, b2 >, < a, c1 >, < a, c2 >, < b, c1 >, < b, c2 >, < b, a1 >, < b, a2 >, < c, a1 >, < c, a2 >, < c, b1 >, < c, b2 >, < b1, >, < b2, >, < c1, >, < c2, >, < c1, b >, < c2, b >, < a1, b >, < a2, b >, < a1, c >, < a2, c >, < b1, c >, < b2, c >} cities shown map, training set necessarily exhaust cities.', 'example 95 cities “satellites” hubs, know nonstop ﬂights satellite city hub.', 'learning program given set positive instances, ξ+, pairs cities nonstop ﬂights set negative instances, ξ−, pairs cities nonstop ﬂights.', 'b c c1 c2 b1 b2 a1 a2 figure 7.3 airline route map want learning program induce program computing value relation nonstop.']\n",
      "\n",
      "['satellite {< a1, a, >, < a2, >, < b1, b >, < b2, b >, < c1, c >, < c2, c >} pairs cities mentioned map relation satellite.', 'inductive logic programming description relation extensional form—it explicitly names pairs relation pairs relation.', 'use notation satellite(x,y) express pair < x, y > relation satellite.', 'desire learn nonstop relation logic program terms background relations, hub satellite, given extensional form.', 'knowing predicate nonstop two-place predicate, inner loop algorithm initializes ﬁrst clause nonstop(x,y) - .', 'following positive instances ξ covered nonstop(x,y) - hub(x) {< a, b >, < a, c >, < b, c >, < b, >, < c, >, < c, b >, < a, a1 >, < a, a2 >, < b, b1 >, < b, b2 >, < c, c1 >, < c, c2 >} compute covering, interpret logic program nonstop(x,y) - hub(x) pairs cities ξ, pairs given background relation hub ground facts.', 'assume learning program following extensional deﬁnitions relations hub satellite hub {< >, < b >, < c >} cities mentioned map assumed relation hub.']\n",
      "\n",
      "['program contains clauses nonstop(x,y) - hub(x), hub(y) - satellite(x,y) program suﬃcient cover following positive instances {< a, a1 >, < a, a2 >, < b, b1 >, < b, b2 >, < c, c1 >, < c, c2 >}', 'positive instances covered clause {< a, a1 >, < a, a2 >, < a1, >, < a2, >, < b, b1 >, < b, b2 >, < b1, b >, < b2, b >, < c, c1 >, < c, c2 >, < c1, c >, < c2, c >} positive instances covered nonstop(x,y) - hub(x), hub(y) removed ξ form ξcur pass inner loop.', 'cover following positive instances ξcur {< a1, >, < a2, >, < b1, b >, < b2, b >, < c1, c >, < c2, c >} instances removed ξcur pass inner loop.', 'ξcur consists negative instances ξ plus positive instances (listed above) covered.', 'example 97 {< a, b1 >, < a, b2 >, < a, c1 >, < a, c2 >, < c, a1 >, < c, a2 >, < c, b1 >, < c, b2 >, < b, a1 >, < b, a2 >, < b, c1 >, < b, c2 >} thus, clause necessary literal added.', 'following positive instances covered nonstop(x,y) - hub(x), hub(y) {< a, b >, < a, c >, < b, c >, < b, >, < c, >, < c, b >} longer negative instances ξ covered clause nonstop(x,y) - hub(x), hub(y) necessary, terminate ﬁrst pass inner loop.', 'order attempt cover them, inner loop creates clause c, initially set nonstop(x,y) - .']\n",
      "\n",
      "['relation canfly satisﬁed following pairs postive instances {< b1, b >, < b1, b2 >, < b1, c >, < b1, c1 >, < b1, c2 >, < b, b1 >, < b2, b1 >, < c, b1 >, < c1, b1 >, < c2, b1 >, < b2, b >, < b2, c >, < b2, c1 >, < b2, c2 >, < b, b2 >, < c, b2 >, < c1, b2 >, < c2, b2 >, < b, c >, < b, c1 >, < b, c2 >, < c, b >, < c1, b >, < c2, b >, < c, c1 >, < c, c2 >, < c1, c >, < c2, c >, < c1, c2 >, < c2, c1 >}', 'again, b c hub cities, b1 b2 satellites b, c1 c2 satellites c. introduced new cities, b3 c3.', '7.4 inducing recursive programs induce recursive program, allow addition literal having predicate letter head clause.', 'note program applied (perhaps good generalization) cities be- sides partial map—so long evaluate relations hub satellite cities.', 'inductive logic programming pass inner loop, add clause nonstop(x,y) - satellite(y,x).', 'mechanisms ensure program terminate; sure new literal diﬀerent variables head literal.', 'clause necessary, program containing clauses suﬃcient, procedure terminates with nonstop(x,y) - hub(x), hub(y) - satellite(x,y) - satellite(y,x) clause necessary, program suﬃcient, pro- gram consistent instances training set.', 'example continues airline map, map somewhat simpler order reduce size extensional relations used.']\n",
      "\n",
      "['suﬃcient cover following positive instances {< b1, b2 >, < b1, c >, < b1, c1 >, < b1, c2 >, < b2, b1 >, < c, b1 >, < c1, b1 >, < c2, b1 >, < b2, c >, < b2, c1 >, < b2, c2 >, < c, b2 >, < c1, b2 >, < c2, b2 >, < b, c1 >,', 'inducing recursive programs 99 b c c1 c2 b1 b2 b3 c3 figure 7.4 airline route map closed-world assumption map, negative instances canfly be {< b3, b2 >, < b3, b >, < b3, b1 >, < b3, c >, < b3, c1 >, < b3, c2 >, < b3, c3 >, < b2, b3 >, < b, b3 >, < b1, b3 >, < c, b3 >, < c1, b3 >, < c2, b3 >, < c3, b3 >, < c3, b2 >, < c3, b >, < c3, b1 >, < c3, c >, < c3, c1 >, < c3, c2 >, < b2, c3 >, < b, c3 >, < b1, c3 >, < c, c3 >, < c1, c3 >, < c2, c3 >} induce canfly(x,y) extensionally deﬁned background relation nonstop given earlier (modiﬁed required reduced airline map) canfly (recursively).']\n",
      "\n",
      "['measure gives comparison quinlan’s based adding literal increases odds instance drawn random covered new clause positive instance odds adding literal.', 'inductive logic programming < b, c2 >, < c1, b >, < c2, b >, < c1, c2 >, < c2, c1 >} thus, add clause program.', 'interpreter attempts estab- lish nonstop(b3,z) z. background facts match, clause cover < b3, b >.', 'program suﬃcient consistent; is canfly(x,y) - nonstop(x,y) - nonstop(x,z), canfly(z,y) 7.5 choosing literals add ﬁrst practical ilp systems quinlan’s foil [quinlan, 1990].', 'foil, quinlan suggested candidate literals compared information-like measure—similar measures inducing decision trees.', 'is, p =(number positive instances covered clause)/(total number instances covered clause).', 'interpreter, clause canfly(x,y) - nonstop(x,z) covers positive instances covered ﬁrst clause, covers negative instances < b2, b3 >, < b, b3 >.', 'interpreter attempts establish nonstop(b1,z) z. nonstop(b1, b), example, background fact, interpreter returns t—which means instance < b1, b2 > covered.', 'inner loop, ﬁrst create clause canfly(x,y) - nonstop(x,z) introduces new variable z. digress brieﬂy describe program containing clause unbound variables body interpreted.', 'major problem involves deciding select literal add inner loop (from literals allowed).', 'let p estimate probability instance drawn random covered clause adding literal positive instance.']\n",
      "\n",
      "['splitting single variable, split node involves asking mutually exclusive exhaustive subsets value variable belongs.', 'let pl denote probability instance drawn random instances covered new clause (with l added) positive.', 'reader refer [pazzani & kibler, 1992, lavraˇc & dˇzeroski, 1994, muggleton, 1991, muggleton, 1992].', 'quinlan discusses post-processing pruning methods presents experi- mental results method applied learning recursive relations lists, learning rules chess endgames card game eleusis, standard tasks mentioned machine learning literature.', 'relationships ilp decision tree induction101 selecting literal, l, add clause, instances previously covered covered; positive negative.', '7.6 relationships ilp decision tree induction generic ilp algorithm understood type decision tree induction.', 'example, node tested variable xi, xi values drawn {a, b, c, d, e, f}, possible split (among many) according value xi value {a, b, c} {d, e, f}.', 'ﬁnding literal high value λl, quinlan’s foil system restricts choice literals that a) contain variable used, b) place restrictions variables literal selected predicate letter literal induced (in order prevent inﬁnite recursion), c) survive pruning test based values λl literals selected far.', 'it turns value quinlan’s information theoretic measure increases monotonically λl, use instead.)', 'example, node tested variables xi xj, xi xj values drawn {a, b, c, d, e, f}, possible binary split', 'specializing clause way fails cover negative instances previously covered covers positive instances previously covered result high value λl. (', 'is, deﬁne λl = ol/o, want literal gives high value λl.']\n",
      "\n",
      "['intensional deﬁnition terms logic program relation r head set clauses bodies involve background relations.', 'background relation r1 satisﬁed patterns; ﬁltered right (to relation r2), rest ﬁltered left (more happens later).', 'relation, r, distinguishes positive negative patterns ξ given terms following logic program r - r1, r2, r3', 'desire construct intensional deﬁnition r terms r1, . . . ,', 'inductive logic programming (among many) according < xi, xj > satisﬁed relation {< a, c >, < c, d >}. (', 'note subset method forming single- variable splits equivalently framed 1-ary relations—which usually called properties.)', 'actually, decision trees decision lists—a special case decision trees, refer trees discussions.)', 'broad outline, method inducing intensional version rela- tion r illustrated considering decision tree shown fig.', 'framework, ilp problem follows given training set, ξ, positively negatively labeled patterns components drawn set variables {x, y, z, . . .}.', 'right-going patterns ﬁltered sequence relational tests positively labeled patterns sat- isfy relation—in case r3.', 'correspond clause created ﬁrst pass inner loop generic ilp algo- rithm.)', 'is, subset patterns satisfying relations, r1, r2, r3 contains positive instances ξ. (', 'rk, positively labeled patterns ξ satisﬁed r negatively labeled patterns are.', 'diagram, patterns ξ ﬁrst ﬁltered decision tree top- level node 1.', 'positively labeled patterns ξ form extensional deﬁnition relation, r. given background relations, r1, . . . ,', 'ξ2 ﬁltered top-level node 2 manner, node 2 satisﬁed positively labeled samples ξ2.', 'generic ilp algorithm understood decision tree induction, node decision tree sub-decision tree, sub- decision tree consists nodes binary splits variables background relations, ri.', 'example, ξ4 contains negatively labeled patterns union ξ1 ξ3 contains positively labeled patterns.']\n",
      "\n",
      "['setting problem, training set, ξ expressed set 2- dimensional vectors components x y. values components range cities {a, b, c, a1, a2, b1, b2, c1, c2} (for simplicity) allow patterns x y value.', 'relationships ilp decision tree induction103 r1 r2 r3 t t t f f f t f r4 r5 t t f f t f u u1 u2 = u < u1 u3 u4= u2 < u3 node 1 node 2 (only positive instances satisfy tests) (only positivel instances satisfy tests) (only negative instances) figure 7.5 decision tree ilp - r4, r5 apply sort decision-tree induction procedure problem generating logic program relation nonstop (refer fig.', 'values x y categorical, decision-tree induction diﬃcult task—involving need invent relations', 'before, relation, nonstop, contains following pairs cities, positive instances {< a, b >, < a, c >, < b, c >, < b, >, < c, >, < c, b >, < a, a1 >, < a, a2 >, < a1, >, < a2, >, < b, b1 >, < b, b2 >, < b1, b >, < b2, b >, < c, c1 >, < c, c2 >, < c1, c >, < c2, c >} pairs cities named map fig.']\n",
      "\n",
      "['select relations way select literals; available tests, selection based leads largest value λri.']\n",
      "\n",
      "['bibliographical historical remarks 105 hub(x) t f u node 1 (top level) {<a,b>, <a,c>, <b,c>, <b,a>, <c,a>, <c,b>} hub(y) t t f node 2 (top level) satellite(x,y) f t t {<a1,a>, <a2,a>, <b1,b>, <b2,b>, <c1,c>, <c2,c>} f {<a,a1>, <a,a2>,<b,b1>, <b,b2>, <c,c1>, <c,c2>} satellite(y,x) f f t node 3 (top level) t {only negative instances} (only positive instances) (only positive instances) (only positive instances) f figure 7.6 decision tree airline route problem']\n",
      "\n",
      "[]\n",
      "\n",
      "['8.1 notation assumptions pac learn- ing theory assume training set ξ n-dimensional vectors, xi, = 1, . . . ,', 'm, labeled (by 1 0) according target function, f, unknown learner.', 'gave intuitive arguments support claim seeing small fraction possible inputs (and values) guess correctly values subsequent inputs—if knew function trying guess belonged appropriately restricted subset functions.', 'in literature pac learning theory, target function usually called target concept denoted c, consistent previous notation continue denote f.) problem guess 107', 'chapter 8 computational learning theory chapter posed problem guessing function given set sample inputs values.', 'insight led theory probably approximately correct (pac) learning—initially developed leslie valiant [valiant, 1984].', 'is, given training set sample patterns adequate allow select function, consistent labeled samples, restricted set hypotheses high probability function select approximately correct (small probability error) subsequent samples drawn random according distribution labeled samples drawn.']\n",
      "\n",
      "['assume target function element set functions, c. assume hypothesis, h, element set, h, hypotheses, includes set, c, target functions.', 'suppose able ﬁnd h classiﬁes m randomly drawn training samples correctly; is, h consistent randomly selected training set, ξ.', 'training occasions, m randomly drawn training samples, h turn approximately correct (for given value ε), not.', 'general, h won’t identical f, strive value h(x) = value f(x) x’s.', 'ﬁnite number hypotheses hypothesis set (as hypothesis sets considered), produce consistent hypothesis set testing training data.', 'class, c, polynomially pac learnable terms h provided exists polynomial-time learning algorithm (polynomial number samples needed, m, dimension, n, 1/ε, 1/δ) pac-learns functions c terms h. initial work pac assumed h = c, later shown func- tions polynomially pac-learned assumption (assuming', 'general, learning algorithm pac-learns functions c terms h iﬀfor function fϵ c, outputs hypothesis hϵ h, probability (1 −δ), εh ≤ε.', 'h probably (except δ) approximately correct (pac) probability approximately correct greater 1−δ, δ conﬁdence parameter.', 'h approximately (except ε ) correct εh ≤ε, ε accuracy parameter.', 'shall m greater bound value depends ε δ, h guaranteed probably approximately correct.', 'quantify notion, deﬁne error h, εh, probability x drawn randomly according p misclassiﬁed εh = x [xh(x)̸=f(x)] p(x) boldface symbols need smaller subscripts math environments.']\n",
      "\n",
      "['pac learning 109 p ̸= np)—but polynomially pac-learned h strict superset c!', 'error hi εhi= probability hi classify pattern error (that is, diﬀerently f classify it).', 'εhb > ε, probability hb (or bad hypothesis) classify pattern correctly (1 −ε).', 'prob[some h ϵ hb classiﬁes m patterns correctly] = p hb ϵ hb prob[hb classiﬁes m patterns correctly |hb ϵ hb] ≤k(1 −ε)m, k = |hb|.', 'c h identical, restrictive deﬁnition properly pac-learnable class class c exists algorithm polynomially pac-learns functions c terms c. 8.2 pac learning 8.2.1 fundamental theorem suppose learning algorithm selects h randomly consistent values f m training patterns.', 'then, probability exists hypothesis h consistent f members ξ error greater ε |h|e−εm.', 'is, prob[hb classiﬁes m patterns correctly |hb ϵ hb] ≤(1 −ε)m.', 'let h set hypotheses, ξ set m ≥1 training examples drawn independently according distribution p, f classiﬁcation function h, ε > 0.', 'probability error randomly selected h greater ε, h consis- tent values f(x) m instances x (drawn according arbitrary p), equal |h|e−εm, |h| number hypotheses h. state result theorem [blumer, et al.,']\n",
      "\n",
      "['taking natural logarithm sides yields ln |h| −εm ≤ln δ m ≥(1/ε)(ln |h| + ln(1/δ)) qed corollary important reasons.', 'computational learning theory is, prob[there bad hypothesis classiﬁes m patterns correctly] ≤k(1 −ε)m.', 'proof ﬁnd bound m guarantees prob[there hypothesis error > ε classiﬁes m patterns correctly] ≤δ.', 'qed corollary theorem is corollary 8.2 given m ≥(1/ε)(ln |h| + ln(1/δ)) independent samples, probability exists hypothesis h consistent f samples error greater ε δ.', 'clearly states select hypothesis consistent m samples assured probability (1 −δ) error ε.', 'possible point confusion bound given corollary upper bound value m needed guarantee polynomial probably ap- proximately correct learning.', 'k ≤|h| (1 −ε)m ≤e−εm, have prob[there bad hypothesis classiﬁes m patterns correctly] = prob[there hypothesis error > ε classiﬁes m patterns correctly] ≤|h|e−εm.']\n",
      "\n",
      "['example, consider following patterns, labeled 1 (from [dietterich, 1990]) (0, 1, 1, 0) (1, 1, 1, 0) (1, 1, 0, 0) processing ﬁrst pattern, h = x1x2x3x4; processing second pattern, h = x2x3x4; ﬁnally, pattern, h = x2x4.', 'n = 50, ε = 0.01 δ = 0.01, m ≥5, 961 guarantees pac learnability.', 'then, |h| = 3n, m ≥(1/ε)(ln(3n) + ln(1/δ)) ≥(1/ε)(1.1n + ln(1/δ)) note bound m increases polynomially n, 1/ε, 1/δ.', 'pac learning 111 8.2.2 examples terms let h set terms (conjunctions literals).', 'then, additional pattern, xi, labeled 1, delete h boolean variables appearing xi sign diﬀerent sign h. processing patterns labeled 1, check patterns labeled 0 sure assigned value 1 h. if, stage algorithm, patterns labeled 0 assigned 1 h, exists term consistently classiﬁes patterns ξ, exit failure.', 'order terms properly pac learnable, additionally ﬁnd time polynomial m n hypothesis h consistent set m patterns labeled value term.', 'initialize boolean function, h, conjunction n literals corresponding values n components x1. (', 'following procedure ﬁnding consistent hypothesis requires o(nm) steps (adapted [dietterich, 1990, page 268]) given training sequence, ξ, m examples.', 'components value 1 corresponding positive literals; components value 0 corresponding negative literals.)']\n",
      "\n",
      "['summary order class functions properly pac- learnable a. algorithm produces consistent hypothesis m n-dimensional samples time polynomial m n. b. sample size, m, needed ensure pac learnability polyno- mial (or better) (1/ε), (1/δ), n showing ln |h| polynomial better number dimensions.', 'np-hard dnf 22n polynomial (all boolean functions) (members class k-2nn two-layer, feedforward neural networks exactly k hidden units output unit.)', 'n = 50, ε = 0.01 δ = 0.01, m ≥173, 748 guarantees pac learnabil- ity.', 'computational learning theory m ≥(1/ε) \\x00n2 ln 2 + ln(1/δ) \\x01 again, note bound m increases polynomially n, 1/ε, 1/δ.', 'adapted [dietterich, 1990, pages 262 268] gives references proofs time complexities.)', 'linearly separable functions properly pac learnable, additionally ﬁnd time polynomial m n hypothesis h consistent set m labeled linearly separable patterns.', 'terms 3n polynomial yes k-term dnf 2o(kn) np-hard (k disjunctive terms) k-dnf 2o(nk) polynomial yes (a disjunction k-sized terms) k-cnf 2o(nk) polynomial yes (a conjunction k-sized clauses) k-dl 2o(nkk lg n) polynomial yes (decision lists k-sized terms) lin.', '8.2.3 properly pac-learnable classes properly pac-learnable classes functions given following table. (']\n",
      "\n",
      "['vapnik-chervonenkis dimension 113 hinted earlier, enlarging class hypotheses makes learning easier.', 'pac learning theory powerful analytic tool, (like complexity theory) deals mainly worst-case results.', 'subset, h, boolean functions able dichotomize arbitrary set, ξ, m boolean patterns 2m ways.', '8.3 vapnik-chervonenkis dimension 8.3.1 linear dichotomies consider set, h, functions, set, ξ, (unlabeled) patterns.', 'general (that is, non-boolean case), subset, h, functions dichotomize set, ξ, m patterns 2m ways, h shatters ξ.', 'ξ include 2n boolean patterns, example, 22n ways dichotomize them, (of course) set possible boolean functions dichotomizes ways.', 'fact class two- layer, feedforward neural networks polynomially pac learnable attack theory networks, successful applications. [', 'measure expressive power set hypotheses, relative ξ, ability arbitrary classiﬁcations patterns ξ.1 m patterns ξ, 2m diﬀerent ways divide patterns disjoint exhaustive subsets.', 'possible enlarging space hy- potheses makes ﬁnding consistent training examples easier.', '8.1, 14 dichotomies 1and, course, hypothesis drawn set arbitrary classiﬁcations set training patterns, little likelihood hypothesis generalize training set.', 'interesting class functions k-2nn poly- nomially pac learnable hypotheses drawn k′-2nn k′ > k. (at time writing, matter undecided.)', 'sim- ilarly, linearly separable functions implemented tlus weight values restricted 0 1 properly pac learnable, unrestricted linearly separable functions are.', 'so, target function k-term-dnf, able ﬁnd hypothesis k-cnf probably approximately correct target function.']\n",
      "\n",
      "['m > n, set m points general position n-dimensional space subset (n+1) points lies (n−1)-dimensional hyperplane.', 'πl(m, n) = 2 n x i=0 c(m −1, i) m > n, = 2m m ≤n', 'm ≤n, set m points general position (m −2)-dimensional hyperplane contains set.', 'note inﬁnite number hyperplanes, are, nevertheless, ﬁnite number ways hyperplanes dichotomize ﬁnite number patterns.', '1 2 3 4 14 dichotomies 4 points 2 dimensions 5 6 7 figure 8.1 dichotomizing points dimensions number dichotomies achievable hyperplanes depends patterns disposed.', 'computational learning theory points dimensions (each separating line yields dichotomies depending points line classiﬁed 1 0). (', 'denote number linear dichotomies m points general position n-dimensional space expression πl(m, n).', 'thus, example, set m ≥4 points general position three-dimensional space lie (two-dimensional) plane.']\n",
      "\n",
      "['reason m = 2(n + 1) called capacity tlu [cover, 1965].', 'special separation found m < 2(n + 1) patterns—almost dichotomy patterns linearly separable.', 'm examples [can correctly classiﬁed by] net w weights (for m >> w), net fraction ε errors new examples chosen [uniform] distribution ε = w/m.”', 'patterns) (dimension) 1 2 3 4 5 1 2 2 2 2 2 2 4 4 4 4 4 3 6 8 8 8 8 4 8 14 16 16 16 5 10 22 30 32 32 6 12 32 52 62 64 7 14 44 84 114 126 8 16 58 128 198 240 note class linear dichotomies shatters m patterns m ≤n + 1.', 'bold-face entries table correspond highest values m linear dichotomies shatter m patterns n dimensions.', 'm > 2(n + 1), randomly selected dichotomy m points certainly linearly separable.', 'm < 2(n + 1), dichotomy m points certainly linearly separable.', '8.2 plot pλ(n+1),n versus λ n, λ = m/(n + 1).', 'analogous results generalizing abilities neural networks developed [baum & haussler, 1989] given intuitive experimen- tal justiﬁcation [baum, 1994, page 438] “the results indicate following heuristic rule holds.', 'number training patterns exceeds capacity, fact tlu separates training patterns according labels means terms tlu generalize new patterns.', 'note large n (say n > 30) quickly pm,n falls 1 0 m goes 2(n + 1).', '8.3.2 capacity let pm,n = πl(m,n) 2m = probability randomly selected dichotomy (out 2m possible dichotomies m patterns n dimensions) linearly separable.', 'sure separation found forced training set generalizes well, case linearly separable functions separate m training patterns.']\n",
      "\n",
      "['in case class linearly separable functions, maximum number achieved m points general position.)', 'number dichotomies will, course, depend disposition m points n-dimensional space; πh(m, n) maximum possible arrangements m points. (', 'set ξ be, example, {0.5, 2.5, - 2.3, 3.14}, hypotheses set [1, 4.5].', 'general, let denote maximum number dichotomies set m n-dimensional patterns hypotheses h πh(m, n).', 'maximum number called vapnik-chervonenkis (vc) dimension denoted vcdim(h) [vapnik & chervonenkis, 1971].', 'computational learning theory 0 1 2 3 4 10 20 30 40 50 0 0.25 0.5 0.75 1 0 1 2 3 4 10 20 30 40 50 0 25 .5 75 1 ph(n + 1), n h n figure 8.2 probability random dichotomy linearly separable 8.3.3 general capacity result corollary 7.2 gave expression number training patterns suﬃcient guarantee required level generalization—assuming function guessing function belonging class known ﬁnite cardinality.', 'hypothesis label points 2.5 3.14 1 points - 2.3 0.5 0.', 'class, h, maximum value m πh(m, n) = 2m, is, h shatters m patterns.', 'example, let calculate vc dimension hypothesis space single intervals real line—used classify points real line.']\n",
      "\n",
      "['vapnik-chervonenkis dimension 117 set hypotheses (single intervals real line) arbitrarily classify points.', 'soon 2 training patterns real line provided know classiﬁcation function trying guess single interval, begin good generalization.', 'dichotomy vcdim(h) fewer patterns general position n dimensions achieved hypothesis h, vcdim(h) patterns training set order hypothesis consistent training set suﬃciently constrained imply good generalization.', 'baum, 1994, page 438] gives experimental evidence proposition “ . . .', '8.3.4 facts speculations vc dimen- sion • ﬁnite number, |h|, hypotheses h, then vcdim(h) ≤log(|h|) • vc dimension terms n dimensions n. • suppose generalize example hypothesis set single intervals real line.', 'multilayer [neural] nets vc dimension roughly equal total number [adjustable] weights.”', 'figure 8.3 dichotomizing points interval vc dimension useful measure expressive power hypothesis set.', 'hypothesis space consisting conjunctions tests (called axis-parallel hyper-rectangles) vc dimension bounded by n ≤vcdim ≤2n • seen, tlus n inputs vc dimension n + 1. • [']\n",
      "\n",
      "['computational learning theory 8.4 vc dimension pac learning theorems connect idea vc dimension pac learn- ing [blumer, et al.,', '1988] theorem 8.5 pac learning algorithm examine ω(1/ε lg(1/δ) + vcdim(h)) training patterns.', 'theorem gives lower (necessary) bound number training patterns required pac learning [ehrenfeucht, et al.,', 'theorem 8.4 set hypotheses, h, properly pac learnable if a. m ≥(1/ε) max [4 lg(2/δ), 8 vcdim lg(13/ε)], b. algorithm outputs hypothesis h ϵ h consistent training set polynomial (in m n) time.', 'second theorems improves bound number training patterns needed linearly separable functions linear n. previous example training patterns needed ensure pac learnability linearly separable function n = 50, ε = 0.01, δ = 0.01, obtained m ≥173, 748.', 'n = 50, ε = 0.01, δ = 0.01, m ≥16, 551 ensures pac learnability.']\n",
      "\n",
      "['encoding involves description point separately; other, shorter, encodings involve description clusters points point cluster described given cluster belongs to.', 'ﬁrst set (a) naturally partitionable classes, second (b) diﬃcult partition all, (c) problematic.', 'setting, assume want encode description set points, ξ, message minimal length.', 'partition separates ξ r mutually exclusive exhaustive subsets, ξ1, . . . ,', 'speciﬁc techniques described chapter explicitly use mdl principles, mdl method applied success.', 'stages • form r-way partition set ξ unlabeled training patterns (where value r, itself, need induced patterns).']\n",
      "\n",
      "['patterns features numeric, distance measure ordinary euclidean distance points n-dimensional space.', 'figure 9.1 unlabeled patterns divided mutually exclusive exhaustive subsets, ξ1, . . . ,', 'suppose r randomly chosen cluster seekers, c1, . . . ,', '9.2 clustering methods 9.2.1 method based euclidean distance unsupervised learning methods use measure similarity patterns order group clusters.']\n",
      "\n",
      "['pattern, xi, presented, ﬁnd cluster seeker, cj, closest xi closer xi cj ←−(1 −αj)cj + αjxi αj learning rate parameter j-th cluster seeker; determines far cj moved xi.', 'clustering methods 121 u11 u12 u21 u22 u23 u31 u32 u11 f u12 = u1 u21 f u22 f u23 = u2 u31 f u32 = u3 u1 f u2 f u3 = u figure 9.2 hierarchy clusters one-by-one.', 'intuitively, cluster seeker gets reasonably clustered set patterns (and cluster seeker located), converge center gravity cluster.', 'adjustment rule, cluster seeker center gravity (sample mean) set patterns far moved.', 'example, set αj = 1/(1 + mj) use rule mj ←−mj +1.']\n",
      "\n",
      "['like partition set patterns clusters sum sample variances (badnesses) clusters small.', 'course cluster pattern, sample variances zero, arrange measure badness partition increase number clusters.', 'unsupervised learning u u2 u11 u12 u31 u32 u21 u22 u23 u1 u3 figure 9.3 displaying hierarchy tree cluster seekers converged, classiﬁer implied now- labeled patterns ξ based voronoi partitioning space (based distances cluster seekers).', 'measure badness, v , cluster patterns, {xi}, computing sample variance deﬁned by v = (1/k) x (xi −m)2 m sample mean cluster, deﬁned be m = (1/k) x xi k number points cluster.']\n",
      "\n",
      "['hand, cluster seekers, ci, deﬁnes cluster sample variance larger δ, place new cluster seeker, cj, random location somewhat adjacent ci reset masses ci cj zero.', 'elaborations basic cluster-seeking procedure allow number clus- ter seekers vary depending distances depending sample variances clusters.', 'values parameters ε δ set depending relative weights given sample variances numbers clusters.', 'commonly technique compute standard deviation (i.e., square root variance) components entire training set normalize values components adjusted standard deviations equal.', 'clustering methods 123 c1 c2 c3 separating boundaries figure 9.4 minimum-distance classiﬁcation clusters number way somewhat similar principle minimal description length discussed earlier.', 'way badness par- tition ultimately decrease decreasing total sample variance comparatively little penalty additional cluster seeker.', 'example, distance, dij, cluster seekers, ci cj, falls threshold ε, replace single cluster seeker placed center gravity (taking account respective masses).', 'way decrease overall badness partition reducing number clusters compara- tively little penalty increased variance.']\n",
      "\n",
      "['described follows a. begin set unlabeled patterns ξ list, l, clusters.', 'assuming conditional independence pattern components, xi, quantity maximized is s(x, ci) = p(x1|ci)p(x2|ci) · · · p(xn|ci)p(ci) p(xj|ci) estimated sample statistics patterns clusters expression. (', 'decide clusters arbitrary pattern, x, assigned selecting ci probability, p(ci|x), largest, providing p(ci|x) larger ﬁxed threshold, δ.', 'unsupervised learning 9.2.2 method based probabilities suppose partition training set, ξ, r mutually exclusive exhaustive clusters, c1, . . . ,', 'is, cmax ←−cmax ∪{x} update sample statistics p(x1|cmax), p(x2|cmax), . . . ,', 'before, deﬁne sample mean cluster, ci, be mi = (1/ki) x xjϵ ci xj ki number patterns ci.', 'b) s(x, cmax) ≤δ, create new cluster, cnew = {x} add cnew l. 3.']\n",
      "\n",
      "['9.3 hierarchical clustering methods 9.3.1 method based euclidean distance suppose set, ξ, unlabeled training patterns.', 'shortest distance cluster vectors, ci cj, form new cluster, c, consisting union ci cj.', 'hierarchical clustering methods 125 d. sample statistics clusters changed entire iteration ξ, terminate clusters l; 2.', 'shortest distance pattern, xi, cluster vector, cj (representing cluster, cj), form new cluster, c, consisting union cj {xi}.', 'collect xi xj cluster, c, eliminate xi xj ξ replace cluster vector, c, equal average xi xj.', 'ternary tree formed instead searches points ξ triangle deﬁned patterns minimal area.', 'reduce number points ξ time, ultimately terminate tree clusters rooted cluster containing points original training set.', 'clusters organized hierarchically binary tree cluster 9 root, clusters 7 8 descendants root, on.', 'smallest distance pairs patterns, form new cluster, c, replace pair patterns ξ average.']\n",
      "\n",
      "['unsupervised learning 1 2 3 5 4 6 7 8 9 figure 9.5 agglommerative clustering 9.3.2 method based probabilities probabilistic quality measure partitions develop measure goodness partitioning based accurately guess pattern given partition in.', 'then, probability guess i-th component correctly is x j probability(guess vij)pi(vij|ck) = x j [pi(vij|ck)]2 average number (the n) components values guessed correctly method given sum probabilities components x x x j [pi(vij|ck)]2', 'before, compute sample statistics p(xi|ck) probability values component given class assigned partitioning.', 'suppose use following probabilistic guessing rule values components vector x given class k. guess xi = vij probability pi(vij|ck).', 'suppose given partitioning ξ r classes, c1, . . . ,', 'suppose component xi x values vij, index j steps domain component.']\n",
      "\n",
      "['finally, dividing number clusters produces ﬁnal z value partition, z(p1) = 3/2.', 'second partition, p2, gives following sample probabilities p1(v11 = 1|c1) = 1 p2(v21 = 1|c1) = 1/2 p3(v31 = 1|c1) = 1 summing values components (0 1) gives (1)2 + (0)2 = 1 component 1, (1/2)2 + (1/2)2 = 1/2 component 2, (1)2 + (0)2 = 1 component 3.', 'similar calculations yield z(p3) = 1 z(p4) = 3/4, method evaluating partitions favor placing patterns single cluster.', 'summing values components (0 1) gives (1/2)2 + (1/2)2 = 1/2.', 'hierarchical clustering methods 127 given partitioning r classes, goodness measure, g, parti- tioning average expression classes g = x k p(ck) x x j [pi(vij|ck)]2 p(ck) probability pattern class ck.', 'let’s evaluate z values follow- ing ones p1 = {a, b, c, d}, p2 = {{a, b}, {c, d}}, p3 = {{a, c}, {b, d}}, p4 = {{a}, {b}, {c}, {d}}.', 'order penalize measure having large number classes, divide r overall “quality” measure partitioning z = (1/r) x k p(ck) x x j [pi(vij|ck)]2 example use measure trivially simple clustering three-dimensional patterns shown fig.', 'finally, dividing number clusters produces ﬁnal z value partition, z(p2) = 1 1/4, high z(p1).', 'sample probabilities pi(vi1 = 1) pi(vi0 = 0) equal 1/2 components.']\n",
      "\n",
      "['method uses z values place patterns vari- ous nodes; sample statistics update z values pattern placed node.', 'arrange times dur- ing process non-empty node tree (besides successors) exactly successor.', 'algorithm follows a. start tree root node contains patterns ξ single successor node.', 'unsupervised learning x2 x3 x1 b c d figure 9.6 patterns 3-dimensional space iterative method hierarchical clustering evaluating partitionings m patterns selecting best computationally intractable.', 'general, successors node, η, labeled mutually exclusive exhaustive subsets pattern set labelling node η.', 'following iterative method based hi- erarchical clustering procedure called cobweb [fisher, 1987].', 'best host determined tentatively placing xi successors calculating resulting z value']\n",
      "\n",
      "['node merging happen nodes having parent merged overall increase quality resulting classiﬁcation performed successors parent.', 'ﬁrst, program at- tempted ﬁnd categories (we class 1 class 2) united states senators based votes (yes no) issues.', 'ﬁnal classiﬁcation tree order dependent, cobweb proce- dure incorporates node merging splitting.', 'issue class 1 class 2 toxic waste yes budget cuts yes sdi reduction yes contra aid yes line-item veto yes mx production yes', 'e. best host node, η, place xi η, generate successor node η, generate sibling node η, 2.', 'node splitting heuristic node splitting consider replacing best host group siblings host’s successors.', 'g. best host non-empty, non-singleton node, η, place xi η, set µ η, 4.', 'merging improves z value, new node containing union patterns merged nodes replaces merged nodes, nodes merged installed successors new node.', 'f. best host non-empty, singleton (tip) node, η, place xi η, create successor node η containing singleton pattern η, create successor node η containing xi, create successor node η, create successor nodes new non-empty successors η, 2.']\n",
      "\n",
      "['n0 soybean diseases n1 diaporthe stem canker n2 charcoal rot n3 n31 rhizoctonia rot n32 phytophthora rot figure 9.7 taxonomy induced soybean diseases 9.4 bibliographical historical remarks added.', 'unsupervised learning second experiment, program attempted classify soybean dis- eases based characteristics.']\n",
      "\n",
      "['multi-step prediction, expect prediction accuracy better better increases m. 10.2 supervised temporal-diﬀerence meth- ods training method naturally suggests use actual value z time m + 1 (once known) supervised learning procedure 131', 'chapter 10 temporal-diﬀerence learning 10.1 temporal patterns prediction prob- lems chapter, consider problems wish learn predict future value quantity, z, n-dimensional input pattern, x. problems, patterns occur temporal sequence, x1, x2, . . .,', 'components xi features values available time, t = i. distinguish kinds prediction problems.', 'one, desire predict value z time t = + 1 based input xi i. example, wish predict aspects tomorrow’s weather based set measurements today.', 'm. example, wish series predictions aspect weather new year’s day, based measurements taken day new year’s.', 'kind prediction problem, desire sequence predictions value z ﬁxed time, t = m + 1, based xi, = 1, . . . ,']\n",
      "\n",
      "['widrow-hoﬀrule results f(x, w) = x • w. then (∆w)i = c(z −fi)xi interesting form (∆w)i developed note (z −fi) = m x k=i (fk+1 −fk) deﬁne fm+1 = z. substituting formula (∆w)i yields (∆w)i = c(z −fi) ∂fi ∂w', 'assume prediction, f(x), depends vector modiﬁable weights, w. dependence explicit, write f(x, w).', 'su- pervised learning, consider procedures following type xi, prediction f(xi, w) computed compared z, learning rule (whatever is) computes change, (∆wi), w. then, taking account weight changes pattern sequence having predictions old weight vector, change w follows w ←−w + m x i=1 (∆w)i attempting minimize squared error z f(xi, w) gradient descent, weight-changing rule pattern is (∆w)i = c(z −fi) ∂fi ∂w c learning rate parameter, fi prediction z, f(xi, w), time t = i, ∂fi ∂w is, deﬁnition, vector partial derivatives ( ∂fi ∂w1 , . . . ,', 'is, seek learn function, f, f(xi) close possible z i. typically, need training set, ξ, consisting sequences.', 'method better supervised learning important problems base learning diﬀerence f(xi+1) f(xi) diﬀerence z f(xi).', '∂fi ∂wn ) wi individual components w. (the expression ∂fi ∂w written ∇wfi.)', 'reader recall equivalent expression (∆w)i deriving backpropagation formulas training multi-layer neural networks.']\n",
      "\n",
      "['λ < 1, degrees unsupervised learning, prediction function strives prediction like successive ones (whatever be).', 'here, λ term gives exponentially decreasing weight diﬀerences later time t = i. λ = 1, rule began—weighting diﬀerences equally, λ →0, weight (fi+1 −fi) diﬀerence.', 'intermediate values λ account diﬀerently weighted diﬀerences future pairs successive predictions.', 'case f(x, w) = x • w, temporal diﬀerence form widrow-hoﬀrule is (∆w)i = cxi m x k=i (fk+1 −fk) reason writing (∆w)i temporal-diﬀerence form permit interesting generalization follows (∆w)i = c ∂fi ∂w m x k=i λ(k−i)(fk+1 −fk) 0 < λ ≤1.', 'supervised temporal-difference methods 133 = c ∂fi ∂w m x k=i (fk+1 −fk) form, instead diﬀerence prediction value z, use diﬀerences successive predictions—thus phrase temporal-diﬀerence (td) learning.', 'shall soon unsupervised procedures result better learning supervised ones important class problems.', 'td(1) considered pure supervised learning procedure, sensitive ﬁnal value z provided teacher.', 'td(0), error diﬀerence successive predic- tions, td(1), error diﬀerence ﬁnally revealed value z prediction.', 'interesting compare extreme cases td(0) (∆w)i = c(fi+1 −fi) ∂fi ∂w td(1) (∆w)i = c(z −fi) ∂fi ∂w extremes handled learning mechanism; error term diﬀerent.']\n",
      "\n",
      "['write expression weight change rule takes account (∆w)i w ←−w + m x i=1 c ∂fi ∂w m x k=i λ(k−i)(fk+1 −fk) interchanging order summations yields w ←−w + m x k=1 c k x i=1 λ(k−i)(fk+1 −fk) ∂fi ∂w = w + m x k=1 c(fk+1 −fk) k x i=1 λ(k−i) ∂fi ∂w interchanging indices k ﬁnally yields w ←−w + m x i=1 c(fi+1 −fi) x k=1 λ(i−k) ∂fk ∂w if, earlier, want use expression form w ←−w+pm i=1(∆w)i, write (∆w)i = c(fi+1 −fi) x k=1 λ(i−k) ∂fk ∂w now, let ei = pi k=1 λ(i−k) ∂fk ∂w, develop computationally eﬃcient recurrence equation ei+1 follows ei+1 = i+1 x k=1 λ(i+1−k) ∂fk ∂w = ∂fi+1 ∂w + x k=1 λ(i+1−k) ∂fk ∂w', 'temporal-difference learning 10.3 incremental computation (∆w)i rewrite formula (∆w)i, (∆w)i = c ∂fi ∂w m x k=i λ(k−i)(fk+1 −fk) allow type incremental computation.']\n",
      "\n",
      "['equation computed incrementally, (∆w)i depends pair successive predictions [weighted] sum past values ∂fi ∂w.', 'case, sequences temporally presented patterns contain important information ignored conventional supervised method widrow-hoﬀrule.', 'similarly, xf sequence, equally likely sequence terminates z = 1 vector xe.', 'sutton [sutton, 1988, page 19] gives interesting example involving random walk, repeat here.', '10.1, sequences vectors, x, generated follows start vector xd; vector sequence equally likely adjacent vectors diagram.', 'vector xc (or xe), equally likely vectors adjacent xc (or xe).', 'xb sequence, equally likely sequence terminates z = 0 vector xc.', '10.4 experiment td methods td prediction methods [especially td(0)] suited situations patterns generated dynamic process.', 'experiment td methods 135 = ∂fi+1 ∂w + λei rewriting (∆w)i terms, obtain (∆w)i = c(fi+1 −fi)ei where e1 = ∂f1 ∂w e2 = ∂f2 ∂w + λe1 etc.', 'saves substantially memory, longer necessary individually remember past values ∂fi ∂w.”', 'quoting sutton [sutton, 1988, page 15] (about diﬀerent equation, quote applies equally one) “. . .']\n",
      "\n",
      "['weight vector increments summed sequences presented, sum change weight vector pass sequences.', 'temporal-difference learning 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 z = 0 z = 1 xb xc xd xe xf typical sequences xdxcxdxexf 1 xdxcxbxcxdxexdxexf 1 xdxexdxcxb 0 figure 10.1 markov process random walk example markov process; transitions state state j occur probabilities depend j. given set sequences generated process training set, want able predict value z x test sequence.', 'experiments process, sutton linear predictor, f(x, w) = x • w. learning problem ﬁnd weight vector, w, minimizes mean-squared error z predicted value z. given ﬁve diﬀerent values x on, following predictions f(xb) = w1, f(xc) = w2, f(xd) = w3, f(xe) = w4, f(xf ) = w5, wi i-th component weight vector. (', 'process repeated (using training sequences) (quoting sutton) “the procedure longer produced signiﬁcant changes weight vector.', 'note values predictions limited 1 0—even z values—because minimizing mean-squared error.)', 'weight vector increments, (∆w)i, computed pattern presentation weight changes sequences presented.']\n",
      "\n",
      "['root-mean-squared diﬀerences best learned predictions (over c) optimal ones plotted fig.', 'answer widrow-hoﬀprocedure minimizes error training set; necessarily minimize error future experience. [', 'even though, ﬁxed, small c, weight vector converged vector, converge somewhat diﬀerent vector diﬀerent values c.) convergence, predictions ﬁnal weight vector com- pared optimal predictions transition probabilities.', '0.10 0.12 0.14 0.16 0.18 0.20 0.0 0.1 0.3 0.5 0.7 0.9 1.0 h error best c widrow-hoff td(1) td(0) (adapted sutton, p. 20, 1988) figure 10.2 prediction errors td(λ) notice widrow-hoﬀprocedure perform versions td(λ) λ < 1!', 'compute proba- bilities 1/6, 1/3, 1/2, 2/3, 5/6 xb, xc, xd, xe, xf , respectively.', 'known that, repeated presentations, widrow-hoﬀprocedure minimizes rms error predictions actual outcomes training set ([widrow & stearns, 1985]).', 'experiment td methods 137 ﬁnal value [for 100 diﬀerent training sets random sequences], independent initial value.” (']\n",
      "\n",
      "['temporal-difference learning matching future experience—those consistent maximum- likelihood estimate underlying markov process.”', 'dayan [dayan, 1992] extended result theorem 9.1 td(λ) arbitrary λ 0 1. (', 'sutton conjectures c approach 0 training progresses, variance predictions approach 0 also.', '10.6 intra-sequence weight updating standard weight updating rule td(λ) methods is w ←−w + m x i=1 c(fi+1 −fi) x k=1 λ(i−k) ∂fk ∂w weight update occurs entire sequence observed.', '10.5 theoretical results possible analyze performance linear-prediction td(λ) methods markov processes.', 'instead, modify rule that, pair predictions, fi+1 = f(xi+1, wi) fi = f(xi, wi).', 'theorem 10.1 (sutton, page 24, 1988) absorbing markov chain, linearly independent set observation vectors {xi} non- terminal states, exists ε > 0 positive c < ε initial weight vector, predictions linear td(0) (with weight updates sequence) converge expected value optimal (maximum likelihood) predictions true process.', 'method truly incremental (in analogy weight updating rules neural nets), desirable change weight vector pattern presentation.', 'fi = f(xi, wi−1), rule prediction diﬀerence, (fi+1 −fi), sensitive changes x changes w lead instabilities.', 'obvious extension is wi+1 ←−wi + c(fi+1 −fi) x k=1 λ(i−k) ∂fk ∂w fi+1 computed making weight change; is, fi+1 = f(xi+1, wi).', 'expected values predictions converge, predictions converge vary expected values depending recent experience.']\n",
      "\n",
      "['linear td(0) method regarded technique training simple network consisting single dot product unit (and threshold sigmoid function).', 'b. = 1, ..., m, do (a) fi ←−xi • w (we compute fi anew time use value fi+1 previous time through.) (', 'weight changing rule i-th weight vector j-th layer weights form before, namely w(j) ←−w(j) + cδ(j) x(j−1) δ(j) given recursively by δ(j) = f (j) (1 −f (j) ) mj+1 x l=1 δ(j+1) l w(j+1) il w(j+1) il l-th component i-th weight vector (j +1)-th layer weights.', 'td(0) change network weights according expression wi+1 = wi + c(fi+1 −fi) ∂fi ∂w change standard backpropagation weight- changing rule diﬀerence term desired output output unit ﬁnal (k-th) layer, (d −f (k)), replaced diﬀerence term successive outputs, (fi+1 −fi).', 'b) fi+1 ←−xi+1 • w (c) di+1 ←−fi+1 −fi (d) w ←−w + c di+1xi (if fi computed changed weight vector, value closer fi+1 desired.)', 'intra-sequence weight updating 139 td(0) linear predictors, rule is wi+1 = wi + c(fi+1 −fi)xi rule implemented follows a. initialize weight vector, w, arbitrarily.', 'change direct eﬀect expression δ(k) becomes δ(k) = 2(f ′(k) −f (k))f (k)(1 −f (k)) f ′(k) f (k) successive outputs network.']\n",
      "\n",
      "['40 hidden units 2 x 24 cells 4 output units hidden output units sigmoids learning rate c = 0.1; initial weights chosen randomly <0.5 +0.5.', 'temporal-difference learning 10.7 example application td-gammon program called td-gammon [tesauro, 1992] learns play backgammon training neural network temporal-diﬀerence methods.', 'bar, board, moves 198 inputs 1 2 3 # > 3 . . .', 'p3 = pr(black wins) p4 = pr(black gammons) p1 = pr(white wins) p2 = pr(white gammons) estimated payoff d = p1 + 2p2 < p3 < 2p4 no.', 'network trained minimize error actual payoﬀand estimated payoﬀ, actual payoﬀis deﬁned df = p1 + 2p2 −p3 −2p4, pi actual probabilities outcomes deﬁned ﬁgure. . . .', 'estimated probabilities figure 10.3 td-gammon network td-gammon learned network select results best predicted payoﬀ. is, stage game ﬁnite set moves possible lead set, {x}, new board positions.']\n",
      "\n",
      "['bibliographical historical remarks 141 predicted payoﬀis selected white’s (and smallest black’s).', 'td(1), network trained that, t, output, dt, input xt tended expected ﬁnal payoﬀ, df, given input.', 'dt network’s estimate payoﬀ time t (before made), dt+1 estimate time t + 1 (after made), weight adjustment rule is ∆wt = c(dt+1 −dt) t x k=1 λt−k ∂dk ∂w wt vector weights network time t, ∂dk ∂w gradient dk weight space. (', 'made, network weights adjusted predicted payoﬀfrom original position closer resulting position.', 'td-gammon (with 40 hidden units, λ = 0.7, c = 0.1) won 66.2% 10,000 games sun microsystems gammontool 55% 10,000 games neural network trained expert moves.', 'commenting later version td- gammon, incorporating special features inputs, tesauro said “it appears strongest program seen author.”', 'special cases clear, recall td(0), network trained that, t, output, dt, input xt tended expected output, dt+1, input xt+1.', 'for layered, feedforward network, td-gammon, weight changes weight vectors layer expressed usual manner.)']\n",
      "\n",
      "[]\n",
      "\n",
      "['moment, assume mapping states vectors one-to-one, and, fact, use notation x refer state environment input vector.', 'maximize rewards, need able predict actions change inputs, particular, actions lead rewards.', 'formalize problem following way robot exists environment consisting set, s, states.', 'chapter 11 delayed-reinforcement learning 11.1 general problem imagine robot exists environment sense act.', 'assume robot’s sensory apparatus constructs input vector, x, environment, informs robot state environment in.', 'learner’s goal ﬁnd policy, π(x), maps input vectors actions way maximizes rewards accumulated time.', 'assume discrete time model; input vector time t = xi, action taken time ai, expected reward, ri, received t = depends action taken state, ri = r(xi, ai).']\n",
      "\n",
      "['robot receives input vector (x1, x2) telling cell in; capable actions, n, e, s, w moving robot cell up, right, down, left, respectively.', 'chapter describe methods learning optimal policies based reward values received learner.', 'let’s suppose robot lands goal cell gets reward, immediately transported random cell, quest reward continues.', 'example, input robot (1,3), robot chooses action w, input robot (1,3) receives reward −1.', 'way displaying policy grid-world robot arrow cell indicating direc- tion robot cell.', 'delayed-reinforcement learning xi ri learner environment (reward) (state) (action) ai figure 11.1 reinforcement learning 11.2 example “grid world,” shown fig.']\n",
      "\n",
      "['then, want maximize expected future reward deﬁne v π(x) as v π(x) = e \" ∞ x i=0 γirπ(x) # case, v π(x) value policy π input x.', 'markovian environments, example, probability action state xi lead state xj given transition probability p[xj|xi, a].', 'optimal policy maximizes v π(x) inputs, x. general, want consider case rewards, ri, random variables eﬀects actions environmental states random.', 'temporal discounting optimal policies 145 r g 1 2 3 4 5 6 7 1 2 3 4 5 6 7 8 figure 11.2 grid world 11.3 temporal discounting optimal poli- cies delayed reinforcement learning, assumes rewards distant future valuable immediate rewards.', 'suppose policy π(x) maps input vectors actions, let rπ(x) reward received i-th time step begins executing policy π starting state x. total reward accumulated time steps policy π beginning state x is v π(x) = ∞ x i=0 γirπ(x) reason temporal discount factor sum ﬁnite.']\n",
      "\n",
      "['famous “optimality equation” v π∗(x) = max \" r(x, a) + γ x x′ p[x′|x, a]v π∗(x′) # theory dynamic programming (dp) [bellman, 1957, ross, 1983] assures optimal policy, π∗, satisﬁes equation.', 'delayed-reinforcement learning r g 1 2 3 4 5 6 7 1 2 3 4 5 6 7 8 figure 11.3 optimal policy grid world action prescribed π taken state x leads state x′ (randomly according transition probabilities), write v π(x) terms v π(x′) follows v π(x) = r[x, π(x)] + γ x x′ p[x′|x, π(x)]v π(x′) (in summary) γ = discount factor, v π(x) = value state x policy π, r[x, π(x)] = expected immediate reward received execute action prescribed π state x, p[x′|x, π(x)] = probability environment transitions state x′ execute action prescribed π state x. words, value state x policy π expected value immediate reward received executing action recommended π plus average value (under π) states accessible x. optimal policy, π∗(and others!),']\n",
      "\n",
      "['11.4 q-learning watkins [watkins, 1989] proposed technique calls incremental dynamic programming.', 'q-learning 147 provides methods calculating v π∗(x) π∗, assuming know average rewards transition probabilities.', 'model actions, is, knew state, x, action a, state, x′ resulted, use method called value iteration ﬁnd optimal policy.', 'is, π∗(x) = arg max \" r(x, a) + γ x x′ p[x′|x, a]v π∗(x′) # but, course, assuming know average rewards transition probabilities, ﬁnd method eﬀectively learns them.', 'knew transition probabilities, average rewards, v π∗(x) x a, easy implement optimal policy.', 'value iteration works follows begin assigning, randomly, estimated value ˆv (x) state, x. i-th step process, suppose state xi (that is, input i-th step xi), estimated value state xi i-th step ˆvi(xi).', 'suppose subsequent state having highest estimated value x′ i. update estimated value, ˆvi(xi), state xi follows ˆvi(x) = (1 −ci) ˆvi−1(x) + ci h ri + γ ˆvi−1(x′ i) x = xi, = ˆvi−1(x) otherwise.', 'adjustment moves value ˆvi(xi) increment (depend- ing ci) closer h ri + γ ˆvi(x′ i) .', 'simply select maximizes r(x, a) + γ p x′ p[x′|x, a]v π∗(x′).', 'assuming ˆvi(x′ i) good estimate vi(x′ i), adjustment helps estimates consistent.', 'providing 0 < ci < 1 visit state inﬁnitely often, process value iteration converge optimal values.', 'let a; π stand policy chooses action once, chooses actions according policy π.']\n",
      "\n",
      "['suppose action state x leads state x′. deﬁnitions q v , easy that qπ(x, a) = r(x, a) + γe[v π(x′)] r(x, a) average value immediate reward received execute action state x. optimal policy (and others), version optimality equation terms q values qπ∗(x, a) = max h r(x, a) + γe h qπ∗(x′, a) ii actions, a, states, x. now, optimal q values (for x), implement optimal policy simply selecting action maximized r(x, a) + γe \\x02 qπ∗(x′, a) \\x03 .', 'i-th episode, agent • observes current state xi, • selects [using method described below] performs action ai, • observes subsequent state x′ i, • receives immediate reward ri,', 'quote (with minor notational changes) [watkins & dayan, 1992, page 281] “in q-learning, agent’s experience consists sequence dis- tinct stages episodes.', 'is, π∗(x) = arg max h r(x, a) + γe h qπ∗(x′, a) ii watkins’ proposal amounts td(0) method learning q values.', 'delayed-reinforcement learning optimal value state x given by v π∗(x) = max qπ∗(x, a) equation holds optimal policy, π∗. optimal policy given by π∗(x) = arg max qπ∗(x, a) note action makes qπ(x, a) larger v π(x), improve π changing π(x) = a. making change basis powerful learning rule shall describe shortly.']\n",
      "\n",
      "['q learning strengthens usual td methods, however, td (applied reinforcement problems value iteration) requires one-step lookahead, model eﬀects actions, q learning not.', 'imagine q values predictions ultimate (inﬁnite horizon) total reward, learning procedure described exactly td(0) method learning predict q values.', 'q-learning 149 • adjusts qi−1 values learning factor ci, according to qi(x, a) = (1 −ci)qi−1(x, a) + ci[ri + γvi−1(x′ i)] x = xi = ai, = qi−1(x, a) otherwise, vi−1(x′) = max b [qi−1(x′, b)] best agent thinks state x′. . . .', 'convenient notation (proposed [schwartz, 1993]) representing change q value is q(x, a) β ←−r + γv (x′) q(x, a) new q value input x action a, r immediate reward action taken response input x, v (x′) maximum (over actions) q value state reached action taken state x, β fraction way new q value, q(x, a), adjusted equal r + γv (x′).', 'current q values, qi(x, a), agent selects action maximizes qi(x, a).', 'deﬁne ni(x, a) index (episode number) i-th time action tried state x. then, have', 'watkins dayan [watkins & dayan, 1992] prove that, certain con- ditions, q values computed learning procedure converge optimal ones (that is, ones optimal policy based).', 'q value adjusted closer (by determined ci) sum immediate reward plus discounted maximum (over actions) q values state entered.']\n",
      "\n",
      "['considered strong condition way states actions selected—however, stochastic con- ditions theorem, method guaranteed ﬁnd optimal policy weaker conditions.', 'sequence episodes forms basis learning include inﬁnite number episodes starting state action.', 'relationships q learning, dynamic programming, control described [barto, bradtke, & singh, 1994].', 'again, quote [watkins & dayan, 1992, page 281] “the important condition implicit convergence theorem . . .', 'note, however, episodes need form continuous sequence—that x′ episode need x episode.”', 'delayed-reinforcement learning theorem 11.1 (watkins dayan) markov problems states {x} actions {a}, given bounded rewards |rn| ≤r, learning rates 0 ≤cn < 1, ∞ x i=0 cni(x,a) = ∞, ∞ x i=0 \\x02 cni(x,a) \\x032 < ∞ x a, qn(x, a) →q∗ n(x, a) n →∞, x a, probability 1, q∗ n(x, a) corresponds q values optimal policy.', '11.5 discussion, limitations, extensions q-learning 11.5.1 illustrative example q-learning procedure requires maintain table q(x, a) values state-action pairs.', 'deﬁnition optimal q values state depends recursively expected values q values subsequent states (and expected values rewards), expected values explicitly computed procedure.', 'instead, values approximated iterative sampling actual stochastic mechanism produces successor states.']\n",
      "\n",
      "['maximum q value occurs = w, robot moves west cell (1,3)—receiving immediate reward.', 'q learning is, date, successful technique temporal credit assignment, related method, called bucket brigade algorithm, proposed [holland, 1986].', 'general problem associating rewards state-action pairs called temporal credit assignment problem—how credit reward apportioned actions leading it?', 'learning rate parameter c = 0.5 γ = 0.9, q value q((2, 3), w) adjusted 7 5.75.', 'q learning gradually reinforces actions con- tribute positive rewards increasing associated q values.', 'maximum q value cell (1,3) 5, learning mechanism attempts value q((2, 3), w) closer discounted value 5 plus immediate reward (which 0 case).', 'typically, example, rewards occur somewhat actions lead them— phrase delayed-reinforcement learning.', 'random walk happens stumble rewarding states q learning begin produce q values useful, and, then, q values work way outward rewarding states.', 'learning problems similar faced agent grid world thoroughly studied sutton proposed architecture, called dyna, solving [sutton, 1990].', 'agent’s model world obtained q learning actual world, planning accomplished q learning model world.', 'imagine better better approximations optimal q values gradually propagate states producing rewards states agent fre- quently visits.', 'discussion, limitations, extensions q-learning151 x q(x, a) r(x, a) (2,3) w 7 0 (2,3) n 4 0 (2,3) e 3 0 (2,3) s 6 0 (1,3) w 4 -1 (1,3) n 5 0 (1,3) e 2 0 (1,3) s 4 0 suppose robot cell (2,3).']\n",
      "\n",
      "['mentioned, convergence theorem q learning require on-line learning; indeed, special precautions taken ensure on-line learning meets conditions theorem.', 'example, grid-world problem, imagine selecting action randomly ac- cording probability distribution actions (n, e, s, w).', 'then, generalized “goal” maximizing discounted future reward instead simply achieving particular condition.', 'on-line learning discovers good paths rewards, agent ﬁxate discover policy leads possibly greater long-term reward.', '11.5.2 random actions pattern presentation sequence patterns caused agent’s action response pattern, called on-line learning method.', 'methods, also, proposed dealing exploration, in- cluding making unvisited states intrinsically rewarding “interval estimate,” related uncertainty estimate state’s value [kaelbling, 1993].', 'example, ﬁrst ﬁnd action prescribed q values choose action probability 1/2, choose orthogonal actions probability 3/16 each, choose opposite action probability 1/8.', 'possibility presents interesting way generalize classical notion “goal” ai planning systems—even learning.', 'way force exploration perform occasional random actions (in- stead single action prescribed current q values).', 'goal maintenance, particular state, expressed terms reward earned agent state performed action transitioned state step.', 'policy modiﬁed “simulated annealing” gradually increase probabil- ity action prescribed q values time goes on.', 'instead representing goal condition achieved, represent “goal struc- ture” set rewards given achieving conditions.', 'reinforcement learning phraseology, problem referred problem exploitation (of learned behavior) versus exploration (of possibly better behavior).']\n",
      "\n",
      "['optimum q values problem (whatever be) complex computed linear machine, layered neural network used.', 'wi q(a1, x) q(a2, x) q(ar, x) figure 11.4 net computes q values neural net agent r actions select from.', 'y y y trainable weights y wi r dot product units q(ai, x) = x .', 'q values (as function input pattern x action ai) computed dot products weight vectors (one action) input vector.', 'weight adjustments according td(0) procedure bring q value action selected closer sum immediate reward (if any) (discounted) maximum q value input pattern.', 'discussion, limitations, extensions q-learning153 11.5.3 generalizing inputs large problems impractical maintain table like grid-world example.']\n",
      "\n",
      "['possible reinstate markov framework (over x’s) x includes current sensory precepts information agent’s memory.', 'perceptual alias- ing, longer guarantee q learning result useful action policies, let optimal ones.', 'case, longer markov problem; is, x vector, given action, depend sequence previous ones immediately preceding one.', 'delayed-reinforcement learning interesting examples delayed-reinforcement training simulated actual robots requiring structural credit assignment reported [lin, 1992, mahadevan & connell, 1992].', 'input vector results agent’s perceptual apparatus (as assume does), reason suppose uniquely identiﬁes environmental state.', '11.5.5 scaling problems diﬃculties far prohibited wide application reinforcement learn- ing large problems. (', '11.5.4 partially observable states far, identiﬁed input vector, x, actual state envi- ronment.', 'use random actions • favor states visited recently • separate learning phase use phase • employ teacher guide exploration b. slow time convergence • combine learning prior knowledge; use estimates q values (rather random values) initially • use hierarchy actions; learn primitive actions ﬁrst freeze useful sequences macros learn use macros', 'the td-gammon program, mentioned chap- ter, probably unique terms success high-dimensional problem.)', 'researchers attempted deal problem variety methods including attempting model “hid- den” states internal memory [lin, 1993].']\n",
      "\n",
      "['bibliographical historical remarks 155 • employ teacher; use graded “lessons”—starting near rewards backing away, use examples good behavior [lin, 1992] • use eﬃcient computations; e.g. updates episode [moore & atkeson, 1993] c. large state spaces • use hand-coded features • use neural networks • use nearest-neighbor methods [moore, 1990] d. temporal discounting problems.', 'use learning method based average rewards [schwartz, 1993] e. “transfer” learning .', 'small γ learner greedy present rewards indiﬀerent future; large γ slows learning. •', 'reinforcement learning replaced “planner” uses action model produce plans achieve goals.', 'learned depends reward struc- ture; rewards change, learning start over. •', 'separate learning parts learn “action model” predicts actions change states (and constant prob- lems), learn “values” states reinforcement learn- ing diﬀerent set rewards.']\n",
      "\n",
      "[]\n",
      "\n",
      "['chapter 12 explanation-based learning 12.1 deductive learning learning methods studied far, typically training set ex- haust version space.', 'logic, deductive system conclusions logically follow set input facts, system sound.1 contrast inductive deductive systems logical setting, suppose set facts (the training set) includes following formulas {round(obj1), round(obj2), round(obj3), round(obj4), ball(obj1), ball(obj2), ball(obj3), ball(obj4)} learning system forms conclusion (∀x)[ball(x) ⊃round(x)] in- ductive.', 'sense, implicitly knew φ along, inherent knowing ∆. yet, φ obvious given ∆, 1logical reasoning systems sound, example non-monotonic reasoning, produce inductive conclusions logically follow input facts.', 'suppose logical proposition, φ, logically follows set facts, ∆. circumstances process deducing φ ∆results learning φ?']\n",
      "\n",
      "['strictly speaking, speed-up learning result system able decisions that, principle, learning took place.', 'surface, real diﬀerence experience-based hypotheses chess player makes constitutes good play kind learning studying far.', 'let suppose proof constructed depend given triangle right triangle; case learn general fact.', 'extreme case, chess player said learn chess optimal play inherent rules chess.', '12.2 domain theories types information present inductive methods studied information inherent training samples information domain implied “bias” (for example, hypothesis set choose functions).', 'typically, smaller hypothesis set (that is, priori information function sought), dependent information supplied training set (that is, fewer samples).', 'ebl, specialize parts domain theory explain particular ex- ample, generalize explanation produce element domain theory useful similar examples.', 'example, suppose given theorems geometry asked prove sum angles right triangle 180 degrees.']\n",
      "\n",
      "['or, loan oﬃcer bank, ask sorts things s/he looks making decision loan, encode knowledge set rules expert system, use expert system decisions.', 'knowledge loan oﬃcer originated set “policies” (the domain theory), application policies specialized eﬃcient experience special cases loans district.', 'example 159 domain theory example (x p) prove x p specialize explanation (proof) generalize new domain rule things \"like\" x p y like x complex proof process trivial proof y p figure 12.1 ebl process data people known good bad credit risks train classiﬁer decisions.']\n",
      "\n",
      "['sees(x, y) ∧habile(x) ⊃fixes(x, y) (a habile individual entity ﬁx entity.)', 'suppose given number facts num5, as robot(num5) r2d2(num5) age(num5, 5) manufacturer(num5, gr) . . .', 'explanation-based learning suppose domain theory logical sentences taken together, help deﬁne robot classiﬁed robust. (', 'example, let’s suppose domain theory includes sentences fixes(u, u) ⊃robust(u) (an individual ﬁx robust.)', 'having found proof particular robot, able derive new sentence use allows faster conclusion.']\n",
      "\n",
      "['example 161 fixes(u, u) => robust(u) robust(num5) fixes(num5, num5) sees(num5,num5) habile(num5) sees(x,y) & habile(x) => fixes(x,y) robot(w) => sees(w,w) robot(num5) r2d2(x) => habile(x) r2d2(num5) figure 12.2 proof tree told robust(num5) true, attempt ﬁnd proof assertion facts num5 domain theory.', 'fact, con- struct following rule explanation robot(num5) ∧r2d2(num5) ⊃robust(num5) explanation allowed prune attributes num5 irrelevant (at deciding robust(num5)).']\n",
      "\n",
      "['explanation-based learning examination proof shows proof structure, sentences domain theory, independently talking num5 individual.', 'apply rules proof forward direction, keeping track substitutions imposed general uniﬁers proof. (', 'clearly, certain assumptions, general rule easily con- clude robust individual original proof process was.', 'substitutions applied variables tip nodes root node yield general rule robot(r) ∧r2d2(r) ⊃robust(r).', 'sole role example instance ebl provide template proof help guide generalization process.', 'process num5 example generalized variable [dejong & mooney, 1986] identity elimination (the precise identity num5 turned irrelevant). (', '12.4 evaluable predicates domain theory includes number predicates occuring formula trying prove custom- arily describe individual.', 'example, replace robot(num5) robot(r) r2d2(num5) r2d2(s) redo proof—using explanation proof template.', 'basing generalization process examples helps insure learn rules matched distribution problems occur.', 'the generalization process described ex- ample based [dejong & mooney, 1986] diﬀers [mitchell, et al.,', 'note occurrence sees(r, r) node tree forces uniﬁcation x y domain rule, sees(x, y)∧habile(y) ⊃fixes(x, y).', 'generalize proof process replaces constants tip nodes proof tree variables works upward—using uniﬁcation constrain values variables needed obtain proof.']\n",
      "\n",
      "['finding new rule corresponds ﬁnding simpler expression formula proved terms evaluable predicates.', 'evaluable predicates 163 robust(r) fixes(r, r) sees(r,r) habile(s) robot(r) r2d2(s) {r/w} {s/x} {r/x, r/y, r/s} {r/u} robot(w) => sees(w,w) r2d2(x) => habile(x) sees(x,y) & habile(x) => fixes(x,y) fixes(u, u) => robust(u) r2d2(r) applying {r/s} figure 12.3 generalized proof tree “extensional,” logical rules “intensional.”', 'logical rules serve connect data base predicates higher level abstractions described (if deﬁned) rules.', 'evaluable predicates correspond components input pattern vector; predicates domain theory correspond hidden units.', 'typically look truth values formulas containing intensional predicates; derived rules database.', 'domain theory useful connecting formulas want prove truth values “looked up” evaluated.', 'usage reﬂects fact predicates data base deﬁned extension—we explicitly list tuples sastisfying relation.']\n",
      "\n",
      "['12.4, consider problem ﬁnding plan robot room r1 fetch box, b1, going adjacent room, r2, pushing', 'considering examples (num5 num6), arises, want generalize rules like robot(u)∧[c3po(u)∨r2d2(u)] ⊃robust(u)?', 'explanation-based learning 12.5 general proofs examining domain theory example reveals alternative rule been robot(u) ∧c3po(u) ⊃robust(u).', '12.7.1 macro-operators planning automatic planning systems, eﬃciency enhanced chain- ing sequence operators macro-operators.', 'example [dejong & mooney, 1986] structural generalization (via disjunctive augmen- tation ).', 'seeing number similar examples, willing induce formula bionic(u) ⊃[c3po(u) ∨r2d2(u)] case rule disjunction replaced robot(u) ∧bionic(u) ⊃robust(u).', 'adding new rule decreases depth shortest proof increases number formulas domain theory.', '12.6 utility ebl known theorem proving complexity ﬁnding proof depends number formulas domain theory depth shortest proof.', 'exam- ple process creating macro-operators based techniques explored [fikes, et al.,', 'example, eﬃciency retrieved evaluable predicate, say, bionic(u) domain theory contained r2d2(x) ⊃bionic(x) c3po(x) ⊃ bionic(x).']\n",
      "\n",
      "['r1 r2 r3 d1 d2 b1 initial state inroom(robot, r1) inroom(b1,r2) connects(d1,r1,r2) connects(d1,r2,r1) . . .', 'figure 12.4 initial state robot problem construct plan set strips operators include gothru(d, r1, r2) preconditions inroom(robot, r1), connects(d, r1, r2) delete list inroom(robot, r1) add list inroom(robot, r2) pushthru(b, d, r1, r2) preconditions inroom(robot, r1), connects(d, r1, r2), inroom(b, r1) delete list inroom(robot, r1), inroom(b, r1) add list inroom(robot, r2), inroom(b, r2) backward-reasoning strips system produce plan shown fig.']\n",
      "\n",
      "['preconditions generalized plan are inroom(robot, r1) connects(d1, r1, r2) connects(d2, r2, r4) inroom(b, r4) inroom(b1,r1) pushthru(b1,d,r1,r1) inroom(robot, r1), connects(d, r1, r1), inroom(b1, r1) inroom(robot, r2), connects(d1, r2, r1), inroom(b1, r2) {r2/r1, d1/d} gothru(d2, r3, r2) inroom(robot, r3), connects(d2, r3, r2), connects(d1, r2, r1), inroom(b1, r2) {r1/r3, d1/d2} inroom(robot, r1), connects(d1, r1, r2), connects(d1, r2, r1), inroom(b1, r2) r1 r2 r3 d1 d2 gothru(d1,r1,r2) pushthru(b1,d1,r2,r1) b1 plan figure 12.5 plan robot problem related technique chains sequences operators form general ones chunking mechanism soar [laird, et al.,', 'explanation-based learning connects(d1, r1, r2) connects(d1, r2, r1) inroom(b1, r2) saving speciﬁc plan, valid speciﬁc constants mentions, useful saving general one.']\n",
      "\n",
      "['system called prodigy, minton proposed ebl learn eﬀective ways control search [minton, 1988].', 'meta theory includes statements control choice subgoal work on, oper- ator apply, etc.', 'prodigy strips-like system solves planning problems blocks-world, simple mobile robot world, job-shop scheduling.', 'applications 167 inroom(b1,r4) pushthru(b1,d2,r2,r4) inroom(robot, r2), connects(d1, r1, r2), connects(d2, r2, r4), inroom(b1, r4) gothru(d1, r1, r2) inroom(robot, r1), connects(d1, r1, r2), connects(d2, r2, r4), inroom(b1, r4) figure 12.6 generalized plan 12.7.2 learning search control knowledge use creating macro-operators, ebl methods improve eﬃciency planning way also.', 'producing plan, analyzes successful unsuccessful choices attempts explain terms domain theory.']\n",
      "\n",
      "['explanation-based learning (and (current −node node) (candidate −goal node (on x y)) (candidate −goal node (on y z))) (prefer goal (on y z) (on x y)) prodigy keeps statistics learned rules used, savings (in time ﬁnd plans), cost application.', 'minton [minton, 1990] shown overall advantage rules (as having rules hand-coded search control rules).']\n",
      "\n",
      "['computational learning theory natural learning systems, volume 1 constraints prospects, pp.', 'anderson, 1958] anderson, t. w., introduction multivariate statistical analysis, new york john wiley, 1958. [', 'baum, 1994] baum, e., “when k-nearest neighbor backpropagation accurate feasible-sized sets examples?”', 'innovative applications artiﬁcial intelligence, menlo park, ca aaai press, 1992. [', 'aha, 1991] aha, d., kibler, d., albert, m., “instance-based learning algorithms,” machine learning, 6, 37-66, 1991. [', 'bibliography [acorn & walden, 1992] acorn, t., walden, s., “smart support man- agement automated reasoning technology compaq customer ser- vice,” proc.', 'bellman, 1957] bellman, r. e., dynamic programming, princeton princeton university press, 1957. [', 'anderson & bower, 1973] anderson, j. r., bower, g. h., human asso- ciative memory, hillsdale, nj erlbaum, 1973. [', 'barto, bradtke, & singh, 1994] barto, a., bradtke, s., singh, s., “learn- ing act real-time dynamic programming,” appear ar- tiﬁcial intelligence, 1994. [', 'baum & haussler, 1989] baum, e, haussler, d., “what size net gives valid generalization?”', 'bollinger & duﬃe, 1988] bollinger, j., duﬃe, n., computer control machines processes, reading, ma addison-wesley, 1988.']\n",
      "\n",
      "['reprinted shavlik, j. dietterich, t., readings machine learn- ing, san francisco morgan kaufmann, 1990, pp 452-467.', 'brent, 1990] brent, r. p., “fast training algorithms multi-layer neural nets,” numerical analysis project manuscript na-90-03, computer sci- ence department, stanford university, stanford, 94305, march 1990. [', 'dayan & sejnowski, 1994] dayan, p., sejnowski, t., “td(λ) converges probability 1,” machine learning, 14, pp.', '3-10), contract da 36-039 sc-78343, sri international, menlo park, ca, june 1962 september 1962. [', 'dasarathy, 1991] dasarathy, b. v., nearest neighbor pattern classiﬁcation techniques, ieee computer society press, 1991. [', 'cover & hart, 1967] cover, t., hart, p., “nearest neighbor pattern clas- siﬁcation,” ieee trans.', '1984] breiman, l., friedman, j., olshen, r., stone, c., classiﬁcation regression trees, monterey, ca wadsworth, 1984. [', 'reprinted shavlik, j. dietterich, t., readings machine learning, morgan kaufmann, san francisco, pp.', 'dayan, 1992] dayan, p., “the convergence td(λ) general λ,” machine learning, 8, 341-362, 1992. [', 'carbonell, 1983] carbonell, j., “learning analogy,” machine learning artiﬁcial intelligence approach, michalski, r., carbonell, j., mitchell, t., (eds.),', 'cover, 1965] cover, t., “geometrical statistical properties systems linear inequalities applications pattern recognition,” ieee trans.', 'dejong & mooney, 1986] dejong, g., mooney, r., “explanation-based learning alternative view,” machine learning, 1145-176, 1986.']\n",
      "\n",
      "['1990] dietterich, t., hild, h., bakiri, g., “a compara- tive study id3 backpropagation english text-to-speech map- ping,” proc.', 'report prepared onr contract 3438(00), sri in- ternational, menlo park, ca, april 1966. [', 'evans & fisher, 1992] evans, b., fisher, d., process delay analyses decision-tree induction, tech.', 'efron, 1982] efron, b., jackknife, bootstrap resampling plans, philadelphia siam, 1982. [', 'etzioni, 1993] etzioni, o., “a structural theory explanation-based learn- ing,” artiﬁcial intelligence, 601, pp.', 'report cs92-06, department com- puter science, vanderbilt university, tn, 1992. [', 'fahlman & lebiere, 1990] fahlman, s., lebiere, c., “the cascade- correlation learning architecture,” touretzky, d., (ed.),', 'bibliography 171 [dietterich & bakiri, 1991] dietterich, t. g., bakiri, g., “error-correcting output codes general method improving multiclass induc- tive learning programs,” proc.', 'duda, 1966] duda, r. o., “training linear machine mislabeled patterns,” sri tech.', 'duda & fossum, 1966] duda, r. o., fossum, h., “pattern classiﬁcation iteratively determined linear piecewise linear discriminant functions,” ieee trans.']\n",
      "\n",
      "['1993] fayyad, u. m., weir, n., djorgovski, s., “skicat machine learning system automated cataloging large scale sky surveys,” proc.', '1972] fikes, r., hart, p., nilsson, n., “learning execut- ing generalized robot plans,” artiﬁcial intelligence, pp 251-288, 1972.', 'reprinted shavlik, j. dietterich, t., readings machine learn- ing, san francisco morgan kaufmann, 1990, pp 468-486. [', 'fisher, 1987] fisher, d., “knowledge acquisition incremental conceptual clustering,” machine learning, 2139-172, 1987.', 'for longer version paper see fayyad, u. djorgovski, g., weir, n., “automating analysis cataloging sky surveys,” fayyad, u., et al.(eds.),', 'gallant, 1986] gallant, s. i., “optimal linear discriminants,” eighth inter- national conf.', 'reprinted shavlik, j. dietterich, t., readings machine learning, san francisco morgan kaufmann, 1990, pp.', 'genesereth & nilsson, 1987] genesereth, m., nilsson, n., logical founda- tions artiﬁcial intelligence, san francisco morgan kaufmann, 1987. [', 'haussler, 1988] haussler, d., “quantifying inductive bias ai learning al- gorithms valiant’s learning framework,” artiﬁcial intelligence, 36177-221, 1988.', 'fu, 1994] fu, l., neural networks artiﬁcial intelligence, new york mcgraw-hill, 1994. [', 'gluck & rumelhart, 1989] gluck, m. rumelhart, d., neuroscience connectionist theory, developments connectionist theory, hills- dale, nj erlbaum associates, 1989. [', '1977] friedman, j. h., bentley, j. l., finkel, r. a., “an algorithm finding best matches logarithmic expected time,” acm trans.', 'feigenbaum, 1961] feigenbaum, e. a., “the simulation verbal learning be- havior,” proceedings western joint computer conference, 19121- 132, 1961. [']\n",
      "\n",
      "['hirsh, 1994] hirsh, h., “generalizing version spaces,” machine learning, 17, 5-45, 1994. [', 'ma- chine learning artiﬁcial intelligence approach, volume 2, chapter 20, san francisco morgan kaufmann, 1986. [', 'ieee pwer engineering society summer meeting, san francisco, ca, 1987. [', 'koza, 1994] koza, j., genetic programming ii automatic discovery reusable programs, cambridge, ma mit press, 1994.', 'koza, 1992] koza, j., genetic programming programming comput- ers means natural selection, cambridge, ma mit press, 1992. [', 'bibliography 173 [haussler, 1990] haussler, d., “probably approximately correct learning,” proc.', 'kohavi, 1994] kohavi, r., “bottom-up induction oblivious read-once de- cision graphs,” proc.', 'holland, 1986] holland, j. h., “escaping brittleness; possibilities general-purpose learning algorithms applied parallel rule-based systems.”', 'hunt, marin, & stone, 1966] hunt, e., marin, j., stone, p., experiments induction, new york academic press, 1966. [', 'hertz, krogh, & palmer, 1991] hertz, j., krogh, a, palmer, r., introduc- tion theory neural computation, lecture notes, vol.', 'hebb, 1949] hebb, d. o., organization behaviour, new york john wiley, 1949. [', 'holland, 1975] holland, j., adaptation natural artiﬁcial systems, ann arbor university michigan press, 1975. (', 'kolodner, 1993] kolodner, j., case-based reasoning, san francisco morgan kaufmann, 1993. [', 'kaelbling, 1993] kaelbling, l. p., learning embedded systems, cambridge, ma mit press, 1993. [']\n",
      "\n",
      "['computational learning theory natural learning systems, volume 1 constraints prospects, pp.', 'langley, 1996] langley, p., elements machine learning, san francisco morgan kaufmann, 1996. [', '1986] laird, j., rosenbloom, p., newell, a., “chunking soar anatomy general learning mechanism,” machine learn- ing, 1, pp.', 'mcculloch & pitts, 1943] mcculloch, w. s., pitts, w. h., “a logical cal- culus ideas immanent nervous activity,” bulletin mathe- matical biophysics, vol.', 'minton, 1988] minton, s., learning search control knowledge explanation-based approach, kluwer academic publishers, boston, ma, 1988.', 'maass & tur´an, 1994] maass, w., tur´an, g., “how fast thresh- old gate learn?,”', 'lin, 1992] lin, l., “self-improving reactive agents based reinforcement learning, planning, teaching,” machine learning, 8, 293-321, 1992. [', 'lavraˇc & dˇzeroski, 1994] lavraˇc, n., dˇzeroski, s., inductive logic pro- gramming, chichester, england ellis horwood, 1994. [', 'mahadevan & connell, 1992] mahadevan, s., connell, j., “automatic programming behavior-based robots reinforcement learn- ing,” artiﬁcial intelligence, 55, pp.', 'marchand & golea, 1993] marchand, m., golea, m., “on learning sim- ple neural concepts halfspace intersections neural decision lists,” network, 467-85, 1993. [', 'michie, 1992] michie, d., “some directions machine intelligence,” unpub- lished manuscript, turing institute, glasgow, scotland, 1992. [', 'littlestone, 1988] littlestone, n., “learning quickly irrelevant at- tributes abound new linear-threshold algorithm,” machine learn- ing 2 285-318, 1988. [']\n",
      "\n",
      "['muggleton, 1991] muggleton, s., “inductive logic programming,” new gen- eration computing, 8, pp.', 'natarjan, 1991] natarajan, b., machine learning theoretical approach, san francisco morgan kaufmann, 1991.', 'bibliography 175 [minton, 1990] minton, s., “quantitative results concerning utility explanation-based learning,” artiﬁcial intelligence, 42, pp.', 'reprinted shavlik, j. dietterich, t., readings machine learning, san francisco morgan kaufmann, 1990, pp.', 'muggleton, 1992] muggleton, s., inductive logic programming, london aca- demic press, 1992. [', 'moore, 1992] moore, a., “fast, robust adaptive control learning forward models,” moody, j., hanson, s., lippman, r., (eds.),', 'moore & atkeson, 1993] moore, a., atkeson, c., “prioritized sweeping reinforcement learning data time,” machine learn- ing, 13, pp.', 'mueller & page, 1988] mueller, r. page, r., symbolic computing lisp prolog, new york john wiley & sons, 1988. [', 'muroga, 1971] muroga, s., threshold logic applications, new york wiley, 1971. [', '1994] moore, a. w., hill, d. j., johnson, m. p., “an em- pirical investigation brute force choose features, smoothers, function approximators,” hanson, s., judd, s., petsche, t., (eds.),', 'advances neural information processing systems 4, san francisco morgan kaufmann, 1992. [']\n",
      "\n",
      "['pomerleau, 1993] pomerleau, d, neural network perception mobile robot guidance, boston kluwer academic publishers, 1993. [', 'quinlan, 1986] quinlan, j. ross, “induction decision trees,” machine learning, 181–106, 1986.', 'quinlan, 1987] quinlan, j. r., “generating production rules decision trees,” ijcai-87 proceedings tenth intl.', 'pomerleau, 1991] pomerleau, d., “rapidly adapting artiﬁcial neural net- works autonomous navigation,” lippmann, p., et al. (', '176 bibliography [nilsson, 1965] nilsson, n. j., “theoretical experimental investigations trainable pattern-classifying systems,” tech.', 'peterson, 1961] peterson, w., error correcting codes, new york john wiley, 1961. [', 'quinlan & rivest, 1989] quinlan, j. ross, rivest, ron, “inferring deci- sion trees minimum description length principle,” informa- tion computation, 80227–248, march, 1989. [', 'reprinted shavlik, j. dietterich, t., readings machine learning, san francisco morgan kaufmann, 1990, pp.', 'pazzani & kibler, 1992] pazzani, m., kibler, d., “the utility knowl- edge inductive learning,” machine learning, 9, 57-94, 1992. [', 'pagallo & haussler, 1990] pagallo, g. haussler, d., “boolean feature dis- covery empirical learning,” machine learning, vol.5, no.1, pp.', 'radc-tr- 65-257, final report contract af30(602)-3448, rome air develop- ment center (now rome laboratories), griﬃss air force base, new york, september, 1965. [', 'quinlan, 1990] quinlan, j. r., “learning logical deﬁnitions relations,” machine learning, 5, 239-266, 1990.', 'nilsson, 1990] nilsson, n. j., mathematical foundations learning ma- chines, san francisco morgan kaufmann, 1990. (', 'oliver, dowe, & wallace, 1992] oliver, j., dowe, d., wallace, c., “infer- ring decision graphs minimum message length principle,” proc.', 'this book reprint learning machines foundations trainable pattern-classifying systems, new york mcgraw-hill, 1965.) [']\n",
      "\n",
      "['shavlik, mooney, & towell, 1991] shavlik, j., mooney, r., towell, g., “symbolic neural learning algorithms experimental compar- ison,” machine learning, 6, pp.', 'rivest, 1987] rivest, r. l., “learning decision lists,” machine learning, 2, 229-246, 1987. [', 'com- putational learning theory natural learning systems, volume 1 constraints prospects, pp.', 'quinlan, 1994] quinlan, j. r., “comparing connectionist symbolic learn- ing methods,” hanson, s., drastal, g., rivest, r., (eds.),', 'ross, 1983] ross, s., introduction stochastic dynamic programming, new york academic press, 1983. [', 'russell & norvig 1995] russell, s., norvig, p., artiﬁcial intelligence modern approach, englewood cliﬀs, nj prentice hall, 1995. [', 'samuel, 1959] samuel, a., “some studies machine learning game checkers,” ibm journal research development, 3211-229, july 1959. [', 'rosenblatt, 1958] rosenblatt, f., principles neurodynamics, washington spartan books, 1961. [', 'ridgway, 1962] ridgway, w. c., adaptive logic system generalizing properties, phd thesis, tech.', 'sejnowski, koch, & churchland, 1988] sejnowski, t., koch, c., church- land, p., “computational neuroscience,” science, 241 1299-1306, 1988. [', 'schwartz, 1993] schwartz, a., “a reinforcement learning method max- imizing undiscounted rewards,” proc.', 'shavlik & dietterich, 1990] shavlik, j. dietterich, t., readings ma- chine learning, san francisco morgan kaufmann, 1990.', 'bibliography 177 [quinlan, 1993] quinlan, j. ross, c4.5 programs machine learning, san francisco morgan kaufmann, 1993. [', 'rissanen, 1978] rissanen, j., “modeling shortest data description,” auto- matica, 14465-471, 1978. [', 'rumelhart, hinton, & williams, 1986] rumelhart, d. e., hinton, g. e., williams, r. j., “learning internal representations error propa- gation,” rumelhart, d. e., mcclelland, j. l., (eds.)']\n",
      "\n",
      "['178 bibliography [sutton & barto, 1987] sutton, r. s., barto, a. g., “a temporal- diﬀerence model classical conditioning,” proceedings ninth annual conference cognitive science society, hillsdale, nj erl- baum, 1987. [', 'watkins & dayan, 1992] watkins, c. j. c. h., dayan, p., “technical note q-learning,” machine learning, 8, 279-292, 1992.', 'tesauro, 1992] tesauro, g., “practical issues temporal diﬀerence learn- ing,” machine learning, 8, nos.', 'sutton, 1990] sutton, r., “integrated architectures learning, planning, reacting based approximating dynamic programming,” proc.', 'unger, 1989] unger, s., essence logic circuits, englewood cliﬀs, nj prentice-hall, 1989. [', 'sutton, 1988] sutton, r. s., “learning predict methods temporal diﬀerences,” machine learning 3 9-44, 1988. [', 'towell & shavlik, 1992] towell g., shavlik, j., “interpretation artiﬁ- cial neural networks mapping knowledge-based neural networks rules,” moody, j., hanson, s., lippmann, r., (eds.),', 'taylor, michie, & spiegalhalter, 1994] taylor, c., michie, d., spiegal- halter, d., machine learning, neural statistical classiﬁcation, paramount publishing international. [', 'towell, shavlik, & noordweier, 1990] towell, g., shavlik, j., noordweier, m., “reﬁnement approximate domain theories knowledge-based artiﬁcial neural networks,” proc.', 'utgoﬀ, 1989] utgoﬀ, p., “incremental induction decision trees,” machine learning, 4161–186, nov.,', 'vapnik & chervonenkis, 1971] vapnik, v., chervonenkis, a., “on uniform convergence relative frequencies, theory probability applications, vol.', 'various editors, 1989-1994] advances neural information processing sys- tems, vols 1 6, san francisco morgan kaufmann, 1989 -1994. [']\n",
      "\n",
      "['winder, 1962] winder, r., threshold logic, phd dissertation, princeton uni- versity, princeton, nj, 1962. [', 'bibliography 179 [watkins, 1989] watkins, c. j. c. h., learning delayed rewards, phd thesis, university cambridge, england, 1989. [', 'widrow, 1962] widrow, b., “generalization storage networks ada- line neurons,” yovits, jacobi, goldstein (eds.),', 'widrow & lehr, 1990] widrow, b., lehr, m. a., “30 years adaptive neural networks perceptron, madaline backpropagation,” proc.', 'widrow & stearns, 1985] widrow, b., stearns, s., adaptive signal pro- cessing, englewood cliﬀs, nj prentice-hall. [', 'werbos, 1974] werbos, p., regression new tools prediction analysis behavioral sciences, ph.d.', 'weiss & kulikowski, 1991] weiss, s., kulikowski, c., computer systems learn, san francisco morgan kaufmann, 1991. [']\n",
      "\n",
      "['supervised machine learning algorithms, provide labeled data, example, prediction stock market prices, unsupervised need labeled data, example, classification emails spam non-spam.', 'lesser variables parameters, variance reduced ● cross-validation methods like k-folds ● model parameters likely cause overfitting, techniques regularization like lasso penalize parameters steve nouri \\u200b \\u200bhttps//www.linkedin.com/in/stevenouri/', 'whereas, use regression analysis dealing continuous data, example predicting stock prices certain point time.', 'overfitting situation occurs model learns training set well, taking random fluctuations training data concepts.', 'examples include decision trees, k-nearest neighbors, topic models latent dirichlet analysis.', '100 machine learning s & answers steve nouri q1 explain difference supervised unsupervised machine learning?', 'involves cost term features involved objective function ● making simple model.']\n",
      "\n",
      "['easiest ways handle missing corrupted data drop rows columns replace entirely value.', 'useful methods pandas ● isnull() dropna() help find columns/rows missing data drop ● fillna() replace wrong values placeholder value q7 explain ensemble learning.', 'algorithms methods finding set parameters minimize loss function evaluating parameters data making adjustments.', 'split given data set different sections namely,’training set’ ‘test set’. ‘', \"stochastic gradient descent, you'll evaluate 1 training sample set parameters updating them.\", 'complex models prone overfitting (high variance) expressive close truth (low bias).', 'ensemble learning, base models like classifiers regressors generated combined better results.', 'predictive models tradeoff bias (how model fits data) variance (how model changes based changes inputs).', 'training set small, model right bias low variance work better likely overfit.']\n",
      "\n",
      "['models low bias high variance tend perform better work fine complex relationships.', 'fourier transform converts signal time frequency domain — it’s common way extract features audio signals time series sensor data.', 'l2 regularization tends spread error terms, l1 binary/sparse, variables assigned 1 0 weighting.', 'deep learning subset machine learning concerned neural networks use backpropagation certain principles neuroscience accurately model large sets unlabelled semi-structured data.', 'term ‘false positive,’ word ‘positive’ refers ‘yes’ row predicted value confusion matrix.', \"test set small, you'll unreliable estimation model performance (performance statistic high variance).\", 'fourier transform finds set cycle speeds, amplitudes, phases match time signal.']\n",
      "\n",
      "['applications supervised machine learning include ● email spam detection train model historical data consists emails categorized spam spam.', '● healthcare diagnosis providing images disease, model trained detect person suffering disease not.', 'generative model learn categories data discriminative model simply learn distinction different categories data.', 'case semi-supervised learning, training data contains small labeled data large unlabeled data.', 'supervised learning uses data completely labeled, unsupervised learning uses training data.', '● fraud detection training model identify suspicious patterns, detect instances possible fraud.', '● sentiment analysis refers process algorithms documents determine they’re positive, neutral, negative sentiment.', '● example, ecommerce website suggest items buy, based prior purchases made, spending habits, items wishlist, customers’ purchase habits, on.']\n",
      "\n",
      "['latent dirichlet allocation (lda) common method topic modeling, classifying documents subject matter.', 'lda generative model represents documents mixture topics probability distribution possible words. \"', 'algorithm assumes presence feature class related presence feature (absolute independence features), given class variable.', 'new features, principal components, sequentially maximize variance represented (i.e. principal component variance, second principal component second most, on).', 'weighted average precision recall model, results tending 1 best, tending 0 worst.']\n",
      "\n",
      "['● time email hit inbox, spam filter use statistical analysis algorithms like decision trees svm determine likely email spam ● likelihood high, label spam, email won’t hit inbox steve nouri \\u200b \\u200bhttps//www.linkedin.com/in/stevenouri/', 'building spam filter involves following process ● email spam filter fed thousands emails ● emails label ‘spam’ ‘not spam.’', 'simple restatement fundamental problem machine learning possibility overfitting training data carrying noise data test set, providing inaccurate generalizations.', 'fixed rule choose algorithm classification problem, follow guidelines ● accuracy concern, test different algorithms cross-validate ● training dataset small, use models low variance high bias ● training dataset large, use models high variance little bias q28 design email spam filter?', 'main methods avoid overfitting 1- model simpler reduce variance taking account fewer variables parameters, removing noise training data.', '3- use regularization techniques lasso penalize certain model parameters they’re likely cause overfitting.', 'use classification regression wanted results reflect belongingness data points dataset certain explicit categories (ex wanted know male female correlated male female names.)', '● supervised machine learning algorithm determine type emails marked spam based spam words like lottery, free offer, money, refund, etc.', 'classification produces discrete values dataset strict categories, regression gives continuous results allow better distinguish differences individual points.']\n",
      "\n",
      "['roc (receiver operating characteristic) performance plot binary classifiers true positive rate (y-axis) vs. false positive rate (x- axis).', 'split dataset training test sets, use cross-validation techniques segment dataset composite sets training test sets data.', '● based accuracy model, use algorithm highest accuracy testing models q29 evaluation approaches work gauge effectiveness machine learning model?', 'lot machine learning interview s type involve implementation machine learning models company’s problems.', \"auc area roc curve, it's common performance metric evaluating binary classification models.\", \"it's equivalent expected probability uniformly drawn random positive ranked uniformly drawn random negative.\", 'what’s important demonstrate understand nuances model measured choose right performance measures right situations.', 'you’ll research company industry in-depth, especially revenue drivers company has, types users company takes context industry it’s in.']\n",
      "\n",
      "['● precision = (true positive) / (true positive + false positive) recall ● recall ratio number events recall number total events.', 'begin leaf nodes popular pruning algorithm called reduced error pruning, which ● starting leaves, node replaced popular class ● prediction accuracy affected, change kept ● advantage simplicity speed steve nouri \\u200b \\u200bhttps//www.linkedin.com/in/stevenouri/', 'advantages\\u200b neural networks (specifically deep nns) led performance breakthroughs unstructured datasets images, audio, video.', \"example, want detect type cancer that's prevalent 1% population, build model achieves 99% accuracy simply classifying cancer-free.\", 'precision ● precision ratio events correctly recall total number events recall (mix correct wrong recalls).', 'decision tree builds classification (or regression) models tree structure, datasets broken ever-smaller subsets developing decision tree, literally tree-like way branches nodes.', '● recall = (true positive) / (true positive + false negative) q36 decision tree classification?', 'q33 area roc curve (auroc) better raw accuracy out-of-sample evaluation metric?']\n",
      "\n",
      "['mechanisms similar first, means order k-nearest neighbors work, need labeled data want classify unlabeled point (thus nearest neighbor part).', 'reduce dimensionality combining features feature engineering, removing collinear features, algorithmic dimensionality reduction.', 'here, it’s important remember while, model needs checked sure it’s working correctly.', 'k-means clustering requires set unlabeled points threshold algorithm unlabeled points gradually learn cluster groups computing mean distance different points.', 'spotify shopped amazon recognize recommendation system it’s information filtering system predicts user want hear based choice patterns provided user.', 'stages building machine learning model are ● model building choose suitable algorithm model train according requirement ● model testing\\u200b check accuracy model test data ● applying mode \\u200bmake required changes testing use final model real-time projects.', 'machine learning relates study, design, development algorithms computers capability learn explicitly programmed.', 'data mining defined process unstructured data tries extract knowledge unknown interesting patterns.']\n",
      "\n",
      "['pca (principal components analysis), kpca ( kernel-based principal component analysis) ica ( independent component analysis) important feature extraction techniques dimensionality reduction.', 'statistical learning techniques allow learning function predictor set observed data predictions unseen future data.', 'bias term measures closely average classifier produced learning algorithm matches target function.', 'different types techniques machine learning ● supervised learning ● unsupervised learning ● semi-supervised learning ● reinforcement learning ● transduction ● learning learn q45 given data set.', 'know, normal distribution, ~68% data lies 1 standard deviation mean (or mode, median), leaves ~32% data unaffected.', 'techniques provide guarantees performance learned predictor future unseen data based statistical assumption data generating process.']\n",
      "\n",
      "['conceptually, say, lasso regression (l1) variable selection parameter shrinkage, ridge regression parameter shrinkage end including coefficients model.', 'context confusion matrix, type error occurs classify value positive (1) actually negative (0).', 'hence, classifier run unseen sample, couldn’t find patterns returned predictions higher error.', 'type error committed null hypothesis true reject it, known ‘false positive’.', 'convex hull created, maximum margin hyperplane (mmh) perpendicular bisector convex hulls.', 'type ii error committed null hypothesis false accept it, known ‘false negative’.', 'quote islr’s authors hastie, tibshirani asserted that, presence variables medium / large sized effect, use lasso regression.', 'case linearly separable data, convex hull represents outer boundaries groups data points.', 'training error 0.00 means classifier mimicked training data patterns extent, available unseen data.']\n",
      "\n",
      "['example think chessboard, movement bishop rook calculated manhattan distance respective vertical & horizontal movements.', 'simple words, ordinary square(ols) method linear regression approximates parameters resulting minimum distance actual predicted values.', 'vif = variance model / variance model single independent variable calculate ratio independent variable.', 'maximum likelihood helps choosing values parameters maximizes likelihood parameters likely produce observed data.', 'q53 suggest treating categorical variable continuous variable result better predictive model?', 'ols maximum likelihood methods respective regression methods approximate unknown parameter (coefficient) value.']\n",
      "\n",
      "['● multi-class classification problems, svms require one-vs-rest method, scalable memory intensive.', 'couple drawbacks linear model ● linear model holds strong assumptions true application.', 'simplest example cross-validation split data groups training data testing data, use training data build model testing data test model.', 'couple reasons random forest better choice model support vector machine ● random forests allow determine feature importance.', 'assumes linear relationship, multivariate normality, little multicollinearity, auto-correlation, homoscedasticity ● linear model can’t discrete binary outcomes.']\n",
      "\n",
      "['large error gradients accumulate result large changes neural network weights training, called exploding gradient problem.', 'causality applies situations action, x, causes outcome, y, correlation relating action (x) action(y) x necessarily cause y. q67 exploding gradient problem backpropagation technique?', 'rotation pca important maximizes separation variance obtained components interpretation components easier.', 'explain kernel trick kernel way computing dot product vectors 𝐱x 𝐲y (possibly high dimensional) feature space, kernel functions called “generalized dot product” kernel trick method linear classifier solve non-linear problem transforming linearly inseparable data linearly separable ones higher dimension.', 'answer yes random forest ensemble method takes weak decision trees strong learner.', 'associative rule mining techniques discover patterns data like features (dimensions) occur features (dimensions) correlated.', 'marginalizationarginalisation summing probability random variable x given joint probability distribution x variables.']\n",
      "\n",
      "['learning rate compensates penalises hyperplanes making wrong moves expansion rate deals finding maximum separation area classes.', 'rsquared represents variance captured virtual linear regression line respect total variance captured dataset.', 'context data science aiml, pruning refers process reducing redundant branches decision tree.', 'handle outliers, cap threshold, use transformations reduce skewness data remove outliers anomalies errors.', 'pruning involves turning branches decision tree leaf nodes removing leaf nodes original branch.', 'discover outliers tools functions like box plot, scatter plot, z-score, iqr score etc.', 'goals model training identify signal ignore noise model given free rein minimize error, possibility suffering overfitting.', 'q72 linear regression line stop rotating finds optimal spot fitted data?', 'decision trees prone overfitting, pruning tree helps reduce size minimizes chances overfitting.']\n",
      "\n",
      "['data augmentation technique synthesizing new data modifying existing data way target changed, changed known way.', 'example, ocr, flips change text won’t beneficial; however, resizes small rotations help.', 'visualization ● univariate visualization ● bivariate visualization ● multivariate visualization missing value treatmen\\u200bt – replace missing values mean/median outlier detection – use boxplot identify distribution outliers, apply iqr set boundary iqr q78 data augmentation?', 'modifications images ● resize ● horizontal vertical flip ● rotate ● add noise ● deform ● modify colors problem needs customized data augmentation pipeline.', 'exploratory data analysis (eda) helps analysts understand data better forms foundation better models.', 'inductive logic programming (ilp) subfield machine learning uses logic programming representing background knowledge examples.', 'difference inductive machine learning deductive machine learning follows machine-learning model learns examples set observed instances draw generalized conclusion deductive learning model draws conclusion conclusion drawn.']\n",
      "\n",
      "['firstly, need clear picture data, constraints, problems heading different machine learning algorithms.', 'that, try optimize hyperparameters ways – grid search, random search, bayesian optimization.', 'important steps follow achieve good working model data collection, data preparation, choosing machine learning model, training model, model evaluation, parameter tuning lastly prediction.', 'mainly based artificial neural network data taken input technique makes intuitive decisions artificial neural network.', 'following step data categorization step, two-step process – categorization input categorization output.', 'q81 difference machine learning deep learning machine learning branch computer science method implement artificial intelligence.', 'secondly, understand type kind data plays primary role deciding algorithm use.']\n",
      "\n",
      "['train multi-layered neural network prime motivation backpropagation learn appropriate internal demonstrations.', 'easy, sure skip closely related artificial intelligence thereby, ai interview s. machine learning algorithms easy serialize.', 'convex function continuous function, value midpoint interval given domain numerical mean values ends interval.', \"bayesian networks referred 'belief networks' 'casual networks', represent graphical model probability relationship set variables.\", 'true positive rate machine learning percentage positives properly acknowledged, recall count results correctly identified relevant.', 'genetic programming software systems implement algorithm uses random mutation, fitness function, crossover, multiple generations evolution resolve user-defined task.']\n",
      "\n",
      "['surely, people going engage machine learning near future q93 define sampling.', 'entropy measure uncertainty associated random variable y. expected number bits required communicate value variable.', '● locates certain patterns data makes certain predictions provide answers matters.', 'decision boundary decision surface hypersurface divides underlying feature space subspaces, class.', 'bayesian networks relate variables (e.g., speech signals protein sequences) called dynamic bayesian networks.', 'bayesian logic program consists components ● logical contains set bayesian clauses, capture qualitative structure domain.', 'intents machine learning stated below, ● system gets information established computations well-founded decisions outputs.', 'navigation system considered examples machine learning calculate distance places optimization techniques.']\n",
      "\n",
      "['learning curve, training error cross-validating error plotted number training data points.', 'aim generative model generate new samples distribution new data instances, whereas, discriminative model highlights differences different kinds data instances.', 'references 1 springboard.com 2 \\u200bsimplilearn.com 3 geeksforgeeks.org 4 \\u200belitedatascience.com 5 analyticsvidhya.com 6 \\u200bguru99.com 7 \\u200bintellipaat.com 8 towardsdatascience.com 9 mygreatlearning.com 10 \\u200bmindmajix.com 11 toptal.com 12 \\u200bglassdoor.co.in 13 \\u200budacity.com 14 educba.com 15 \\u200banalyticsindiamag.com 16 \\u200bubuntupit.com 17 \\u200bjavatpoint.com 18 quora.com 19 hackr.io 20 kaggle.com steve nouri \\u200b \\u200bhttps//www.linkedin.com/in/stevenouri/', 'in-depth knowledge statistics, probability, data modelling, programming language, cs, application ml libraries algorithms, software design required successful machine learning engineer.', 'feature engineering process transforming raw data features better represent underlying problem predictive models, resulting improved model accuracy unseen data.']\n",
      "\n",
      "['a. count word document b. vector notation word c. speech tag d. basic dependency grammar e. answer e)all features text corpus.', 'a. lemmatization b. soundex c. cosine similarity d. n-grams answer a) lemmatization helps base form word, e.g. playing -> play, eating -> eat, etc.other options meant different purposes.', 'a. lemmatization b. euclidean distance c. cosine similarity d. n-grams answer b) c) distance word vectors computed cosine similarity euclidean distance.', 'created document term matrix input data 20k documents machine learning model.', 'e.g. cosine angle words “football” “cricket” closer 1 compared angle words “football” “new delhi” q3.']\n",
      "\n",
      "['latent dirichlet allocation a. 1 b. 2, 3 c. 1, 3 d. 1, 2, 3 answer d) q5.', 'dissimilarity words expressed cosine similarity values significantly higher 0.5 a. true b. false answer a) q7.', 'a. speech tagging b. skip gram n-gram extraction c. continuous bag words d. dependency parsing constituency parsing answer d) q6.', 'named entity help extract organization, time, date, city, etc..type entities given sentence, speech helps extract noun, verb, pronoun, adjective, etc..from given sentence tokens.', 'a. detecting objects image b. facial recognition c. speech biometric d. text summarization', 'text parsing techniques noun phrase detection, verb phrase detection, subject detection, object detection nlp.', 'following keyword normalization techniques nlp a. stemming b. speech c. named entity recognition d. lemmatization answer a) d) speech (pos) named entity recognition(ner) keyword normalization techniques.']\n",
      "\n",
      "['correct value product tf (term frequency) idf (inverse-document- frequency), term “hello” appears approximately one-third total documents?', 'nlp, process removing words like “and”, “is”, “a”, “an”, “the” sentence called a. stemming b. lemmatization c. stop word d. answer c) lemmatization, stop words a, an, the, etc.. removed.', 'steve nouri https//www.linkedin.com/in/stevenouri/ answer (d) a) b) computer vision use cases, c) speech use case.', 'nlp, algorithm decreases weight commonly words increases weight words collection documents a. term frequency (tf) b. inverse document frequency (idf) c. word2vec d. latent dirichlet allocation (lda) answer b) q11.', 'a. kt * log(3) b. t * log(3) / k c. k * log(3) / t d. log(3) / kt answer (c) formula tf k/t formula idf log(total docs / docs containing “data”) = log(1 / (⅓)) = log (3) correct choice klog(3)/t q10.', 'corpus n documents, randomly chosen document contains total t terms term “hello” appears k times.']\n",
      "\n",
      "['q14 identify odd a. nltk b. scikit learn c. spacy d. bert answer d) ones mentioned nlp libraries bert, word embedding q15 tf-idf helps establish?', '● idf obtained dividing total number documents number documents containing term taking logarithm quotient.', 'nlp, process converting sentence paragraph tokens referred stemming a. true b. false answer b) statement describes process tokenization stemming, false.', 'nlp, tokens converted numbers giving neural network a. true b. false answer a) nlp, words converted number feeding neural network.', 'tf-idf takes account number times word appears document offset number documents appear corpus.', 'a. frequently occurring word document b. important word document answer b) tf-idf helps establish important particular word context document corpus.']\n",
      "\n",
      "['q18 text mining, converting text tokens converting integer floating-point vectors a. countvectorizer b. tf-idf c. bag words d. ners answer a) countvectorizer helps above, applicable.', 'q16 nlp, process identifying people, organization given sentence, paragraph called a. stemming b. lemmatization c. stop word removal d. named entity recognition answer d) q17 following pre-processing technique nlp a. stemming lemmatization b. converting lowercase c. removing punctuations d. removal stop words e. sentiment analysis answer e) sentiment analysis pre-processing technique.']\n",
      "\n",
      "['d) answer c) bert (bidirectional encoder representations transformer) supports context modelling previous sentence context taken consideration.', 'bert model uses previous sentence arrive context.word2vec glove word embeddings, provide context.', 'following word embeddings custom trained specific subject nlp a. word2vec b. bert', 'steve nouri https//www.linkedin.com/in/stevenouri/ print(vector.toarray()) output [[1 1 1 1 2 1 1 1 1 1 1 1 1 1]] second section interview s covers advanced nlp techniques word2vec, glove word embeddings, advanced models gpt, elmo, bert, xlnet based s, explanations.', 'nlp, words represented vectors called neural word embeddings a. true b. false answer a) word2vec, glove based models build word embedding vectors multidimensional.', 'nlp, bidirectional context supported following embedding a. word2vec b. bert c. glove d. answer b) bert provides bidirectional context.']\n",
      "\n",
      "['word embeddings capture multiple dimensions data represented vectors a. true b. false answer a) q24.', 'following better choice address nlp use cases semantic similarity, reading comprehension, common sense reasoning a. elmo b. open ai’s gpt c. ulmfit answer b) open ai’s gpt able learn complex pattern data transformer models attention mechanism suited complex use cases semantic similarity, reading comprehensions, common sense reasoning.', 'language biases introduced historical data training word embeddings, example bias a. new delhi india, beijing china b. man computer, woman homemaker answer a) statement b) bias buckets woman homemaker, statement a) biased statement.', 'steve nouri https//www.linkedin.com/in/stevenouri/ c. glove d. answer b) bert allows transform learning existing pre-trained models custom trained given specific subject, unlike word2vec glove existing word embeddings used, transfer learning text possible.', 'nlp, word embedding vectors help establish distance tokens a. true b. false answer a) use cosine similarity establish distance vectors represented word embeddings q25.']\n",
      "\n",
      "['trains independent lstm language model left right right left shallowly concatenates a. gpt b. bert c. ulmfit d. elmo', 'following architecture trained faster needs training data a. lstm based language modelling b. transformer architecture answer b) transformer architectures supported gpt onwards faster train needed data training too.', 'a. glove b. word2vec c. elmo d. nltk answer c) emlo word embeddings supports word multiple embeddings, helps word different context captures context meaning word unlike glove word2vec.', 'q30 given token, input representation sum embedding token, segment position embedding a. elmo b. gpt c. bert d. ulmfit answer c) bert uses token, segment position embedding.', 'steve nouri https//www.linkedin.com/in/stevenouri/ c. open ai’s gpt d. ulmfit answer c) ulmfit lstm based language modeling architecture.']\n",
      "\n",
      "['● sentiment analysis ● language translation (english german, chinese english, etc..) ● document summarization ● answering ● sentence completion ● attribute extraction (key information extraction documents) ● chatbot interactions ● topic classification ● intent extraction ● grammar sentence correction ● image captioning ● document ranking ● natural language inference q35.', 'a. openai gpt b. elmo c. bert d. ulmfit answer c)bert transformer architecture models relationship word words sentence generate attention scores.', 'attention scores later weights weighted average words’ representations fed fully-connected network generate new representation.', 'uses unidirectional language model producing word embedding a. bert b. gpt c. elmo d. word2vec answer b) gpt unidirectional model word embedding produced training information flow left right.', 'transformer model pays attention important word sentence a. true b. false', 'steve nouri https//www.linkedin.com/in/stevenouri/ answer d) elmo tries train independent lstm language models (left right right left) concatenates results produce word embedding.']\n",
      "\n",
      "['series nlp model forms family algorithms wide range classification tasks including sentiment prediction, filtering spam, classifying documents more.', 'outperformed bert 20 tasks achieves state art results 18 tasks including sentiment analysis, answering, natural language inference, etc.', 'steve nouri https//www.linkedin.com/in/stevenouri/ answer a) attention mechanisms transformer model model relationship words provide weights important word.', 'compared discriminative models like logistic regression, naive bayes model takes lesser time train.', 'algorithm perfect use working multiple classes text classification data dynamic changes frequently.', 'permutation language models feature a. bert b. emmo c. gpt d. xlnet answer d) xlnet provides permutation-based language modelling key difference bert.', 'a. bert b. xlnet c. gpt-2 d. elmo answer b) xlnet given best accuracy models.', 'transformer xl uses relative positional embedding a. true b. false a) instead embedding having represent absolute position word, transformer xl uses embedding encode relative distance words.']\n",
      "\n",
      "['text summarization intends create summary given piece text outlines main points document.', 'toolkit contains powerful libraries work different ml techniques break understand human language.', 'information extraction context natural language processing refers technique extracting structured information automatically unstructured sources ascribe meaning it.', 'models information extraction includes ● tagger module ● relation extraction module ● fact extraction module ● entity extraction module', 'text summarization proved blessing machines summarise large volumes text time time-consuming.', 'difference nltk spacey follows ● nltk collection programs choose from, spacey contains best- suited algorithm problem toolkit ● nltk supports wider range languages compared spacey (spacey supports 7 languages) ● spacey object-oriented library, nltk string processing library ● spacey support word vectors nltk q43.', 'nltk natural language toolkit series libraries programs symbolic statistical natural language processing.', 'steve nouri https//www.linkedin.com/in/stevenouri/ dependency parsing, known syntactic parsing nlp process assigning syntactic structure sentence identifying dependency parses.', 'dependency parsing needs resolve ambiguities order effectively assign syntactic structure sentence.']\n",
      "\n",
      "['parts speech tagging better known pos tagging refers process identifying specific words document group speech, based context.', 'pos tagging known grammatical tagging involves understanding grammatical structures identifying respective component.', 'steve nouri https//www.linkedin.com/in/stevenouri/ ● sentiment analysis module ● network graph module ● document classification & language modeling module q44.', 'masked language models help learners understand deep representations downstream tasks taking output corrupt input.', 'entity recognition commonly known ner process identifying specific entities text document informative unique context.', 'best nlp tools open sources are ● spacy ● textblob ● textacy ● natural language toolkit ● retext ● nlp.js ● stanford nlp ● cogcompnlp q49.', 'model creates occurrence matrix documents sentences irrespective grammatical structure word order.']\n",
      "\n",
      "['fact, ner involves entity chunking extraction entities segmented categorise different predefined classes.', 'pragmatic analysis deals outside word knowledge, means knowledge external documents and/or queries.', 'pragmatics analysis focuses described reinterpreted actually meant, deriving aspects language require real-world knowledge.', 'n-gram nlp simply sequence n words, conclude sentences appeared frequently, example, let consider progression words ● new york (2 gram) ● golden compass (3 gram) ● hotel (4 gram) sequence, easily conclude sentence (a) appeared frequently sentences, sentence(c) seen often.', 'perplexity high low; low perplexity ethical inability deal complicated problem high perplexity terrible failure deal complicated high.', 'word \"perplexed\" means \"puzzled\" \"confused\", perplexity general means inability tackle complicated problem specified.']\n",
      "\n",
      "['stop words was, were, is, am, the, a, an, how, why, more.', 'in terms computational complexity, self-attention layers faster recurrent layers sequence length n smaller representation dimensionality d, case sentence representations state-of-the-art models machine translations, word-piece byte-pair representations.” —']\n",
      "\n",
      "['example understand unigrams, bigrams, trigrams, refer diagram q62 steps involved solving nlp problem?', 'steve nouri https//www.linkedin.com/in/stevenouri/ latent semantic indexing mathematical technique improve accuracy information retrieval process.', 'design lsi algorithms allows machines detect hidden (latent) correlation semantics (words).', '● b regular expressions, + b regular expression language {a, b}.', 'suppose, b regular expressions, following true them ● {ɛ} regular language, ɛ regular expression it.']\n",
      "\n",
      "['q64 case processing natural language, normally mentioned common terminology nlp binding language terminology properly.', 'key factors given below ● vectors weights google word vectors, length tf-idf, varieties documents, word vectors, tf-idf.', 'q65 explain briefly word2vec word2vec embeds words lower-dimensional vector space shallow neural network.', '● analysis syntactic way mainly helps maintaining ordering properly available words.', 'help identifying human it’s fictional real, kind reality identification organization, events geographic location etc.', '● classification text learning supervising, set train, set validation dev, set define test, feature individual text, lda.', 'result set word-vectors vectors close vector space similar meanings based context, word-vectors distant differing meanings.', '● analysis sentiment know features sentiment, entities available sentiment, sentiment common dictionary.', 'major components explained below ● extraction entity actually identifying extracting critical data available information help segmentation provided sentence identifying entity.', '● reading machine language extraction possible entity, linking individual entity, dbpedia, libraries like pikes fred.']\n",
      "\n",
      "['names suggest, additive attention weighted sum multiplicative attention weighted multiplier hidden weights.', 'output time step passed decoder, resulting loss information learned previous time steps.', 'training process, model learns weights attention mechanisms recognize relative importance time step.', 'use attention encoder-decoder networks, fixed-dimensional vector passed decoder function vectors outputted intermediary steps.', 'encoder-decoder structure deep learning model architecture responsible state art solutions, including machine translation.', 'preprocessing steps commonly nlp tasks ● case normalization convert input case (lowercase uppercase) way reducing text canonical form ● punctuation/stop word/white space/special characters removal don’t think words characters relevant, remove reduce feature space ● lemmatizing/stemming reduce words inflectional forms (i.e. walks → walk) trim vocabulary ● generalizing irrelevant information replace numbers <number> token names <name> token q68 encoder-decoder structure work language modelling?']\n",
      "\n",
      "['word embeddings trained large corpus (for instance, extensive web scrape billions words), model vocabulary include common misspellings design.', 'terms found model vocabulary mapped “closest” vocabulary term using ● edit distance strings ● phonetic distance word pronunciations ● keyword distance catch common typos q72 following models perform tweet classification regards context mentioned above?', 'a) naive bayes b) svm c) solution (c) since, given data tweets information, means target variable present.', 'steve nouri https//www.linkedin.com/in/stevenouri/ diving productionization aspect, ideal machine learning service have ● endpoint(s) business systems use inference ● feedback mechanism validating model predictions ● database store predictions ground truths feedback ● workflow orchestrator (upon signal) re-train load new model serving based records database + prior training data ● form model version control facilitate rollbacks case bad deployments ● post-production accuracy error monitoring q71 handle misspellings text input?']\n",
      "\n",
      "['selection number topics directly proportional size data, number topic terms directly proportional size data.', 'number topic terms directly proportional size data a) 0 b) 25 c) 50 d) 75 e) 100 solution (a) lda unsupervised learning model, lda latent dirichlet allocation, linear discriminant analysis.', 'converting words lowercase affect dimensionality data a) 1 b) 2 c) 3 d) 1 2 e) 2 3 f) 1, 2 3 solution (d) choices b correct stopword removal decrease number features matrix, normalization words reduce redundant features, and, converting words lowercase decrease dimensionality.', 'a) frequency count terms b) vector notation sentence c) speech tag d) dependency grammar e) solution (e) techniques purpose engineering features model.']\n",
      "\n",
      "['time complexity lstm seq_length*hidden² time complexity transfomer seq_length²*hidden hidden size seq_length(which normally case), transfomer faster lstm.', 'better explained fastai videos) q78 difference learning latent features svd getting embedding vectors deep network?', '● exploding gradient(solved gradient clipping) ● dying relu — learning activation 0 (solved parametric relu) ● mean variance activations 0 1.(partially solved subtracting 0.5 activation.', 'number parameters lstm model bias 4(𝑚h+h²+h) 𝑚 input vectors size h output vectors size a.k.a.', 'steve nouri https//www.linkedin.com/in/stevenouri/ q76 latent dirichlet allocation model text classification purposes, alpha beta hyperparameter represent- a) alpha number topics documents, beta number terms topics false b) alpha density terms generated topics, beta density topics generated terms false c) alpha number topics documents, beta number terms topics false d) alpha density topics generated documents, beta density terms generated topics true solution (d) option d correct q77 problem relu?', 'hidden stores information till time step cell state stores particular information needed future time step.']\n",
      "\n",
      "['batch normalisation allows set higher learning rates, increasing speed training reduces unstability initial starting weights.', 'batchnorm — compute mean var layer minibatch layernorm — compute mean var single sample layer independently q85 transformer block layernorm instead batchnorm?', 'q88 tell language model doesn’t use dropout albert v2 — throws light fact lot assumptions granted necessarily true.', 'hard sharing train task time update weights losses soft sharing train task time.', 'looking advantages layernorm, robust batch size works better works sample level batch level.', 'steve nouri https//www.linkedin.com/in/stevenouri/ learning rate warm-up learning rate schedule low (or lower) learning rate beginning training avoid divergence unreliable gradients beginning.', 'batchnorm computes mean variance layer minibatch layernorm computes mean variance sample layer independently.', 'not great s checks awareness) ● lm tuning task text ● weight dropout ● discriminative learning rates layers ● gradual unfreezing layers ● slanted triangular learning rate schedule followed explaining help.']\n",
      "\n",
      "['● gpt bidirectional concept masking ● bert adds sentence prediction task training segment embedding q91 differences bert albert v2?', '● weights residual layers initially scaled factor 1/√n n number residual layers.', '● serve gpu/tpu/fpga ● 16 bit quantisation served gpu fp16 support ● pruning reduce parameters ● knowledge distillation (to smaller transformer model simple neural network) ● hierarchical softmax/adaptive softmax ● cache results explained here.', 'steve nouri https//www.linkedin.com/in/stevenouri/ ● layer normalization moved input sub-block, similar residual unit type “building block” (differently original type “bottleneck”, batch normalization applied weight layers).', 'k results tf-idf similarity rank results ● semantic encoding + cosine similarity ● model trained ranking']\n",
      "\n",
      "['recurrent neural networks, additional loop node loop essentially includes time component network well.', 'like normalizing input helps improve logistic regression model, normalize activations hidden layers deep learning model well q100 backpropagation different rnn compared ann?', 'interviewee things transfer learning latest models need talk having neutral class good accuracy/f1 still, model classify positive negative.', 'interviewee talk create dataset training strategies like selection language model, language model fine-tuning datasets multi- task learning.', 'regular expression representation natural language form mathematical expressions containing character sequence.', 'interviewer asked fundamentals deep learning architectures, key topic improving deep learning model’s performance.', 'hand, regular grammar generator natural language, defining set defined rules syntax strings natural language follow.', 'steve nouri https//www.linkedin.com/in/stevenouri/ q96 k results tf-idf similarity rank results ● semantic encoding + cosine similarity ● model trained ranking q97 sentiment classifier?']\n",
      "\n",
      "['100 python interview s prepare 2019 updated aug 14,2019 379.7k views python certiõcation sought-after skill programming domain.', 'mymock interview service real tech jobs \\ue913 mock interview latest tech domains i.e java, ai, devops,etc interviewed leading tech experts real time assement report video recording python mock interview \\ue90b \\ue921 follow linkedin more steve nouri https//www.linkedin.com/in/stevenouri/', 'compiled list python interview s classiõed 7 sections, namely basic interview s oops interview s basic python programs python libraries interview s web scraping interview s data analysis interview s multiple choice s (mcq) moving ahead, recording python interview s instructor shared experience expertise help crack python interview python interview s answers | python training | edureka aayushi johari technophile likes writing diàerent technologies spreading knowledge.', '100+ s python programming basics help diàerent expertise levels reap maximum beneõt blog.']\n",
      "\n",
      "['python dynamically typed, means don’t need state types variables declare like that.', 'things like x=111 x=\"i\\'m string\" error python suited object orientated programming allows deõnition classes composition inheritance.', 'classes õrst class objects writing python code quick running slower compiled languages.', 'numpy package good example this, it’s quick lot number crunching isn’t actually python python õnds use spheres – web applications, automation, scientiõc modeling, big data applications more.', 'means that, unlike languages like c variants, python need compiled run.', 'fortunately ，python allows inclusion c based extensions bottlenecks optimized away are.', 'python interview s answers | python interview preparati python interview s answers | python interview preparati… \\ue921', 'syntax list_1 = [10, ‘chelsea’, 20] syntax tup_1 = (10, ‘chelsea’ , 20) q2.']\n",
      "\n",
      "['global variables variables declared outside function global space called global variables.', 'commonly built-in modules are os sys math random data time json q10.what local variables global variables python?', 'example a=2 def add() b=3 c=a+b print(c) add() output 5 try access local variable outside function add(), throw error.', 'int() – converts data type integer type öoat() – converts data type öoat type ord() – converts characters integer hex() – converts integers hexadecimal oct() – converts integer octal tuple() – function convert tuple.']\n",
      "\n",
      "['ans install python windows, follow steps install python link https//www.python.org/downloads/ this, install pc.', 'look location python installed pc following command command prompt cmd python.', 'but, arrays hold single data type elements lists hold data type elements.', \"example import array arr my_array=arr.array('i',[1,2,3,4]) my_list=[1,'abc',1.20] print(my_array) print(my_list) output array(‘i’, [1, 2, 3, 4]) [1, ‘abc’, 1.2] q16.\", 'class employee def __init__(self, name, age,salary) self.name = self.age = age self.salary = 20000 e1 = employee(\"xyz\", 23, 20000) # e1 instance class employee. #__', 'example def newfunc() print(\"hi, welcome edureka\") newfunc(); #calling function output hi, welcome edureka q17.what __init__?']\n",
      "\n",
      "['self variable init method refers newly created object methods, refers object method called.', 'continue allows skipping loop speciõc condition met control transferred beginning loop pass need block code syntactically, want skip execution.', \"ans consider example shown below random import shuffle x = ['keep', 'the', 'blue', 'flag', 'flying', 'high'] shuffle(x) print(x) output following code below. ['\", \"example import array arr my_array=arr.array('i',[1,2,3,4,5]) my_array[-1] output array(‘i’, [5, 4, 3, 2, 1]) [-1] reprints reversed copy ordered data structures array list.\"]\n",
      "\n",
      "['need hold ctrl key left click place want include # character type # once.', 'means xrange doesn’t actually generate static list run-time like range does.', 'diàerence range returns python list object x range returns xrange object.', 'example #comments python start like print(\"comments python start #\") output comments python start # q27.', 'uniform(a, b) chooses öoating point number deõned range [a,b).iyt returns öoating point number 3.', 'especially true memory sensitive system cell phone working with, range use memory create array integers, result memory error crash program.', 'ans pickle module accepts python object converts string representation dumps õle dump function, process called pickling.', 'means gigantic range you’d like generate list for, billion, xrange function use.']\n",
      "\n",
      "[\"dict={'country''india','capital''delhi','pm''modi'} print dict[country] india print dict[capital] delhi print dict[pm] modi q37.\", 'help() function help() function display documentation string facilitates help related modules, keywords, attributes, etc.', 'ans help() dir() functions accessible python interpreter viewing consolidated dump built-in functions.', 'is returns true 2 operands true (example “a” ‘a’) not returns inverse boolean value in checks element present sequence q34.', 'code divides 2 numbers \"\"\" x=8 y=4 z=x/y print(z) output 2.0 q33.', 'python exits, especially python modules having circular references objects objects referenced global namespaces de-allocated freed.']\n",
      "\n",
      "['index negative number starts ‘-1’ represents index sequence ‘-2’ penultimate index sequence carries forward like positive number.', 'negative index remove new-line spaces string allow string character given s[-1].', 'sub() – õnds substrings regex pattern matches replace diàerent string subn() – similar sub() returns new string no.', 'kwargs don’t know keyword arguments passed function, pass values dictionary keyword arguments.', 'ans built-in types python follows – integers floating-point complex numbers strings boolean built-in functions q45.', 'support (fairly) eþcient insertion, deletion, appending, concatenation, python’s list comprehensions easy construct manipulate.', 'numbers positive uses ‘0’ uses õrst index ‘1’ second index process goes like that.', 'syntax ternary operator given as [on_true] [expression] [on_false]x, y = 25, 50big = x x < y y example expression gets evaluated like x<y y, case x<y true value returned big=x incorrect big=y sent result.', 'ans use *args aren’t sure arguments going passed function, want pass stored list tuple arguments function. **']\n",
      "\n",
      "['deep copy makes execution program slower making certain copies object called.', 'certain limitations don’t support “vectorized” operations like elementwise addition multiplication, fact contain objects diàering types mean python store type information element, execute type dispatching code operating element.', 'references point original objects changes member class aàect original copy it.', 'numpy array faster lot built numpy, ffts, convolutions, fast searching, basic statistics, linear algebra, histograms, etc.', \"example a=arr.array('d', [1.1 , 2.1 ,3.1] ) a.append(3.4) print(a) a.extend([4.5,6.3,6.8]) print(a) a.insert(2,3.8) print(a) output array(‘d’, [1.1, 2.1, 3.1, 3.4]) array(‘d’, [1.1, 2.1, 3.1, 3.4, 4.5, 6.3, 6.8]) array(‘d’, [1.1, 2.1, 3.8, 3.1, 3.4, 4.5, 6.3, 6.8]) q47.\", \"example a=arr.array('d', [1.1, 2.2, 3.8, 3.1, 3.7, 1.2, 4.6]) print(a.pop()) print(a.pop(3)) a.remove(1.1) print(a) output 4.6 3.1 array(‘d’, [2.2, 3.8, 3.7, 1.2]) q48.\", 'ans shallow copy new instance type gets created keeps values copied new instance.']\n",
      "\n",
      "['python multi-threading package want multi-thread speed code up, it’s usually good idea use it.', 'import modules ways- example import array #importing original module import array arr # importing alias array import * #imports present array module oops interview s q55.', 'happens quickly human eye like threads executing parallel, taking turns cpu core.', 'multi-level inheritance – derived class d1 inherited base class base1, d2 inherited base2.', 'ans compiling linking allows new extensions compiled properly error linking passes compiled procedure.', 'class inheriting called super- class class inherited called derived / child class.']\n",
      "\n",
      "['so, instance, parent class method named abc child class method abc having parameters variables.', 'python lays concept preõxing variable, function method single double underscore imitate behavior protected private access speciõers.', 'consider example # m.py class myclass def f(self) print \"f()\" run monkey-patch testing like this import m def monkey_f(self) print \"monkey_f()\" m.myclass.f = monkey_f obj = m.myclass() obj.f() output below monkey_f() see, changes behavior f() myclass function deõned, monkey_f(), outside module m. q58.', 'ans python, term monkey patch refers dynamic modiõcations class module run-time.', 'example class employee def __init__(self, name) self.name = e1=employee(\"abc\") print(e1.name) output abc q57.']\n",
      "\n",
      "[\"def pyfunc(r) x range(r) print(' '*(r-x-1)+'*'*(2*x+1)) pyfunc(9) output * *** ***** ******* ********* *********** ************* *************** ***************** q67.\", 'def bs(a)&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# = list &nbsp; b=len(a)-1&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# minus 1 compare 2 adjacent values &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; x range(b) &nbsp; &nbsp; &nbsp; y range(b-x) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; a[y]>a[y+1] &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; a[y],a[y+1]=a[y+1],a[y] &nbsp; return a=[32,5,3,6,7,54,87] bs(a) output [3, 5, 6, 7, 32, 54, 87] q66.', 'class a &nbsp; pass obj=a() obj.name=\"xyz\" print(\"name = \",obj.name) output = xyz q64.']\n",
      "\n",
      "['a=int(input(\"enter number\"))&nbsp; &nbsp; &nbsp; a>1 &nbsp; x range(2,a) &nbsp; &nbsp; &nbsp; if(a%x)==0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; print(\"not prime\") &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; break &nbsp; else &nbsp; &nbsp; &nbsp; print(\"prime\") else &nbsp; print(\"not prime\") output enter number 3 prime q69.', 'a=input(\"enter sequence\") b=a[-1] a==b &nbsp; print(\"palindrome\") else &nbsp; print(\"not palindrome\") output enter sequence 323 palindrome q70.', '# enter number terms needed&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#0,1,1,2,3,5.... a=int(input(\"enter terms\")) f=0&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#first element series s=1&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#second element series a<=0 &nbsp; print(\"the requested series \",f) else &nbsp; print(f,s,end=\" \") &nbsp; x range(2,a) &nbsp; &nbsp; &nbsp; next=f+s&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; print(next,end=\" \") &nbsp; &nbsp; &nbsp; f=s &nbsp; &nbsp; &nbsp; s=next</pre> output enter terms 5 0 1 1 2 3 q68.', 'open(some_large_file) fh count = 0 text = fh.read() character text character.isupper() count += 1 try transform single line.']\n",
      "\n",
      "[\"a0 = dict(zip(('a','b','c','d','e'),(1,2,3,4,5))) a1 = range(10)a2 = sorted([i a1 a0]) a3 = sorted([a0[s] s a0]) a4 = [i a1 a3] a5 = {ii*i a1} a6 = [[i,i*i] a1] print(a0,a1,a2,a3,a4,a5,a6) ans following õnal outputs a0, a1, … a6 a0 = {'a' 1, 'c' 3, 'b' 2, 'e' 5, 'd' 4} # order vary a1 = range(0, 10) a2 = [] a3 = [1, 2, 3, 4, 5] a4 = [1, 2, 3, 4, 5] a5 = {0 0, 1 1, 2 4, 3 9, 4 16, 5 25, 6 36, 7 49, 8 64, 9 81} a6 = [[0, 0], [1, 1], [2, 4], [3, 9], [4, 16], [5, 25], [6, 36], [7, 49], [8, 64], [9, 81]] python libraries interview s q73.\", 'ans django flask map url’s addresses typed web browsers functions python.', 'django consists prewritten code, user need analyze flask gives users create code, therefore, making simpler understand code.', 'ans flask web microframework python based “werkzeug, jinja2 good intentions” bsd license.', 'ans following code sort list python list = [\"1\", \"4\", \"0\", \"6\", \"9\"] list = [int(i) list] list.sort() print (list) q72.', 'flask simpler compared django but, flask lot meaning need specify details, django lot need work.']\n",
      "\n",
      "[\"add following lines code setting.py õle databases = { 'default' { 'engine' 'django.db.backends.sqlite3', 'name' os.path.join(base_dir, 'db.sqlite3'), } } q78.\", 'figure python interview s – django architecture developer provides model, view template maps url django magic serve user.', 'ans use command edit mysite/setting.py, normal python module module level representing django settings.', 'case sqlite database, case, database õle computer, absolute path, including õle õle.', 'database server— postgresql, mysql, oracle, mssql—and want use sqlite, use database’s administration tools create new database django project.', 'case database choice diàerent following keys database ‘default’ item match database connection settings.', 'template contains variables replaced values template evaluated tags (% tag %) control logic template.', 'ans use write view django django.http import httpresponse import datetime def current_datetime(request) = datetime.datetime.now() html = \"<html><body>it %s</body></html> % return httpresponse(html) returns current date time, html document q79.', 'django uses sqlite default; easy django users won’t require type installation.']\n",
      "\n",
      "['example, check google webcache age edureka.co you’d use following url http//webcache.googleusercontent.com/search?q=cacheedureka.co q84.', 'ans use following code save image locally url address import urllib.request urllib.request.urlretrieve(\"url\", \"local-filename.jpg\") q83.', 'figure python interview s – django framework data stored client side.', 'ans use following url format http//webcache.googleusercontent.com/search?q=cacheurlgoeshere sure replace “urlgoeshere” proper web address page site cache want retrieve time for.', 'django abstracts process sending receiving cookies, placing session id cookie client side, storing related data server side.', 'proxy models use model, want modify python level behavior model, changing model’s õelds.', 'abstract base classes style want parent’s class hold information don’t want type child model.']\n",
      "\n",
      "['ans calculate percentiles following code import numpy np = np.array([1,2,3,4,5]) p = np.percentile(a, 50) #returns 50th percentile, e.g. median print(p) output 3 q89.', 'ideal world, numpy contain array data type basic operations indexing, sorting, reshaping, basic elementwise functions, et cetera.', 'ans indices n maximum values numpy array code import numpy np arr = np.array([1, 3, 2, 4, 5]) print(arr.argsort()[-3][-1]) output [ 4 3 1 ] q88.', 'bs4 import beautifulsoup import requests import sys url = \\'http//www.imdb.com/chart/top\\' response = requests.get(url) soup = beautifulsoup(response.text) tr = soup.findchildren(\"tr\") tr = iter(tr) next(tr) movie tr title = movie.find(\\'td\\', {\\'class\\' \\'titlecolumn\\'} ).find(\\'a\\').contents[0] year = movie.find(\\'td\\', {\\'class\\' \\'titlecolumn\\'} ).find(\\'span\\', {\\'class\\' \\'secondaryinfo\\'}).contents[0] rating = movie.find(\\'td\\', {\\'class\\' \\'ratingcolumn imdbrating\\'} ).find(\\'strong\\').contents[0] row = title + \\' - \\' + year + \\' \\' + \\' \\' + rating print(row) code help scrap data imdb’s 250 list data analysis – python interview s q85.', 'ans map function executes function given õrst argument elements iterable given second argument.']\n",
      "\n",
      "['however, numpy’s important goals compatibility, numpy tries retain features supported predecessors.', 'a) / b) // c) % d) mentioned b) // operands integer python chops fraction gives round oà value, accurate answer use öoor division.', 'a) abc = 1,000,000 b) b c = 1000 2000 3000 c) a,b,c = 1000, 2000, 3000 d) a_b_c = 1,000,000 b) b c = 1000 2000 3000 spaces allowed variable names.', '2.5 answer, use öoor division //. so, 5//2 = 2.5 q93.', 'matplotlib provides basic 3d plotting mplot3d subpackage, mayavi provides wide range high-quality 3d visualization features, utilizing powerful vtk engine.', 'a) indicate private variables class b) confuse interpreter c) indicate global variables d) slow execution a) indicate private variable class python concept private variables, leading underscores indicate variables accessed outside class.', 'a) 31 characters b) 63 characters c) 79 characters d) d) identiõers length.', 'multiple correct answers possible) a) d = {} b) d = {“john”40, “peter”45} c) d = {40”john”, 45”peter”} d) d = (40”john”, 45”50”) b, c & d. dictionaries created specifying keys values.', 'ans like 2d plotting, 3d graphics scope numpy scipy, 2d case, packages exist integrate numpy.']\n",
      "\n",
      "['use libraries like pandas, numpy, matplotlib, scipy, scikit, pyspark master concepts like python machine learning, scripts, sequence, web scraping big data analytics leveraging apache spark.', 'open õle cscores.txt writing, use a) outõle = open(“cscores.txt”, “r”) b) outõle = open(“cscores.txt”, “w”) c) outõle = open(õle = “cscores.txt”, “r”) d) outõle = open(õle = “cscores.txt”, “o”) b) location contains double slashes ( ) w indicate õle written to.', '= 1 raise \"someerror\" else print(\"someerror occured\") \"someerror\" print (\"someerror occured\") a) someerror occured b) someerror occured c) invalid code d) c) invalid code new exception class inherit baseexception.', 'a) error b) c) 25 d) 2 c) 25 index -1 corresponds index list.', 'f = range (5) open(\"data.txt\", \"w\") f > 2 break print f.closed a) true b) false c) d) error a) true statement open õle guarantees õle object closed block exits.', 'a) b) exception occurs c) exception occurs d) exception occurs block c) exception occurs executed exception occurs.', 'wish learn python gain expertise quantitative analysis, data mining, presentation data numbers transforming career data scientist role, check interactive, live-online python certiõcation training.']\n",
      "\n",
      "['python inbuilt garbage collector, recycle unused memory frees memory makes available heap space.', 'converts source code written programmer intermediate language, translated machine language executed.', '1 / 7 read click >>>>> ai related cheatsheets tutorials place https//www.linkedin.com/pulse/all-cheatsheets-one-place-vipul-patel/', 'benefits pythons simple easy, portable, extensible, build-in data structure open source.', 'pickle module accepts python object converts string representation dumps file dump function, process called pickling.']\n",
      "\n",
      "['mutable immutable types pythons built types mutable built-in types list sets dictionaries immutable built-in types 2 / 7', 'pychecker static analysis tool detects bugs python source code warns style complexity bug.']\n",
      "\n",
      "['python documentation string known docstring, way documenting python 3 / 7', 'supports sharing setups, automation testing, shutdown code tests, aggregation tests collections etc.', 'mechanism select range items sequence types like list, tuple, strings etc.', 'pass means, no-operation python statement, words place holder compound statement, blank left written there.']\n",
      "\n",
      "[\"local variables variable assigned new value function's body, it's assumed local.\", 'xrange returns xrange object range returns list, uses memory matter range size is.']\n",
      "\n",
      "['generate random numbers python, need import command import random random.random() returns random floating point number range [0,1) 31) explain access module written python c?', 'python require explicit memory management interpreter allocates memory new variables free automatically provide easy readability use square brackets easy-to-learn beginners having built-in data types saves programming time effort declaring variables 34) mention use split function python?', 'access module written python c following method, module = =pyimport_importmodule(\"\"); 32) mention use // operator python?', 'floor divisionoperator , dividing operands result quotient showing digits decimal point.', \"http//career.guru99.com/ python script executable unix, need things, script file's mode executable line begin # ( #!\", 'python comprises huge standard library internet platforms like email, html, etc.']\n",
      "\n",
      "['features include flask wtf integration wtforms secure form csrf token global csrf protection internationalization integration recaptcha supporting file upload works flask uploads 38) explain common way flask script work?', 'flask web micro framework python based “werkzeug, jinja 2 good intentions” bsd licensed.', 'common way flask script work import path application path python file 39) explain access sessions flask?']\n",
      "\n",
      "['mvc perfect fit flask, pattern mvc consider following example powered tcpdf (www.tcpdf.org) 7 / 7']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for page in tqdm(pages_and_metadata, total=len(pages_and_metadata)):\n",
    "    print(page[\"sentences\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3244522b7b680f87",
   "metadata": {},
   "source": [
    "## 2.6 Update the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8943b0ff4b313750",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:42:07.860161Z",
     "start_time": "2025-02-10T16:42:07.846917Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef39ec71a18a4d7abb87bc5761c97b5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/641 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for page in tqdm(pages_and_metadata, total=len(pages_and_metadata)):\n",
    "    pages_and_metadata[page[\"page_number\"]][\"number_of_sentences\"] = len(page[\"sentences\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b9813d9d02986948",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:42:08.918920Z",
     "start_time": "2025-02-10T16:42:08.912408Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_number\n",
      "raw_text\n",
      "number_of_characters\n",
      "number_of_tokens\n",
      "number_of_words\n",
      "formatted_text\n",
      "sentences\n",
      "number_of_sentences\n",
      "sentence_chunk\n",
      "embeddings\n"
     ]
    }
   ],
   "source": [
    "for key in pages_and_metadata[0].keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2d4bf7c69a5ea0",
   "metadata": {},
   "source": [
    "## 2.7 Converting sentences to sentence_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "22ed0b39d46cf55e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:42:11.689414Z",
     "start_time": "2025-02-10T16:42:11.669184Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ab028f71423409ab64ade5416bd81c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/641 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for page in tqdm(pages_and_metadata, total=len(pages_and_metadata)):\n",
    "    sentences = pages_and_metadata[page[\"page_number\"]][\"sentences\"]\n",
    "    sentence_chunk = [sentences[i : i+SENTENCE_CHUNKS] for i in range(0, len(sentences), SENTENCE_CHUNKS)]\n",
    "    page[\"sentence_chunk\"] = sentence_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5b03d9f534a696c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:42:12.995127Z",
     "start_time": "2025-02-10T16:42:12.968943Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11636c92eb344068b608e2cf271d3902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/641 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['boltzmann developed simple learning algorithms allow find important information presented complex regularities data.', 'banking industry giving loans primary source making money time repayment rate good profit, risk huge losses.', 'says sample means, sample variance sample standard deviation converge trying estimate.', 'cluster sampling technique difficult study target population spread wide area simple random sampling applied.', 'systematic sampling, list progressed circular manner reach end list, progressed again.', 'banks don’t want lose good customers point time, don’t want acquire bad customers.', 'consider restricted boltzmann machines, single algorithm feature detectors faster compared others.']]\n",
      "\n",
      "[['predictor variables money spent election campaigning particular candidate, time spent campaigning, etc.', 'studying target population spread wide area difficult applying simple random sampling ineffective, technique cluster sampling used.', 'couple layers added input output size layer smaller size pertaining input layer.', 'logistic regression referred logit model technique predict binary outcome linear combination predictor variables.', 'addition giving insights effective efficient manner, data visualization way restricted bar, line stereotypic graphs.']]\n",
      "\n",
      "[['fine line simple insightful dashboard awesome looking 0 fruitful insight dashboards.', 'method comparing testing versions systems applications determine version application performs better.', 'businesses like e-commerce, retail, government organizations contain large customer transaction information outdated needs cleaned.', 'different types data existing data source dirty data, clean data, mixed clean dirty data sample clean data.', 'data cleaning important data science end results outcomes data analysis come existing data useless unimportant need cleaned periodically required.', 'area data science, a/b testing know variable existing variables order optimize increase outcome goal.', 'depending size data, suitable tools methods clean data database big data environment.', 'data cleaning reduces data redundancy gives good results data analysis large customer information exists cleaned periodically.', 'modern data science applications rely machine learning model learner learns existing data.', 'learning design principles help build effective efficient visualizations tableau prep tool drastically increase time focusing important part.']]\n",
      "\n",
      "[['resampling cases estimating accuracy sample statistics subsets accessible data drawing randomly replacement set data points', 'involved process determining sample size needed detecting effect given size cause certain degree assurance.', 'scenarios machine learning finds applications real world ecommerce understanding customer churn, deploying targeted advertising, remarketing.', 'multiple comparisons way comparing variables case customer interests causes false positives resulting requirement correction confidence level seller area e-commerce.', 'techniques statistical power analysis sample size estimation widely deployed making statistical judgment accurate evaluates size needed experimental effects practice.', 'search engine ranking pages depending personal preferences searcher finance evaluating investment opportunities & risks, detecting fraudulent transactions medicare designing drugs depending patient’s history needs robotics machine learning handling situations ordinary social media understanding relationships recommending connections extraction information framing s getting answers databases web.']]\n",
      "\n",
      "[['answer candidate’s need additional training basic programming languages platforms transferable skills.', 'vital understand cost time money train candidate knowledgeable languages applications required position.', 'statistics machine learning, common tasks fit model set training data, able reliable predictions general untrained data.', 'substituting labels data points performing significance tests validating models random subsets (bootstrapping, cross-validation) 21.', 'firm uses advanced technology address everyday problems consumers businesses alike, admire.', 'underfitting occurs statistical model machine learning algorithm capture underlying trend data.', 'answers look include interest data mining respect company’s innovative practices desire apply analytical skills solve real-world issues data “i passion working data-driven, innovative companies.', 'devise complex models algorithms lend prediction commercial use known predictive analytics.', 'candidate pursuing position passionate data believe company, elements determine candidate’s performance.']]\n",
      "\n",
      "[['coming square data science projects seen lot companies project kaggle problems.', 'advances handling huge data, best platform engage graphical capacities comes functional graphical capacities limited knowledge field.', 'unsupervised learning, hand, type machine learning inferences drawn datasets containing input data labeled responses.', 'unsupervised learning uses anomaly detection, clustering, latent variable models, neural networks.', 'useful customize plots better tool management benefits release updates regards controlled conditions.', 'following differences types machine learning algorithms – supervised learning makes use decision trees, k-nearest neighbor algorithm, neural networks, regression, support vector machines.', 'enables – supervised learning enables classification regression, unsupervised learning enables classification, dimension reduction, density estimation use – supervised learning prediction, unsupervised learning finds use analysis 26.', 'prediction rate provides low prediction training error test error leads high business problem, error rate training set high error rate test set high, conclude overfitting model.', 'problem faced hands-on analysis data science poor understanding problem hand concentrating tools, end results aspects project.']]\n",
      "\n",
      "[['method, implementation python programming technique important method machine learning technique area data science.', 'r seen data science area data analysis standalone servers computing separately.', 'linear regression called regression analysis comes area statistical sciences integrated data science.', 'recommender system today widely deployed multiple fields like movie recommendations, music preferences, social tags, research articles, search queries on.', 'dependent variable outcome variable response variable independent variable predictor variable explanatory variable.', 'python better cases repeated tasks jobs data manipulations r programming querying retrieving datasets customized data analysis.', 'process linear regression method predict variable called target variable making best relationship dependent variable independent variable.', 'type system works based person’s past behavior order build model future.', 'python preferred cases general-purpose programming language found applications data science too.', 'python preferred types data science applications time r programming preferred cases high complex data applications.'], ['linear regression technique supervised machine learning algorithmic process area data science.', 'example real life, depending expenses occurred financial year monthly expenses, predictions happen calculating approximate upcoming months financial years expenses.', 'predictive analytics area statistical sciences existing information extracted processed predict trends outcomes pattern.']]\n",
      "\n",
      "[['statistics help data scientists look data patterns, hidden insights convert big data big insights.', 'overfitting comes light data associated complexity, means associated parameters relative number observations.', 'systematic sampling, list actually circular manner selection starts end reaches final, cycle goes on.', 'filtering process recommender systems find patterns information numerous data sources, agents, collaborating perspectives.', 'unnderfittinng happens machine learning algorithm statistical model unable focus underlying insights data.', 'overfitting, statistical model help letting know random noise errors instead underlying relationship.', 'model overfitted performed poor predictive performance acts overly minor fluctuations training data.', 'data scientists learn consumer behavior, interest, engagement, retention finally conversion power insightful statistics.', 'systematic sampling technique, resembles follows systematic way, samples selected ordered sampling frame.', 'machine learning statistics, common task undergo fit model set training data.'], ['method helps sense data random creating order interpreting results bell-shaped graph.']]\n",
      "\n",
      "[['activation function helps introducing nonlinearity neural network enables neural network learn complex functions.', 'machine learning, feature vectors represent numeric symbolic characteristics, called features, object mathematical, easily analyzable way.', 'recommender systems widely areas like news, movies, social tags, music, products, etc.', 'movie recommenders netflix, imdb, & bookmyshow, product recommender e-commerce sites like ebay, amazon, flipcart, youtube video recommendations, game recommendations.', 'recommender systems treated information filtering systems work predict likeness user product.', 'cluster sampling technique difficult study target population spread wide area simple random sampling applied.', 'connection acts similar synapses human brain helps transmitting signals artificial neurons.', 'algorithm learns training data knowledge applied test data, referred supervised learning.', 'artificial neural networks work based nodes called artificial neurons connected another.']]\n",
      "\n",
      "[['outlier values, simply outliers, data points statistics don’t belong certain population.', 'outlier values assessed individually assessing large set outlier values require substitution 99th 1st percentile values.', 'pushing information readers brain, need figure easily help consume dashboard chart.', 'following steps involved analytics project understand business problem explore data familiar it.', 'prepare data modeling detecting outliers, treating missing values, transforming variables, etc.', 'popular ways treating outlier values change value brought range simply remove value note extreme values outlier values.', 'moment, start explaining simple things mission making complex simple goes away.']]\n",
      "\n",
      "[['cleaning data multiple sources transform format data analysts data scientists work cumbersome process – number data sources increases, time clean data increases exponentially number sources volume data generated sources.', 'data mining working unlimited data extract level unusual unknown patterns identified.', 'define key performance indicators product playing product, think this key metrics product want optimize?', 'data scientist’s role certain companies involves working closely product teams help define, measure, report metrics.', 'machine learning method study closely relates design, development concerning algorithms provide ability certain computers capacity learn.', 'data scientist’s time goes cleaning data – data gets bigger, time takes clean.', 'survey relied service unit, drawn telephone directories car registration lists. •', 'cleaning right foundation analysis, time takes clean data, alone, makes important.']]\n",
      "\n",
      "[['methods assess results logistic regression analysis- classification matrix look true negatives false positives.', 'satellite tables map id’s physical description connected central fact table id fields; tables known lookup tables, principally useful real- time applications, save lot memory.', 'focus makes path data scientist unique – mentor preferred method data extraction.', 'data science extraction knowledge large volumes data structured unstructured, continuation field data mining predictive analytics, known knowledge discovery data mining.', 'steps involved build decision trees bootstrapped training samples data tree, time split considered, random sample mm predictors chosen split candidates, pp predictors', 'data scientists are, all, numbers-based problem-solvers, so, it’s important determine example problem you’ve solved ahead time.', 'hash table (hash map) kind data structure implement associative array, structure map keys values.', 'ideally, hash function assign key unique bucket, possible keys generate identical hash causing keys point bucket.']]\n",
      "\n",
      "[['linear regression statistical technique score variable y predicted score second variable x. x referred predictor variable y criterion variable.', 'goal cross-validation term data set test model training phase (i.e. validation data set) order limit problems like overfitting insight model generalize independent data set.', 'model validation technique evaluating outcomes statistical analysis generalize independent data set.', 'explain utilize coding language know, r sql, language helps complete certain tasks.', 'best possible answer python pandas library provides easy use data structures high-performance data analysis tools.', 'simple terms, differences summarized as- training set fit parameters i.e. weights.', 'validation set considered training set parameter selection avoid overfitting model built.', 'focus data science – means extracting insights numbers – explain makes personal.']]\n",
      "\n",
      "[['method, error form end network inside it, brings efficient computation gradient.', 'consists below-mentioned steps forward data propagation data training derivatives computed help output target.', 'univariate analysis descriptive analysis differentiate number variables involved given point time.', 'easier assessed individually outlier values outlier values number values required substituted 1st 99th percentile values.', 'wide format method, subject, repeated responses recorded single row, recorded response separate column.', 'sampling bias bias arises select particular people non-random selection samples happened.']]\n",
      "\n",
      "[['unlike supervised learning, special teacher predefined data machine quickly learn from.', 'helpful areas backgrounds objective exactly forecasted, people want estimate accurately model work real-time.', 'main ambition cross-validation test model test model training phase limit problems like overfitting insights generalize independent data set.', 'supervised learning supervised learning process training machines labeled right kind data.', 'data data bias separate set data taken support conclusion eliminates terrible data based arbitrary grounds, instead generally relying generally stated criteria.', 'attrition bias attrition bias defined error occurs unequal loss participants randomized controlled trial (rct).', 'time interval trial terminated earlier actual time (probably ethical reasons) extreme value finally taken consideration significant value variables similar mean.', 'batch gradient descent backpropagation method, consider data calculating gradient executes update iteration.', 'mentioned different variants backpropagation stochastic gradient descent module, help single training example updating parameters calculation gradient.', 'easier assessed individually outlier values outlier values number values required substituted 1st 99th percentile values.'], ['data science defined multidisciplinary subject extract meaningful insights different types data employing scientific methods scientific processes algorithms.']]\n",
      "\n",
      "[['boltzmann developed simple learning algorithms allow find important information presented complex regularities data.', 'time interval trial terminated early extreme value (often ethical reasons), extreme value likely reached variable largest variance, variables similar mean.', 'types selection bias include sampling bias systematic error non-random sample population causing members population likely included resulting biased sample.', 'cumbersome process number data sources increases, time taken clean data increases exponentially number sources volume data generated sources.', 'data specific subsets data chosen support conclusion rejection bad data arbitrary grounds, instead according previously stated generally agreed criteria.', 'consider restricted boltzmann machines, single algorithm feature detectors faster compared others.', 'attrition attrition bias kind selection bias caused attrition (loss participants) discounting trial subjects/tests run completion.', 'data cleaning help analysis because cleaning data multiple sources helps transform format data analysts data scientists work with.']]\n",
      "\n",
      "[['also, data cleaning solely 80% total time required carrying data analysis task.', 'stochastic gradient descent use single training example calculation gradient update parameters.', 'main reasons increase data generated sources growth hardware resources required run models gpus multiple times faster help build bigger deeper deep learning models comparatively time required previously.', 'it’s variant stochastic gradient descent instead single training example, mini-batch samples used.', 'important ones are cleaning data different sources helps transforming data format easy work data cleaning increases accuracy machine learning model', 'data cleaning daunting task fact increase number data sources, time required cleaning data increases exponential rate.', 'simple terms, differences summarized as; training set fit parameters i.e. weights test set assess performance model i.e. evaluating predictive power generalization.', 'validation set considered training set parameter selection avoid overfitting model built.']]\n",
      "\n",
      "[['matplotlib provides primary 3d plotting mplot3d subpackage, mayavi produces wide range high-quality 3d visualization features, utilizing powerful vtk engine.', 'prediction rate high inconsistency training error test error leads ta high business problem, error rate training set low error rate ithe n test set high, conclude overfitting model.', 'best statistical functions, graphical user interface, come price tag readily adopted smaller enterprises r best r open source tool generously academia research community.', 'tableau prep reduce lot time like parent software (tableau) creating impressive visualizations.', 'known logit model, logistic regression statistical technique predicting binary outcome linear combination predictor variables.', 'mainly to increase data generation sources growth hardware resources required running deep learning models caffe, chainer, keras, microsoft cognitive toolkit, pytorch, tensorflow popular deep learning frameworks today.', 'like 2d plotting, 3d graphics scope numpy scipy, 2d example, packages exist integrate numpy.', 'tool lot potentials taking professionals data cleaning, merging step creating final usable data linked tableau desktop getting visualization business insights.', 'deep learning wide array uses, ranging social network filtering medical image analysis speech recognition.', 'deep learning paradigm machine learning displays great degree analogy functioning human brain.'], ['linear regression form statistical technique score variable y predicted basis score second variable x, referred predictor variable.']]\n",
      "\n",
      "[['especially useful data extremities certain region don’t data points specific point.', 'extrapolation determination estimation known set values facts extending taking area region unknown.', 'strictly speaking, database design includes detailed logical model database include physical design choices storage parameters.', 'python python powerful open source programming language easy learn, works tools technologies.', 'data modeling – data modeling (or modeling) software engineering process creating data model information system applying formal data modeling techniques.', 'interpolation, hand, method determining certain value falls certain set values sequence values.']]\n",
      "\n",
      "[['univariate analyses descriptive statistical analysis techniques differentiated based number variables involved given point time.', 'selection bias product inadequately improperly randomized data leading data sets representative whole.', 'example, pie charts sales based territory involve variable analysis referred univariate analysis.', '3 rejected marry good person based predictive model happen meet him/her years realize false negative?', '1 assume airport ‘a’ received high-security threats based certain characteristics identify particular passenger threat not.']]\n",
      "\n",
      "[['sampling bias systematic error resulting non-random sample populace causing certain members likely included results biased sample.', 'elbow curve graph contains point represents point post aren’t decrements wss.', 'data – results specific data subsets selected supporting conclusion rejection bad data arbitrarily.', 'defining number clusters clustering algorithm, wss plotted range pertaining number clusters.', 'recommender systems subclass information filtering systems, meant predicting preferences ratings awarded user product.', 'time interval – trial ended extreme value, usually ethical reasons, extreme value likely reached variable variance, variables similar mean.', 'primary objective clustering group similar identities way entities group similar other, groups remain different another.', 'attrition – caused attrition, i.e. loss participants, discounting trial subjects tests didn’t run completion.']]\n",
      "\n",
      "[['want update algorithm when want model evolve data streams infrastructure underlying data source changing case non-stationarity planning data science certification r – programming?', 'important understand data scientists able communicate findings, work team environment skills perform task.', 'unsupervised learning type machine learning algorithm draw inferences datasets consisting input data labeled responses.', 'p-value – 0.05 denotes weak evidence null hypothesis means null hypothesis rejected.', 'gaussian distribution exponential family distributions, lot them, sort ease use, cases, person machine learning solid grounding statistics, utilized appropriate 111.', 'p-value -0.05 denotes strong evidence null hypothesis means null hypothesis rejected.', 'here’re 100 data science foundations s. free practice test know stand.', 'algorithms clustering, anomaly detection, neural networks, latent variable models data science mock interviews interviews industry expertspersonalized detailed interview feedback access exclusive curated content e.g. example, fruit clustering categorize “fruits soft skin lots dimples”, “fruits shiny hard skin” “elongated yellow fruits”.', 'helps determine candidate’s experience holistic perspective reveals experience demonstrating interpersonal, communication technical skills.']]\n",
      "\n",
      "[['time series algorithms like arima, arimax, sarima, holts winters interesting learn use solve lot complex problems businesses.', 'validation set training set parameter selection avoiding overfitting machine learning model developed.', 'underfitting occurs, statistical model machine learning algorithm fails capturing underlying trend data.', 'boltzmann machine features simple learning algorithm enables discover fascinating features representing complex regularities present training data.', 'working visualization projects helps develop key skills data scientist possess i.e. thinking shoes end-user.', 'overfitted model overreacts minor fluctuations training data, underfit model under-reacts bigger fluctuations.', 'occurrence – statistical model machine learning algorithm excessively complex, result overfitting.', 'simple learning algorithm involved boltzmann machine slow networks layers feature detectors.', 'order reliable predictions general untrained data machine learning statistics, required fit model set training data.', 'poor predictive performance – overfitting underfitting yield poor predictive performance, way different.'], ['contrary, test set meant evaluating testing performance trained machine learning model.', 'following differences overfitting underfitting definition – statistical model suffering overfitting describes random error noise place underlying relationship.']]\n",
      "\n",
      "[['highlights r programming environment include following extensive collection tools data analysis operators performing calculations matrix array data analysis technique graphical representation highly developed simple effective programming language extensively supports machine learning applications acts connecting link software, tools, datasets create high-quality reproducible analysis flexible powerful provides robust package ecosystem diverse needs useful solve data-oriented problem 120.', 'r programming language includes set software suite graphical representation, statistical computing, data manipulation, calculation.', 'data cleansing essential data science data prone error human negligence, corruption transmission storage things.', 'data cleansing takes huge chunk time effort data scientist multiple sources data emanates speed comes.', 'data cleansing extensively deals process detecting correcting data records, ensuring data complete accurate components data irrelevant deleted modified needs.']]\n",
      "\n",
      "[['arti+cial intelligence going create 2.3 million jobs 2020 crack job interview come set deep learning interview s. divided article sections basic deep learning interview s advance deep learning interview s basics deep learning interview s q1.', 'traditional ml algorithms solve lot cases, useful working high dimensional data, large number inputs outputs.', 'deep learning interview s know 1.3k views kurt updated 22,2019 deep learning hottest topics 2018-19 good reason.', 'machine learning subset ai technique uses statistical methods enable machines improve experience.', 'second major challenge tell computer features look play important role predicting outcome achieve better accuracy so.', 'dendrite receives signals neurons cell body sums inputs axon transmit signals cells similarly, perceptron receives multiple inputs, applies transformations functions provides output.', 'example, case handwriting recognition, large input different type inputs associated different type handwriting.']]\n",
      "\n",
      "[['cost function measure accuracy neural network respect given training sample expected output.', 'activation function decides neuron activated calculating weighted sum adding bias it.', 'activation functions like linear identity unit binary step sigmoid logistic tanh relu softmax q6.', 'gradient descent optimization algorithm minimize function iteratively moving direction steepest descent defined negative gradient.', 'stochastic gradient descent uses single training example calculate gradient update parameters.', 'repeat steps 2 3 wj (t+1) – updated weight wj (t) – old weight d – desired output y – actual output x – input q7.', 'weights determine slope classifier line, bias allows shift line left right.', 'mini-batch gradient descent mini-batch gradient variation stochastic gradient descent instead single training example, mini-batch samples used.', 'mini-batches allows help approximate gradient entire training set helps avoid local minima.']]\n",
      "\n",
      "[['1 2 3 4 5 6 7 8 9 10 11 12 13 params = [weights_hidden, weights_output, bias_hidden, bias_output] def sgd(cost, params, lr=0.05) grads = t.grad(cost=cost, wrt=params) updates = [] p, g zip(params, grads) updates.append([p, p - g * lr]) return updates updates = sgd(cost, params)', 'network single input layer single output layer, zero multiple hidden layers.', 'data normalization important preprocessing step, rescale values +t speci+c range assure better convergence backpropagation.', 'composed input layer receive signal, output layer makes decision prediction input, two, arbitrary number hidden layers true computational engine mlp.', 'output nodes output nodes collectively referred “output layer” responsible computations transferring information network outside world.', 'bad weight initialization prevent network learning good weight initialization helps giving quicker convergence better overall error.', 'input nodes input nodes provide information outside world network referred “input layer”.', 'hidden nodes hidden nodes perform computations transfer information input nodes output nodes.']]\n",
      "\n",
      "[['batch size mini batch size number sub-samples given network parameter update happens.', 'hyperparameters variables determine network structure(eg number hidden units) variables determine network trained(eg learning rate).', 'feed-forward neural network type neural network architecture connections “fed forward”, i.e. form cycles.', 'activation function activation functions introduce nonlinearity models, allows deep learning models learn nonlinear prediction boundaries.', 'network hyperparameters number hidden layers hidden units layer regularization techniques increase accuracy.', 'likely better performance dropout larger network, giving model opportunity learn independent representations.', 'number hidden layers network weight initialization activation function learning rate momentum number epochs batch size q20.', 'network weight initialization ideally, better use different weight initialization schemes according activation function layer.', 'number epochs number epochs number times training data shown network training.', 'generally, use small dropout value 20%-50% neurons 20% providing good starting point.'], ['training hyperparameters learning rate learning rate de+nes quickly network updates parameters.', 'reasons be learning rate low regularization parameter high stuck local minima', 'term “feed-forward” input input layer travels input hidden hidden output layer.']]\n",
      "\n",
      "[['deep learning frameworks tensorflow caffe microsoft cognitive toolkit/cntk torch/pytorch mxnet chainer keras q24.', 'general, deep learning deal high dimensional data sets dimensions refer different features present data set.', 'operations assigned different nodes computational graph performed parallel, thus, providing better performance terms computations.', 'tensorflow auto differentiation capabilities advanced support threads, asynchronous computation, queue es.', 'convolutional neural network (cnn, convnet) class deep neural networks, commonly applied analyzing visual imagery.', 'layered concepts understand convolutional neural networks convolution convolution layer comprises set independent +lters.', 'basically, think computational graph alternative way conceptualizing mathematical calculations takes place tensorflow program.']]\n",
      "\n",
      "[['features instantiation parameters like position, size, orientation, deformation, velocity, hue, texture more.', 'connectedness neurons fully connected layer connections activations previous layer, seen regular neural networks.', 'obviously, improper inaccurate results, expect layers complete network perform nicely produce accurate results.', 'recurrent networks type arti+cial neural network designed recognize patterns sequences data, text, genomes, handwriting, spoken word, numerical times series data.', 'unlike standard feedforward neural networks, lstm feedback connections “general purpose computer”.', 'pooling function progressively reduce spatial size representation reduce number parameters computation network.', 'recurrent neural networks use backpropagation algorithm training internal memory, rnn’s able remember important things input received, enables precise predicting what’s coming next.', 'long short-term memory(lstm) arti+cial recurrent neural network architecture +eld deep learning.', 'exploding gradients problem large error gradients accumulate result large updates neural network model weights training.', 'magnitudes gradients accumulate, unstable network likely occur, cause poor prediction results model reports useful ever.'], ['earlier layers network important responsible learn detecting simple patterns actually building blocks network.', 'means neurons earlier layers learn slowly compared neurons later layers hierarchy.']]\n",
      "\n",
      "[['feature variation extracts required features image generates output removing noise unnecessary interruption.', 'autoencoder neural network unsupervised machine learning algorithm applies backpropagation, setting target values equal inputs.']]\n",
      "\n",
      "[['algorithm useful dimensionality reduction, classification, regression, collaborative filtering, feature learning, topic modeling.', 'restricted boltzmann machine undirected graphical model plays major role deep learning framework recent times.', 'task training minimize error reconstruction, i.e. find efficient compact representation input data.', 'deep autoencoder composed two, symmetrical deep-belief networks shallow layers representing encoding half net.', 'rbm shares similar idea, uses stochastic units particular distribution instead deterministic distribution.', 'autoencoder simple 3-layer neural network output units directly connected input units.']]\n",
      "\n",
      "[['browse world’s leading job boards, you’ll find it’s heart in-demand tech careers today. “', 'everyone’s trying figure ways optimize businesses practices, automate day-to-day lives little bit easier, little bit productive functional,” www.springboard.com 20 mins read', 'o interview prep 40 artificial intelligence s ver decade, artificial intelligence (ai) grown pipe dream driving force fourth industrial revolution.']]\n",
      "\n",
      "[['artificial intelligence s introduction ai ai interview internship, there’s good chance interviewer try break ice feel com- fortable asking “simple” general interest s. types s usually cover basics, sound straightforward, sure don’t stumped (seemingly simple s require answer', 'artificial intelligence s categories it’s broad area computer science, ai s popping job interview scenarios.', 'easier navigate space, curated list s artificial intelligence divided multiple categories.', 'so, job interview related data science, machine learning (ml), deep learning (dl), bet artifi- cial intelligence s come up.', 'you’re hoping data science career ladder looking start machine learning internship, sure brush ai interview s answers walk interview oozing confidence.']]\n",
      "\n",
      "[['open-source modular programming language python leads ai industry simplicity predictable coding behavior.', 'it’s amazon’s alexa self-driving car, goal mimic human intelligence lightning speed (and reduced rate error).']]\n",
      "\n",
      "[['popularity attributed open-source libraries like mat- plotlib numpy, efficient frameworks scikit-learn, practical version libraries like tensorflow vtk.', 'happens, men- tion following • java • julia • haskell • lisp reading 5 best programming languages ai 4.']]\n",
      "\n",
      "[['• human-level intelligence • processes data clustering association weak ai • great performing simple tasks • uses supervised unsupervised learning • scope minimal reading what’s difference weak strong ai?', 'current application ml ai based idea enable access data machines observe learn themselves.']]\n",
      "\n",
      "[['compelling examples ai applications are • chatbots • facial recognition • image tagging • natural language processing • sales prediction • self-driving cars • sentiment analysis reading ask ai experts applications ai?', 'interviewer prods provide real-world examples, list following • amazon product recommendations • fraud detection • search ranking • spam detection • spell correction reading explain machine learning data mining non-computer science people?']]\n",
      "\n",
      "[['example, human-versus- machine scenario, judge tasked identifying ter- minal occupied human occupied computer based individual performance.', 'reading tensorflow tutorial scratch building deep learning model fashion mnist dataset (part 1) 11.', 'game theory, developed american mathematician josh nash, essential ai plays underlying role smart algorithms improve time.', 'it’s com- prehensive highly adaptable ecosystem libraries, tools, community resources help developers build deploy ml-pow- ered applications.']]\n",
      "\n",
      "[['you’re naturally passionate ai every- thing related it, knowledge current industry trends.', 'devops replaced calling aiops allows developers engage accurate root cause analysis combining big data, ml, visualization.', 'data scientists space aware following games • symmetric vs. asymmetric • perfect vs. imperfect information • cooperative vs. non-cooperative • simultaneous vs. sequential • zero-sum vs. non-zero-sum reading connection game theory ai?']]\n",
      "\n",
      "[['common ones are • ai replace humans • ai systems aren’t safe • ai lead significant unemployment']]\n",
      "\n",
      "[['perspective systems theory, good knowledge represen- tation system following • acquisition efficiency acquire incorporate new data • inferential adequacy derive knowledge representation structures like symbols new knowledge learned old knowledge • inferential efficiency enable addition data existing knowledge structures help inference process • representation adequacy represent knowledge required specific domain reading knowledge representation ai 15.', 'variety keys relational database, including • alternate keys candidate keys exclude primary keys.', 'ai-based technology able complete tasks—for example, analyzing zettabytes data second—it needs humans gather data define pat- terns identification.']]\n",
      "\n",
      "[['• artificial keys created assigning unique number occurrence record aren’t compound standalone keys. •', 'compound keys combining multiple elements develop unique identifier construct isn’t single data element uniquely identifies occurrences construct.', 'foreign keys groups fields database record point key field group fields create key database record that’s usually different table.']]\n",
      "\n",
      "[['however, recent years grown go-to programming language following • machine learning • predictive analytics • simple data analytics • statistics data science projects, following packages python stand- ard library life easier accelerate deliveries • numpy (to process large multidimensional arrays, extensive collections high-level mathematical functions, matrices) • pandas (to leverage built-in methods rapidly combining, filtering, grouping data) • scipy (to extend numpy’s capabilities solve tasks related integral calculus, linear algebra, probability theory)', 'relative, ai produces actions, ml produces predictions, data sci- ence produces insights.', 'artificial intelligence s statistics ai, ml, data science great deal overlap, it’s crucial cover bases ai interview.']]\n",
      "\n",
      "[['disadvantages linear models, main ones are • errors linearity assumptions • lacks autocorrelation • can’t solve overfitting problems • can’t use calculate outcomes binary outcomes reading limitations linear regression modeling data analysis?', 'known social filtering, approach essentially makes sug- gestions based recommendations preferences peo- ple share similar interests.', 'collaborative filtering described process finding pat- terns available information build personalized recommenda- tions.', 'reading 20 python interview s answers—start pre- paring ideal job 17.', 'exam- ple, object’s numerical features list numbers taken output neural network layer.']]\n",
      "\n",
      "[['means like matrix, row multiple columns (or single column multiple rows) like [1,2,3,5,6,3,2,0].', 'it’s difficult predict ai interview unfold, fol- low asking list keys dictionary, respond following', 'ai data science, feature vectors represent numeric symbolic characteristics object mathematical terms seamless analysis.']]\n",
      "\n",
      "[['random forest data construct that’s applied ml projects develop large number random decision trees analyzing var- iables.', 'obtain list keys dictionary, you’ll use following function keys() mydict={‘a’1,’b’2,’c’3,’e’5} mydict.keys() dict_keys([‘a’, ‘b’, ‘c’, ‘e’]) reading sort python dictionaries key value 21.', 'it’s important consider reduce biases you’ll want smart algorithms accurate predic- tions based data.']]\n",
      "\n",
      "[['excellent tool ai ml projects work large labeled unlabeled data sets large number attributes.']]\n",
      "\n",
      "[['example, better sense covariance covariance matrix, eigenvector help identify direction covariances going.', 'artificial intelligence s programming ai interview s bound enter sphere programming sooner later.', 'popular known principal component analysis dimensionality reduction (e.g., eigenfaces face recognition).', 'example, normalize features 0 1 1 100, helps accelerate learning cycle.']]\n",
      "\n",
      "[['it’s data structure implements associative array abstract data type map key values.', 'algorithm techniques leveraged are • learning learn • reinforcement learning (deep adversarial networks, q-learning, temporal difference) • semi-supervised learning', 'array, actual table data stored, mapping function that’s known hash function.']]\n",
      "\n",
      "[['• supervised learning (decision trees, linear regression, naive bayes, nearest neighbor, neural networks, support vector machines) • transduction • unsupervised learning (association rules k-means clustering) reading types machine learning algorithms know 27.', 'defined problem statement, identify appropriate algorithm following • classification algorithm • clustering algorithm • regression algorithm • recommendation algorithm algorithm use depend specific problem you’re trying solve.', 'step essential it’ll help ensure fully understand type problem input output problem want solve.']]\n",
      "\n",
      "[['interviewer follows methods avoid overfitting, mention cross-valida- tion techniques k-folds cross-validation.', 'reading choose ml algorithm machine learning ques- tions & answers – iii 28.', 'ing algorithm choose k-means algorithm achieve goal filtering spam email system.', 'regular- ization techniques like lasso help penalize model parameters likely lead overfitting.', 'examples aren’t necessary answering s artificial intelligence, help easier point across.']]\n",
      "\n",
      "[['option cross-validation technique seg- ment data set composite training test sets data.', 'statistical ml, k-nearest neighbor support vector machine good examples inductive learning.', 'literals (top-down) inductive learning • arithmetic literals • equality inequality • predicates deductive learning, smart algorithms draw conclusions fol- lowing truth-generating structure (major premise, minor premise, conclusion) improve based previous decisions.']]\n",
      "\n",
      "[['• confusion matrix • accuracy • precision • recall sensitivity • specificity • f1 score part, use measures accuracy, confusion matrix, f1 score.', 'pandas, isnull() dropna() handy tools find missing cor- rupted data drop values.', 'however, it’ll critical demonstrate understand nuances model measured choosing right performance measure match problem.']]\n",
      "\n",
      "[['(source) reading build simple neural network 9 lines python code 34.']]\n",
      "\n",
      "[['source) reading drawing flower python turtle artificial intelligence s general ai interest tech talent shortage created fierce demand skills, land “dream job” it’ll help demonstrate pas- sion field.', 'scheduled ai interview startup established tech giant, ready wide-ranging ques- tions like ones listed below.']]\n",
      "\n",
      "[['excellent place start following sciencedirect track published research papers what’s pipeline.', 'example, talk ai jour- ney started weekend hobby grew space years.', 'fact, step summary research experience research papers ready share interviewing panel.']]\n",
      "\n",
      "[['dyann daley (founder ceo predict align prevent), siddha ganju (solutions architect nvidia), dr.', 'example, you’re interested use ai medical images, health analyt- ics interesting use cases • detecting fractures musculoskeletal injuries • aiding diagnosis neurological diseases • flagging thoracic complications conditions • screening common cancers 38.']]\n",
      "\n",
      "[['you’re genuinely passionate field, worked projects know find free data sets.', 'company training data collected sebastian thrun, ceo kitty hawk corporation co-founder (and ceo) udacity.', 'example, freely available public data sets know (without conducting google search) • celebfaces (with 200,000 celebrity images 40 attribute annotations) • cifar (with 60,000 images map 10 different classes) • youtube-8m (with 4,000 annotated entities taken enormous data set youtube videos) researchers released hundreds free resources like actual network architecture weights exam- ples.', 'talk ai projects you’ve worked free time, interviewer probably ask sourced data sets.', 'google recaptcha source labeled data store- fronts traffic signs years now.']]\n",
      "\n",
      "[]\n",
      "\n",
      "[['machine learning interview s categories we’ve traditionally seen machine learning interview s pop categories.', 'm achine learning interview s integral data science interview path data scientist, machine learning engineer, data engi- neer.', 'order help resolve that, curated created list key s machine learning interview.', 'springboard created free guide data science interviews, know exactly trip candidates!', 'general interest machine learning you’ll asked what’s going industry latest machine learning trends.']]\n",
      "\n",
      "[['essentially, model complex add variables, you’ll lose bias gain variance — order optimally reduced error, you’ll tradeoff bias variance.', 'leads algorithm highly sensitive high degrees variation training data, lead model overfit data.', 'lead model underfit- ting data, making hard high predictive accuracy generalize knowledge training set test set.', 'we’ve divided guide machine learning interview s categories mentioned easily information need comes machine learning inter- view s. machine learning interview s algorithms/theory algorithms s test grasp theory machine learning.', 'bias-variance decomposition essentially decomposes learning error algorithm adding bias, variance bit irreducible error noise underlying dataset.', 'reading bias-variance tradeoff (wikipedia) bias error erroneous overly simplistic assumptions learning algorithm you’re using.']]\n",
      "\n",
      "[['reading receiver operating characteristic (wikipedia) roc curve graphical representation contrast true positive rates false positive rate thresholds.', 'quora) k-nearest neighbors supervised classification algorithm, k-means clustering unsupervised clustering algorithm.', 'mechanisms similar first, means order k-nearest neighbors work, need labeled data want classify unlabeled point (thus nearest neighbor part).', 'k-means clustering requires set unlabeled points threshold algorithm unlabeled points gradually learn cluster groups computing mean distance different points.', 'example, order classification (a supervised learning task), you’ll need label data you’ll use train model classify data labeled groups.', 'critical difference knn needs labeled points supervised learning, k-means doesn’t — unsu- pervised learning.']]\n",
      "\n",
      "[['model (true positives) vs fall-out probability trig- ger false alarm (false positives).', 'reading precision recall (wikipedia) recall known true positive rate positives model claims compared actual number positives data.', 'you’d perfect recall (there actually 10 apples, predicted 10) 66.7% precision 15 events predicted, 10 (the apples) correct.', 'precision known positive pre- dictive value, measure accurate positives model claims compared number positives actually claims.', 'easier think recall precision context case you’ve predicted 10 apples 5 oranges case 10 apples.']]\n",
      "\n",
      "[['mathematically, it’s expressed true positive rate condition sample divided sum false positive rate population true positive rate condition.', 'says (.6 * 0.05) (true pos- itive rate condition sample) / (.6*0.05)(true positive rate condition sample) + (.5*0.95) (false positive rate population) = 0.0594 5.94% chance getting flu.', '60% chance actually having flu flu test, people flu, test false 50% time, overall population 5% chance having flu.', 'reading intuitive (and short) explanation bayes’ theorem (betterexplained) bayes’ theorem gives posterior probability event given known prior knowledge.', 'bayes’ theorem basis branch machine learning notably includes naive bayes classifier.']]\n",
      "\n",
      "[['quora) despite practical applications, especially text mining, naive bayes considered “naive” makes assumption virtually impossible real-life data conditional probabil- ity calculated pure product individual probabilities components.', 'important consider you’re faced machine learning inter- view s. q7- “naive” bayes naive?', 'quora commenter whimsically, naive bayes classifier figured liked pickles ice cream probably naively recommend pickle ice cream.', 'quora) l2 regularization tends spread error terms, l1 binary/sparse, variables assigned 1 0 weighting.']]\n",
      "\n",
      "[['sure choice sure explain different algorithms simply effectively five-year-old grasp basics!', 'type tests understanding communi- cate complex technical nuances poise ability sum- marize quickly efficiently.', 'briefly stated, type error means claiming happened hasn’t, type ii error means claim happening fact is.', 'machine learning inter- view s attempt lob basic s sure you’re game you’ve prepared bases.', 'clever way think think type error telling man pregnant, type ii error means tell pregnant woman isn’t carrying baby.']]\n",
      "\n",
      "[['fourier transform converts signal time frequency domain — it’s common way extract features audio signals time series sen- sor data.', 'fourier transform finds set cycle speeds, amplitudes phases match time signal.', 'fourier transform generic method decompose generic func- tions superposition symmetric functions.', 'reading deep learning (wikipedia) deep learning subset machine learning concerned neural networks use backpropagation certain principles']]\n",
      "\n",
      "[['sense, deep learning represents unsupervised learning algorithm learns representations data use neural nets.', 'reading k-fold cross-validation time-series model selection (crossvalidated) instead standard k-folds cross-validation, pay attention fact time series randomly distributed data — inherently ordered chronological order.', 'you’ll want like forward chaining you’ll able model past data look forward-facing data. •', 'stack overflow) generative model learn categories data discrimina- tive model simply learn distinction different catego- ries data.', 'pattern emerges later time periods example, model pick effect doesn’t hold earlier years!', 'fold 1 training [1], test [2] • fold 2 training [1 2], test [3] • fold 3 training [1 2 3], test [4] • fold 4 training [1 2 3 4], test [5] • fold 5 training [1 2 3 4 5], test [6] q16- decision tree pruned?']]\n",
      "\n",
      "[['reading accuracy paradox (wikipedia) tests grasp nuances machine learning model performance!', 'pruning happen bottom-up top-down, approaches reduced error pruning cost complexity prun- ing.', 'however, useless predictive model — model designed find fraud asserted fraud all!', 'reading pruning (decision trees) pruning happens decision trees branches weak predictive power removed order reduce complexity model increase predictive accuracy decision tree model.', 's like help demonstrate understand model accuracy isn’t be-all end-all model performance.', 'example, wanted detect fraud massive dataset sample millions, accurate model likely predict fraud vast minority cases fraud.']]\n",
      "\n",
      "[['use classification regression wanted results reflect belongingness data points dataset certain explicit categories (ex wanted know male female correlated male female names.)', 'reading regression vs classification (math stackexchange) classification produces discrete values dataset strict catego- ries, regression gives continuous results allow better distinguish differences individual points.', 'reading 8 tactics combat imbalanced classes machine learning dataset (machine learning mastery) imbalanced dataset have, example, classification test 90% data class.', 'weighted average precision recall model, results tending 1 best, tending 0 worst.']]\n",
      "\n",
      "[['list examples ensemble methods, bagging boosting “bucket models” method demonstrate increase predictive power.', 'main methods avoid overfitting 1- model simpler reduce variance taking account fewer variables parameters, removing noise training data.', 'quora) simple restatement fundamental problem machine learning possibility overfitting training data carrying noise data test set, providing inaccu- rate generalizations.', '3- use regularization techniques lasso penalize certain model parameters they’re likely cause overfitting.', 'reading ensemble learning (wikipedia) ensemble techniques use combination learning algorithms optimize better predictive performance.', 'typically reduce overfit- ting models model robust (unlikely influ- enced small changes training data).']]\n",
      "\n",
      "[['reading evaluating logistic regression (crossvalidated), lo- gistic regression plain english subsection above.', 'learn springboard’s ai / machine learning bootcamp, kind come job guaran- tee.)', 'allows useful attribute calculating coordinates higher dimensions computationally cheaper explicit calculation said coordinates.', 'what’s important demonstrate understand nuances model measured choose right performance measures right situations.', 'reading kernel method (wikipedia) kernel trick involves kernel functions enable higher- dimension spaces explicitly calculating coordinates points dimension instead, kernel functions compute inner products images pairs data feature space.', 'split dataset training test sets, per- haps use cross-validation techniques segment dataset composite sets training test sets data.', 'kernel trick enables effectively run algorithms high-dimensional space lower-dimensional data. (']]\n",
      "\n",
      "[['reading 50 open source tools big data (datamation) you’ll want familiar meaning big data different companies different tools they’ll want.', 'spark big data tool demand now, able handle immense datasets speed.', 'machine learning interview s programming machine learning interview s test knowledge programming principles need implement machine learning principles practice.', 'reading handling missing data (o’reilly) find missing/corrupted data dataset drop rows columns, decide replace value.', 'machine learning interview s tend technical s test logic programming skills section focuses latter.', 'pandas, useful methods isnull() dropna() help find columns data missing corrupted data drop values.', 'want fill invalid values placeholder value (for example, 0), use fillna() method.']]\n",
      "\n",
      "[['look pseudocode frameworks peril-l visualization tools web sequence diagrams help demonstrate ability write code reflects parallelism.', 'demanded, look job descriptions tools pop up you’ll want invest familiarizing them.', 'shuffling linked list involves changing points direct — meanwhile, shuffling array complex takes memory.', 'reading array versus linked list (stack overflow) array ordered collection objects.', 'reading writing pseudocode parallel programming (stack overflow) kind demonstrates ability think parallel- ism handle concurrency programming imple- mentations dealing big data.', 'reading hash table (wikipedia) hash table data structure produces associative array.']]\n",
      "\n",
      "[['stack overflow) lot machine learning interview s type involve implementation machine learning models company’s problems.', 'you’ll asked create case studies extend knowledge company industry you’re applying machine learning skills.', 'related 20 python interview s machine learning interview s company/industry specific machine learning interview s deal imple- ment general machine learning knowledge specific compa- ny’s requirements.', 'reading 31 free data visualization tools (springboard) what’s important define views properly vis- ualize data personal preferences comes tools.', 'popular tools include r’s ggplot, python’s seaborn matplotlib, tools plot.ly tableau.']]\n",
      "\n",
      "[['cially revenue drivers company has, types users company takes context industry it’s in.', 'example, interviewing music-streaming startup spotify, remark skills developing bet- ter recommendation model increase user retention, increase revenue long run.', 'startup metrics slideshare linked help understand exactly performance indicators important startups tech companies think revenue growth.']]\n",
      "\n",
      "[['interviewer trying gauge you’d valuable member team grasp nuances certain things set way company’s data process based company- indus- try-specific conditions.', 'overview deep learning nature scions deep learning (from hinton bengio lecun) good reference paper overview what’s happening deep learning — kind paper want cite.', 'machine learning interview s general machine learning interest series machine learning interview s attempts gauge passion interest machine learning.', 'keeping latest scientific literature machine learning want demonstrate interest machine learning position.']]\n",
      "\n",
      "[['reading 19 free public data sets data science project (springboard) machine learning interview s like try heart machine learning interest.', 'familiarity case solution help demonstrate you’ve paid attention machine learning while.', 'quora) quora thread contains examples, decision trees categorize people different tiers intelligence based iq scores.', 'team won called bellkor 10% improvement ensem- ble different methods win.', 'sure summary research experience papers ready — explanation background lack formal research experience don’t.', 'related point, organizations hiring machine learn- ing positions look formal experience field.', 'reading netflix prize (wikipedia) netflix prize famed competition netflix offered $1,000,000 better collaborative filtering algorithm.']]\n",
      "\n",
      "[['related 40 artificial intelligence interview s looking land role machine learning engineer?', 'you’re missing any, check quandl economic financial data, kaggle’s datasets collection great list.', 'find springboard’s machine learning engineer- ing career track, kind come job guarantee.', 'building training data collected sebastian thrun googlex — obtained grad students driving buggies desert dunes!', 'reading waymo tech machine learning interview s like test knowledge different machine learning methods, inven- tiveness don’t know answer.', 'nature paper describes accomplished “monte-carlo tree search deep neural networks trained supervised learning, human expert games, reinforcement learning games self- play.”', 'reading mastering game deep neural networks tree search (nature) alphago beating lee sidol, best human player go, best-of- series truly seminal event history machine learn- ing deep learning.']]\n",
      "\n",
      "[['browse world’s leading job boards, you’ll find it’s heart in-demand tech careers today. “', 'everyone’s trying figure ways optimize businesses practices, automate day-to-day lives little bit easier, little bit productive functional,” www.springboard.com 20 mins read', 'o interview prep 40 artificial intelligence s ver decade, artificial intelligence (ai) grown pipe dream driving force fourth industrial revolution.']]\n",
      "\n",
      "[['artificial intelligence s introduction ai ai interview internship, there’s good chance interviewer try break ice feel com- fortable asking “simple” general interest s. types s usually cover basics, sound straightforward, sure don’t stumped (seemingly simple s require answer', 'artificial intelligence s categories it’s broad area computer science, ai s popping job interview scenarios.', 'easier navigate space, curated list s artificial intelligence divided multiple categories.', 'so, job interview related data science, machine learning (ml), deep learning (dl), bet artifi- cial intelligence s come up.', 'you’re hoping data science career ladder looking start machine learning internship, sure brush ai interview s answers walk interview oozing confidence.']]\n",
      "\n",
      "[['open-source modular programming language python leads ai industry simplicity predictable coding behavior.', 'it’s amazon’s alexa self-driving car, goal mimic human intelligence lightning speed (and reduced rate error).']]\n",
      "\n",
      "[['popularity attributed open-source libraries like mat- plotlib numpy, efficient frameworks scikit-learn, practical version libraries like tensorflow vtk.', 'happens, men- tion following • java • julia • haskell • lisp reading 5 best programming languages ai 4.']]\n",
      "\n",
      "[['• human-level intelligence • processes data clustering association weak ai • great performing simple tasks • uses supervised unsupervised learning • scope minimal reading what’s difference weak strong ai?', 'current application ml ai based idea enable access data machines observe learn themselves.']]\n",
      "\n",
      "[['compelling examples ai applications are • chatbots • facial recognition • image tagging • natural language processing • sales prediction • self-driving cars • sentiment analysis reading ask ai experts applications ai?', 'interviewer prods provide real-world examples, list following • amazon product recommendations • fraud detection • search ranking • spam detection • spell correction reading explain machine learning data mining non-computer science people?']]\n",
      "\n",
      "[['example, human-versus- machine scenario, judge tasked identifying ter- minal occupied human occupied computer based individual performance.', 'reading tensorflow tutorial scratch building deep learning model fashion mnist dataset (part 1) 11.', 'game theory, developed american mathematician josh nash, essential ai plays underlying role smart algorithms improve time.', 'it’s com- prehensive highly adaptable ecosystem libraries, tools, community resources help developers build deploy ml-pow- ered applications.']]\n",
      "\n",
      "[['you’re naturally passionate ai every- thing related it, knowledge current industry trends.', 'devops replaced calling aiops allows developers engage accurate root cause analysis combining big data, ml, visualization.', 'data scientists space aware following games • symmetric vs. asymmetric • perfect vs. imperfect information • cooperative vs. non-cooperative • simultaneous vs. sequential • zero-sum vs. non-zero-sum reading connection game theory ai?']]\n",
      "\n",
      "[['common ones are • ai replace humans • ai systems aren’t safe • ai lead significant unemployment']]\n",
      "\n",
      "[['perspective systems theory, good knowledge represen- tation system following • acquisition efficiency acquire incorporate new data • inferential adequacy derive knowledge representation structures like symbols new knowledge learned old knowledge • inferential efficiency enable addition data existing knowledge structures help inference process • representation adequacy represent knowledge required specific domain reading knowledge representation ai 15.', 'variety keys relational database, including • alternate keys candidate keys exclude primary keys.', 'ai-based technology able complete tasks—for example, analyzing zettabytes data second—it needs humans gather data define pat- terns identification.']]\n",
      "\n",
      "[['• artificial keys created assigning unique number occurrence record aren’t compound standalone keys. •', 'compound keys combining multiple elements develop unique identifier construct isn’t single data element uniquely identifies occurrences construct.', 'foreign keys groups fields database record point key field group fields create key database record that’s usually different table.']]\n",
      "\n",
      "[['however, recent years grown go-to programming language following • machine learning • predictive analytics • simple data analytics • statistics data science projects, following packages python stand- ard library life easier accelerate deliveries • numpy (to process large multidimensional arrays, extensive collections high-level mathematical functions, matrices) • pandas (to leverage built-in methods rapidly combining, filtering, grouping data) • scipy (to extend numpy’s capabilities solve tasks related integral calculus, linear algebra, probability theory)', 'relative, ai produces actions, ml produces predictions, data sci- ence produces insights.', 'artificial intelligence s statistics ai, ml, data science great deal overlap, it’s crucial cover bases ai interview.']]\n",
      "\n",
      "[['disadvantages linear models, main ones are • errors linearity assumptions • lacks autocorrelation • can’t solve overfitting problems • can’t use calculate outcomes binary outcomes reading limitations linear regression modeling data analysis?', 'known social filtering, approach essentially makes sug- gestions based recommendations preferences peo- ple share similar interests.', 'collaborative filtering described process finding pat- terns available information build personalized recommenda- tions.', 'reading 20 python interview s answers—start pre- paring ideal job 17.', 'exam- ple, object’s numerical features list numbers taken output neural network layer.']]\n",
      "\n",
      "[['means like matrix, row multiple columns (or single column multiple rows) like [1,2,3,5,6,3,2,0].', 'it’s difficult predict ai interview unfold, fol- low asking list keys dictionary, respond following', 'ai data science, feature vectors represent numeric symbolic characteristics object mathematical terms seamless analysis.']]\n",
      "\n",
      "[['random forest data construct that’s applied ml projects develop large number random decision trees analyzing var- iables.', 'obtain list keys dictionary, you’ll use following function keys() mydict={‘a’1,’b’2,’c’3,’e’5} mydict.keys() dict_keys([‘a’, ‘b’, ‘c’, ‘e’]) reading sort python dictionaries key value 21.', 'it’s important consider reduce biases you’ll want smart algorithms accurate predic- tions based data.']]\n",
      "\n",
      "[['excellent tool ai ml projects work large labeled unlabeled data sets large number attributes.']]\n",
      "\n",
      "[['example, better sense covariance covariance matrix, eigenvector help identify direction covariances going.', 'artificial intelligence s programming ai interview s bound enter sphere programming sooner later.', 'popular known principal component analysis dimensionality reduction (e.g., eigenfaces face recognition).', 'example, normalize features 0 1 1 100, helps accelerate learning cycle.']]\n",
      "\n",
      "[['it’s data structure implements associative array abstract data type map key values.', 'algorithm techniques leveraged are • learning learn • reinforcement learning (deep adversarial networks, q-learning, temporal difference) • semi-supervised learning', 'array, actual table data stored, mapping function that’s known hash function.']]\n",
      "\n",
      "[['• supervised learning (decision trees, linear regression, naive bayes, nearest neighbor, neural networks, support vector machines) • transduction • unsupervised learning (association rules k-means clustering) reading types machine learning algorithms know 27.', 'defined problem statement, identify appropriate algorithm following • classification algorithm • clustering algorithm • regression algorithm • recommendation algorithm algorithm use depend specific problem you’re trying solve.', 'step essential it’ll help ensure fully understand type problem input output problem want solve.']]\n",
      "\n",
      "[['interviewer follows methods avoid overfitting, mention cross-valida- tion techniques k-folds cross-validation.', 'reading choose ml algorithm machine learning ques- tions & answers – iii 28.', 'ing algorithm choose k-means algorithm achieve goal filtering spam email system.', 'regular- ization techniques like lasso help penalize model parameters likely lead overfitting.', 'examples aren’t necessary answering s artificial intelligence, help easier point across.']]\n",
      "\n",
      "[['option cross-validation technique seg- ment data set composite training test sets data.', 'statistical ml, k-nearest neighbor support vector machine good examples inductive learning.', 'literals (top-down) inductive learning • arithmetic literals • equality inequality • predicates deductive learning, smart algorithms draw conclusions fol- lowing truth-generating structure (major premise, minor premise, conclusion) improve based previous decisions.']]\n",
      "\n",
      "[['• confusion matrix • accuracy • precision • recall sensitivity • specificity • f1 score part, use measures accuracy, confusion matrix, f1 score.', 'pandas, isnull() dropna() handy tools find missing cor- rupted data drop values.', 'however, it’ll critical demonstrate understand nuances model measured choosing right performance measure match problem.']]\n",
      "\n",
      "[['(source) reading build simple neural network 9 lines python code 34.']]\n",
      "\n",
      "[['source) reading drawing flower python turtle artificial intelligence s general ai interest tech talent shortage created fierce demand skills, land “dream job” it’ll help demonstrate pas- sion field.', 'scheduled ai interview startup established tech giant, ready wide-ranging ques- tions like ones listed below.']]\n",
      "\n",
      "[['excellent place start following sciencedirect track published research papers what’s pipeline.', 'example, talk ai jour- ney started weekend hobby grew space years.', 'fact, step summary research experience research papers ready share interviewing panel.']]\n",
      "\n",
      "[['dyann daley (founder ceo predict align prevent), siddha ganju (solutions architect nvidia), dr.', 'example, you’re interested use ai medical images, health analyt- ics interesting use cases • detecting fractures musculoskeletal injuries • aiding diagnosis neurological diseases • flagging thoracic complications conditions • screening common cancers 38.']]\n",
      "\n",
      "[['you’re genuinely passionate field, worked projects know find free data sets.', 'company training data collected sebastian thrun, ceo kitty hawk corporation co-founder (and ceo) udacity.', 'example, freely available public data sets know (without conducting google search) • celebfaces (with 200,000 celebrity images 40 attribute annotations) • cifar (with 60,000 images map 10 different classes) • youtube-8m (with 4,000 annotated entities taken enormous data set youtube videos) researchers released hundreds free resources like actual network architecture weights exam- ples.', 'talk ai projects you’ve worked free time, interviewer probably ask sourced data sets.', 'google recaptcha source labeled data store- fronts traffic signs years now.']]\n",
      "\n",
      "[]\n",
      "\n",
      "[['machine learning interview s categories we’ve traditionally seen machine learning interview s pop categories.', 'm achine learning interview s integral data science interview path data scientist, machine learning engineer, data engi- neer.', 'order help resolve that, curated created list key s machine learning interview.', 'springboard created free guide data science interviews, know exactly trip candidates!', 'general interest machine learning you’ll asked what’s going industry latest machine learning trends.']]\n",
      "\n",
      "[['essentially, model complex add variables, you’ll lose bias gain variance — order optimally reduced error, you’ll tradeoff bias variance.', 'leads algorithm highly sensitive high degrees variation training data, lead model overfit data.', 'lead model underfit- ting data, making hard high predictive accuracy generalize knowledge training set test set.', 'we’ve divided guide machine learning interview s categories mentioned easily information need comes machine learning inter- view s. machine learning interview s algorithms/theory algorithms s test grasp theory machine learning.', 'bias-variance decomposition essentially decomposes learning error algorithm adding bias, variance bit irreducible error noise underlying dataset.', 'reading bias-variance tradeoff (wikipedia) bias error erroneous overly simplistic assumptions learning algorithm you’re using.']]\n",
      "\n",
      "[['reading receiver operating characteristic (wikipedia) roc curve graphical representation contrast true positive rates false positive rate thresholds.', 'quora) k-nearest neighbors supervised classification algorithm, k-means clustering unsupervised clustering algorithm.', 'mechanisms similar first, means order k-nearest neighbors work, need labeled data want classify unlabeled point (thus nearest neighbor part).', 'k-means clustering requires set unlabeled points threshold algorithm unlabeled points gradually learn cluster groups computing mean distance different points.', 'example, order classification (a supervised learning task), you’ll need label data you’ll use train model classify data labeled groups.', 'critical difference knn needs labeled points supervised learning, k-means doesn’t — unsu- pervised learning.']]\n",
      "\n",
      "[['model (true positives) vs fall-out probability trig- ger false alarm (false positives).', 'reading precision recall (wikipedia) recall known true positive rate positives model claims compared actual number positives data.', 'you’d perfect recall (there actually 10 apples, predicted 10) 66.7% precision 15 events predicted, 10 (the apples) correct.', 'precision known positive pre- dictive value, measure accurate positives model claims compared number positives actually claims.', 'easier think recall precision context case you’ve predicted 10 apples 5 oranges case 10 apples.']]\n",
      "\n",
      "[['mathematically, it’s expressed true positive rate condition sample divided sum false positive rate population true positive rate condition.', 'says (.6 * 0.05) (true pos- itive rate condition sample) / (.6*0.05)(true positive rate condition sample) + (.5*0.95) (false positive rate population) = 0.0594 5.94% chance getting flu.', '60% chance actually having flu flu test, people flu, test false 50% time, overall population 5% chance having flu.', 'reading intuitive (and short) explanation bayes’ theorem (betterexplained) bayes’ theorem gives posterior probability event given known prior knowledge.', 'bayes’ theorem basis branch machine learning notably includes naive bayes classifier.']]\n",
      "\n",
      "[['quora) despite practical applications, especially text mining, naive bayes considered “naive” makes assumption virtually impossible real-life data conditional probabil- ity calculated pure product individual probabilities components.', 'important consider you’re faced machine learning inter- view s. q7- “naive” bayes naive?', 'quora commenter whimsically, naive bayes classifier figured liked pickles ice cream probably naively recommend pickle ice cream.', 'quora) l2 regularization tends spread error terms, l1 binary/sparse, variables assigned 1 0 weighting.']]\n",
      "\n",
      "[['sure choice sure explain different algorithms simply effectively five-year-old grasp basics!', 'type tests understanding communi- cate complex technical nuances poise ability sum- marize quickly efficiently.', 'briefly stated, type error means claiming happened hasn’t, type ii error means claim happening fact is.', 'machine learning inter- view s attempt lob basic s sure you’re game you’ve prepared bases.', 'clever way think think type error telling man pregnant, type ii error means tell pregnant woman isn’t carrying baby.']]\n",
      "\n",
      "[['fourier transform converts signal time frequency domain — it’s common way extract features audio signals time series sen- sor data.', 'fourier transform finds set cycle speeds, amplitudes phases match time signal.', 'fourier transform generic method decompose generic func- tions superposition symmetric functions.', 'reading deep learning (wikipedia) deep learning subset machine learning concerned neural networks use backpropagation certain principles']]\n",
      "\n",
      "[['sense, deep learning represents unsupervised learning algorithm learns representations data use neural nets.', 'reading k-fold cross-validation time-series model selection (crossvalidated) instead standard k-folds cross-validation, pay attention fact time series randomly distributed data — inherently ordered chronological order.', 'you’ll want like forward chaining you’ll able model past data look forward-facing data. •', 'stack overflow) generative model learn categories data discrimina- tive model simply learn distinction different catego- ries data.', 'pattern emerges later time periods example, model pick effect doesn’t hold earlier years!', 'fold 1 training [1], test [2] • fold 2 training [1 2], test [3] • fold 3 training [1 2 3], test [4] • fold 4 training [1 2 3 4], test [5] • fold 5 training [1 2 3 4 5], test [6] q16- decision tree pruned?']]\n",
      "\n",
      "[['reading accuracy paradox (wikipedia) tests grasp nuances machine learning model performance!', 'pruning happen bottom-up top-down, approaches reduced error pruning cost complexity prun- ing.', 'however, useless predictive model — model designed find fraud asserted fraud all!', 'reading pruning (decision trees) pruning happens decision trees branches weak predictive power removed order reduce complexity model increase predictive accuracy decision tree model.', 's like help demonstrate understand model accuracy isn’t be-all end-all model performance.', 'example, wanted detect fraud massive dataset sample millions, accurate model likely predict fraud vast minority cases fraud.']]\n",
      "\n",
      "[['use classification regression wanted results reflect belongingness data points dataset certain explicit categories (ex wanted know male female correlated male female names.)', 'reading regression vs classification (math stackexchange) classification produces discrete values dataset strict catego- ries, regression gives continuous results allow better distinguish differences individual points.', 'reading 8 tactics combat imbalanced classes machine learning dataset (machine learning mastery) imbalanced dataset have, example, classification test 90% data class.', 'weighted average precision recall model, results tending 1 best, tending 0 worst.']]\n",
      "\n",
      "[['list examples ensemble methods, bagging boosting “bucket models” method demonstrate increase predictive power.', 'main methods avoid overfitting 1- model simpler reduce variance taking account fewer variables parameters, removing noise training data.', 'quora) simple restatement fundamental problem machine learning possibility overfitting training data carrying noise data test set, providing inaccu- rate generalizations.', '3- use regularization techniques lasso penalize certain model parameters they’re likely cause overfitting.', 'reading ensemble learning (wikipedia) ensemble techniques use combination learning algorithms optimize better predictive performance.', 'typically reduce overfit- ting models model robust (unlikely influ- enced small changes training data).']]\n",
      "\n",
      "[['reading evaluating logistic regression (crossvalidated), lo- gistic regression plain english subsection above.', 'learn springboard’s ai / machine learning bootcamp, kind come job guaran- tee.)', 'allows useful attribute calculating coordinates higher dimensions computationally cheaper explicit calculation said coordinates.', 'what’s important demonstrate understand nuances model measured choose right performance measures right situations.', 'reading kernel method (wikipedia) kernel trick involves kernel functions enable higher- dimension spaces explicitly calculating coordinates points dimension instead, kernel functions compute inner products images pairs data feature space.', 'split dataset training test sets, per- haps use cross-validation techniques segment dataset composite sets training test sets data.', 'kernel trick enables effectively run algorithms high-dimensional space lower-dimensional data. (']]\n",
      "\n",
      "[['reading 50 open source tools big data (datamation) you’ll want familiar meaning big data different companies different tools they’ll want.', 'spark big data tool demand now, able handle immense datasets speed.', 'machine learning interview s programming machine learning interview s test knowledge programming principles need implement machine learning principles practice.', 'reading handling missing data (o’reilly) find missing/corrupted data dataset drop rows columns, decide replace value.', 'machine learning interview s tend technical s test logic programming skills section focuses latter.', 'pandas, useful methods isnull() dropna() help find columns data missing corrupted data drop values.', 'want fill invalid values placeholder value (for example, 0), use fillna() method.']]\n",
      "\n",
      "[['look pseudocode frameworks peril-l visualization tools web sequence diagrams help demonstrate ability write code reflects parallelism.', 'demanded, look job descriptions tools pop up you’ll want invest familiarizing them.', 'shuffling linked list involves changing points direct — meanwhile, shuffling array complex takes memory.', 'reading array versus linked list (stack overflow) array ordered collection objects.', 'reading writing pseudocode parallel programming (stack overflow) kind demonstrates ability think parallel- ism handle concurrency programming imple- mentations dealing big data.', 'reading hash table (wikipedia) hash table data structure produces associative array.']]\n",
      "\n",
      "[['stack overflow) lot machine learning interview s type involve implementation machine learning models company’s problems.', 'you’ll asked create case studies extend knowledge company industry you’re applying machine learning skills.', 'related 20 python interview s machine learning interview s company/industry specific machine learning interview s deal imple- ment general machine learning knowledge specific compa- ny’s requirements.', 'reading 31 free data visualization tools (springboard) what’s important define views properly vis- ualize data personal preferences comes tools.', 'popular tools include r’s ggplot, python’s seaborn matplotlib, tools plot.ly tableau.']]\n",
      "\n",
      "[['cially revenue drivers company has, types users company takes context industry it’s in.', 'example, interviewing music-streaming startup spotify, remark skills developing bet- ter recommendation model increase user retention, increase revenue long run.', 'startup metrics slideshare linked help understand exactly performance indicators important startups tech companies think revenue growth.']]\n",
      "\n",
      "[['interviewer trying gauge you’d valuable member team grasp nuances certain things set way company’s data process based company- indus- try-specific conditions.', 'overview deep learning nature scions deep learning (from hinton bengio lecun) good reference paper overview what’s happening deep learning — kind paper want cite.', 'machine learning interview s general machine learning interest series machine learning interview s attempts gauge passion interest machine learning.', 'keeping latest scientific literature machine learning want demonstrate interest machine learning position.']]\n",
      "\n",
      "[['reading 19 free public data sets data science project (springboard) machine learning interview s like try heart machine learning interest.', 'familiarity case solution help demonstrate you’ve paid attention machine learning while.', 'quora) quora thread contains examples, decision trees categorize people different tiers intelligence based iq scores.', 'team won called bellkor 10% improvement ensem- ble different methods win.', 'sure summary research experience papers ready — explanation background lack formal research experience don’t.', 'related point, organizations hiring machine learn- ing positions look formal experience field.', 'reading netflix prize (wikipedia) netflix prize famed competition netflix offered $1,000,000 better collaborative filtering algorithm.']]\n",
      "\n",
      "[['related 40 artificial intelligence interview s looking land role machine learning engineer?', 'you’re missing any, check quandl economic financial data, kaggle’s datasets collection great list.', 'find springboard’s machine learning engineer- ing career track, kind come job guarantee.', 'building training data collected sebastian thrun googlex — obtained grad students driving buggies desert dunes!', 'reading waymo tech machine learning interview s like test knowledge different machine learning methods, inven- tiveness don’t know answer.', 'nature paper describes accomplished “monte-carlo tree search deep neural networks trained supervised learning, human expert games, reinforcement learning games self- play.”', 'reading mastering game deep neural networks tree search (nature) alphago beating lee sidol, best human player go, best-of- series truly seminal event history machine learn- ing deep learning.']]\n",
      "\n",
      "[['neural networks extract features fed algorithms clustering classification; think deep neural networks components larger machine-learning applications involving algorithms reinforcement learning, classification regression.)', \"tips & stories inbox \\ue602 directory beginner's guide neural networks deep learning contents neural network definition concrete examples neural network elements key concepts deep neural networks example feedforward networks & backprop multiple linear regression updaters custom layers, activation functions loss functions logistic regression & classifiers loss functions deeplearning4j neural networks & artificial intelligence neural network definition neural networks set algorithms, modeled loosely human brain, designed recognize patterns.\", 'help group unlabeled data according similarities example inputs, classify data labeled dataset train on. (', 'patterns recognize numerical, contained vectors, real-world data, images, sound, text time series, translated.']]\n",
      "\n",
      "[['known “universal approximator”, learn approximate unknown function f(x) = y input x output y, assuming related (by correlation causation, example).', 'outcomes labels applied data example, spam not_spam email filter, good_guy bad_guy fraud detection, angry_customer happy_customer customer relationship management.', 'process learning, neural network finds right f, correct manner transforming x y, f(x) = 3x + 12 f(x) = 9x - 0.1.', 'classification classification tasks depend labeled datasets; is, humans transfer knowledge dataset order neural network learn correlation labels data.', 'is, find labeled data, create labeled dataset (with service like aws mechanical turk figure mighty.ai) spam labeled spam, order teach algorithm correlation labels inputs?', 'detect faces, identify people images, recognize facial expressions (angry, joyful) identify objects images (stop signs, pedestrians, lane markers…) recognize gestures video detect voices, identify speakers, transcribe speech text, recognize sentiment voices classify text spam (in emails), fraudulent (in insurance claims); recognize sentiment text (customer feedback) labels humans generate, outcomes care correlate data, train neural network.']]\n",
      "\n",
      "[['token, exposed right data, deep learning able establish correlations present events future events.', 'node place computation happens, loosely patterned neuron human brain, fires encounters sufficient stimuli.', 'brief overview deep learning use cases, let’s look neural nets of.', 'given time series, deep learning read string number predict number likely occur next.', 'neural network elements deep learning use “stacked neural networks”; is, networks composed layers.', 'we’re moving world smarter agents combine neural networks algorithms like reinforcement learning attain goals.', 'hardware breakdowns (data centers, manufacturing, transport) health breakdowns (strokes, heart attacks based vital stats data wearables) customer churn (predicting likelihood customer leave, based web activity metadata) employee turnover (ditto, employees) better predict, better prevent pre-empt.', 'predictive analytics regressions classification, deep learning able establish correlations between, say, pixels image person.', 'node combines input data set coefficients, weights, amplify dampen input, assigning significance inputs regard task algorithm trying learn; e.g. input helpful classifying data error?']]\n",
      "\n",
      "[['advance neural net, complex features nodes recognize, aggregate recombine features previous layer.', 'pairing model’s adjustable weights input features assign significance features regard neural network classifies clusters input.', 'deep-learning networks, layer nodes trains distinct set features based previous layer’s output.', 'earlier versions neural networks perceptrons shallow, composed input output layer, hidden layer between.', 'key concepts deep neural networks deep-learning networks distinguished commonplace single- hidden-layer neural networks depth; is, number node layers data pass multistep process pattern recognition.', 'deep buzzword algorithms like read sartre listen bands haven’t heard yet.', 'layer’s output simultaneously subsequent layer’s input, starting initial input layer receiving data.', 'products summed sum passed node’s so-called activation function, determine extent signal progress network affect ultimate outcome, say, act classification.']]\n",
      "\n",
      "[['word unstructured data raw media; i.e. pictures, texts, video audio recordings.', 'all, neural nets capable discovering latent structures unlabeled, unstructured data, vast majority data world.', 'apply idea data types deep learning cluster raw text emails news articles.', 'time series data generated smart phone, provide insight users’ health habits; generated autopart, prevent catastrophic breakdowns.', 'example, deep learning million images, cluster according similarities cats corner, ice breakers another, photos grandmother.', 'emails angry complaints cluster corner vector space, satisfied customers, spambot messages, cluster others.', 'therefore, problems deep learning solves best processing clustering world’s raw, unlabeled media, discerning similarities anomalies data human organized relational database to.', 'deep-learning networks perform automatic feature extraction human intervention, unlike traditional machine-learning algorithms.', 'makes deep-learning networks capable handling large, high-dimensional data sets billions parameters pass nonlinear functions.', 'given feature extraction task teams data scientists years accomplish, deep learning way circumvent chokepoint limited experts.']]\n",
      "\n",
      "[['step neural network involves guess, error measurement slight update weights, incremental adjustment coefficients, slowly learns pay attention important features.', 'given raw data form image, deep-learning network decide, example, input data 90 percent likely represent person.', 'collection weights, start end state, called model, attempt model data’s relationship ground-truth labels, grasp data’s structure.', 'deep learning’s ability process learn huge quantities unlabeled data distinct advantage previous algorithms.', 'process, neural networks learn recognize correlations certain relevant features optimal results – draw connections feature signals features represent, reconstruction, labeled data.', 'training unlabeled data, node layer deep network learns features automatically repeatedly trying reconstruct input draws samples, attempting minimize difference network’s guesses probability distribution input data itself.', 'starting line race state weights initialized, finish line state parameters capable producing sufficiently accurate classifications predictions.', 'models normally start bad end bad, changing time neural network updates parameters.', 'recipe higher performance data net train on, accurate likely be. (', 'example feedforward networks goal neural net arrive point error fast possible.'], ['deep-learning networks end output layer logistic, softmax, classifier assigns likelihood particular outcome label.', 'deep-learning network trained labeled data applied unstructured data, giving access input machine-learning nets.']]\n",
      "\n",
      "[['like child born knowing much, exposure life experience, slowly learn solve problems world.', 'you think neural network miniature enactment scientific method, testing hypotheses trying – scientific method blindfold on.', 'neural takes guess compares ground-truth data, effectively asking expert “did right?”', 'input * weight = guess weighted input results guess input is.', \"error * weight's contribution error = adjustment pseudo-mathematical formulas account key functions neural networks scoring input, calculating loss applying update model – begin three-step process again.\", 'multiple linear regression despite biologically inspired name, artificial neural networks math code, like machine-learning algorithm.', 'network measures error, walks error model, adjusting weights extent contributed error.', 'fact, understands linear regression, methods learn statistics, understand neural net works.', 'imagine time add unit x, dependent variable y_hat increases proportionally, matter far x axis.', 'simplest form, linear regression expressed y_hat = bx + y_hat estimated output, x input, b slope intercept line vertical axis two-dimensional graph. ('], ['to concrete x radiation exposure y cancer risk; x daily pushups y_hat total weight benchpress; x fertilizer y_hat size crop.)', 'ground truth - guess = error difference network’s guess ground truth error.', 'neural network corrective feedback loop, rewarding weights support correct guesses, punishing weights lead err.']]\n",
      "\n",
      "[['gradient descent commonly optimization function adjusts weights according error caused called “gradient descent.”', 'here’s why node merely performed multiple linear regression, y_hat increase linearly limit x’s increase, doesn’t suit purposes.', 'gradient word slope, slope, typical form x-y graph, represents variables relate other rise run, change money change time, etc.', 'it’s typically expressed like this y_hat = b_1*x_1 + b_2*x_2 + b_3*x_3 + (to extend crop example above, add sunlight rainfall growing season fertilizer variable, affecting y_hat.)', 'output nodes, squashed s-shaped space 0 1, passed input layer feed forward neural network, signal reaches final layer net, decisions made.', 'is, inputs mixed different proportions, according coefficients, different leading node subsequent layer.', 'trying build node switch (like neuron…) turns off, depending let signal input pass affect ultimate decisions network.', 'particular case, slope care describes relationship network’s error single weight; i.e. is, error vary weight adjusted.', 'binary decision expressed 1 0, logistic regression non-linear function squashes input translate space 0 1.']]\n",
      "\n",
      "[['chain rule calculus states feedforward network, relationship net’s error single weight look like this is, given variables, error weight, mediated variable, activation, weight passed, calculate change weight affects change error calculating change activation affects change error, change weight affects change activation.', 'relationship network error weights derivative, de/dw, measures degree slight change weight causes slight change error.', 'essence learning deep learning that adjusting model’s weights response error produces, can’t reduce error more.', 'weight factor deep network involves transforms; signal weight passes activations sums layers, use chain rule calculus march networks activations outputs finally arrive weight , relationship overall error.', 'optimization algorithms examples optimization algorithms include adadelta adagrad adam nesterovs rmsprop sgd conjugate gradient hessian free lbfgs line gradient descent activation functions activation function determines output node generate, based input.']]\n",
      "\n",
      "[['output layer condense signals $67.59 spent diapers, 15 visits website, range 0 1; i.e. probability given input labeled not.', 'neural networks working labeled data produce binary output, input receive continuous.', 'is, signals network receives input span range values include number metrics, depending problem seeks solve.', 'examples include cube elu hardsigmoid hardtanh identity leakyrelu rationaltanh relu rrelu sigmoid softmax softplus softsign tanh custom layers, activation functions loss functions deeplearning4j, major ai frameworks skymind supports alongside keras, includes custom layers, activations loss functions.', 'output node produces possible outcomes, binary output values 0 1, input variable deserves label .', 'input bases decision include customer spent amazon week, customer visits site.', 'node output layer represents label, node turns according strength signal receives previous layer’s input parameters.']]\n",
      "\n",
      "[['mse mean squared error linear regression expll exponential log likelihood poisson regression xent cross entropy binary classification mcxent multiclass cross entropy rmse_xent rmse cross entropy squared_loss squared loss negativeloglikelihood negative log likelihood neural networks & artificial intelligence circles, neural networks thought “brute force” ai,', 'input correlates negatively output value flipped negative sign e’s exponent, negative signal grows, quantity e x larger, pushing entire fraction closer zero.', 'input x triggers label grows, expression e x shrinks zero, leaving fraction 1/1, 100%, means approach (without reaching) absolute certainty label applies.', 'that’s you’re feeding logistic regression layer output layer neural network classifier.', 'imagine that, having x exponent, sum products weights corresponding inputs – total signal passing net.', 'set different thresholds prefer – low threshold increase number false positives, higher increase number false negatives – depending like err.', 'that’s input exponent e denominator – exponents force results greater zero.']]\n",
      "\n",
      "[['effective, eyes inefficient approach modeling, can’t assumptions functional dependencies output input.', 'reading recipe training neural networks, andrej karpathy interactive demo learn build ai applications interactive learning portal.', 'try company press kit contact press privacy platform skil subscriptions documentation community support international english japanese follow facebook twitter linkedin', 'moreover, algorithms hinton’s capsule networks require far fewer instances data converge accurate model; is, present research potential resolve brute force nature deep learning.', 'said, gradient descent recombining weight find best match – method pathfinding shrinks relevant weight space, number updates required computation, orders magnitude.']]\n",
      "\n",
      "[]\n",
      "\n",
      "[]\n",
      "\n",
      "[['that’s data science interview s cover bunch different topics (data science interdisciplinary field, all) cheeky interviewers love throw odd curveball.']]\n",
      "\n",
      "[]\n",
      "\n",
      "[]\n",
      "\n",
      "[['step hitting curveballs park coming, coming you’ve got confident rest game.', 'that’s data science interview s cover bunch different topics (data science interdisciplinary field, all) cheeky interviewers love throw odd curveball.']]\n",
      "\n",
      "[['remember hundred-odd different examples serve confuse more, plus comes didn’t study for?']]\n",
      "\n",
      "[['technical s 1.1 mathematics 1.2 statistics 1.3 coding 1.4 machine learning 2.']]\n",
      "\n",
      "[['technical s strong grasp mathematics, statistics, coding, machine learning data scientist.']]\n",
      "\n",
      "[]\n",
      "\n",
      "[]\n",
      "\n",
      "[['prepared answer quick (mental) maths s, as • sum numbers 1 100? •']]\n",
      "\n",
      "[]\n",
      "\n",
      "[]\n",
      "\n",
      "[['unlikely you’ll given equation solve, you’ll asked simply worded requires conceptual preparation answer.']]\n",
      "\n",
      "[['examples are • consider extension rock, paper, scissors n options instead 3 options.', 'values n possible construct fair game, ‘fair’ mean player plays equal number moves beat lose it? •']]\n",
      "\n",
      "[]\n",
      "\n",
      "[]\n",
      "\n",
      "[]\n",
      "\n",
      "[]\n",
      "\n",
      "[]\n",
      "\n",
      "[]\n",
      "\n",
      "[]\n",
      "\n",
      "[]\n",
      "\n",
      "[]\n",
      "\n",
      "[]\n",
      "\n",
      "[]\n",
      "\n",
      "[]\n",
      "\n",
      "[]\n",
      "\n",
      "[['check following machine learning s we’ve picked you • difference supervised unsupervised machine learning? •']]\n",
      "\n",
      "[['additionally, stumble way specific way vague s as • explain difference gaussian mixture model k- means. •']]\n",
      "\n",
      "[]\n",
      "\n",
      "[['countless data science s interviewer going waste time asking dozens s gauge candidate them.']]\n",
      "\n",
      "[['practical experience qs practical experience s, designed shed light pace work, experiences, habits.', 'avoid having sift catalogue experiences spot, mind experiences versatile – ones exemplify different skills based .']]\n",
      "\n",
      "[]\n",
      "\n",
      "[['practical experience qs course, vice-versa • so, r preferred programming language.']]\n",
      "\n",
      "[]\n",
      "\n",
      "[['like job interview, employers interested handle workplace situations, work team good fit company.', 'behavioural s asked indirectly, example, interviewer pose broad s motivation tasks enjoy.']]\n",
      "\n",
      "[['instead asking hypothetical s (“how deal with…”), interviewer hoping elicit meaningful response pushing chat real-life past event.', 'devote 10% answer time) known star technique, steps help present answers clear succinct fashion.']]\n",
      "\n",
      "[]\n",
      "\n",
      "[]\n",
      "\n",
      "[]\n",
      "\n",
      "[]\n",
      "\n",
      "[['given market sizing s, called guestimates some, term sounds like need stab dark, case.', 'reaching conclusion require degree guesswork estimation, process use difficult requires rigid logic.']]\n",
      "\n",
      "[['single correct answer s like chances interviewer doesn’t know exact answer, either.']]\n",
      "\n",
      "[['industry booming such, companies constantly adapting interview sessions (what common today hardly asked 2 years).', 'data science interview s vary peculiarities, types s remain same, having base knowledge types good preparation allow logically tackle interviewer sleeve.']]\n",
      "\n",
      "[['authors 365 data science online educational career website offers incredible opportunity find way data science world matter previous knowledge experience.', 'we, authors, committed educators believe curio- sity hindered inability access good learning resources.', 'courses cover necessary topics build data science skills ground up, including mathematics statistics, python, r, sql, data visualization, machine deep learning.', 'comprehensive programs suit needs aspiring bi analysts, data analysts, data scientists.']]\n",
      "\n",
      "[['comprehensive data science curriculum grow data science skillset training 365 data science program comprehensive set courses, work help student learn need expert data scientist months.', 'training includes sought-after skills, including • fundamentals mathematics • probability • intro data & data science • tableau • sql • r • python • machine learning program consists 45 hours on-demand video, split 12 courses, real-life business examples, 300 exercises.']]\n",
      "\n",
      "[]\n",
      "\n",
      "[['machine learning, statistical model describes random error noise instead underlying relationship ‘overﬁtting’ occurs.', 'technique, model usually given dataset known data training (training data set) run dataset unknown data model tested.', 'machine learning branch computer science deals system programming order automatically learn improve experience.', 'lot data overﬁtting avoided, overﬁtting happens relatively small dataset, try learn it.', 'model excessively complex, overﬁtting normally observed, having parameters respect number training data types.', 'machine learning relates study, design development algorithms computers capability learn explicitly programmed.', 'method dataset splits section, testing training datasets, testing dataset test model while, training dataset, datapoints come model.', 'inductive machine learning involves process learning examples, system, set observed instances tries induce general rule.', 'https//career.guru99.com/ 50 machine learning interview s & answers 1) machine learning?', 'while, data mining deﬁned process unstructured data tries extract knowledge unknown interesting patterns.']]\n",
      "\n",
      "[['a) model building b) model testing c) applying model 10) standard approach supervised learning?', 'areas information science like machine learning, set data discover potentially predictive relationship known ‘training set’.', 'a) decision trees b) neural networks (back propagation) c) probabilistic networks d) nearest neighbor e) support vector machines 8) diﬀerent algorithm techniques machine learning?', 'diﬀerent approaches machine learning a) concept vs classiﬁcation learning b) symbolic vs statistical learning', 'training set examples given learner, test set test accuracy hypotheses generated learner, set example held learner.', 'diﬀerent types techniques machine learning a) supervised learning b) unsupervised learning c) semi-supervised learning d) reinforcement learning e) transduction f) learning learn 9) stages build hypotheses model machine learning?']]\n",
      "\n",
      "[['classiﬁer machine learning system inputs vector discrete continuous feature values outputs single discrete value, class.', 'a) classiﬁcations b) speech recognition c) regression d) predict time series e) annotate strings 16) algorithm independent machine learning?', 'naïve bayes classiﬁer converge quicker discriminative models like logistic regression, need training data.', 'machine learning mathematical foundations independent particular classiﬁer learning algorithm referred algorithm independent machine learning?', 'artiﬁcial intelligence addition machine learning, covers aspects like knowledge representation, natural language processing, planning, robotics etc.', 'designing developing algorithms according behaviours based empirical data known machine learning.', 'a) artiﬁcial intelligence b) rule based inference 14) explain function ‘unsupervised learning’?', 'a) find clusters data b) find low-dimensional representations data c) find interesting directions data d) interesting coordinates correlations e) find novel observations/ database cleaning 15) explain function ‘supervised learning’?']]\n",
      "\n",
      "[['methods predicting good probabilities supervised learning a) platt calibration b) isotonic regression methods designed binary classiﬁcation, trivial.', 'diﬀerence heuristics decision trees evaluate average quality number disjointed sets rule learners evaluate quality set instances covered candidate rule.', 'a) computer vision b) speech recognition c) data mining d) statistics e) informal retrieval f) bio-informatics 21) genetic programming?', 'inductive logic programming (ilp) subﬁeld machine learning uses logical programming representing background knowledge examples.', 'process selecting models diﬀerent mathematical models, describe data set known model selection.']]\n",
      "\n",
      "[['paradigms ensemble methods a) sequential ensemble methods b) parallel ensemble methods 36) general principle ensemble method bagging boosting ensemble method?', 'a) combining binary classiﬁers b) modifying binary incorporate multiclass learning 32) ensemble learning?', 'ﬁrst component logical ; consists set bayesian clauses, captures qualitative structure domain.', 'general principle ensemble method combine predictions models built given learning algorithm order improve robustness single model.', 'solve particular computational program, multiple models classiﬁers experts strategically generated combined.', 'instance based learning algorithm referred lazy learning algorithm delay induction generalization process classiﬁcation performed.']]\n",
      "\n",
      "[['important components relational evaluation techniques a) data acquisition b) ground truth acquisition c) cross validation technique d) query type e) scoring metric f) signiﬁcance test 43) diﬀerent methods sequential supervised learning?', 'pca (principal components analysis), kpca ( kernel based principal component analysis) ica ( independent component analysis) important feature extraction techniques dimensionality reduction.', 'machine learning statistics, dimension reduction process reducing number random variables considerations divided feature selection feature extraction 41) support vector machines?', 'incremental learning method ability algorithm learn new data available classiﬁer generated available dataset.', 'bias term measures closely average classiﬁer produced learning algorithm matches target function.', 'diﬀerent methods solve sequential supervised learning problems a) sliding-window methods b) recurrent sliding windows c) hidden markow models d) maximum entropy markow models e) conditional random ﬁelds']]\n",
      "\n",
      "[['areas robotics information processing sequential prediction problem arises a) imitation learning b) structured prediction c) model based reinforcement learning 45) batch statistical learning?', 'pac (probably approximately correct) learning learning framework introduced analyze learning algorithms statistical eﬃciency.', 'f) graph transformer networks 44) areas robotics information processing sequential prediction problem arises?', 'statistical learning techniques allow learning function predictor set observed data predictions unseen future data.', 'techniques machine learning a) genetic programming b) inductive learning 50) popular application machine learning day day basis?', 'recommendation engine implemented major ecommerce websites uses machine learning guru99 provides free online tutorial courses like', 'a) sequence prediction b) sequence generation c) sequence recognition d) sequential decision 48) sequence learning?', 'techniques provide guarantees performance learned predictor future unseen data based statistical assumption data generating process.']]\n",
      "\n",
      "[['java mis mongodb bigdata cassandra web services sqlite jsp informatica accounting sap training python excel asp net hbase project management test management business analyst ethical hacking pmp live project soapui photoshop manual testing mobile testing data warehouse r tutorial tableau devops aws jenkins agile testing rpa junit software engineering selenium ccna angularjs nodejs plsql']]\n",
      "\n",
      "[['select model family train different training samples resulting different models ml family. •', 'new data comes, prediction 10 models combine predictions finally joint prediction.', 'ensemble learning says, build multiple models select best 2, 3 10.', 'bagging ensemble learning general, machine learning problems try find best possible optimal model given problem.', 'building/training different models means below • select different model families knn, decision trees, linear regression etc. •', 'means finding best possible model given model family, example, finding best possible decision tree finding best possible knn model.', 'time try model families available, come best possible regression model, best possible knn model, best possible svm model etc.']]\n",
      "\n",
      "[['o weighted mean build models different set features different samples features high importance generate optimal model need assign weight models combining.', 'combine predictions models means • regression (weighted) mean, median, max, min o regression problem mean, median mode models outcomes final outcome.']]\n",
      "\n",
      "[['suppose build 5 models outcome y1, y2, y3, y4 y5 respectively.', 'create new sample replacement strategy means mix original dataset sample created step 1 create new fresh sample.', 'standard deviation square root variance hance variance reduced factor 1/n. combiner bagging reduces model variance.', 'this hyperparameter, mathematical formula determine this) important thought, different k-fold cross-validation technique.', 'hance bagging high variance machine learning algorithms like decision trees, knn neural networks.', 'recall central limit theorem, says large population size good number (greater 30 optimal) samples means samples mean follow normal distribution population mean.', 'however preferable use high variance algorithms) • suited high variance algorithms. •', 'variance reduction averaging set observations reduces variance – central limit theorem. (', 'important standard deviation samples reduced square root n n sample size.', 'important points bagging • algorithm independent general-purpose technique, work machine learning algorithms. ('], ['remember outcomes y1, y2, y3, y4, y5 predicted models trained different samples big dataset.', 'note taking mean explain concept, median, mode etc based problem set.']]\n",
      "\n",
      "[['random feature subspaces build different models using • different subset training data (create samples replacement) • random subset features! (', 'limitation loss interpretability, example, decision tree single model, easy interpret build multiple decision trees different sample forest decide outcome combining outputs models lose interpretability. •', 'k-fold cross-validation, use training data different iteration choosing different hyperparameter values.', \"feature dominates build multiple decision trees exactly (think if-else rule decision trees) bagging won't work properly.\", 'in bagging time features) • ml algorithm training things gets change.', 'different feature selection different sampling assign different weight models depending dominating features.']]\n",
      "\n",
      "[['means include features sample training data set) advantages • de-correlates models ensemble • improve accuracy prediction • of-course reduces model variance.', 'bagged trees help reduce variance; random forests so… random forests • sample replacement (shift 1 training set multiple training sets) • train model training set • tree uses random subset feature random forest • dt predicts • mean / majority vote prediction final prediction • faster bagging (fewer splits evaluate tree)', 'recommended heuristics select m p m = sqrt(p) • m = p approach reduces bagged trees. (', 'random feature spaces decision trees known random forests algorithm • decision trees high variance. •', 'choose # models build # features (m) sample p available features. •']]\n",
      "\n",
      "[['thank like posts machine learning, connect follow blog https//ashutoshtripathi.com/ linkedin https//www.linkedin.com/in/ashutoshtripathi1/ instagram https//www.instagram.com/ashutosh_ai/ medium articles https//medium.com/@ashutosh.optimistic']]\n",
      "\n",
      "[['linear relationship dependent variables regressors, meaning model creating actually fits data, 2.', 'linear regression good tool quick predictive analysis example, price house depends myriad factors, size location.', 'basically, interaction effect factor (input variable) dependent variable (output variable) differs levels factor.”', 'selection (or ‘sampling’) bias occurs ‘active,’ sense sample data gathered prepared modeling characteristics representative true, future population cases', 'order relationship variables, need build linear regression, predicts line best fit help conclude factors positive negative relationship.', 'data sampling statistical analysis technique select, manipulate analyze representative subset data points identify patterns trends larger data set examined.”', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x data science interview s statistics 1.']]\n",
      "\n",
      "[['the binomial distribution consists probabilities possible numbers successes n trials independent events probability π (the greek letter pi) occurring.”', 'differences supervised unsupervised learning follows; supervised learning unsupervised learning input data labelled.', 'is, active selection bias occurs subset data systematically (i.e., non-randomly) excluded analysis.”', 'data science blend tools, algorithms, machine learning principles goal discover hidden patterns raw data.', 'the gaussian distribution exponential family distributions, lot them, sort ease use, cases, person machine learning solid grounding statistics, utilized appropriate.”']]\n",
      "\n",
      "[['train model time model makes simplified assumptions target function easier understand.', 'low bias machine learning algorithms — decision trees, k-nn svm high bias machine learning algorithms — linear regression, logistic regression variance variance error introduced model complex machine learning algorithm, model learns noise training data set performs badly test data set.', 'time interval trial terminated early extreme value (often ethical reasons), extreme value likely reached variable largest variance, variables similar mean.', 'bias-variance trade-off goal supervised machine learning algorithm low bias low variance achieve good prediction performance.', 'data specific subsets data chosen support conclusion rejection bad data arbitrary grounds, instead according previously stated generally agreed criteria.', 'sampling bias systematic error non-random sample population causing members population likely included resulting biased sample.', 'attrition attrition bias kind selection bias caused attrition (loss participants) discounting trial subjects/tests run completion.', 'continue model complex, end over-fitting model model start suffering high variance.']]\n",
      "\n",
      "[['k-nearest neighbour algorithm low bias high variance, trade-off changed increasing value k increases number neighbours contribute prediction turn increases bias model.', 'support vector machine algorithm low bias high variance, trade-off changed increasing c parameter influences number violations margin allowed training data increases bias decreases variance.']]\n",
      "\n",
      "[['false-negative(fn) — incorrect negative prediction basic measures derived confusion matrix 1.', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x predicted labels usually match observed labels real-world scenarios.', 'f-score(harmonic mean precision recall) = (1+b)(prec.rec)/(b²prec+rec) b commonly 0.5, 1, 2.']]\n",
      "\n",
      "[['figure normal distribution bell curve random variables distributed form symmetrical, bell-shaped curve.', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x statistics interview s q5.', 'however, chances data distributed central value bias left right reaches normal distribution form bell-shaped curve.']]\n",
      "\n",
      "[['likeliness probability called confidence level confidence coefficient represented 1 — alpha, alpha level significance.', 'covariance covariance items vary it’s measure indicates extent random variables change cycle.', 'statistical term; explains systematic relation pair random variables, changes variable reciprocal corresponding change variable.', 'method moments maximum likelihood estimator methods derive point estimators population parameters.', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x covariance correlation mathematical concepts; approaches widely statistics.', 'correlation correlation considered described best technique measuring estimating quantitative relationship variables.', 'a/b testing fantastic method figuring best online promotional marketing strategies business.', 'hypothesis testing randomized experiment variables b. goal a/b testing identify changes web page maximize increase outcome interest.']]\n",
      "\n",
      "[['follow steve nouri ai data science posts https//lnkd.in/gzu463x example identifying click-through rate banner ad.', 'case children, 4 equally likely possibilities bb, bg, gb gg; b = boy g = girl letter denotes child. ,', 'low p-value (≤ 0.05) indicates strength null hypothesis means reject null hypothesis.', 'remaining 3 possibilities bg, gb & bb, find probability case girls.', 'high p-value (≥ 0.05) indicates strength null hypothesis means accept null hypothesis p-value 0.05 indicates hypothesis way.', 'simple scenario exclude combination (6,6), i.e., roll die 6 appears twice. •', 'probability seeing shooting star 15 minutes = 1 – p( seeing shooting star ) = 1 – 0.2 = 0.8 probability seeing shooting star period hour = (0.8) ^ 4 = 0.4096 probability seeing shooting star hour = 1 – p( seeing star ) = 1 – 0.4096 = 0.5904 q12.']]\n",
      "\n",
      "[['statistics machine learning, common tasks fit model set training data, able reliable predictions general untrained data.', 'resampling cases • estimating accuracy sample statistics subsets accessible data drawing randomly replacement set data points • substituting labels data points performing significance tests • validating models random subsets (bootstrapping, cross-validation) q17.', 'probability selecting fair coin = 999/1000 = 0.999 probability selecting unfair coin = 1/1000 = 0.001 selecting 10 heads row = selecting fair coin * getting 10 heads + selecting unfair coin p (a) = 0.999 * (1/2)^5 = 0.999 * (1/1024) = 0.000976 p (b) = 0.001 * 1 = 0.001 p( / + b ) = 0.000976 / (0.000976 + 0.001) = 0.4939 p( b / + b ) = 0.001 / 0.001976 = 0.5061 probability selecting head = p(a/a+b) * 0.5 + p(b/a+b) * 1 = 0.4939 * 0.5 + 0.5061 = 0.7531 q15.', 'seasonality = ( true positives ) / ( positives actual dependent variable ) q16.']]\n",
      "\n",
      "[['combat overfitting underfitting, resample data estimate model accuracy (k-fold cross-validation) having validation dataset evaluate model.', 'data scientist masters program explore curriculum regularisation process adding tuning parameter model induce smoothness order prevent overfitting.', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x overfitting, statistical model describes random error noise instead underlying relationship.', 'underfitting occurs statistical model machine learning algorithm capture underlying trend data.']]\n",
      "\n",
      "[['example, researching lack exercise leads weight gain, lack exercise = independent variable weight gain = dependent variable.', 'logical error focusing aspects support surviving process casually overlooking work lack prominence.', 'says sample means, sample variance sample standard deviation converge trying estimate.', 'roc curve graphical representation contrast true positive rates false-positive rates thresholds.', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x theorem describes result performing experiment large number times.']]\n",
      "\n",
      "[['tf–idf value increases proportionally number times word appears document offset frequency word corpus, helps adjust fact words appear frequently general.', 'tf–idf short term frequency-inverse document frequency, numerical statistic intended reflect important word document collection corpus.']]\n",
      "\n",
      "[['univariate analyses descriptive statistical analysis techniques differentiated based number variables involved given point time.', 'prefer python following reasons • python best option pandas library provides easy use data structures high-performance data analysis tools. •', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x clear output probability distribution element non-negative sum components 1.', 'example, pie charts sales based territory involve variable analysis referred univariate analysis.', 'cumbersome process number data sources increases, time taken clean data increases exponentially number sources volume data generated sources. •', 'data cleaning help analysis because • cleaning data multiple sources helps transform format data analysts data scientists work with. •']]\n",
      "\n",
      "[['send free voucher mail directly 100 customers minimum purchase condition assume 20% profit sold items $10,000.', 'satellite tables map ids physical names descriptions connected central fact table id fields; tables known lookup tables principally useful real-time applications, save lot memory.', 'absence cancerous cell, chemotherapy certain damage normal healthy cells lead severe diseases, cancer.', 'false negatives cases wrongly classify events non-events, a.k.a type ii error.', 'false positives cases wrongly classified non-event event a.k.a type error. •', 'cluster sampling technique difficult study target population spread wide area simple random sampling applied.', 'issue send $1000 gift vouchers customers actually purchased marked having $10,000 worth purchase.', 'systematic sampling, list progressed circular manner reach end list, progressed again.', 'example 2 let’s e-commerce company decided $1000 gift voucher customers assume purchase $10,000 worth items.', 'assume patient comes hospital tested positive cancer, based lab prediction actually doesn’t cancer.']]\n",
      "\n",
      "[['goal cross-validation term data set test model training phase (i.e. validation data set) order limit problems like overfitting insight model generalize independent data set.', 'example 1 assume airport ‘a’ received high-security threats based certain characteristics identify particular passenger threat not.', 'banking industry giving loans primary source making money time repayment rate good profit, risk huge losses.', 'devise complex models algorithms lend prediction commercial use known predictive analytics.', 'banks don’t want lose good customers point time, don’t want acquire bad customers.', 'simple terms, differences summarized as; training set fit parameters i.e. weights test set assess performance model i.e. evaluating predictive power generalization.', 'validation set considered training set parameter selection avoid overfitting model built.', 'example 3 rejected marry good person based predictive model happen meet him/her years realize false negative?', 'cross-validation model validation technique evaluating outcomes statistical analysis generalize independent dataset.']]\n",
      "\n",
      "[['svm stands support vector machine, supervised machine learning algorithm regression classification.', 'algorithms clustering, anomaly detection, neural networks latent variable models e.g. example, fruit clustering categorize “fruits soft skin lots dimples”, “fruits shiny hard skin” “elongated yellow fruits”.', 'bayes’ theorem describes probability event, based prior knowledge conditions related event.', 'unsupervised learning type machine learning algorithm draw inferences datasets consisting input data labelled responses.', 'algorithms support vector machines, regression, naive bayes, decision trees, k-nearest neighbor algorithm neural networks e.g. built fruit classifier, labels “this orange, apple banana”, based showing classifier examples apples, oranges bananas.']]\n",
      "\n",
      "[['diagram, thinner lines mark distance classifier closest data points called support vectors (darkened data points).', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x n-dimensional space value feature value particular coordinate.']]\n",
      "\n",
      "[['id3 uses entropy information gain entropy decision tree built top-down root node involve partitioning data homogenious subsets.', 'breaks data set smaller smaller subsets time associated decision tree incrementally developed.', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x decision tree supervised machine learning algorithm mainly regression classification.']]\n",
      "\n",
      "[['pruning technique machine learning search algorithms reduces size decision trees removing sections tree provide little power classify instances.', 'predictor variables money spent election campaigning particular candidate, time spent campaigning, etc.', 'linear regression statistical technique score variable y predicted score second variable x. x referred predictor variable y criterion variable.', 'so, remove sub-nodes decision node, process called pruning opposite process splitting.', 'logistic regression referred logit model technique predict binary outcome linear combination predictor variables.']]\n",
      "\n",
      "[['supervised machine learning algorithm, train model labelled data set, training explicitly provide correct labels algorithm tries learn pattern input output.', 'recommender systems subclass information filtering systems meant predict preferences ratings user product.', 'recommender systems widely movies, news, research articles, products, social tags, music, etc.', 'examples include movie recommenders imdb, netflix & bookmyshow, product recommenders e- commerce sites like amazon, ebay & flipkart, youtube video recommendations game recommendations xbox.', 'process filtering recommender systems find patterns information collaborating viewpoints, data sources multiple agents.', 'can’t count outcomes binary outcomes • overfitting problems can’t solve q54.', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x follow ai data science posts https//lnkd.in/gzu463x q53.']]\n",
      "\n",
      "[['follow steve nouri ai data science posts https//lnkd.in/gzu463x example collaborative filtering predict rating particular user based his/her ratings movies others’ ratings movies.', 'number outlier values assessed individually large number outliers, values substituted 99th 1st percentile values.', 'concept widely recommending movies imdb, netflix & bookmyshow, product recommenders e-commerce sites like amazon, ebay & flipkart, youtube video recommendations game recommendations xbox.', 'patterns identified, missing values substituted mean median values (imputation) simply ignored.', '80% values variable missing answer dropping variable instead treating missing values.', 'prepare data modelling detecting outliers, treating missing values, transforming variables, etc.']]\n",
      "\n",
      "[['widely approach data scientists use hierarchical clustering create dendrograms identify distinct groups there.', 'bagging bagging tries implement similar learners small sample populations takes mean predictions.', 'objective clustering group similar entities way entities group similar groups different other.', 'red circled point graph i.e. number cluster =6 point don’t decrement wss. •', 'ensemble learning basically combining diverse set learners(individual models) improvise stability predictive power model.']]\n",
      "\n",
      "[['random forest versatile machine learning method capable performing regression classification tasks.', 'type ensemble learning method, group weak models combine form powerful model.', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x boosting boosting iterative technique adjusts weight observation based classification.']]\n",
      "\n",
      "[['follow steve nouri ai data science posts https//lnkd.in/gzu463x random forest, grow multiple trees opposed single tree.', 'fold 1 training[1], test[2] fold 1 training[1 2], test[3] fold 1 training[1 2 3], test[4] fold 1 training[1 2 3 4], test[5] q66.', 'forest chooses classification having votes(overall trees forest) case regression, takes average outputs different trees.', 'case time series data, use techniques like forward=chaining — model past data look forward-facing data.', 'steps involved • build decision trees bootstrapped training samples data • tree, time split considered, random sample mm predictors chosen split candidates, pp predictors • rule thumb split m=p√m=p • predictions majority rule q65.', 'box cox transformation statistical technique transform non-normal dependent variables normal shape.', 'instead k-fold cross-validation, aware fact time series randomly distributed data — inherently ordered chronological order.']]\n",
      "\n",
      "[['follow steve nouri ai data science posts https//lnkd.in/gzu463x techniques assume normality.', 'want update algorithm when • want model evolve data streams infrastructure • underlying data source changing • case non-stationarity • algorithm underperforms/ results lack accuracy q68.', 'numpy array property create mapping complete data set, doesn’t load complete data set memory.', 'normality important assumption statistical techniques, data isn’t normal, applying box-cox means able run broader number tests.', 'box-cox transformation named statisticians george box sir david roxbee cox collaborated 1964 paper developed technique.']]\n",
      "\n",
      "[['machine learning field computer science gives computers ability learn explicitly programmed.', 'reinforcement learning deep learning subfield machine learning concerned algorithms inspired structure function brain called artificial neural networks.']]\n",
      "\n",
      "[['follow steve nouri ai data science posts https//lnkd.in/gzu463x deep learning years, major breakthroughs techniques came recent years.', 'neural networks adapt changing input network generates best possible result needing redesign output criteria.', 'main reasons • increase data generated sources • growth hardware resources required run models gpus multiple times faster help build bigger deeper deep learning models comparatively time required previously.']]\n",
      "\n",
      "[['batch – refers pass entire dataset neural network once, divide dataset batches.', 'determines network trained structure network (such number hidden units, learning rate, epochs, etc.).', 'learning rate set high, causes undesirable divergent behaviour loss function drastic updates weights.', 'referred “loss” “error,” cost function measure evaluate good model’s performance is.', 'learning rate low, training model progress slowly making minimal updates weights.']]\n",
      "\n",
      "[['rnns type artificial neural networks designed recognise pattern sequence data time series, stock market government agencies etc.', 'convolutional layer – layer performs convolutional operation, creating smaller picture windows data.', 'networks rnn feed-forward named way channel information series mathematical orations performed nodes network.', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x • iteration – 10,000 images data batch size 200.', 'performs down-sampling operations reduce dimensionality creates pooled feature map sliding filter matrix input matrix.']]\n",
      "\n",
      "[['decision recurrent neural network reached time t-1 affects decision reach moment later time t. recurrent networks sources input, present recent past, combine determine respond new data, life.', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x recurrent networks, hand, input, current input example see, perceived previously time.', 'long-short-term memory (lstm) special kind recurrent neural network capable learning long- term dependencies, remembering information long periods default behaviour.', 'steps lstm network • step 1 network decides forget remember. •']]\n",
      "\n",
      "[['follow steve nouri ai data science posts https//lnkd.in/gzu463x neural networks, mlps input layer, hidden layer, output layer.', 'means input layers, data coming in, activation function based nodes weights added together, producing output.', 'single layer perceptron classify linear separable classes binary output (0,1), mlp classify nonlinear classes.']]\n",
      "\n",
      "[['method, error end network weights inside network allowing efficient computation gradient.', 'stochastic gradient descent use single training example calculation gradient update parameters. •', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x training rnn, exponentially growing (very large) error gradients accumulate result large updates neural network model weights training, they’re known exploding gradients.', 'it’s variant stochastic gradient descent instead single training example, mini-batch samples used.', 'following steps data scientist masters program weekday / weekend batchessee batch details • forward propagation training data • derivatives computed output target • propagate computing derivative error wrt output activation • previously calculated derivatives output • update weights q90.']]\n",
      "\n",
      "[['purpose libraries scientific computation numpy tabular data pandas data modelling & preprocessing scikit learn time-series analysis statsmodels text processing regular expressions, nltk deep learning tensorflow, pytorch q94.', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x • keras • caffe • chainer q92.', 'restricted boltzmann machines” algorithm single layer feature detectors makes faster rest.', 'auto-encoders simple learning networks aim transform inputs outputs minimum possible error.', 'boltzmann machines simple learning algorithm allows discover interesting features represent complex regularities training data.']]\n",
      "\n",
      "[['batch normalization technique improve performance stability neural networks normalizing inputs layer mean output activation zero standard deviation one.', 'tensorflow provides c++ python apis, making easier work faster compilation time compared deep learning libraries like keras torch.', 'arrays data different dimensions ranks fed input neural network called “tensors.”', 'batch gradient descent stochastic gradient descent batch gradient computes gradient entire dataset.', 'dropout technique dropping hidden visible units network randomly prevent overfitting data (typically dropping 20 cent nodes).']]\n",
      "\n",
      "[['logistic regression measures relationship dependent variable (our label want predict) independent variables (our features) estimating probability underlying logistic function (sigmoid).', 'network nodes node operates, nodes represent mathematical operations, edges represent tensors.', 'supervised learning unsupervised learning • uses known labeled data input • supervised learning feedback mechanism • commonly supervised learning algorithms decision trees, logistic regression, support vector machine • uses unlabeled data input • unsupervised learning feedback mechanism • commonly unsupervised learning algorithms k-means clustering, hierarchical clustering, apriori algorithm 102.']]\n",
      "\n",
      "[['follow steve nouri ai data science posts https//lnkd.in/gzu463x formula graph sigmoid function shown 103.', 'calculate information gain attributes (we gain information sorting different objects other) 4.', \"repeat procedure branch decision node branch finalized example, let's want build decision tree decide accept decline job offer.\"]]\n",
      "\n",
      "[[\"use regularization techniques, lasso, penalize certain model parameters they're likely cause overfitting 106.\", 'follow steve nouri ai data science posts https//lnkd.in/gzu463x clear decision tree offer accepted if • salary greater $50,000 • commute hour • incentives offered 104.', \"randomly select 'k' features total of'm' features k << m 2. '\", \"build forest repeating steps 'n' times create 'n' number trees 105.\", 'split data different packages decision tree different groups data, random forest brings trees together.']]\n",
      "\n",
      "[['follow steve nouri ai data science posts https//lnkd.in/gzu463x univariate univariate data contains variable.', 'example height students height (in cm) 164 167.3 170 174.2 178 180 patterns studied drawing conclusions mean, median, mode, dispersion range, minimum, maximum, etc.']]\n",
      "\n",
      "[['follow steve nouri ai data science posts https//lnkd.in/gzu463x temperature (in celcius) sales 20 2,000 25 2,100 26 2,300 28 2,400 30 2,600 36 3,100 here, relationship visible table temperature sales directly proportional other.']]\n",
      "\n",
      "[['wrapper methods involves • forward selection test feature time adding good fit • backward selection test features start removing works better • recursive feature elimination recursively looks different features pair wrapper methods labor-intensive, high-end computers needed lot data analysis performed wrapper method.', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x 3 2 1,100 $600,000 3.5 5 1,500 $900,000 4 3 2,100 $1,200,000 patterns studied drawing conclusions mean, median, mode, dispersion range, minimum, maximum, etc.', 'main methods feature selection filter methods involves • linear discrimination analysis • anova • chi-square best analogy selecting features \"bad data in, bad answer out.\"']]\n",
      "\n",
      "[['following ways handle missing data values data set large, simply remove rows missing data values.', 'numbers multiples five, print \"fizzbuzz\" code shown below note range mentioned 51, means zero 50.', 'smaller data sets, substitute missing values mean average rest data pandas data frame python.', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x multiples three, print \"fizz\" instead number multiples five, print \"buzz.\"']]\n",
      "\n",
      "[['plot1 = [1,3] plot2 = [2,5] euclidean distance calculated follows euclidean_distance = sqrt( (plot1[0]-plot2[0])**2 + (plot1[1]-plot2[1])**2 ) 111.', 'dimensionality reduction refers process converting data set vast dimensions data fewer dimensions (fields) convey similar information concisely.', '-2 -4 2 -2 1 2 4 2 5 characteristic equation shown expanding determinant (-2 – λ) [(1-λ) (5-λ)-2x2] + 4[(-2) x (5-λ) -4x2] + 2[(-2) x 2-4(1-λ)] =0 - λ3 + 4λ2 + 27λ – 90 = 0, λ3 - 4 λ2 -27 λ + 90 = 0 algebraic equation built eigenvectors.', \"removes redundant features; example, there's point storing value different units (meters inches).\"]]\n",
      "\n",
      "[['steps maintain deployed model are monitor constant monitoring models needed determine performance accuracy.', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x 33 – 4 x 32 - 27 x 3 +90 = 0 hence, (λ - 3) factor λ3 - 4 λ2 - 27 λ +90 = (λ – 3) (λ2 – λ – 30) eigenvalues 3,-5,6 (λ – 3) (λ2 – λ – 30) = (λ – 3) (λ+5) (λ-6), calculate eigenvector λ = 3 x = 1, -5 - 4y + 2z =0, -2 - 2y + 2z =0 subtracting equations 3 + 2y = 0, subtracting second equation y = -(3/2) z = -(1/2) similarly, calculate eigenvectors -5 6.']]\n",
      "\n",
      "[['split different areas collaborative filtering example, last.fm recommends tracks users similar interests play often.', 'commonly seen amazon making purchase; customers notice following message accompanied product recommendations \"users bought bought…\" content-based filtering example pandora uses properties song recommend music similar properties.', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x new models compared determine model performs best.', \"idea elbow method run k- means clustering data set 'k' number clusters.\"]]\n",
      "\n",
      "[['p-value typically ≤ 0.05 indicates strong evidence null hypothesis; reject null hypothesis.', 'example, data points clustered zero 10, point lies 100, remove point.', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x sum squares (wss), defined sum squared distance member cluster centroid.', 'p-value typically > 0.05 indicates weak evidence null hypothesis, accept null hypothesis.']]\n",
      "\n",
      "[['follow steve nouri ai data science posts https//lnkd.in/gzu463x graph, variance constant time.', 'formula accuracy is accuracy = (true positive + true negative) / total observations = (262 + 347) / 650 = 609 / 650 = 0.93 result, accuracy 93 percent.']]\n",
      "\n",
      "[['collaborative filtering explains behavior users purchase history terms ratings, selection, etc.', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x consider confusion matrix previous .', \"precision = (true positive) / (true positive + false positive) = 262 / 277 = 0.94 recall rate = (true positive) / (total positive + false negative) = 262 / 288 = 0.90 122. '\", 'example, sales page shows certain number people buy new phone buy tempered glass time.']]\n",
      "\n",
      "[['forger try different techniques sell fake wine sure specific techniques past shop owner’s check.', 'forger’s goal create wines indistinguishable authentic ones shop owner intends tell wine real accurately let understand example help image.', 'apart technical s, interviewer hit simple ones check overall confidence, likes following.', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x suppose wine shop purchasing wine dealers, resell later.', 'discriminator generator cnn keeps keys producing images closer appearance real images discriminator tries determine difference real fake images ultimate aim discriminator learn identify real fake images.']]\n",
      "\n",
      "[['want predict probability death heart disease based risk factors age, gender, blood cholesterol level.', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x hence, evaluate model performance, use sensitivity (true positive rate), specificity (true negative rate), f measure determine class wise performance classifier.', '0, 0, 0, 1, 1, 1, 1, 1] choose correct answer.', \"k-means clustering • linear regression • k-nn (k-nearest neighbor) • decision trees k nearest neighbor algorithm compute nearest neighbor doesn't value, computes nearest neighbor based features.\", 'following machine learning algorithms inputting missing values categorical continuous variables? •', 'formula calculating entropy is putting p=5 n=8, entropy = = -(5/8 log(5/8) + 3/8 log(3/8)) 127.', \"you're dealing k-means clustering linear regression, need pre- processing, otherwise, they'll crash.\"]]\n",
      "\n",
      "[['decision trees looking grouping people specifically different similarities, indicates value k. therefore, k-means clustering (answer a) appropriate algorithm study.', 'grape, apple} frequent itemset answer a {grape, apple} frequent itemset 130.', 'run association rules algorithm dataset, rules {banana, apple} => {grape} {apple, orange} => {grape} found relevant.']]\n",
      "\n",
      "[['recommender systems subclass information filtering systems meant predict preferences ratings user product.', 'factor called root cause deduction problem-fault-sequence averts final undesirable event recurring.', \"student's t-test answer a one-way anova additional data science interview s basic concepts 131.\", \"machine learning, feature vectors represent numeric symbolic characteristics (called features) object mathematical way that's easy analyze.\"]]\n",
      "\n",
      "[['statistical hypothesis testing randomized experiments variables, b. objective a/b testing detect changes web page maximize increase outcome strategy.', 'recommender systems use filtering process find patterns information collaborating perspectives, numerous data sources, agents.', 'satellite tables map ids physical names descriptions connected central fact table id fields; tables known', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x cross-validation model validation technique evaluating outcomes statistical analysis generalize independent data set.', 'states sample mean, sample variance sample standard deviation converge trying estimate.', 'goal cross-validation term data set test model training phase (i.e. validation data set) limit problems like overfitting gain insight model generalize independent data set.', \"assumption linearity errors • can't count outcomes binary outcomes • overfitting problems can't solve 141.\"]]\n",
      "\n",
      "[['follow steve nouri ai data science posts https//lnkd.in/gzu463x lookup tables principally useful real-time applications, save lot memory.', 'want update algorithm when • want model evolve data streams infrastructure • underlying data source changing • case non-stationarity 145.', 'resampling cases • estimating accuracy sample statistics subsets accessible data, drawing randomly replacement set data points • substituting labels data points performing significance tests • validating models random subsets (bootstrapping, cross-validation) 147.']]\n",
      "\n",
      "[['ability write small, clean functions (important developer), preferably pure functions don’t alter objects. •', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x survivorship bias logical error focusing aspects support surviving process casually overlooking lack prominence.', 'tree, time split considered, random sample mm predictors chosen split candidates pp predictors 3.', 'good understanding built-in data types especially lists, dictionaries, tuples, sets. •', 'credit kdnuggets, simplilearn, edureka, guru99, hackernoon, datacamp, nitin panwar, michael rundell', 'following important skills possess come handy performing data analysis python. •', 'scikit-learn cheat sheet** • ability write efficient list comprehensions instead traditional loops. •']]\n",
      "\n",
      "[['arti+cial intelligence going create 2.3 million jobs 2020 crack job interview come set deep learning interview s. divided article sections basic deep learning interview s advance deep learning interview s basics deep learning interview s q1.', 'traditional ml algorithms solve lot cases, useful working high dimensional data, large number inputs outputs.', 'deep learning interview s know 1.3k views kurt updated 22,2019 deep learning hottest topics 2018-19 good reason.', 'machine learning subset ai technique uses statistical methods enable machines improve experience.', 'second major challenge tell computer features look play important role predicting outcome achieve better accuracy so.', 'dendrite receives signals neurons cell body sums inputs axon transmit signals cells similarly, perceptron receives multiple inputs, applies transformations functions provides output.', 'example, case handwriting recognition, large input different type inputs associated different type handwriting.']]\n",
      "\n",
      "[['cost function measure accuracy neural network respect given training sample expected output.', 'activation function decides neuron activated calculating weighted sum adding bias it.', 'activation functions like linear identity unit binary step sigmoid logistic tanh relu softmax q6.', 'gradient descent optimization algorithm minimize function iteratively moving direction steepest descent defined negative gradient.', 'stochastic gradient descent uses single training example calculate gradient update parameters.', 'repeat steps 2 3 wj (t+1) – updated weight wj (t) – old weight d – desired output y – actual output x – input q7.', 'weights determine slope classifier line, bias allows shift line left right.', 'mini-batch gradient descent mini-batch gradient variation stochastic gradient descent instead single training example, mini-batch samples used.', 'mini-batches allows help approximate gradient entire training set helps avoid local minima.']]\n",
      "\n",
      "[['1 2 3 4 5 6 7 8 9 10 11 12 13 params = [weights_hidden, weights_output, bias_hidden, bias_output] def sgd(cost, params, lr=0.05) grads = t.grad(cost=cost, wrt=params) updates = [] p, g zip(params, grads) updates.append([p, p - g * lr]) return updates updates = sgd(cost, params)', 'network single input layer single output layer, zero multiple hidden layers.', 'data normalization important preprocessing step, rescale values +t speci+c range assure better convergence backpropagation.', 'composed input layer receive signal, output layer makes decision prediction input, two, arbitrary number hidden layers true computational engine mlp.', 'output nodes output nodes collectively referred “output layer” responsible computations transferring information network outside world.', 'bad weight initialization prevent network learning good weight initialization helps giving quicker convergence better overall error.', 'input nodes input nodes provide information outside world network referred “input layer”.', 'hidden nodes hidden nodes perform computations transfer information input nodes output nodes.']]\n",
      "\n",
      "[['batch size mini batch size number sub-samples given network parameter update happens.', 'hyperparameters variables determine network structure(eg number hidden units) variables determine network trained(eg learning rate).', 'feed-forward neural network type neural network architecture connections “fed forward”, i.e. form cycles.', 'activation function activation functions introduce nonlinearity models, allows deep learning models learn nonlinear prediction boundaries.', 'network hyperparameters number hidden layers hidden units layer regularization techniques increase accuracy.', 'likely better performance dropout larger network, giving model opportunity learn independent representations.', 'number hidden layers network weight initialization activation function learning rate momentum number epochs batch size q20.', 'network weight initialization ideally, better use different weight initialization schemes according activation function layer.', 'number epochs number epochs number times training data shown network training.', 'generally, use small dropout value 20%-50% neurons 20% providing good starting point.'], ['training hyperparameters learning rate learning rate de+nes quickly network updates parameters.', 'reasons be learning rate low regularization parameter high stuck local minima', 'term “feed-forward” input input layer travels input hidden hidden output layer.']]\n",
      "\n",
      "[['deep learning frameworks tensorflow caffe microsoft cognitive toolkit/cntk torch/pytorch mxnet chainer keras q24.', 'general, deep learning deal high dimensional data sets dimensions refer different features present data set.', 'operations assigned different nodes computational graph performed parallel, thus, providing better performance terms computations.', 'tensorflow auto differentiation capabilities advanced support threads, asynchronous computation, queue es.', 'convolutional neural network (cnn, convnet) class deep neural networks, commonly applied analyzing visual imagery.', 'layered concepts understand convolutional neural networks convolution convolution layer comprises set independent +lters.', 'basically, think computational graph alternative way conceptualizing mathematical calculations takes place tensorflow program.']]\n",
      "\n",
      "[['features instantiation parameters like position, size, orientation, deformation, velocity, hue, texture more.', 'connectedness neurons fully connected layer connections activations previous layer, seen regular neural networks.', 'obviously, improper inaccurate results, expect layers complete network perform nicely produce accurate results.', 'recurrent networks type arti+cial neural network designed recognize patterns sequences data, text, genomes, handwriting, spoken word, numerical times series data.', 'unlike standard feedforward neural networks, lstm feedback connections “general purpose computer”.', 'pooling function progressively reduce spatial size representation reduce number parameters computation network.', 'recurrent neural networks use backpropagation algorithm training internal memory, rnn’s able remember important things input received, enables precise predicting what’s coming next.', 'long short-term memory(lstm) arti+cial recurrent neural network architecture +eld deep learning.', 'exploding gradients problem large error gradients accumulate result large updates neural network model weights training.', 'magnitudes gradients accumulate, unstable network likely occur, cause poor prediction results model reports useful ever.'], ['earlier layers network important responsible learn detecting simple patterns actually building blocks network.', 'means neurons earlier layers learn slowly compared neurons later layers hierarchy.']]\n",
      "\n",
      "[['feature variation extracts required features image generates output removing noise unnecessary interruption.', 'autoencoder neural network unsupervised machine learning algorithm applies backpropagation, setting target values equal inputs.']]\n",
      "\n",
      "[['algorithm useful dimensionality reduction, classification, regression, collaborative filtering, feature learning, topic modeling.', 'restricted boltzmann machine undirected graphical model plays major role deep learning framework recent times.', 'task training minimize error reconstruction, i.e. find efficient compact representation input data.', 'deep autoencoder composed two, symmetrical deep-belief networks shallow layers representing encoding half net.', 'rbm shares similar idea, uses stochastic units particular distribution instead deterministic distribution.', 'autoencoder simple 3-layer neural network output units directly connected input units.']]\n",
      "\n",
      "[['map function executes function given argument elements iterable given second argument.', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x interview series #2 python programming numpy 1.', 'use numpy.bincount() >>> arr = numpy.array([0, 5, 5, 0, 2, 4, 3, 0, 0, 5, 4, 1, 9, 9]) >>> numpy.bincount(arr) argument bincount() consist booleans positive integers.', 'generate array ‘100’ random numbers sampled standard normal distribution numpy np.random.rand(100) create 100 random numbers generated standard normal distribution mean 0 standard deviation 1.', 'axis = 0 meant reading rows, axis = 1 meant reading columns', 'python numpy arrays considered instead list fast, consume memory convenient lots functionality.', \"python numpy supports nan definition nan system dependent systems don't round support like older cray vax computers.\", 'nan, short “not number”, special floating point value defined ieee-754 specification.']]\n",
      "\n",
      "[['>>> import numpy np >>> arr = np.array([11, 22, 33, 44 ,55 ,66, 77]) >>> perc = np.percentile(arr, 40) #returns 40th percentile >>> print(perc) 13.', 'numpy contain array data type basic operations indexing, sorting, reshaping, basic element wise functions, et cetera.', 'choose column 2 example >>> import numpy np >>> arr = np.array([[1, 2, 3], [4, 5, 6], [0,0,1]]) >>> arr[arr[,1].argsort()] # output >>> array([[0, 0, 1], [1, 2, 3], [4, 5, 6]]) 11.', \">>> = np.array([5, 4, 3, 2, 1]) >>> b = np.array([4, 8, 9, 10, 1]) # 'a' remove 'b' >>> np.setdiff1d(a,b) # output >>> array([5, 3, 2]) 10.\", 'numpy package library python, adding support large, multi-dimensional arrays matrices, large collection high level mathematical functions.', 'simple words, numpy optimized version python lists like financial functions, linear algebra, statistics, polynomials, sorting searching etc.', '>>> import numpy np >>> arr = np.array([9, 10, 1, 2, 0]) >>> reverse_arr = arr[-1] 12.']]\n",
      "\n",
      "[['given array a, condition arr > 3 returns boolean array false interpreted 0 python numpy.', '>>> import numpy np >>> arr = np.array([[9,8,7],[6,5,4],[3,2,1]]) >>> arr > 3 >>> array([[true, true, true], [ true, true, true], [false, false, false]], dtype=bool) 17.', 'write numpy program calculate difference maximum minimum values given array second axis.', '>>> import numpy np >>> = np.arange(4).reshape((2,2)) >>> max_val = np.amax(a) >>> min_val = np.amin(a) 18.', 'size attribute helpful determining length numpy array >>> arr = numpy.zeros((1,0)) >>> arr.size 15.', 'find median numpy flattened array >>> import numpy np >>> arr = np.arange(16).reshape((4, 5)) >>> res = np.median(arr)', '>>> import numpy np >>> arr = np.arange(16).reshape((4, 7)) >>> res = np.ptp(arr, 1) 19.']]\n",
      "\n",
      "[['develop numpy program compute histogram nums bins >>> import numpy np >>> nums = np.array([0.5, 0.7, 1.0, 1.2, 1.3, 2.1]) >>> bins = np.array([0, 1, 2, 3]) >>> np.histogram(nums, bins) 24.', 'write numpy program true division element-wise array inputs >>> import numpy np >>> x = np.arange(10) >>> np.true_divide(x, 3)', 'compute compute pearson product-moment correlation coefficients given numpy arrays >>> import numpy np >>> x = np.array([0, 1, 3]) >>> y = np.array([2, 4, 5]) >>> cross_corr = np.corrcoef(x, y) 23.', 'write numpy program compute mean, standard deviation, variance given array second axis import numpy np >>> import numpy np >>> x = np.arange(16) >>> mean = np.mean(x) >>> std = np.std(x) >>> var= np.var(x) 21.', 'calculate covariance matrix numpy arrays >>> import numpy np >>> x = np.array([2, 1, 0]) >>> y = np.array([2, 3, 3]) >>> cov_arr = np.cov(x, y) 22.', 'powers array values element-wise >>> import numpy np >>> x = np.arange(7) >>> np.power(x, 3) 25.']]\n",
      "\n",
      "[['returns new object new index produced equivalent current one, value copy false.']]\n",
      "\n",
      "[['follow steve nouri ai data science posts https//lnkd.in/gzu463x vectorization process running operations entire array.', '>>> import pandas pd >>> ds = pd.series([2, 4, 6, 8, 10])', 'mention different types data structures pandas pandas provide data structures, supported pandas library, series, dataframes.', 'time series pandas time series ordered sequence data basically represents quantity changes time.', \"write pandas program 5 rows given dataframe >>> import pandas pd >>> exam_data = {'name' ['anastasia', 'dima', 'katherine', 'james', 'emily', 'michael', 'matthew', 'laura', 'kevin', 'jonas'],} labels = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j'] >>> df = pd.dataframe(exam_data , index=labels) >>> df.iloc[5] 37.\", 'dataframe.to_numpy(self, dtype=none, copy=false) dtype parameter defines data type pass array copy ensures returned value view array.', '>>> import pandas pd >>> pd.series([2, 4, 6, 8, 10]) 38.', 'develop pandas program create display one-dimensional array- like object containing array data.', 'pandas number vectorized functions like aggregations, string functions optimized operate specifically series dataframes.', \"write python program convert panda module series python list it's type.\"]]\n",
      "\n",
      "[[\">>> import pandas pd >>> s1 = pd.series(['100', '200', 'python', '300.12', '400']) >>> s2 = pd.to_numeric(s1, errors='coerce') >>> s2 42.\", '>>> import pandas pd >>> ds1 = pd.series([2, 4, 6, 8, 10]) >>> ds2 = pd.series([1, 3, 5, 7, 9]) >>> sum = ds1 + ds2 >>> sub = ds1 - ds2 >>> mul = ds1 * ds2 >>> div = ds1 / ds2 40.', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x >>> type(ds) >>> ds.tolist() >>> type(ds.tolist()) 39.', '>>> import pandas pd >>> ds1 = pd.series([2, 4, 6, 8, 10]) >>> ds2 = pd.series([1, 3, 5, 7, 10]) >>> ds1 == ds2 >>> ds1 > ds2 >>> ds1 < ds2 41.', \"write pandas program convert series lists series >>> import pandas pd >>> s = pd.series([ ['red', 'black'], ['red', 'green', 'white'] , ['yellow']]) >>> s = s.apply(pd.series).stack().reset_index(drop=true) 43.\", 'write pandas program create subset given series based value condition >>> import pandas pd >>> s = pd.series([0, 1,2,3,4,5,6,7,8,9,10]) >>> n = 6']]\n",
      "\n",
      "[['write pandas program display frequent value given series replace “replaced” series.', '>>> import pandas pd >>> sr1 = pd.series([1, 2, 3, 4, 5]) >>> sr2 = pd.series([2, 4, 6, 8, 10]) >>> result = sr1[~sr1.isin(sr2)] >>> result 46.', \">>> import pandas pd >>> import numpy np >>> np.random.randomstate(100) >>> num_series = pd.series(np.random.randint(1, 5, [15])) >>> result = num_series[~num_series.isin(num_series.value_counts().index[1])] = 'replaced' 48.\", \"develop pandas code alter order index given series >>> import pandas pd >>> s = pd.series(data = [1,2,3,4,5], index = ['a', 'b', 'c','d','e']) >>> s.reindex(index = ['b','a','c','d','e']) 45.\", '>>> import pandas pd >>> import numpy np >>> num_series = pd.series(np.random.randint(1, 10, 9)) >>> result = np.argwhere(num_series % 5==0) 49.', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x >>> new_s = s[s < n] >>> new_s 44.']]\n",
      "\n",
      "[[\"follow steve nouri ai data science posts https//lnkd.in/gzu463x # importing pandas library >>> import pandas pd >>> info = {'one' pd.series([1, 2, 3, 4, 5], index=['a', 'b', 'c', 'd', 'e']), 'two' pd.series([1, 2, 3, 4, 5, 6], index=['a', 'b', 'c', 'd', 'e', 'f'])} >>> info = pd.dataframe(info) # add new column existing dataframe object >>> info['three']=pd.series([20,40,60],index=['a','b','c']) 50.\", 'xrange range exact terms functionality.the difference range returns python list object x range returns xrange object.', 'python exits, especially python modules having circular references objects objects referenced global namespaces']]\n",
      "\n",
      "[['python lays concept prefixing variable, function method single double underscore imitate behavior protected private access specifiers.', 'doesn’t allow copies provides good functions perform set operations like union, difference etc.', 'kwargs don’t know keyword arguments passed function, pass values dictionary keyword arguments.', 'generator function normal function contains yield expression function definition making generator function.', 'use *args aren’t sure arguments going passed function, want pass stored list tuple arguments function. **', 'shallow copy new instance type gets created keeps values copied new instance.']]\n",
      "\n",
      "[['apparently contributing python open-source community requires follow style guidelines sincerely strictly.', 'slicing python mechanism select range items sequence types like strings, list, tuple, etc.', 'decorators python essentially functions add functionality existing function python changing structure function itself.', 'pythonpath environment variable set add additional directories python look modules packages.', 'pep official design document providing information python community, describing new feature python processes.', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x arguments pass value python supports pass reference.']]\n",
      "\n",
      "[['access module written python c following method, module = =pyimport_importmodule(\"<modulename>\"); 73.', 'provides environment, document code, run it, look outcome, visualize data results leaving environment.', 'converts source code written programmer intermediate language, translated machine language executed.', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x .py files contain source code program.', 'pickle module accepts python object converts string representation dumps file dump function, process called pickling.']]\n",
      "\n",
      "[['ipywidgets package provides common user interface controls exploring code data interactively.', 'third-party extensions magic commands exist, example, %%cython magic allows write cython code directly notebook.']]\n",
      "\n",
      "[[\">>> data = 'this string want pass different notebook' >>> %store data # stored 'data' (str) # new notebook >>> %store -r data >>> print(data) 95.\", 'export contents cell/show contents external script %%writefile magic saves contents cell external file. %']]\n",
      "\n",
      "[]\n",
      "\n",
      "[['follow steve nouri ai data science posts https//lnkd.in/gzu463x references [1] https//www.edureka.co [2] https//www.kausalvikash.in [3] https//www.wisdomjobs.com [4] https//blog.edugrad.com [5]https//stackoverflow.com [6]http//www.ezdev.org [7]https//www.techbeamers.com [8]https//www.w3resource.com [9]https//www.javatpoint.com [10]https//analyticsindiamag.com [11]https//www.onlineinterviews.com [12]https//www.geeksforgeeks.org [13]https//www.springpeople.com [14]https//atraininghub.com [15]https//www.interviewcake.com [16]https//www.techbeamers.com [17]https//www.tutorialspoint.com [18]https//programmingwithmosh.com [19]https//www.interviewbit.com [20]https//www.guru99.com [21]https//hub.packtpub.com [22]https//analyticsindiamag.com [23]https//www.dataquest.io [24]https//www.infoworld.com']]\n",
      "\n",
      "[['machine learning, statistical model describes random error noise instead underlying relationship ‘overﬁtting’ occurs.', 'technique, model usually given dataset known data training (training data set) run dataset unknown data model tested.', 'machine learning branch computer science deals system programming order automatically learn improve experience.', 'lot data overﬁtting avoided, overﬁtting happens relatively small dataset, try learn it.', 'model excessively complex, overﬁtting normally observed, having parameters respect number training data types.', 'machine learning relates study, design development algorithms computers capability learn explicitly programmed.', 'method dataset splits section, testing training datasets, testing dataset test model while, training dataset, datapoints come model.', 'inductive machine learning involves process learning examples, system, set observed instances tries induce general rule.', 'https//career.guru99.com/ 50 machine learning interview s & answers 1) machine learning?', 'while, data mining deﬁned process unstructured data tries extract knowledge unknown interesting patterns.']]\n",
      "\n",
      "[['a) model building b) model testing c) applying model 10) standard approach supervised learning?', 'areas information science like machine learning, set data discover potentially predictive relationship known ‘training set’.', 'a) decision trees b) neural networks (back propagation) c) probabilistic networks d) nearest neighbor e) support vector machines 8) diﬀerent algorithm techniques machine learning?', 'diﬀerent approaches machine learning a) concept vs classiﬁcation learning b) symbolic vs statistical learning', 'training set examples given learner, test set test accuracy hypotheses generated learner, set example held learner.', 'diﬀerent types techniques machine learning a) supervised learning b) unsupervised learning c) semi-supervised learning d) reinforcement learning e) transduction f) learning learn 9) stages build hypotheses model machine learning?']]\n",
      "\n",
      "[['classiﬁer machine learning system inputs vector discrete continuous feature values outputs single discrete value, class.', 'a) classiﬁcations b) speech recognition c) regression d) predict time series e) annotate strings 16) algorithm independent machine learning?', 'naïve bayes classiﬁer converge quicker discriminative models like logistic regression, need training data.', 'machine learning mathematical foundations independent particular classiﬁer learning algorithm referred algorithm independent machine learning?', 'artiﬁcial intelligence addition machine learning, covers aspects like knowledge representation, natural language processing, planning, robotics etc.', 'designing developing algorithms according behaviours based empirical data known machine learning.', 'a) artiﬁcial intelligence b) rule based inference 14) explain function ‘unsupervised learning’?', 'a) find clusters data b) find low-dimensional representations data c) find interesting directions data d) interesting coordinates correlations e) find novel observations/ database cleaning 15) explain function ‘supervised learning’?']]\n",
      "\n",
      "[['methods predicting good probabilities supervised learning a) platt calibration b) isotonic regression methods designed binary classiﬁcation, trivial.', 'diﬀerence heuristics decision trees evaluate average quality number disjointed sets rule learners evaluate quality set instances covered candidate rule.', 'a) computer vision b) speech recognition c) data mining d) statistics e) informal retrieval f) bio-informatics 21) genetic programming?', 'inductive logic programming (ilp) subﬁeld machine learning uses logical programming representing background knowledge examples.', 'process selecting models diﬀerent mathematical models, describe data set known model selection.']]\n",
      "\n",
      "[['paradigms ensemble methods a) sequential ensemble methods b) parallel ensemble methods 36) general principle ensemble method bagging boosting ensemble method?', 'a) combining binary classiﬁers b) modifying binary incorporate multiclass learning 32) ensemble learning?', 'ﬁrst component logical ; consists set bayesian clauses, captures qualitative structure domain.', 'general principle ensemble method combine predictions models built given learning algorithm order improve robustness single model.', 'solve particular computational program, multiple models classiﬁers experts strategically generated combined.', 'instance based learning algorithm referred lazy learning algorithm delay induction generalization process classiﬁcation performed.']]\n",
      "\n",
      "[['important components relational evaluation techniques a) data acquisition b) ground truth acquisition c) cross validation technique d) query type e) scoring metric f) signiﬁcance test 43) diﬀerent methods sequential supervised learning?', 'pca (principal components analysis), kpca ( kernel based principal component analysis) ica ( independent component analysis) important feature extraction techniques dimensionality reduction.', 'machine learning statistics, dimension reduction process reducing number random variables considerations divided feature selection feature extraction 41) support vector machines?', 'incremental learning method ability algorithm learn new data available classiﬁer generated available dataset.', 'bias term measures closely average classiﬁer produced learning algorithm matches target function.', 'diﬀerent methods solve sequential supervised learning problems a) sliding-window methods b) recurrent sliding windows c) hidden markow models d) maximum entropy markow models e) conditional random ﬁelds']]\n",
      "\n",
      "[['areas robotics information processing sequential prediction problem arises a) imitation learning b) structured prediction c) model based reinforcement learning 45) batch statistical learning?', 'pac (probably approximately correct) learning learning framework introduced analyze learning algorithms statistical eﬃciency.', 'f) graph transformer networks 44) areas robotics information processing sequential prediction problem arises?', 'statistical learning techniques allow learning function predictor set observed data predictions unseen future data.', 'techniques machine learning a) genetic programming b) inductive learning 50) popular application machine learning day day basis?', 'recommendation engine implemented major ecommerce websites uses machine learning guru99 provides free online tutorial courses like', 'a) sequence prediction b) sequence generation c) sequence recognition d) sequential decision 48) sequence learning?', 'techniques provide guarantees performance learned predictor future unseen data based statistical assumption data generating process.']]\n",
      "\n",
      "[['java mis mongodb bigdata cassandra web services sqlite jsp informatica accounting sap training python excel asp net hbase project management test management business analyst ethical hacking pmp live project soapui photoshop manual testing mobile testing data warehouse r tutorial tableau devops aws jenkins agile testing rpa junit software engineering selenium ccna angularjs nodejs plsql']]\n",
      "\n",
      "[['ii year – sem (2021-2022) department computer science engineering malla reddy college engineering & technology (autonomous institution – ugc, govt.', 'india) (affiliated jntuh, hyderabad, approved aicte - accredited nba & naac – ‘a’ grade - iso 90012015 certified) maisammaguda, dhulapally (post via.']]\n",
      "\n",
      "[['2 malla reddy college engineering & technology department computer science engineering syllabus ii year m. tech.', 'unit - ii artificial neural networks -introduction, neural network representation, appropriate problems neural network learning, perceptions, multilayer networks propagation algorithm.', 'dimensionality reduction feature selection, principal component analysis, linear discriminate analysis, factor analysis, independent component analysis, multidimensional scaling, manifold learning.', 'unit - v genetic algorithms different search methods induction - explanation-based learning prior knowledge reduce sample complexity.', 'unit -iv pattern comparison techniques-temporal patterns, dynamic time warping methods,clustering, introduction clustering, k-means clustering, k-mode clustering.', 'cse – sem l/t/p/ c 3 / - / - 3 (r20d5803) machine learning objectives 1.', 'course explains machine learning techniques decision tree learning, bayesian learning etc.', 'decision tree learning-introduction, decision tree representation, appropriate problems decision tree learning, basic decision tree learning algorithm hypothesis space search decision tree learning, inductive bias decision tree learning, issues decision tree learning.', 'instance-based learning-introduction, k-nearest neighbor learning, locally weighted regression, radial basis functions, case-based reasoning, remarks lazy eager learning.', 'discussion propagation algorithm, illustrative example face recognition unit - iii bayesian learning-introduction, byes theorem, bayes theorem concept learning maximum likelihood squared error hypotheses, maximum likelihood hypotheses predicting probabilities, minimum description length principle, bayes optimal classifier, gibs algorithm, naïve bayes classifier, example learning classify text, bayesian belief networks, em algorithm.'], ['unit - introduction well-posed learning problems, designing learning system perspectives issues machine learning concept learning general specific ordering introduction,a concept learning task, concept learning search, find-s finding maximally specific hypothesis, version spaces candidate elimination algorithm, remarks version spaces candidate elimination, inductive bias.']]\n",
      "\n",
      "[['mehryar mohri, afshin rostamizadeh, ameet talwalkar ” foundations machine learning”,mit press,2012 references 1.', 'fundamentals speech recognition lawrence rabiner biing – hwang juang .ethem alpaydin, ”introduction machine learning”, mit press, prentice hall india, 3 rd edition2014.']]\n",
      "\n",
      "[['25 s. unit topic page 1 ii artificial neural networks -introduction, neural network representation 26 2 ii appropriate problems neural network learning 28 3 ii perceptions, multilayer networks & propagation algorithm.', '29 4 ii discussion propagation algorithm 34 malla reddy college engineering & technology department computer science engineering', '4 index s. unit topic page 1 introduction well-posed learning problems 1 2 concept learning task, concept learning search 6 3 find-s finding maximally specific hypothesis 15 4 version spaces candidate elimination algorithm 17 5 remarks version spaces candidate elimination, inductive bias 21 6 decision tree learning-introduction, decision tree representation 22 7 appropriate problems decision tree learning 23 8 decision tree learning algorithm, issues decision tree learning.']]\n",
      "\n",
      "[['57 malla reddy college engineering & technology department computer science engineering', 'instance-based learning-introduction 51 7 iii k-nearest neighbor learning, locally weighted regression 55 8 iii radial basis functions, case-based reasoning 56 9 iii remarks lazy eager learning.', '5 s. unit topic page 1 iii bayesian learning-introduction ,bayes theorem & concept learning maximum 36 2 iii maximum likelihood hypotheses predicting probabilities(map) 42 3 iii gibs algorithm, naïve bayes classifier 46 4 iii minimum description length principle , bayes optimal classifier 47 5 iii example learning classify text, bayesian belief networks 50 6 iii em algorithm.']]\n",
      "\n",
      "[['76 s. unit topic page 1 v genetic algorithms different search methods induction 78 2 v explanation-based learning prior knowledge reduce sample complexity.', '79 3 v dimensionality reduction 82 4 v principal component analysis 84 5 v linear discriminate analysis, factor analysis, 85 6 v independent component analysis multidimensional scaling, manifold learning.', '6 s. unit topic page 1 iv pattern comparison techniques-temporal patterns, 58 2 iv dynamic time warping methods 61 3 iv clustering 67 5 iv k-means clustering 69 6 iv k-mode clustering.', '86 malla reddy college engineering & technology department computer science engineering']]\n",
      "\n",
      "[['scientists discovered good idea reward machine job expected way came reinforcement learning. •', 'machine learning broadly categorized following headings machine learning evolved left right shown diagram. •', 'department cse mrcet 1 unit-i machine learning field study gives computers capability learn explicitly programmed.', 'soon, data available days humongous conventional techniques developed far failed analyse big data provide predictions.']]\n",
      "\n",
      "[['given feature value x1 output y1, x2 y2, x3 y3, on.', 'technique advanced giving incentives deep learning networks awards finally comes deep reinforcement learning.', 'hold child’s hand, foot forward, walk demonstration on, child learns walk own.', 'satisfied machine able predictions desired level accuracy (say 80 90%) stop training machine.', 'machine learns high computing power huge memory resources available today. •', 'based data, let computer figure empirical relationship x y. machine trained way sufficient number data points, ask machine predict y given x. assuming know real value y given x, able deduce machine’s prediction correct.', 'department cse mrcet 2 • thus, came deep learning human brain simulated artificial neural networks (ann) created binary computers. •', 'now, safely use machine predictions unknown data points, ask machine predict y given x know real value y. training comes regression talked earlier.', 'let study categories details supervised learning supervised learning analogous training child walk.']]\n",
      "\n",
      "[['specifically, ask s given huge data set x, “what best groups x?” “', 'arrive answers s, understand number data points machine require deduce strategy large.', 'following principles regression training, train machine classify student based feature – height.', 'however, case unsupervised learning, number data points reasonably accepted learning starts millions.', 'example, set 100 students say, like group groups based heights - short, medium long.', 'department cse mrcet 3 classification use machine learning techniques classification problems.', 'again, use test data verify machine learned technique classification putting developed model production.', 'unsupervised learning unsupervised learning, specify target variable machine, ask machine “what tell x?”.', 'following figure shows boundary yellow red dots determined unsupervised machine learning.', 'however, data continuously flowing social area network, cases data curation impossible task.']]\n",
      "\n",
      "[['department cse mrcet 4 able determine class black dots fairly good accuracy.', 'reinforcement learning consider training pet dog, train pet bring ball us.', 'slowly, dog learns job rightly gives reward dog starts job right way time future.', 'slowly, machine start differentiating right wrong moves iterations learn solve game puzzle better accuracy.']]\n",
      "\n",
      "[['networks successfully applied solving problems computer vision, speech recognition, natural language processing, bioinformatics, drug design, medical image analysis, games.', 'reinforcement learning algorithms like q learning combined deep learning create powerful drl model.', 'deep reinforcement learning deep reinforcement learning (drl) combines techniques deep reinforcement learning.', 'architectures deep learning deep neural networks, deep belief networks, recurrent neural networks, convolutional neural networks.', 'department cse mrcet 5 deep learning deep learning model based artificial neural networks (ann), specifically convolutional neural networks (cnn)s.', 'deep learning requires huge processing power humongous data, generally easily available days.']]\n",
      "\n",
      "[['handwriting recognition problem • task – acknowledging handwritten words portrayal • performance measure – percent words accurately classified • experience – directory handwritten words given classifications 4.', 'better filter emails spam • task – classifying emails spam • performance measure – fraction emails accurately classified spam spam • experience – observing label emails spam spam 2.', 'robot driving problem • task – driving public four-lane highways sight scanners • performance measure – average distance progressed fallacy • experience – order images steering instructions noted observing human driver 5.', 'posed learning problems computer program said learn experience e context task t performance measure p, performance t, measured p, upgrades experience e. problem segregated well-posed learning problem traits – • task • performance measure • experience certain example efficiently defines well-posed learning problems are 1.', 'checkers learning problem • task – playing checkers game • performance measure – percent games won opposer • experience – playing implementation games 3.', 'department cse mrcet 6 got brief introduction machine learning models, let explore slightly deeper algorithms available models.']]\n",
      "\n",
      "[['automatic translation documents • task – translating type language document language • performance measure – able convert language efficiently • experience – training machine large dataset different types languages design learning system looked learning process understood goal learning.', 'final design look game - checkers learning problem apply design choices.', 'face recognition problem • task – predicting different types faces • performance measure – able predict maximum types faces • experience – training machine maximum datasets different face images 7.', 'want design learning system follows learning process, need consider design choices.', 'checkers learning problem, elements be, • task t play checkers • performance measure p total present game won tournament. •', \"type training experience design checker's learning system, type training experience available learning system significant effect success failure learning.\", 'department cse mrcet 7 • task – forecasting different fruits recognition • performance measure – able predict maximum variety fruits • experience – training machine largest datasets fruits images 6.']]\n",
      "\n",
      "[['training experience good \\uf0a2 training examples represent distribution examples final system performance measured?', '\\uf0a2 semi-supervised learner generates game states asks teacher help finding correct board state confusing.', 'department cse mrcet 8 direct indirect training experience case direct training experience, individual board states correct board state given.', 'choosing target function playing checkers game, moment time, decision choosing best different possibilities.', 'teacher not \\uf0a2 supervised training experience labelled, means, board states labelled correct move.', 'case indirect training experience, sequences game final result (win, lose draw) given number games.']]\n",
      "\n",
      "[['function assigns higher scores better board states system successfully learn target function v, easily use select best board position.', 'direct experience checkers learning system, needs learn choose best large search space.', 'let define target value v(b) arbitrary board state b b, follows', 'function v b →r indicating accepts input board set legal board states b produces output real score.', 'let function choose use notation choose move b →m indicate function accepts input board set legal board states b produces output set legal moves m. • indirect experience difficult learn function.']]\n",
      "\n",
      "[['hand, wish pick expressive representation allow representing close approximation possible ideal target function v. hand, expressive representation, training data program require order choose alternative hypotheses represent.', 'b final state game, v (b) = v (b’), b’ best final board state achieved starting b playing optimally end game. (', 'discussion brief, let choose simple representation given board state, function ^v calculated linear combination following board features • x1(b) — number black pieces board b • x2(b) — number red pieces b • x3(b) — number black kings b', 'choosing representation target function specified ideal target function v, choose representation learning program use describe function ^v learn.', '4) recursive definition determine value v(b) particular board state, performs search ahead optimal line play, way end game.', 'allow represent collection rules match features board state, quadratic polynomial function predefined board features, artificial neural network.', 'could, example, allow program represent large table distinct entry specifying value distinct board state.']]\n",
      "\n",
      "[['department cse mrcet 11 • x4(b) — number red kings b • x5(b) — number red pieces threatened black • x6(b) — number black pieces threatened red ^v = w0 + w1 · x1(b) + w2 · x2(b) + w3 · x3(b) + w4 · x4(b) +w5 · x5(b) + w6 · x6(b) w0 w6 numerical coefficients weights obtained learning algorithm.', 'specification machine learning problem time till worked choosing type training experience, choosing target function representation.', 'task t play checkers • performance measure % games won world tournament • training experience e opportunity play • target function v board → r • target function representation ^v = w0 + w1 · x1(b) + w2 · x2(b) + w3 · x3(b) + w4 · x4(b) +w5 · x5(b) + w6 · x6(b) items correspond specification learning task, final items constitute design choices implementation learning program.', 'choosing approximation algorithm target function generating training data — train learning program, need set training data, describing specific board state b training value v_train (b) b. training example ordered pair <b,v_train(b)>.']]\n",
      "\n",
      "[['experiment generator takes current hypothesis (currently learned function) input outputs new problem (an initial board state) performance system explore.', 'department cse mrcet 12 temporal difference (td) learning concept central reinforcement learning, learning happens iterative correction estimated returns accurate target return.', '\\uf056 v_train(b) ← ^v(successor(b)) final design checkers learning system final design checkers learning system naturally described distinct program modules represent central components learning systems.', 'critic takes trace game input outputs set training examples target function.', 'performance system takes new board input outputs trace game played itself.']]\n",
      "\n",
      "[[\"general bounds found relate confidence learned hypotheses training experience character learner's hypothesis space? •\", 'concept learning • inducing general functions specific training examples main issue machine learning. •', 'hypothesis space general-to-specific ordering hypotheses, search efficiently organized taking advantage naturally occurring structure hypothesis space.', 'concept learning problem searching predefined space potential hypotheses hypothesis best fits training examples. •', 'department cse mrcet 13 checkers example raises number generic s machine learning.', 'concept learning acquiring definition general category given sample positive negative training examples category. •', 'field machine learning, book, concerned answering s following • algorithms exist learning general target functions specific training examples?', 'best strategy choosing useful training experience, choice strategy alter complexity learning problem? •']]\n",
      "\n",
      "[['department cse mrcet 14 inferring boolean-valued function training examples input output. •', 'selecting hypothesis representation, designer learning algorithm implicitly defines space hypotheses program represent learn.', 'task learn predict value enjoy sport arbitrary day, based values attribute values.', 'concept learning search • concept learning viewed task searching large space hypotheses implicitly defined hypothesis representation. •', 'example concept-learning learning bird-concept given examples birds (positive examples) non-birds (negative examples). •', 'concept learning task enjoy sport training examples set example days, described attributes.']]\n",
      "\n",
      "[['find-s algorithm finds specific hypothesis h consistent positive training examples. –', 'department cse mrcet 15 find-s • find-s algorithm starts specific hypothesis generalize considering positive examples. •', 'find-s algorithm ignores negative example long hypothesis space contains hypothesis describes true target concept, training data contains errors, ignoring negative examples cause problem. •', 'positive training instance x attribute constraint a, h constraint a, satisfied x 3.', 'specific hypothesis represented by {ϕ, ϕ, ϕ, ϕ, ϕ, ϕ} steps involved find-s 1.', 'final hypothesis consistent negative examples correct target concept h, training examples correct.']]\n",
      "\n",
      "[['example positive find initial hypothesis specific update current hypothesis general condition.', 'hence, hypothesis be h = {ϕ, ϕ, ϕ, ϕ, ϕ, ϕ} consider example 1 data example 1 {green, hard, no, wrinkled}.', 'hence, hypothesis becomes h = {green, hard, no, wrinkled} consider example 2']]\n",
      "\n",
      "[['reached point attributes hypothesis general condition, example 6 example 7 result hypothesizes general attributes.', 'hence, given data final hypothesis be final hypothesis h = { ?, ?, ?, ? }.', 'h = {green, hard, no, wrinkled} consider example 3 example negative outcome.', 'hard, no, wrinkled } consider example 5 data present example 5 {green, soft, yes, smooth}.', 'h = {green, hard, no, wrinkled} consider example 4 data present example 4 {orange, hard, no, wrinkled}.', 'compare single attribute initial data mismatch found replace particular attribute general case (“ ?”).', 'compare single attribute initial data mismatch found replace particular attribute general case ( “?” ).']]\n",
      "\n",
      "[['initialize g set maximally general hypotheses h initialize s set maximally specific hypotheses h training example d, • d positive example • remove g hypothesis inconsistent d • hypothesis s s consistent d • remove s s • add s minimal generalizations h s h consistent d, member g general h • remove s hypothesis general hypothesis s • d negative example • remove s hypothesis inconsistent d • hypothesis g g consistent d • remove g g 18\\\\ • add g minimal specializations h g • h consistent d, member s specific h • remove g hypothesis general hypothesis g. candidate- elimintion algorithm version spaces illustrative example', 'department cse mrcet 18 candidate-elimintion algorithm computes version space containing hypotheses h consistent observed sequence training examples.']]\n",
      "\n",
      "[['second training example observed, similar effect generalizing s s2, leaving g unchanged i.e., g2 = g1 =g0', 'update g boundary needed response training example correctly covers example. •', 'department cse mrcet 19 candidate-elimintion algorithm begins initializing version space set hypotheses h; boundary set contain general hypothesis h, g0 ?, ?, ?, ?, ?,', 'training example presented, candidateelimintion algorithm checks s boundary finds overly specific fails cover positive example. •']]\n",
      "\n",
      "[['minimal specialization g2 correctly labels new example negative example, included g3.', 'negative example reveals boundary version space overly general, is, hypothesis g incorrectly predicts new example positive example. •']]\n",
      "\n",
      "[['department cse mrcet 21 • positive example generalizes s boundary version space.', 'results removing member g boundary, member fails cover new positive example processing examples, boundary sets s4 g4 delimit version space hypotheses consistent set incrementally observed training examples. •', 'processing examples, boundary sets s4 g4 delimit version space hypotheses consistent set incrementally observed training examples.']]\n",
      "\n",
      "[['classification trees (yes/no types) seen example classification tree, outcome variable like ‘fit’ ‘unfit’.', 'let’s want predict person fit given information like age, eating habit, physical activity, etc.', 'department cse mrcet decision tree decision trees type supervised machine learning (that explain input corresponding output training data) th e data continuously split according certain parameter.']]\n",
      "\n",
      "[['appropriate problems decision tree learning • instances represented attribute-value pair • target function discrete output values • disjunctive descriptions required • training data contain errors • training data contain missing attribute values. •', 'capability • hypothesis space decision trees complete space finite discrete valued functions. •', 'entropy, called shannon entropy denoted h(s) finite set s, measure uncertainty randomness data.', 'department cse mrcet 23 decision outcome variable continuous, e.g. number like 123.', 'hypothesis space search set possible decision tree, simple complex, hill climbing search.']]\n",
      "\n",
      "[['department cse mrcet 24 • id3 uses training example step statistically based decisions refine current hypothesis. •', 'inductive bias decision tree learning note h power set instances x • inductive bias id3 – approximate inductive bias id3 \\uf0a2 shorter trees preferred larger tress \\uf0a2 bfs-id3 difference (id3 & c-e) && restriction bias preference bias id3 candidate-elimination searches complete hypothesis space incompletely searches incomplete hypothesis space completely inductive bias solely consequence ordering hypotheses search strategy inductive bias solely consequence expressive power hypothesis representation sss restriction bias preference bias candidate-elimination id3 categorical restriction set hypotheses considered preference certain hypotheses']]\n",
      "\n",
      "[['department cse mrcet 25 possibility excluding unknown target function work complete hypothesis space issues decision tree learning • determine deeply grow decision tree • handling continuous attributes • choosing appropriate attribute selection measure • handling training data missing attribute values • handling attributes differing costs • improving computational efficiency']]\n",
      "\n",
      "[['ann applications classification, aim predict class input vector • pattern matching, aim produce pattern best associated given input vector. •', 'control, appropriate action suggested based given input vectors • function approximation/times series modelling, aim learn functional relationships input desired output vectors. •', 'ann architectures • neural networks known universal function approximators • architectures available approximate nonlinear function • different architectures allow generation functions different complexity power \\uf0a2 feed forward networks \\uf0a2 feedback networks \\uf0a2 lateral networks', 'department cse mrcet 26 unit-ii artificial neural networks introduction artificial neural networks (ann) algorithms based brain function model complicated patterns forecast issues.', 'artificial neural network (ann) deep learning method arose concept human brain biological neural networks.']]\n",
      "\n",
      "[['output anns discrete-valued, real-valued, vector multiple real discrete-valued characteristics, target function discrete-valued, real-valued, vector numerous real discrete- valued attributes.', 'number training instances evaluated, settings different learning algorithm parameters contribute extended training periods anns.', 'department cse mrcet 27 advantages artificial neural networks attribute-value pairs represent problems ann.', 'hardware dependence • construction artificial neural networks necessitates use parallel processors. •']]\n",
      "\n",
      "[['1943 mcculloch pitts proposed model neuron perceptron (read [mitchell, section 4.4]) 2.', 'department cse mrcet 28 • precise rule determine structure artificial neural networks. •', '1960s widrow hoff explored perceptron networks (which called “adelines”) delta rule.', 'network’s lifetime unknown • network’s error sample decreased specific amount, training complete. •', 'difficulty presenting issue network • anns capable working numerical data. •']]\n",
      "\n",
      "[['1986 invention backpropagation rumelhart mcclelland, parker earlier on werbos learn nonlinearly-separable data sets.', '1969 minsky papert showed perceptron deal nonlinearly-separable data sets---even represent simple function x-or.', 'multilayer neural network • multiplayer perceptron feed forward neural network hidden layers • network consists input layer source neurons, hidden layer computational neurons, output layer computational neurons. •']]\n",
      "\n",
      "[]\n",
      "\n",
      "[]\n",
      "\n",
      "[['algorithm composed parts repeated pre-set maximal number epochs, ep max. •', 'ii, propagation pass weights network updated- starting hidden output weights followed input hidden weights--with respect sum squares error series weight update rules called delta rule.', 'definition propagation algorithm neural network computes gradient loss function single weight chain rule.', 'i, feed forward pass activation values hidden output units computed. •', 'department cse mrcet 32 propagation overview • propagation works applying gradient descent rule feed forward network. •']]\n",
      "\n",
      "[['department cse mrcet 33 • inputs x, arrive preconnected path • input modelled real weights w. weights usually randomly selected. •', 'calculate error outputs errorb= actual output – desired output • travel output layer hidden layer adjust weights error decreased. •', 'prominent advantages propagation are • propagation fast, simple easy program • parameters tune apart numbers input • flexible method require prior knowledge network • standard method generally works • need special mention features function learned.']]\n",
      "\n",
      "[['propagation algorithm data mining sensitive noisy data • need use matrix-based approach propagation instead mini-batch.', 'propagation algorithm • initialize weights small random values; create random pool training patterns; set ep, number epochs training 0. •', 'department cse mrcet 34 types propagation networks types propagation networks are • static back-propagation • recurrent propagation static back-propagation kind propagation network produces mapping static input static output.', 'disadvantages propagation • actual performance propagation specific problem dependent input data. •', 'recurrent propagation recurrent propagation data mining fed forward fixed value achieved.', 'hidden layer propagating error • update connections • w newji = wjiold + wji w newkj = wkjold + wkj j']]\n",
      "\n",
      "[['new delta rule wpq(t+1) = - e/ wpq + wpq(t) • p q input hidden, or, hidden output units; t time step epoch; momentum parameter regulates inertia weights.', 'propagation momentum • point, propagation disadvantage slow small oscillate widely large. •', 'solve problem, add momentum connection inertia, forcing change direction downhill “force”. •', 'training patterns pool used, set ep = ep+1, ep epmax, create random pool patterns step 2.']]\n",
      "\n",
      "[['case 1 observed, certain coin fair coin, decide probability observing heads 0.50.5 confidence.', 'adjust belief accordingly value hh observed, decide probability observing heads recent observations.', 'thinking model uses recent observations beliefs inclination critical thinking known bayesian thinking.', 'neglect prior beliefs new data, decide probability observing heads h/10h/10 solely depending recent observations.', 'observations experiment fall following cases • case 1 observing 55 heads 55 tails. •', 'department cse mrcet 36 unit - iii introduction bayesian learning imagine situation friend gives new coin asks fairness coin (or probability observing heads) flipping coin once.']]\n",
      "\n",
      "[['department cse mrcet 37 moreover, assume friend allows conduct 1010 coin flips.', 'consequently, quantity pp deviates 0.50.5 indicates biased coin is, pp considered degree-of-fairness coin.', 'bayesian learning comes play occasions, unable use frequentist statistics drawbacks discussed above.', 'famous coin flip experiment flip coin, possible outcomes - heads tails.', 'course, rare possibility coin balances edge falling side, assume possible outcome coin flip discussion.', 'conduct series coin flips record observations i.e. number heads (or tails) observed certain number coin flips.', 'observed heads tails equal frequencies probability observing heads (or tails) 0.50.5, established coin fair coin.', 'conducted sufficient number coin flip trials, determine frequency probability observing heads (or tails).', 'use bayesian learning address drawbacks additional capabilities (such incremental updates posterior) testing hypothesis estimate unknown parameters machine learning models.', 'bayesian learning uses bayes’ theorem determine conditional probability hypotheses given evidence observations.']]\n",
      "\n",
      "[['therefore, pp 0.60.6 (note pp number heads observed number total coin flips).', 'department cse mrcet 38 testing hypothesis true false calculating probability event prolonged experiment known frequentist statistics.', 'hence, according frequencies statistics, coin biased coin — opposes assumption fair coin.', 'such, determining fairness coin probability observing heads example frequentist statistics (a.k.a.', 'new value pp change previous conclusion (i.e. coin biased), observation raises s • confident pp 0.60.6? •']]\n",
      "\n",
      "[['assume true value pp closer 0.550.55 0.60.6 computed observations considerable number trials compared compute latter.', '39 department cse mrcet pp continue change increase number coin flip trails?', 'determine confidence estimated pp value inferred conclusion, situation number trials limited, allow', 'yet, practical conduct experiment infinite number trials stop experiment sufficiently large number trials.', 'confidence estimated pp increase increasing number coin-flips, frequentist statistic facilitate indication confidence estimated pp value.', 'however, increase number trials, different probability values observing heads eventually, discover coin fair coin.', 'attempt understand importance confident measure studying following cases • experiment infinite number trials guarantees pp absolute accuracy (100% confidence).', 'number coin number heads probability observing heads flips 10 6 0.6 50 29 0.58 100 55 0.55 200 94 0.47 500 245 0.49 table 1 - coin flip experiment results increasing number trials table 1 presents possible outcomes hypothetical coin flip experiment increasing number trials.']]\n",
      "\n",
      "[['beliefs play significant role shaping outcome hypothesis test especially limited data.', 'however, frequentist statistics, possible incorporate beliefs past experience increase accuracy hypothesis test.', 'conditional probability - measure probability p(a|b)p(a|b) event given event b occurred. •', 'continuous probability distributions described probability density functions discrete probability distributions represented probability mass functions.', 'joint probability distribution bayes’ theorem bayes’ theorem describes conditional probability event hypothesis computed evidence prior knowledge.', 'department cse mrcet 40 decide accept conclusion extend experiment trials achieves sufficient confidence.', 'moreover, valuable insights prior beliefs (for example, coins usually fair coin biased intentionally, p≈0.5p≈0.5) describes value pp.', 'random variable (stochastic variable) - statistics, random variable variable possible values result random event.', 'provide lengthy explanations mathematical definition lot widely available content use understand concepts. •', 'therefore, possible value random variable probability attached represent likelihood values. •']]\n",
      "\n",
      "[['department cse mrcet 41 test cases, including prior belief rarely observed bugs code.', 'let assume unlikely find bugs code rarely observed bugs code past.', 'however, time applying bayes’ theorem, decide priors means (otherwise use previous posterior new prior).', 'however, intuition goes simple hypothesis test multiple events hypotheses involved (let worry moment).', 'prior represents beliefs gained past experience, refers common sense outcome bayes’ theorem past observations.', 'however, now, let assume p(θ)=pp(θ) term depends test coverage test cases.', 'know value term proper measurements, order continue discussion let assume p(x|¬θ)=0.5p(x|¬θ)=0.5.', 'p(θ)p(θ) - prior probability probability hypothesis θθ true applying bayes’ theorem.', 'past experience observing fewer bugs code, assign prior p(θ)p(θ) higher probability.', 'accordingly, p(x)=1×p+0.5×(1−p)=0.5(1+p)p(x)=1×p+0.5×(1−p)=0.5(1+p) • p(θ|x)p(θ|x) - posteriori probability denotes conditional probability hypothesis θθ observing evidence xx.'], ['θθ xx denote code bug free passes test cases respectively. •']]\n",
      "\n",
      "[['therefore, express hypothesis θmapθmap concluded map follows θmap=argmaxθp(θi|x)=argmaxθ(p(x|θi)p(θi)p(x))θmap=argmaxθp(θi|x) =argmaxθ(p(x|θ i)p(θi)p(x)) argmaxθargmaxθ operator estimates event hypothesis θiθi maximizes posterior probability p(θi|x)p(θi|x).', 'p(¬θ|x)=p(x|¬θ).p(¬θ)p(x)=0.5×(1−p)0.5×(1+p)=(1−p)(1+p)p(¬θ|x)=p(x|¬ θ).p(¬θ) p(x)=0.5×(1−p)0.5×(1+p)=(1−p)(1+p) know conditional probabilities observing bug code observing bug code.', 'department cse mrcet 42 know values terms bayes’ theorem, calculate posterior probability following formula p(θ|x)=1×p0.5(1+p)p(θ|x)=1×p0.5(1+p) calculate probability observing bug, given code passes test cases p(¬θ|x)p(¬θ|x) .']]\n",
      "\n",
      "[['department cse mrcet 43 figure 1 - p(θ|x)p(θ|x) p(¬θ|x)p(¬θ|x) changing p(θ)=pp(θ)=p figure 1 illustrates posterior probabilities possible hypotheses change value prior probability.', 'unlike frequentist statistics belief past experience influence concluded hypothesis, bayesian learning capable incorporating belief improve accuracy predictions.', 'assuming fairly good programmers probability observing bug p(θ)=0.4p(θ)=0.4 , find θmapθmap map=argmaxθ{θp(|x)=0.40.5(1+0.4),¬θp(¬θ|x)=0.5(1−0.4)0.5(1+0.4)}=ar gmaxθ{θp(θ|x)=0.57,¬θp(¬θ|x)=0.43}=θ⟹no bugs present codemap=argmaxθ{θp(|x)=0.40.5(1+0.4),¬θp(¬θ|x)=0.5(1−0.4)0.5(1+0.4 )}=argmaxθ{θp(θ|x)=0.57,¬θp(¬θ|x)=0.43}=θ⟹no bugs present code']]\n",
      "\n",
      "[['however, problem deciding sufficiently large number trials attaching confidence concluded hypothesis.', 'therefore, simplify θmapθmap estimation, denominator posterior computation shown below θmap=argmaxθ(p(x|θi)p(θi))θmap=argmaxθ(p(x|θi)p(θi)) notice map estimation algorithms compute posterior probability hypothesis decide probable hypothesis.', 'binomial likelihood likelihood coin flip experiment given probability observing heads coin flips given fairness coin.', 'therefore, practical implementation map estimation algorithms use approximation techniques, capable finding probable hypothesis computing posteriors computing them.', 'assuming hypothesis space continuous (i.e. fairness coin encoded probability observing heads, coefficient regression model, etc.),', 'department cse mrcet 44 however, p(x)p(x) independent θθ, p(x)p(x) events hypotheses.', 'defined fairness coins (θθ) probability observing heads coin flip, define probability observing heads', 'endless possible hypotheses present smallest range human mind think of, discrete hypothesis space large number possible outcomes event, need find posterior hypothesis order decide probable hypothesis.']]\n",
      "\n",
      "[['rewrite expression single expression follows p(y=y|θ)=θy×(1−θ)1−yp(y=y|θ)=θy×(1−θ)1−y equation represents likelihood single test coin flip experiment.', 'interestingly, likelihood function single coin flip experiment similar bernoulli probability distribution.', 'accordingly p(y=1|θ)=θp(y=0|θ)=(1−θ)p(y=1|θ)=θp(y=0|θ)=(1−θ) defined conditional probabilities outcome above, let try find p(y=y|θ)p(y=y|θ) joint probability observing heads tails p(y=y|θ)={θ, y=11−θ, p(y=y|θ)={θ, y=11−θ, note yy 00 11, θθ lie range [0,1][0,1].', 'bernoulli probability distribution simplification binomial probability distribution single trail, represent likelihood coin flip experiment observe kk number heads nn number trials binomial probability distribution shown below p(k,n|θ)=(nk)θk(1−θ)n−k', 'department cse mrcet 45 tails given fairness coin p(y|θ)p(y|θ) y=1y=1 observing heads y=0y=0 observing tails.']]\n",
      "\n",
      "[['model referred bayes optimal learner, bayes classifier, bayes optimal decision boundary, bayes optimal discriminant function.', 'department cse mrcet 46 maximum likelihood estimation method (mle) likelihood function indicates likely observed sample function possible parameter values.', 'statistical point view, mle usually recommended large samples versatile, applicable models different types data, produces precise estimates.', 'repeat procedure additional n - 1 iterations, alternating drawing new sample conditional probability distribution x conditional probability distribution y, given current value random variable.', 'squares estimation method (lse) squares estimates calculated fitting regression line points data set minimal sum deviations squared (least square error).', 'bayes optimal classifier bayes optimal classifier probabilistic model makes probable prediction new example, given training dataset.', 'gibbs sampling algorithm start selecting initial value random variables x & y. then, sample conditional probability distribution x given y = y⁰ denoted p(x|y⁰).']]\n",
      "\n",
      "[['naive bayes classifier algorithm • naïve bayes algorithm supervised learning algorithm, based bayes theorem solving classification problems.']]\n",
      "\n",
      "[['popular examples naïve bayes algorithm spam filtration, sentimental analysis, classifying articles.', 'naïve bayes classifier simple effective classification algorithms helps building fast machine learning models quick predictions. •', 'solution solve this, consider dataset outlook play 0 rainy yes 1 sunny yes 2 overcast yes 3 overcast yes 4 sunny 5 rainy yes 6 sunny yes', 'department cse mrcet 48 • mainly text classification includes high-dimensional training dataset. •']]\n",
      "\n",
      "[[\"department cse mrcet frequency table weather conditions likelihood table weather condition weather yes overcast 0 5 5/14= 0.35 rainy 2 2 4/14=0.29 sunny 2 3 5/14=0.35 4/14=0.29 10/14=0.71 applying bayes'theorem p(yes|sunny)= p(sunny|yes)*p(yes)/p(sunny) 49 7 overcast yes 8 rainy 9 sunny 10 sunny yes 11 rainy 12 overcast yes 13 overcast yes weather yes overcast 5 0 rainy 2 2 sunny 3 2 total 10 5\"]]\n",
      "\n",
      "[['consider example • figure, alarm ‘a’ – node, installed house person ‘gfg’, rings probabilities i.e burglary ‘b’ fire', 'feature joint probability, probability bayesian belief network derived, based condition — p(attribute/parent) i.e probability attribute, true parent attribute.', 'department cse mrcet 50 p(sunny|yes)= 3/10= 0.3 p(sunny)= 0.35 p(yes)=0.71 p(yes|sunny) = 0.3*0.71/0.35= 0.60 p(no|sunny)= p(sunny|no)*p(no)/p(sunny) p(sunny|no)= 2/4=0.5 p(no)= 0.29 p(sunny)= 0.35 p(no|sunny)= 0.5*0.29/0.35 = 0.41 bayesian belief network graphical representation different probabilistic relationships random variables particular set.']]\n",
      "\n",
      "[['find local maximum likelihood parameters statistical model cases latent variables involved data missing incomplete.', 'expectation-maximization algorithm real-world applications machine learning, common relevant features available learning small subset observable.', 'explained, proposed given paper published 1977 arthur dempster, nan laird, donald rubin.', 'hand, expectation-maximization algorithm latent variables (variables directly observable actually inferred values observed variables) order predict values condition general form probability distribution governing latent variables known us.', 'expectation step (e – step) observed available data dataset, estimate (guess) values missing data.', 'but, drawbacks case, ‘p1’ forget person ‘gfg’, hearing alarm, tendency forget things, quick.', 'so, variables observable not, use instances variable visible observed purpose learning predict value instances observable.', 'alarm parent node probabilities p1 calls ‘p1’ & p2 calls ‘p2’ person nodes. •', 'maximization step (m – step) complete data generated expectation (e) step order update parameters.']]\n",
      "\n",
      "[['step, use observed data order estimate guess values missing incomplete data.', 'step, use complete data generated preceding “expectation” – step order update values parameters.', 'set incomplete observed data given system assumption observed data comes specific model. •', 'department cse mrcet 52 essence expectation-maximization algorithm use available observed data dataset estimate missing data data update values parameters.', 'now, fourth step, checked values converging not, yes, stop repeat step-2 step-3 i.e. “expectation” – step “maximization” – step convergence occurs.']]\n",
      "\n",
      "[['department cse mrcet 53 usage em algorithm • fill missing data sample. •']]\n",
      "\n",
      "[['department cse mrcet 54 instance-based learning machine learning systems categorized instance-based learning systems learn training examples heart generalizes new instances based similarity measure.', 'large memory required store data, query involves starting identification local model scratch.', 'example, create spam filter instance-based learning algorithm, instead flagging emails marked spam emails, spam filter programmed flag emails similar them.']]\n",
      "\n",
      "[['k-nn algorithm stores available data classifies new data point based similarity.', 'working knn algorithm k-nearest neighbours (knn) algorithm uses ‘feature similarity’ predict values new data points means new data point assigned value based closely matches points training set.', 'k-nn algorithm assumes similarity new case/data available cases new case category similar available categories. •', 'knn algorithm training phase stores dataset gets new data, classifies data category similar new data.', 'called lazy learner algorithm learn training set immediately instead stores dataset time classification, performs action dataset. •', 'department cse mrcet 55 k-nearest neighbor(knn) algorithm • k-nearest neighbour simplest machine learning algorithms based supervised learning technique. •', 'understand working help following steps − step 1 − implementing algorithm, need dataset.', 'means new data appears easily classified suite category k- nn algorithm. •', 'step 2 − next, need choose value k i.e. nearest data points.']]\n",
      "\n",
      "[['case-based reasoning classifiers (cbr) use database problem solutions solve new problems.', 'cbr tries combine solutions neighbouring training cases propose solution new case.', 'identical case found, cbr search training cases having components similar new case.', 'new case arrises classify, case-based reasoner(cbr) check identical training case exists.', 'step 4 – end example case based reasoning know nearest neighbour classifiers stores training tuples points euclidean space.', '3.4 − now, assign class test point based frequent class rows.', 'department cse mrcet 56 • 3.1 − calculate distance test data row training data help method namely euclidean, manhattan hamming distance.']]\n",
      "\n",
      "[['cbr intelligent number trade-off accuracy efficiency evolves number stored cases large.', 'problem resolution customer service help desks, cases describe product-related diagnostic problems.', 'medical educations, patient case histories treatments help diagnose treat new patients.', 'lazy learning methods construct different approximation target function encountered query instance.', 'certain point, system’s efficiency suffer time required search process relevant cases increases.', 'lazy learning suitable complex incomplete problem domains, complex target function represented collection complex local approximations.', 'differences eager lazy learning • eager learning methods construct general, explicit description target function based provided training examples. •', 'challenges cbr • finding good similarity metric (eg matching subgraphs) suitable methods combining solutions. •', 'eager learning methods use approximation target function, learned based training examples input queries observed.', 'lazy learning methods simply store data generalizing data postponed explicit request made. •']]\n",
      "\n",
      "[['examples speech recognition, speaker identification, multimedia document recognition (mdr), automatic medical diagnosis.', 'regression algorithms try find relationship variables predict unknown dependent variables based known data.', 'person keeps watching videos related cricket, youtube wouldn’t recommend chess tutorials videos.', 'now, similarities found based statistical analysis, historical data, gained knowledge machine itself.', 'based type data system choose appropriate algorithm classification, regression, regression recognize pattern. •', 'department cse mrcet 58 unit - iv pattern comparison techniques pattern recognition process finding regularities similarities data machine learning data.']]\n",
      "\n",
      "[['clinical settings, having ability know hidden relationships patient data unfold help save life aiding detection conditions obvious clinicians healthcare workers.', 'live ever-changing environment, intelligent system, human robot, encode patterns time, recognize generate temporal patterns.', 'temporal abstraction data mining research fields tried synthesis time oriented data bring understanding hidden relationships exist time oriented events.', 'example, temporal signal sequences movements head, hand, body, piece music, on.', 'temporal patterns temporal patterns pattern comparison techniques defined segment signals recurs frequently temporal signal sequence.', 'paper, propose temporal pattern recognition model based dimension reduction similarity measures maintaining temporal nature raw data introduction temporal pattern processing important intelligent behaviours, including hearing, vision, speech, music motor control.', 'understanding hidden patterns huge challenge exponential search space unique time-series data.']]\n",
      "\n",
      "[['recognition layer typically includes recurrent connections selecting winner self-organization (e.g. winner-take-all) training recognition.', 'speech recognition, example, want rate invariance distinguishing relative durations vowel /i/ (as beet) /i/ (as bit) temporal pattern recognition shared goal stm models input history available simultaneously recognition takes place.', 'department cse mrcet 60 refer syntactic structure, subject-verb-object, component category possible symbols • time duration.', 'recognition scheme essentially template matching, templates formed following hebbian learning wij(t) = wij(t–1) + c si (t)[xj (t) – wij(t–1)] wij connection weight unit xj input layer sequence recognizer si recognition layer.', 'template matching hebbian learning architecture type recognition simply two-layer network input layer incorporates stm, sequence recognition layer unit encodes individual sequence.', 'associative memory approach dynamics hopfield associative memory model characterized evolving memory state similar current input pattern.']]\n",
      "\n",
      "[['input layer, tdnn uses 2 hidden layers output layer unit encodes phoneme.', 'feed forward connections converge input layer successive layer unit specific layer receives inputs limited time window previous layer.', '1989) reported architecture called time delay neural networks (tdnn) spoken phoneme recognition.', 'recognizer encodes different template sequence unique weight vector acting inputs stm.', 'normalized exponential kernel stm, tank hopfield (1987) described recognition network based associative memory dynamics.', 'process dynamic evolution viewed optimization process, minimizes cost function equilibrium reached.', 'dynamic time warping sounds like time traveling kind future technic, however, not.', 'recognition process uses current input sequence (evidence) bias minimization process similar template wins competition, activating corresponding recognizer.', 'department cse mrcet 61 views memory state category, hopfield net performs pattern recognition recalled category recognized pattern.', 'demonstrated good recognition performance stop consonants /b/, /d/, /g/, accuracy speaker dependent recognition reached 98.5%.']]\n",
      "\n",
      "[['however, people speak word different ways, speaks hello slower pace like heeeeeeelloooooo , need algorithm match sound track different lengths able identify come person.', 'suppose want recognise voice person analysing sound track, able collect sound track saying hello scenario.', 'suppose want calculate distance equal-length arrays = [1, 2, 3] b = [3, 2, 2] that?', '= [1, 2, 3] b = [2, 2, 2, 3, 4] match up?']]\n",
      "\n",
      "[['department cse mrcet 63 stock market stock market, people hope able predict future, general machine learning algorithms exhaustive, prediction task requires test training set dimension features.', 'however, speculate stock market, know pattern stock different length reflection klines indicators.']]\n",
      "\n",
      "[['department cse mrcet 64 time series analysis, dynamic time warping (dtw) algorithms measuring similarity temporal sequences, vary speed.', 'idea compare arrays different length build one-to-many many-to-one matches total distance minimised two.', 'dtw applied temporal sequences video, audio, graphics data — indeed, data turned linear sequence analysed dtw.']]\n",
      "\n",
      "[['apply one-to-one match, shown top, mapping perfectly synced tail blue curve left out.', 'department cse mrcet 65 clearly series follow pattern, blue curve longer red.', 'dtw overcomes issue developing one-to-many match troughs peaks pattern perfectly matched, left curves(shown top).']]\n",
      "\n",
      "[['department cse mrcet 66 l > k rules general, dtw method calculates optimal match given sequences (e.g. time series) certain restriction rules(comes wiki) • index sequence matched indices sequence vice versa • index sequence matched index sequence (but match) • index sequence matched index sequence (but match) • mapping indices sequence indices sequence monotonically increasing, vice versa, i.e. sequence, indices indices sequence, index matched index l index j matched index k , vice versa.', 'optimal match denoted match satisfies restrictions rules minimal cost, cost computed sum absolute differences, matched pair indices, values.']]\n",
      "\n",
      "[['unsupervised learning method method draw references datasets consisting input data labelled responses.', 'department cse mrcet 67 introduction clustering basically type unsupervised learning method.', 'clustering task dividing population data points number groups data points groups similar data points group dissimilar data points groups.', 'generally, process find meaningful structure, explanatory underlying processes, generative features, groupings inherent set examples.']]\n",
      "\n",
      "[['department cse mrcet 68 dbscan density-based spatial clustering applications noise data points clustered basic concept data point lies given constraint cluster center.', 'example dbscan (density-based spatial clustering applications noise), optics (ordering points identify clustering structure), etc. •', 'clustering methods • density-based methods methods consider clusters dense region having similarities differences lower dense region space.', 'instance, interested finding representatives homogeneous groups (data reduction), finding “natural clusters” describe unknown properties (“natural” data types), finding useful suitable groupings (“useful” data classes) finding unusual data objects (outlier detection).', 'hierarchical based methods clusters formed method form treetype structure based hierarchy.']]\n",
      "\n",
      "[['clustering operations grids fast independent number data objects example sting (statistical information grid), wave cluster, clique (clustering quest), etc.', 'k means clustering simplest unsupervised learning algorithm solves clustering problem.k-means algorithm partitions n observations k clusters observation belongs cluster nearest mean serving prototype cluster.', 'method optimize objective criterion similarity function distance major parameter example k-means, clarans (clustering large applications based randomized search), etc. •', 'applications clustering different fields • marketing characterize & discover customer segments marketing purposes. •', 'city planning groups houses study values based geographical locations factors present.', 'department cse mrcet 69 examples cure (clustering representatives), birch (balanced iterative reducing clustering hierarchies), etc. •', 'grid-based methods method, data space formulated finite number cells form grid-like structure.']]\n",
      "\n",
      "[['method initialize means random values boundaries data set (if feature x items values [0,3], initialize means values x [0,3]).', 'department cse mrcet 70 • earthquake studies learning earthquake-affected areas determine dangerous zones.', 'categorize item closest mean update mean’s coordinates, averages items categorized mean far.', 'algorithm pseudocode k-mode clustering kmodes clustering unsupervised machine learning algorithms cluster categorical variables.']]\n",
      "\n",
      "[['repeat 2–3 steps re-assignment required example imagine dataset information hair color, eye color, skin color persons.', 'aim group based available information(maybe want suggest styling ideas) hair color, eye color, skin color categorical variables.', 'mismatches) assign observation closest cluster iteratively compare cluster data points observations.', 'let proceed defining number clusters(k)=3 step 1 pick k observations random use leaders/clusters choosing p1, p7, p8 leaders/clusters step 2 calculate dissimilarities(no.']]\n",
      "\n",
      "[['department cse mrcet 72 comparing leader/cluster p1 observation p1 gives 0 dissimilarities comparing leader/cluster p1 observation p2 gives 3(1+1+1) dissimilarities.', 'likewise, calculate dissimilarities matrix shown assign observations closest cluster (cluster dissimilarity)']]\n",
      "\n",
      "[['explanation cluster 1 observations(p1, p2, p5) brunette observed hair color, amber observed eye color, fair observed skin color.', 'repeat steps 2–4 obtaining new leaders, calculate dissimilarities observations newly obtained leaders.', 'department cse mrcet 73 step 2, observations p1, p2, p5 assigned cluster 1; p3, p7 assigned cluster 2; p4, p6, p8 assigned cluster 3.', 'observations cluster 1 marked yellow, cluster 2 marked brick red, cluster 3 marked purple.']]\n",
      "\n",
      "[['department cse mrcet 74 comparing cluster 1 observation p1 gives 1 dissimilarity.']]\n",
      "\n",
      "[['department cse mrcet 75 observations p1, p2, p5 assigned cluster 1; p3, p7 assigned cluster 2; p4, p6, p8 assigned cluster 3.']]\n",
      "\n",
      "[['department cse mrcet 76 vector quantization learning vector quantization ( lvq ) type artificial neural network inspired biological models neural systems.', 'based prototype supervised learning classification algorithm trained network competitive learning algorithm similar self organizing map.', 'architecture learning vector quantization number classes input data n number input features sample given below']]\n",
      "\n",
      "[['first, initializes weights size ( n, c ) c number training samples different labels discarded training samples.', 'department cse mrcet 77 let input data size ( m, n ) m number training example n number features example label vector size ( m, 1 ).', 'weight updation rule given wij = wij(old) - alpha(t) * (x k - wij(old)) alpha learning rate time t, j denotes winning vector, denotes ith feature training example k denotes kth training example input data.', 'algorithm steps involved • weight initialization • 1 n number epochs • select training example • compute winning vector • update winning vector • repeat steps 3, 4, 5 training example. •', 'iterate remaining input data, training example, updates winning vector ( weight vector shortest distance ( e.g euclidean distance ) training example ).']]\n",
      "\n",
      "[['words, algorithm place, set “knowledge data” end users improved, that’s quantity data, filtering noise undesirable results, refinement data points.', 'machine learning systems simple “rote input/output” function, evolve results supply continued use.', 'intelligent exploitation random search provided historical data direct search region better performance solution space.', 'fundamental ways engineers use induction algorithm enhance knowledge acquisition given system.', 'generation consist population individuals individual represents point search space possible solution.', 'different search methods induction field machine learning, induction algorithm represents example mathematical principles development sophisticated computing systems.', 'genetic algorithms simulate process natural selection means species adapt changes environment able survive reproduce generation.', 'department cse mrcet genetic algorithms unit- v genetic algorithms(gas) adaptive heuristic search algorithms belong larger evolutionary algorithms.']]\n",
      "\n",
      "[['department cse mrcet 79 technical descriptions induction algorithms largely territory mathematical scientific journals, basic ideas induction algorithm organize “classification rules” according induction principle separate corollary results different kinds system noise exceptions.', \"general, machine learning use visual dashboards generating new tools users rapidly develop in-depth knowledge given system, it's related marine research, medical diagnosis, e-commerce, kind data-rich system.\", 'idea real-world data filtering, induction algorithms compose different sets rules legitimate results system noise, order distinguish other.', 'like kinds machine learning software, induction algorithms thought form “decision support.” “', 'mind, induction algorithms kinds software products seek refine data produce evolving results human users.', 'sense, use induction algorithm uses induction principle “prove” certain results aid knowledge, provide marked delineations data set (or multiple data sets) – distinctions drive sorts end user capabilities.', 'we consider principal task real-world induction system assisting expert expressing expertise,” write authors turing institute paper induction machine learning 1980s. “', 'setting induction algorithms according certain training examples, stakeholders looking ability systems identify assess consistent rules data represents exceptions rules.']]\n",
      "\n",
      "[['terms machine learning, algorithm aims understand example particular concept generalizations form concepts training examples.', 'ebl architecture • ebl model training • training, model generalizes training example way scenarios lead goal concept, specific cases. (', 'department cse mrcet 80 simple terms, ability gain basic problem-solving techniques observing analysing solutions specific problems.', 'explanation — domain theory eliminate unimportant training example retaining important ones best describe goal concept.']]\n",
      "\n",
      "[['department cse mrcet 81 • ebl model training • post training, ebl model tends directly reach hypothesis space involving goal concept. (']]\n",
      "\n",
      "[['condition, classification problem relies humidity rainfall collapsed underlying feature, aforementioned correlated high degree.', '3d classification problem hard visualize, 2-d mapped simple 2 dimensional space, 1-d problem simple line.', 'department cse mrcet 82 dimensionality reduction intuitive example dimensionality reduction discussed simple e-mail classification problem, need classify e-mail spam not.', 'figure illustrates concept, 3-d feature space split 1-d feature spaces, later, found correlated, number features reduced further.', 'involve large number features, e-mail generic title, content e- mail, e-mail uses template, etc.']]\n",
      "\n",
      "[['department cse mrcet 83 components dimensionality reduction components dimensionality reduction • feature selection this, try find subset original set variables, features, smaller subset model problem.', 'methods dimensionality reduction methods dimensionality reduction include • principal component analysis (pca) • linear discriminant analysis (lda) • generalized discriminant analysis (gda) dimensionality reduction linear non-linear, depending method used.', 'embedded • feature extraction reduces data high dimensional space lower dimension space, i.e. space lesser no.']]\n",
      "\n",
      "[['department cse mrcet 84 principal component analysis method introduced karl pearson.', 'works condition data higher dimensional space mapped data lower dimension space, variance data lower dimensional space maximum.', 'advantages dimensionality reduction • helps data compression, reduced storage space. •']]\n",
      "\n",
      "[['example, possible variations observed variables mainly reflect variations unobserved (underlying) variables.', 'department cse mrcet 85 • know principal components keep- practice, thumb rules applied.', 'observed variables modelled linear combinations potential factors plus \"error\" terms, factor analysis thought special case errors-invariables models.', 'factor analysis statistical method describe variability observed, correlated variables terms potentially lower number observed variables called factors.']]\n",
      "\n",
      "[['multidimensional scaling multidimensional scaling visual representation distances dissimilarities sets objects. “', 'department cse mrcet 86 distances speakers recording ‘n’ speakers’ voice signals.', 'decomposing mixed signal microphone’s recording independent source’s speech signal machine learning technique, independent component analysis. [', 'objects” colors, faces, map coordinates, political persuasion, kind real conceptual stimuli (kruskal wish, 1978).', 'objects similar (or shorter distances) closer graph objects similar (or longer distances).', 'x1, x2, ….., xn ] => [ y1, y2, ….., yn ] where, x1, x2, …, xn original signals present mixed signal y1, y2, …, yn new features independent components independent other.', 'now, microphones’ recordings, want separate ‘n’ speakers’ voice signals room given microphone recorded voice signals coming speaker different intensity difference distances them.']]\n",
      "\n",
      "[['two-dimensional manifold 2-d shape fit higher dimensional space twisting bending it, loosely speaking.', 'term scaling comes psychometrics, abstract concepts (“objects”) assigned numbers according rule (trochim, 2006).', 'department cse mrcet 87 graph, mds serve dimension reduction technique high- dimensional data (buja et.', 'it’s use isn’t limited specific matrix set data; fact, matrix analyzed technique long matrix contains type relational data (young, 2013).', 'think “scaling” fact you’re essentially scaling data (i.e. making simpler creating lower-dimensional data).', 'assign “1” “doesn’t believe global warming”, 10 “firmly believes global warming” scale 2 9 attitudes between.', 'example, data points close high-dimensional space close low- dimensional space (martinez, 2005). “']]\n",
      "\n",
      "[['unsupervised learning algorithm produces low-dimensional embeddings high-dimensional inputs, relating training instance closest neighbor.', 'general, m training instances total, tries find set weights w minimizes squared distance x(i) linear representation.', 'training instance x(i), algorithm finds k nearest neighbors tries express x(i) linear function them.', 'the manifold hypothesis states real-world high-dimensional data lie low dimensional manifolds embedded high-dimensional space.”', 'so, cost function given wi,j =0, j included k closest neighbors i. also, normalizes weights training instance x(i),', 'locally linear embedding (lle) locally linear embedding (lle) manifold learning technique non-linear dimensionality reduction.']]\n",
      "\n",
      "[['choosing d-dimensional coordinates minimize cost function, weights wi,j kept fixed try find optimum coordinates y(i)', 'department cse mrcet 89 finally, high-dimensional training instance x(i) mapped low- dimensional (say, d dimensions) vector y(i) preserving neighborhood relationships.']]\n",
      "\n",
      "[['introduction machine learning early draft proposed textbook nils j. nilsson robotics laboratory department computer science stanford university stanford, 94305 e-mail nilsson@cs.stanford.edu november 3, 1998 copyright c⃝2005 nils j. nilsson material copied, reproduced, distributed written permission copyright holder.']]\n",
      "\n",
      "[]\n",
      "\n",
      "[['23 2.3 summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', '16 2.2 classes boolean functions . . . . . . . . . . . . . . . . . . . .', '9 1.4 sample applications . . . . . . . . . . . . . . . . . . . . . . . . .', '5 1.2.2 input vectors . . . . . . . . . . . . . . . . . . . . . . . . .', '8 1.2.5 noise . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', '15 2.1.2 diagrammatic representations . . . . . . . . . . . . . . .', '17 2.2.1 terms clauses . . . . . . . . . . . . . . . . . . . . . .', '17 2.2.2 dnf functions . . . . . . . . . . . . . . . . . . . . . . . .', '9 1.3 learning requires bias . . . . . . . . . . . . . . . . . . . . . . . .', '13 1.6 bibliographical historical remarks . . . . . . . . . . . . . .'], ['18 2.2.3 cnf functions . . . . . . . . . . . . . . . . . . . . . . . .', '7 1.2.3 outputs . . . . . . . . . . . . . . . . . . . . . . . . . . . .', '3 1.1.3 varieties machine learning . . . . . . . . . . . . . . . .', '5 1.2.1 types learning . . . . . . . . . . . . . . . . . . . . . .', 'contents 1 preliminaries 1 1.1 introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', '23 2.2.6 linearly separable functions . . . . . . . . . . . . . . . .', '1 1.1.2 wellsprings machine learning . . . . . . . . . . . . . .', '4 1.2 learning input-output functions . . . . . . . . . . . . . . . . . .', '8 1.2.4 training regimes . . . . . . . . . . . . . . . . . . . . . . .', '11 1.5 sources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .'], ['21 2.2.4 decision lists . . . . . . . . . . . . . . . . . . . . . . . . .', '22 2.2.5 symmetric voting functions . . . . . . . . . . . . . .', '13 2 boolean functions 15 2.1 representation . . . . . . . . . . . . . . . . . . . . . . . . . . . .', '9 1.2.6 performance evaluation . . . . . . . . . . . . . . . . . . .', '1 1.1.1 machine learning? . . . . . . . . . . . . . . . . .', '24 2.4 bibliographical historical remarks . . . . . . . . . . . . . .', '15 2.1.1 boolean algebra . . . . . . . . . . . . . . . . . . . . . . .']]\n",
      "\n",
      "[['42 4.1.6 training tlu non-linearly-separable training sets 44 4.2 linear machines . . . . . . . . . . . . . . . . . . . . . . . . . . .', '46 4.3.1 motivation examples . . . . . . . . . . . . . . . . . .', '68 5.2 learning belief networks . . . . . . . . . . . . . . . . . . . . . .', '51 4.4 training feedforward networks backpropagation . . . . . . .', '52 4.4.1 notation . . . . . . . . . . . . . . . . . . . . . . . . . . . .', '63 5.1.2 gaussian (or normal) distributions . . . . . . . . . . . .', '70 5.4 bibliographical historical remarks . . . . . . . . . . . . . .', '56 4.4.4 computing changes weights intermediate layers 58 4.4.5 variations backprop . . . . . . . . . . . . . . . . . . .', '70 5.3 nearest-neighbor methods . . . . . . . . . . . . . . . . . . . . . .', '37 4.1.3 error-correction training tlu . . . . . . . . . . . .'], ['52 4.4.2 backpropagation method . . . . . . . . . . . . . . . .', '44 4.3 networks tlus . . . . . . . . . . . . . . . . . . . . . . . . . .', '53 4.4.3 computing weight changes final layer . . . . . .', '32 3.5 bibliographical historical remarks . . . . . . . . . . . . . .', '49 4.3.3 piecewise linear machines . . . . . . . . . . . . . . . . . .', '27 3.2 version graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . .', '34 4 neural networks 35 4.1 threshold logic units . . . . . . . . . . . . . . . . . . . . . . . .', '38 4.1.4 weight space . . . . . . . . . . . . . . . . . . . . . . . . .', '32 3.4 candidate elimination method . . . . . . . . . . . . . . . .', '35 4.1.2 special cases linearly separable functions . . . . . . .'], ['63 5.1.1 background general method . . . . . . . . . . . . . .', '60 4.5 synergies neural network knowledge-based methods 61 4.6 bibliographical historical remarks . . . . . . . . . . . . . .', '40 4.1.5 widrow-hoﬀprocedure . . . . . . . . . . . . . . . . .', '65 5.1.3 conditionally independent binary components . . . . . .', '61 5 statistical learning 63 5.1 statistical decision theory . . . . . . . . . . . . . . . . . .', '35 4.1.1 deﬁnitions geometry . . . . . . . . . . . . . . . . . .', '59 4.4.6 application steering van . . . . . . . . . . . . . . .', '50 4.3.4 cascade networks . . . . . . . . . . . . . . . . . . . . . .', '3 version spaces learning 27 3.1 version spaces mistake bounds . . . . . . . . . . . . . . . .', '46 4.3.2 madalines . . . . . . . . . . . . . . . . . . . . . . . . . . .'], ['29 3.3 learning search version space . . . . . . . . . . . . . . .']]\n",
      "\n",
      "[['101 7.7 bibliographical historical remarks . . . . . . . . . . . . . .', '73 6.2 supervised learning univariate decision trees . . . . . . . . .', '104 8 computational learning theory 107 8.1 notation assumptions pac learning theory . . . . . . .', '81 6.4.3 avoiding overﬁtting decision trees . . . . . . . . . . .', '82 6.4.4 minimum-description length methods . . . . . . . . . . .', '109 8.2.1 fundamental theorem . . . . . . . . . . . . . . . . .', '80 6.4.2 validation methods . . . . . . . . . . . . . . . . . . . . .', '115 8.3.3 general capacity result . . . . . . . . . . . . . .', '94 7.4 inducing recursive programs . . . . . . . . . . . . . . . . . . . .', '75 6.2.3 non-binary attributes . . . . . . . . . . . . . . . . . . . .'], ['84 6.6 problem missing attributes . . . . . . . . . . . . . . . . .', '86 6.8 bibliographical historical remarks . . . . . . . . . . . . . .', '113 8.3.1 linear dichotomies . . . . . . . . . . . . . . . . . . . . . .', '84 6.5 problem replicated subtrees . . . . . . . . . . . . . . . .', '90 7.2 generic ilp algorithm . . . . . . . . . . . . . . . . . . . . . .', '113 8.3.2 capacity . . . . . . . . . . . . . . . . . . . . . . . . . . .', '111 8.2.3 properly pac-learnable classes . . . . . . . . . . .', '6 decision trees 73 6.1 deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', '79 6.4 overﬁtting evaluation . . . . . . . . . . . . . . . . . . . . .', '87 7 inductive logic programming 89 7.1 notation deﬁnitions . . . . . . . . . . . . . . . . . . . . . . .'], ['107 8.2 pac learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', '91 7.3 example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', '75 6.2.2 uncertainty reduction select tests . . . . . . .', '109 8.2.2 examples . . . . . . . . . . . . . . . . . . . . . . . . . . .', '98 7.5 choosing literals add . . . . . . . . . . . . . . . . . . . . . .', '112 8.3 vapnik-chervonenkis dimension . . . . . . . . . . . . . . . .', '117 8.4 vc dimension pac learning . . . . . . . . . . . . . . . . .', '79 6.3 networks equivalent decision trees . . . . . . . . . . . . . . .', '83 6.4.5 noise data . . . . . . . . . . . . . . . . . . . . . . . . .', '118 8.5 bibliographical historical remarks . . . . . . . . . . . . . .'], ['86 6.7 comparisons . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', '80 6.4.1 overﬁtting . . . . . . . . . . . . . . . . . . . . . . . . . .', '100 7.6 relationships ilp decision tree induction . . . . .', '74 6.2.1 selecting type test . . . . . . . . . . . . . . . . . .']]\n",
      "\n",
      "[['125 9.3.2 method based probabilities . . . . . . . . . . . . . .', '153 11.5.4 partially observable states . . . . . . . . . . . . . . . . .', '140 10.8 bibliographical historical remarks . . . . . . . . . . . . . .', '154 11.5.5 scaling problems . . . . . . . . . . . . . . . . . . . . . . .', '131 10.3 incremental computation (∆w)i . . . . . . . . . . . . . .', '120 9.2.2 method based probabilities . . . . . . . . . . . . . .', '130 10 temporal-diﬀerence learning 131 10.1 temporal patterns prediction problems . . . . . . . . . . . .', '150 11.5.1 illustrative example . . . . . . . . . . . . . . . . . . .', '126 9.4 bibliographical historical remarks . . . . . . . . . . . . . .', '138 10.6 intra-sequence weight updating . . . . . . . . . . . . . . . . . .'], ['152 11.5.3 generalizing inputs . . . . . . . . . . . . . . . . . .', '124 9.3 hierarchical clustering methods . . . . . . . . . . . . . . . . . .', '150 11.5.2 random actions . . . . . . . . . . . . . . . . . . .', '138 10.7 example application td-gammon . . . . . . . . . . . . . . .', '131 10.2 supervised temporal-diﬀerence methods . . . . . . . . . . .', '141 11 delayed-reinforcement learning 143 11.1 general problem . . . . . . . . . . . . . . . . . . . . . . . .', '134 10.4 experiment td methods . . . . . . . . . . . . . . . . .', '135 10.5 theoretical results . . . . . . . . . . . . . . . . . . . . . . . . . .', '144 11.3 temporal discounting optimal policies . . . . . . . . . . . .', '9 unsupervised learning 119 9.1 unsupervised learning? . . . . . . . . . . . . . . . . . .'], ['119 9.2 clustering methods . . . . . . . . . . . . . . . . . . . . . . . . . .', '145 11.4 q-learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', '154 11.6 bibliographical historical remarks . . . . . . . . . . . . . .', '147 11.5 discussion, limitations, extensions q-learning . . . . . .', '120 9.2.1 method based euclidean distance . . . . . . . . . .', '125 9.3.1 method based euclidean distance . . . . . . . . . .', '143 11.2 example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .']]\n",
      "\n",
      "[['164 12.7.1 macro-operators planning . . . . . . . . . . . . . . . .', '164 12.7.2 learning search control knowledge . . . . . . . . . . . .', '167 12.8 bibliographical historical remarks . . . . . . . . . . . . . .', '12 explanation-based learning 157 12.1 deductive learning . . . . . . . . . . . . . . . . . . . . . . . . . .', '164 12.6 utility ebl . . . . . . . . . . . . . . . . . . . . . . . . . . . .', '158 12.3 example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', '157 12.2 domain theories . . . . . . . . . . . . . . . . . . . . . . . . . . .', '159 12.4 evaluable predicates . . . . . . . . . . . . . . . . . . . . . . . . .', '162 12.5 general proofs . . . . . . . . . . . . . . . . . . . . . . . . .', '164 12.7 applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .']]\n",
      "\n",
      "[]\n",
      "\n",
      "[['and, treat matters practical importance applications; book handbook machine learning practice.', 'students stanford courses machine learning useful suggestions, colleague, pat langley, teaching assistants, ron kohavi, karl pﬂeger, robert allen, lise getoor.', 'hope future versions cover hopﬁeld nets, elman nets re- current nets, radial basis functions, grammar automata learning, genetic algorithms, bayes networks . . ..']]\n",
      "\n",
      "[['dictionary deﬁnition includes phrases “to gain knowledge, understanding of, skill in, study, instruction, expe- rience,” “modiﬁcation behavioral tendency experience.”', 'regards machines, say, broadly, machine learns changes structure, program, data (based inputs response external information) manner expected future performance improves.', 'tasks involve recognition, diag- nosis, planning, robot control, prediction, etc. “', 'but, example, performance speech-recognition machine improves hearing samples person’s speech, feel justiﬁed case machine learned.', 'likely concepts techniques explored researchers machine learning illuminate certain aspects biological learning.', 'certainly, techniques machine learning derive eﬀorts psychologists precise theories animal human learning computational models.', 'changes, addition record data base, fall comfortably province disciplines necessarily better understood called learning.', 'learning, like intelligence, covers broad range processes dif- ﬁcult deﬁne precisely.', 'machine learning usually refers changes systems perform tasks associated artiﬁcial intelligence (ai).']]\n",
      "\n",
      "[['sensory signals perception actions action computation model planning reasoning goals figure 1.1 ai system ask “why machines learn?', 'like machines able adjust internal structure produce correct outputs large number sample inputs suitably constrain input/output function approximate relationship implicit examples. •', 'are • tasks deﬁned example; is, able specify input/output pairs concise relationship inputs desired outputs.']]\n",
      "\n",
      "[['details statistical the- ory underlying methods found statistical textbooks [anderson, 1958]. •', 'brain models non-linear elements weighted inputs suggested simple models biological neu- rons.', 'dif- ferent traditions bring diﬀerent methods diﬀerent vocabulary assimilated uniﬁed discipline.', 'introduction 3 • human designers produce machines work desired environments used.', 'machines learn knowledge gradually able capture humans want write down. •', 'statistical methods dealing problems considered instances machine learning decision estimation rules depend corpus samples drawn problem environment.', 'continuing redesign ai systems conform new knowledge impractical, machine learning methods able track it.', 'brief listing separate disciplines contributed machine learning; details follow appropriate chapters • statistics long-standing problem statistics best use sam- ples drawn unknown probability distributions help decide distribution new sample drawn.', 'related problem estimate value unknown function new point given values function set sample points.', 'networks elements studied sev- eral researchers including [mcculloch & pitts, 1943, hebb, 1949, rosenblatt, 1958] and, recently [gluck & rumelhart, 1989, sejnowski, koch, & churchland, 1988].']]\n",
      "\n",
      "[['recent work directed discovering rules expert systems decision-tree methods [quinlan, 1990] in- ductive logic programming [muggleton, 1991, lavraˇc & dˇzeroski, 1994].', 'related work led number early decision tree [hunt, marin, & stone, 1966] semantic network [anderson & bower, 1973] methods.', 'samuel developed prominent early pro- gram learned parameters function evaluating board posi- tions game checkers [samuel, 1959].', 'work reinforcement learning traced eﬀorts model reward stimuli inﬂuence learning goal-seeking behavior animals [sutton & barto, 1987].', 'adaptive control theory control theorists study problem con- trolling process having unknown parameters estimated operation.', 'evolutionary models nature, individual animals learn perform better, species evolve better ﬁt individual niches.', 'distinc- tion evolving learning blurred computer systems, techniques model certain aspects biological evolution proposed learning methods improve performance computer programs.', 'early example epam net- work storing retrieving member pair words given [feigenbaum, 1961].', 'ai researchers explored role analogies learning [carbonell, 1983] fu- ture actions decisions based previous exemplary cases [kolodner, 1993].', 'theme saving generalizing results prob- lem solving explanation-based learning [dejong & mooney, 1986, laird, et al.,'], ['genetic algorithms [holland, 1975] genetic programming [koza, 1992, koza, 1994] prominent computational tech- niques evolution.', 'shall important machine learning techniques based networks nonlinear elements—often called neural networks.']]\n",
      "\n",
      "[['consider variety diﬀerent computational structures • functions • logic programs rule sets • finite-state machines • grammars • problem solving systems present methods synthesis structures examples changing existing structures.', 'assume ﬁnd hypothesis, h, closely agrees f members ξ, hypothesis good guess f—especially ξ large.', 'assume priori hypothesized function, h, selected class functions h. know f belongs class subset class.', 'learning input-output functions 5 1.1.3 varieties machine learning orthogonal historical source learning technique important learned.', 'hypothesis function learned denoted h. f h functions vector-valued input x = (x1, x2, . . . ,', 'terminology shall book best introduced discussing problem learning functions, turn matter ﬁrst.', 'one, called supervised learning, know (sometimes approximately) values f m samples training set, ξ.', 'case, change existing structure simply computationally eﬃcient increase coverage situations handle.']]\n",
      "\n",
      "[['sup- pose given values two-dimensional function, f, sample points shown solid circles fig.', 'setting, termed unsupervised learning, simply train- ing set vectors function values them.', 'unsupervised learning methods application taxonomic problems desired invent ways classify data meaningful categories.', 'we regard problem learning function; value function subset input vector be- longs.)', 'problem case, typically, partition training set subsets, ξ1, . . . ,', 'formulas ⊃b b ⊃c, deduce c given a. deductive process, create formula ⊃c—a new formula sanction con-', 'xn h d h figure 1.2 input-output function curve-ﬁtting simple example supervised learning function.']]\n",
      "\n",
      "[['example illustrating categorical values, information student represented values attributes class, major, sex, adviser.', 'components, xi, input vector variously called features, attributes, input variables, components.', 'contrast speed-up learning methods create genuinely new functions—ones diﬀerent results learning before.', '1.2.2 input vectors machine learning methods derive diﬀerent traditions, terminology rife synonyms, book.', 'additionally, categorical values ordered (as {small, medium, large}) unordered (as example given).', 'learning input-output functions 7 -10 -5 0 5 10-10 -5 0 5 10 0 500 1000 1500 -10 -5 0 5 10-10 -5 0 5 10 0 00 00 0 x1 x2 h sample f-value figure 1.3 surface fits points clusions derived formulas previously had.']]\n",
      "\n",
      "[['case, training pattern having value 1 called positive instance, training sample having value 0 called negative instance.', 'input case suitable representation printed character, classiﬁer maps input of, say, 64 categories.', 'alternatively, output categorical value, case pro- cess embodying h variously called classiﬁer, recognizer, categorizer, output called label, class, category, decision.', 'batch method, entire training set available compute function, h. variation method uses entire training set modify current hypothesis iteratively acceptable hypothesis obtained.', '1.2.3 outputs output real number, case process embodying function, h, called function estimator, output called output value estimate.', 'entire training set available member time, use incremental method—selecting training set members arrive. (', 'contrast, incremental method, select member time training set use instance modify current hypothesis.', 'important specialization uses boolean values, regarded special case discrete numbers (1,0) categorical variables (true, false).']]\n",
      "\n",
      "[['present members training set, graph number hypotheses ruled function number diﬀerent patterns presented shown fig.', 'class noise randomly alters value function; attribute noise randomly alters values components input vector.', 'remaining functions constitute called “version space;” we’ll explore concept detail later.', '1.2.6 performance evaluation correct answer inductive learning, important methods evaluate result learning.', 'certainly, example, uncountable number diﬀerent functions having values agree samples shown fig.', 'learning requires bias 9 training instance function current hypothesis previ- ous instance—as classiﬁer decide robot’s action given current set sensory inputs.', 'case, inappropriate insist hypothesized function agree precisely values samples training set.', 'gain insight role bias considering special case learning boolean function n dimensions.', 'case, presented member training set value rule precisely one-half members h—those boolean functions misclassify labeled sample.', 'discuss matter detail later, but, brieﬂy, supervised learning induced function usually evaluated separate set inputs function values called testing set .'], ['order selection limit priori set hypotheses quadratic functions insist chose passed sample points.', '1.3 learning requires bias long reader undoubtedly asked learning function possible all?', 'suppose bias; h set 22n boolean functions, preference ﬁt samples training set.']]\n",
      "\n",
      "[['labeled patterns seen 0 0 2n < j (generalization possible) |hv| = no.', 'depending subset order presentation training patterns, curve hypotheses ruled look like shown fig.', 'functions ruled figure 1.4 hypotheses remaining function labeled patterns presented suppose limited h subset, hc, boolean functions.', 'hypothesis set consists linearly separable functions—those positive negative instances separated linear surface, function remaining hypothsis set consistent training set.', 'so, case, training set contain possible patterns, pin function be—given bias.', 'preliminaries half remaining boolean functions value 1 half value 0 training pattern seen.', 'there, training set sample patterns marked having value 1 small square having value 0 small circle.', 'case possible seeing fewer 2n labeled samples, hypothesis agrees training set.']]\n",
      "\n",
      "[['example, way measuring complex- ity hypothesis, select simplest performed satisfactorily training set.', 'principle occam’s razor, science prefer simple explanations complex ones, type preference bias. (', 'langley, 1992] cites following applications others a. rule discovery variant id3 printing industry problem', 'absolute bias (also called restricted hypothesis-space bias), restricts h deﬁnite subset functions.', 'william occam, 1285-?1349, english philosopher said “non sunt multiplicanda entia praeter necessitatem,” means “entities multiplied unnecessarily.”)', 'functions ruled depends order presentation log2|hc| figure 1.5 hypotheses remaining restricted subset machine learning researchers identiﬁed main varieties bias, ab- solute preference.']]\n",
      "\n",
      "[['b. electric power load forecasting k-nearest-neighbor rule system [jabbour, k., et al.,', 'd. planning scheduling steel mill expertease, marketed (id3-like) system [michie, 1992].', 'c. automatic “help desk” assistant nearest-neighbor system [acorn & walden, 1992].', 'papers on speech recognition, dolphin echo recognition, image processing, bio-engineering, diag- nosis, commodity trading, face recognition, music composition, optical character recognition, control applications [various editors, 1989-1994].', 'b. neuroforecasting centre’s (london business school university col- lege london) trading strategy selection network earned average annual proﬁt 18% conventional system’s 12.3%.', 'additional examples, [hammerstrom, 1993] mentions a. sharp’s japanese kanji character recognition system processes 200 char- acters second 99+% accuracy.', 'preliminaries x1 x2 x3 figure 1.6 training set completely determines linearly separable function [evans & fisher, 1992].']]\n",
      "\n",
      "[['fact come surprise inasmuch machine learning techniques viewed extensions known statistical meth- ods successfully applied years.', 'sources 13 c. fujitsu’s (plus partner’s) neural network monitoring continuous steel casting operation successful operation early 1990.', '1.5 sources rich literature machine learning (a small referenced bibliography), text- books worth mentioning [hertz, krogh, & palmer, 1991, weiss & kulikowski, 1991, natarjan, 1991, fu, 1994, langley, 1996]. [', 'shavlik & dietterich, 1990, buchanan & wilkins, 1993] edited vol- umes containing important papers.', 'established conferences publications papers given appear including • annual conferences advances neural information processing systems • annual workshops computational learning theory • annual international workshops machine learning • annual international conferences genetic algorithms (the proceedings above-listed conferences published morgan kaufmann.) •']]\n",
      "\n",
      "[]\n",
      "\n",
      "[['deﬁnitions compactly given following rules boolean algebra 1 + 1 = 1, 1 + 0 = 1, 0 + 0 = 0, 1 · 1 = 1, 1 · 0 = 0, 0 · 0 = 0, 1 = 0, 0 = 1.', 'chapter 2 boolean functions 2.1 representation 2.1.1 boolean algebra important ideas learning functions easily presented special case boolean functions.', 'arguments values boolean functions expressed terms constants t (true) f (false) instead 1 0, respectively.', 'x1x2 value 1 x1 x2 value 1; x1 x2 value 0, x1x2 value 0. (', 'x1 + x2 value 1 x1 x2 value 1; x1 x2 value 0, x1 + x2 value 0.', 'complement negation variable, x, written x. x value 1 x value 0; x value 1, x value 0.']]\n",
      "\n",
      "[['2.1.2 diagrammatic representations saw chapter boolean function represented labeling vertices cube.', 'vertices having value 1 labeled small square, vertices having value 0 labeled small circle.', 'instead, demorgan’s laws (which veriﬁed deﬁnitions) x1x2 = x1 + x2, x1 + x2 = x1 x2.', '3-dimensional cube 23 = 8 vertices, labeled diﬀerent ways; 2(23) = 256', 'x1 x2 x1 x2 x1 x2 xor (exclusive or) x1x2 x1 + x2 x1x2 + x1x2 parity function x1 x2 x3 x1x2x3 + x1x2x3 + x1x2x3 + x1x2x3 figure 2.1 representing boolean functions cubes hypercube representations, easy boolean functions n dimensions are.']]\n",
      "\n",
      "[['an parity function boolean function value 1 number arguments value 1; value 0.)', 'term function written form l1l2 · · · lk, li literals.', 'rows columns arranged way entries adjacent map correspond vertices adjacent hypercube representation.', '00 01 10 11 00 01 10 11 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 x1,x2 x3,x4 figure 2.2 karnaugh map 2.2 classes boolean functions 2.2.1 terms clauses use absolute bias machine learning, limit class hypotheses.', 'course, visualize hypercubes (for n > 3), surprising properties higher dimensional spaces, careful intuitions gained low dimensions.', 'karnaugh map array values boolean function horizontal rows indexed values variables vertical columns indexed rest.']]\n",
      "\n",
      "[['consider, example, function f = x2x3 + x1 x3 + x2x1x3.', 'number terms size k bounded pk i=0 c(2n, i) = o(nk), c(i, j) = i! (', 'term, t, prime implicant f term, t′, formed taking literal implicant t longer implicant f. (the implicant “divided” term remain implicant.)', 'note planes ﬁgure “cuts oﬀ” group vertices having value 1, cuts oﬀany vertices having value 0.', 'group vertices subface corresponds implicants function, f, implicant corresponds subface dimension.', 'relationship implicants prime implicants geometri- cally illustrated cube representation boolean functions.', 'term dnf expression function called implicant “implies” function (if term value 1, function).', '3n possible clauses fewer pk i=0 c(2n, i) clauses size k less.', 'general, term, t, implicant function, f, f value 1 t does.', 'dnf expression called k-term dnf expression disjunction k terms; class k-dnf size largest term k. examples 2-term 3-term expressions, respectively.'], ['2.2.2 dnf functions boolean function said disjunctive normal form (dnf) written disjunction terms.', 'clause function written form l1 +l2 +· · ·+lk, li literals.', 'thus, x2x3 x1 x3 prime implicants f = x2x3+x1 x3+x2x1x3, x2x1x3 not.', 'examples dnf are f = x1x2+x2x3x4 f = x1x3 + x2 x3 + x1x2x3.']]\n",
      "\n",
      "[['consensus method relies results replace section describing quine-mccluskey method instead. •', 'note term x2x1x3 prime implicant f. (in case, don’t include term function vertex cut oﬀby plane corresponding x2x1x3 cut oﬀby plane corresponding x2x3.)', 'x2 x1 x3 1, 0, 0 1, 0, 1 1, 1, 1 0, 0, 1 f = x2x3 + x1x3 + x2x1x3 = x2x3 + x1x3 x2x3 x1x3 prime implicants figure 2.3 function implicants note boolean functions represented dnf—trivially disjunctions terms size n term corresponds vertices value 1.', 'boolean functions represented dnf term prime implicant, representation unique, shown fig.', '22n functions n dimensions dnf (since boolean function written dnf), 2o(nk) functions k-dnf.', 'express function dnf form, use consensus method ﬁnd expression function term prime implicant.']]\n",
      "\n",
      "[['boolean functions x2 x1 x3 1, 0, 0 1, 0, 1 1, 1, 1 0, 0, 1 f = x2x3 + x1x3 + x1x2 = x1x2 + x1x3 terms prime implicants, unique representation figure 2.4 non-uniqueness representation prime implicants xi · f1 + xi · f2 = xi · f1 + xi · f2 + f1 · f2 f1 f2 terms literal appearing f1 appears complemented f2.', 'f1 · f2 called consensus xi · f1 xi · f2.']]\n",
      "\n",
      "[['ﬁnal form function terms prime implicants is f = x1x2 +x1x3 +x1 x4x5.', 'x1x2 x1x2x3 x1x2x3x4x5 x1x3 x1x2x4x5 x1x4x5 f = x1x2 + + x1x3 x1x4x5 1 2 6 4 5 3 figure 2.5 consensus tree 2.2.3 cnf functions disjunctive normal form dual conjunctive normal form (cnf).', 'process halts, terms remaining t prime implicants f. example let f = x1x2 + x1 x2x3 + x1 x2 x3 x4x5.', 'classes boolean functions 21 consensus method ﬁnding set prime implicants function, f, iterates following operations terms dnf expression f operations applied a. initialize process set, t , terms dnf expression f, b. compute consensus pair terms t add result t , c. eliminate terms t subsumed terms t .']]\n",
      "\n",
      "[['boolean functions example cnf is f = (x1 +x2)(x2 +x3 +x4).', 'f written dnf, application de morgan’s law renders f cnf, vice versa.', 'value 1 x1 = 1, x2 = 0, x3 = 1.', 'decision list size k, size largest term k. class decision lists size k called k-dl.', 'at ti value 1, does; v1 regarded default value decision list.)', 'decision list written ordered list pairs (tq, vq) (tq−1, vq−1) · · · (ti, vi) · · · (t2, v2) (t, v1) vi 0 1, ti terms (x1, . . . ,', 'value decision list value vi ﬁrst ti list value 1. (', 'example decision list is f = (x1x2, 1) (x1 x2x3, 0) x2x3, 1) (1, 0) f value 0 x1 = 0, x2 = 0, x3 = 1.', 'example use linearly separable functions place ti (see [marchand & golea, 1993]).', '2.2.4 decision lists rivest proposed class boolean functions called decision lists [rivest, 1987].'], ['cnf expression called k-clause cnf expression conjunction k clauses; class k-cnf size largest clause k. example 2-clause expression 3-cnf.']]\n",
      "\n",
      "[['classes boolean functions 23 2.2.5 symmetric voting functions boolean function called symmetric invariant permutations input variables.', 'wn) n-dimensional vector weight values, x · w dot (or inner) product vectors.', 'input vectors f value 1 lie half-space (and on) hyperplane orientation normal w position (with respect origin) determined θ.', 'closed-form expression number linearly separable func- tions n dimensions, following table gives numbers n 6.', 'important subclass symmetric functions class voting func- tions (also called m-of-n functions).', 'n, real-valued numbers called weights, θ real-valued number called threshold, thresh(σ, θ) 1 σ ≥θ 0 otherwise. (', 'xn) n-dimensional vector input variables, w = (w1, . . . ,', 'k-voting functions members class linearly separable functions weights unit value threshold depends k. thus, terms clauses special cases linearly separable functions.', '2.2.6 linearly separable functions linearly separable functions expressed follows f = thresh( n x i=1 wixi, θ) wi, = 1, . . . ,', 'k = 1, voting function n-sized clause; k = n, voting function n-sized term; k = (n + 1)/2 n odd k = 1 + n/2 n even, majority function.'], ['parity functions, value 1 depending number input variables value 1 odd symmetric function. (', 'convenient way write linearly separable functions uses vector notation f = thresh(x · w, θ) x = (x1, . . . ,']]\n",
      "\n",
      "[['boolean functions n boolean linearly separable functions functions 1 4 4 2 16 14 3 256 104 4 65,536 1,882 5 ≈4.3 × 109 94,572 6 ≈1.8 × 1019 15,028,134 [muroga, 1971] shown (for n > 1) 2n2 linearly separable functions n dimensions. (', 'dnf (all) k-dl k-dnf k-size- terms terms lin sep figure 2.6 classes boolean functions sizes classes given following table (adapted [dietterich, 1990, page 262])']]\n",
      "\n",
      "[['bibliographical historical remarks 25 class size class terms 3n clauses 3n k-term dnf 2o(kn) k-clause cnf 2o(kn) k-dnf 2o(nk) k-cnf 2o(nk) k-dl 2o[nkk log(n)] lin sep 2o(n2) dnf 22n 2.4 bibliographical historical remarks added.']]\n",
      "\n",
      "[]\n",
      "\n",
      "[['consider following procedure classifying arbitrary input pattern, x pattern class (0 1) majority outputs functions version space.', 'stage process left subset functions consistent patterns presented far; subset version space patterns presented.', 'hypothesis, h, consistent values x ξ h(x) = f(x) x ξ.', 'devices implement- ing function h. incremental training procedure deﬁned presented pattern ξ functions eliminated functions values pattern agree given value.', 'learning procedure, majority equal value pattern presented, mistake made, revise version space accordingly—eliminating (majority the) functions voting incorrectly.', 'chapter 3 version spaces learning 3.1 version spaces mistake bounds ﬁrst learning methods present based concepts version spaces version graphs.', 'given initial hypothesis set h (a subset boolean functions) values f(x) x training set, ξ, version space subset hypotheses, hv, consistent values.']]\n",
      "\n",
      "[['special case, bias limit h terms, log2(3n) = n log2(3) = 1.585n mistakes exhausting version space.', 'version spaces learning h1 h2 hi hk x subset, h, boolean functions rule hypotheses consistent training patterns hj hypotheses ruled constitute version space k = |h| 1 0 figure 3.1 implementing version space original hypothesis set, h. (note, though, number training patterns seen maximum number mistakes greater.)', 'suﬃcient training patterns reduce version space single function, training patterns reduce version space set functions assign values patterns henceforth.', 'result means f term, 1.585n mistakes learning f, number mistakes able decide f term.', 'result (due [littlestone, 1988]) example mistake bound—an important concept machine learning theory.']]\n",
      "\n",
      "[['3.2, example version graph 3-dimensional input space hypotheses restricted terms (with ruled out).', '1” row nodes corresponding terms having literal, row nodes corresponding terms having literals,', 'boolean function, f1, general function, f2, (and f2 speciﬁc f1), f1 value 1 arguments f2 value 1, f1 ̸= f2.', '0 x1 x2 x3 x2 x3 1 x1x2 x3 x1x2 x1 version graph terms x1 x2 x3 (for simplicity, arcs graph shown) (none ruled out) (k = 1) (k = 2) (k = 3) x1 x3 figure 3.2 version graph terms function, denoted “1,” value 1 inputs, corre- sponds node graph. (']]\n",
      "\n",
      "[['3.4, version graph exists learning (1,0,1) value 0 (1, 0, 0) value 1.', 'portrayal graph cluttered arcs shown; node actual graph arc directed nodes general.', '0 x1 x2 x3 x2 x3 1 x1x2 x3 x1x2 x1 new version graph 1, 0, 1 value 0 x1x3 x1x2 x2x3 x1x2x3 x1 x2 x3 x1x3 (only arcs graph shown) ruled nodes figure 3.3 version graph seeing (1, 0, 1) version graph, set hypotheses maximally general set hypotheses maximally speciﬁc.', 'use example version graph changes consider set labeled samples training set, ξ.', '33 = 27 functions altogether (the function “0,” included graph, technically term).']]\n",
      "\n",
      "[['maximally speciﬁc corresponds subface minimal dimension contains members training set labelled 1 members labelled 0.', 'determination possible fact member version space (that member boundary sets) speciﬁc member general boundary set general member speciﬁc boundary set.', 'version graphs 31 0 x1 x2 x3 x2 x3 1 x1x2 x3 x1 x2x3 x1x3 general boundary set (gbs) specific boundary set (sbs) x1x2 specific gbs, general sbs 1, 0, 1 value 0 x1 x2 x3 1, 0, 0 value 1 figure 3.4 version graph seeing (1, 0, 1) (1, 0, 0) boundary sets important provide alternative repre- senting entire version space explicitly, impractical.', '3.4, subface minimal dimension contains (1, 0, 0) contain (1, 0, 1) vertex (1, 0, 0) itself—corresponding function x1x2 x3.', 'maximally general corresponds subface maximal dimension contains members training set labelled 1 members labelled 0.', 'given boundary sets, possible determine hypoth- esis (in prescribed class boolean functions using) member version space.', 'limit boolean functions version space terms, simple matter determine maximally general maximally speciﬁc functions (assuming term version space).']]\n",
      "\n",
      "[['negative instance algorithm specializes elements [gbs] longer cover new instance remain consis- tent past data, removes [sbs] elements mistakenly cover new, negative instance.”', 'version spaces learning maximal dimension contains (1, 0, 0) contain (1, 0, 1) face cube—corresponding function x3.', '3.4 candidate elimination method candidate elimination method, incremental method computing boundary sets.', 'quoting [hirsh, 1994, page 6] “the candidate-elimination algorithm manipulates boundary-set representation version space create boundary sets rep- resent new version space consistent previous instances plus new one.', 'method uses following deﬁnitions (adapted [genesereth & nilsson, 1987]) • hypothesis called suﬃcient value 1 training samples labeled 1, • hypothesis called necessary value 0 training samples labeled 0.', 'compare view top-down versus bottom-up divide-and-conquer covering (or aq) methods decision-tree induction.', 'start general function specialize specialization operators ﬁnds function consistent (or adequately so) set training patterns.', 'positive exmple algorithm generalizes elements [sbs] little possible cover new instance remain consistent past data, removes elements [gbs] cover new instance.']]\n",
      "\n",
      "[['new general boundary set obtained previous re- placing element, hi, specializations.', 'candidate elimination method 33 think deﬁnitions hypothesis implements suﬃ- cient condition training sample value 1 hypothesis value 1 positive instances; hypothesis implements necessary condition training sample value 1 hypothesis value 0 negative instances.', 'hypothesis hg generalization h if a) h speciﬁc hg, b) hg suﬃcient, c) function (including h) speciﬁc hg suﬃcient, d) hg speciﬁc member new general boundary set.', 'b. new vector labelled 0 new speciﬁc boundary set obtained previous ex- cluding elements necessary. (', 'hypothesis hs specialization h if a) h general hs, b) hs necessary, c) function (including h) general hs necessary, d) hs general member new speciﬁc boundary set.', 'again, hs = h, specializations diﬀerent functions general boundary set identical.', 'example, suppose present vectors following order vector label (1, 0, 1) 0 (1, 0, 0) 1 (1, 1, 1) 0 (0, 0, 1) 0', 'hg = h. also, generalizations diﬀerent functions speciﬁc boundary set identical.', 'start (before receiving members training set) function “0” singleton element speciﬁc boundary set function “1” singleton element general boundary set.', 'new speciﬁc boundary set obtained previous re- placing element, hi, generalizations.'], ['receiving new labeled input vector, boundary sets changed follows a. new vector labelled 1 new general boundary set obtained previous ex- cluding elements suﬃcient. (']]\n",
      "\n",
      "[['version spaces learning start general boundary set, “1”, speciﬁc boundary set, “0.”', 'functions, x1, x2, x3, specializations “1” (they necessary, “1” not, general “0”, functions general necessary).', 'order accomodate noisy data, version spaces generalized [hirsh, 1994] allow hypotheses necessarily consistent training set.', '1, 1, 1), labeled 0, change speciﬁc boundary set function necessary.', 'single function generalization “0” (it suﬃcient, “0” speciﬁc it, function (including “0”) speciﬁc suﬃcient, speciﬁc member general boundary set. (', 'seeing ﬁrst sample, (1, 0, 1), labeled 0, speciﬁc boundary set stays “0” (it necessary), change general boundary set {x1, x2, x3}.', 'finally, (0, 0, 1), labeled 0, change speciﬁc boundary set function necessary.', '3.5 bibliographical historical remarks concept version spaces role learning ﬁrst investigated tom mitchell [mitchell, 1982].', 'ideas prac- tical machine learning procedures, provide insight nature hypothesis selection.', 'then, seeing (1, 0, 0), labeled 1, general boundary set changes {x3} (because x1 x2 suﬃcient), speciﬁc boundary set changed {x1x2 x3}.']]\n",
      "\n",
      "[['networks commonly use threshold element encountered chapter study linearly separable boolean functions.', 'called neural networks be- cause non-linear elements inputs weighted sum outputs elements—much like networks biological neurons do.', 'begin treatment neural nets studying threshold element simplest networks, ones composed single threshold element.', 'called adaline (for adaptive linear element) [widrow, 1962, widrow & lehr, 1990], ltu (linear threshold unit), perceptron, neuron. (', '4.1 threshold logic units 4.1.1 deﬁnitions geometry linearly separable (threshold) functions implemented straightforward way summing weighted inputs comparing sum threshold value shown fig.', 'although word “per- ceptron” nowadays refer single tlu, rosenblatt originally deﬁned class networks threshold elements [rosenblatt, 1958].)', 'networks non-linear elements, interconnected adjustable weights, play prominent role machine learning.', 'best implement function device gives outputs prescribed function arbi- trary inputs.', 'chapter describe networks non-linear elements implement input-output functions trained supervised learning methods.', 'output 1 0 depending weighted sum inputs greater equal threshold value, θ.']]\n",
      "\n",
      "[['n + 1)-st component, xn+1, augmented feature vector, y, value 1; (n + 1)-st component, wn+1, augmented weight vector, v, set equal negative desired threshold value. (', 'weighted sum calculated tlu simply represented vector dot product, x•w. (', 'x1 x2 xn+1 = 1 xi w1 w2 wn+1 wi wn x threshold weight xn w threshold \" = 0 f f = thresh( !', 'if pattern weight vectors thought “column” vectors, dot product written xtw, “row” vector xt transpose x.) often, threshold, θ, tlu ﬁxed 0; case, arbitrary thresholds achieved (n + 1)- dimensional “augmented” vectors, y, v, ﬁrst n components x w, respectively. (', 'weights tlu represented n-dimensional weight vector, w = (w1, . . . ,', 'when want emphasize use augmented vectors, we’ll use y,v notation; context discussion makes clear sort vectors talking about, we’ll lapse familiar x,w notation.)', 'unit vector normal hyperplane n = w |w|, |w| = p (w2 1 + . . .', 'hyperplane boundary patterns x•w + wn+1 > 0 patterns x•w + wn+1 < 0.', 'wi xi, 0) = 1 n+1 figure 4.1 threshold logic unit (tlu) n-dimensional feature input vector denoted x = (x1, . . . ,']]\n",
      "\n",
      "[['threshold logic units 37 form hyperplane equation x•n + w |w| = 0.)', 'distance hyperplane origin negative (that is, wn+1 < 0), origin negative hyperplane (that is, x•w + wn+1 < 0).', 'weight +1 input corresponding positive literal, weight −1 input corresponding negative literal. (', 'distance hyperplane origin wn+1 |w| , distance arbitrary point, x, hyperplane x•w+wn+1 |w| .', '4.1.2 special cases linearly separable functions terms term size k implemented tlu weight inputs corresponding variables occurring term.', 'x.w + wn+1 > 0 w x w n = w |w| origin unit vector normal hyperplane w + wn+1 = 0 x n + = 0 x equations hyperplane wn+1 |w| wn+1 w + wn+1 x x.w + wn+1 < 0 figure 4.2 tlu geometry adjusting weight vector, w, changes orientation hyperplane; adjusting wn+1 changes position hyperplane (relative origin).', 'threshold, θ, set equal kp −1/2, kp number positive literals term.']]\n",
      "\n",
      "[['present family incremental training procedures parameter c. methods adjustments weight vector tlu trained makes error training pattern; called error-correction procedures.', 'example, negation clause f = x1 + x2 + x3 term f = x1 x2 x3.', '1,1,1) (1,1,0) x2 x1 x3 f = x1x2 x1 + x2 - 3/2 = 0 equation plane is figure 4.3 implementing term clauses negation clause term.', 'a. start ﬁnite training set, ξ, vectors, yi , binary labels.']]\n",
      "\n",
      "[['note adjustment new dot product (v − ciyi)•yi = v•yi −ciyi•yi, smaller weight adjustment. (', 'threshold logic units 39 f = x1 + x2 + x3 x1 x1 + x2 + x3 < 1/2 = 0 f = x1x2x3 equation plane is x2 x3 figure 4.4 implementing clause b. compose inﬁnite training sequence, σ, vectors ξ labels member ξ occurs inﬁnitely σ.', 'b) yi supposed produce output 0 produces output 1 instead, modify weight vector follows v ←−v −ciyi ci positive real number called learning rate parame- ter (whose value diﬀererent diﬀerent instances family procedures depend i).', 'c) yi supposed produce output 1 produces output 0 instead, modify weight vector follows v ←−v + ciyi case, new dot product (v + ciyi)•yi = v•yi + ciyi•yi, larger weight adjustment.', 'c. repeat forever present vector, yi, σ tlu note response. (']]\n",
      "\n",
      "[['weight points half-spaces deﬁned hyperplane cause corresponding pattern yield dot product 0, weight points half- space cause corresponding pattern yield dot product greater 0.', 'note weight vector v includes wn+1 thresh- old component, threshold tlu changed adjust- ments.', '2) fractional-correction procedure, parameter ci set λ yi•v yi•yi , v weight vector changed.', 'use augmented vectors discussion threshold function compares dot product, yi•v, threshold 0.', '4.1.4 weight space intuitive idea procedures work considering happens augmented weight vector “weight space” corrections made.', 'neural networks v ←−v + ci(di −fi)yi di desired response (1 0) yi , fi actual response (1 0) yi.]', 'identify versions procedure 1) ﬁxed-increment procedure, learning rate parameter, ci, ﬁxed, positive constant i. depending value constant, weight adjustment correct response erroneously classiﬁed feature vector.', 'particular weight vector, v, corresponds point (n + 1)-dimensional weight space.', 'maass & tur´an, 1994] hyperplane-ﬁnding procedure makes o(n2 log n) mistakes.', 'now, pattern vector, yi, consider locus points weight space corresponding weight vectors yielding yi•v = 0.'], ['proved weight vector, v, produces correct output feature vectors ξ, ﬁnite number feature vector presentations, ﬁxed-increment procedure ﬁnd weight vector weight changes.']]\n",
      "\n",
      "[['2 3 4 1 v 0 1 1 2 3 2 3 4 figure 4.6 solution region weight space', 'suppose wanted weight values positive responses patterns y1, y3, y4, negative response pattern y2.', 'threshold logic units 41 y2, y3, y4, respectively, indicate arrow half-space weight vectors dot products greater 0.', '2 3 4 1 v figure 4.5 weight space exists weight vector gives desired responses given set patterns given geometric interpretation.']]\n",
      "\n",
      "[['the boxed numbers show, later purposes, number errors weight vectors regions.)', 'subsequent corrections overshoot solution region, eventually work way far solution region corrections (for ﬁxed increment size) it.', 'starting v1, gives incorrect response pattern y1, v1 v2 direction normal plane 1. (', 'solution region “hyper-wedge” region vertex origin weight space cross-section increases increasing distance origin.', 'neural networks weight vector exists correctly classiﬁes set patterns, half-spaces deﬁned correct responses patterns non- intersection, called solution region.', '2 3 4 1 v v1 v2 v3 v4 v5 v6 figure 4.7 moving solution region 4.1.5 widrow-hoﬀprocedure widrow-hoﬀprocedure (also called lms delta procedure) at- tempts ﬁnd weights minimize squared-error function pat- tern labels dot product computed tlu.', 'ﬁxed-increment error-correction procedure changes weight vector moving normal pattern hyperplane weight vector gives incorrect response.', 'suppose example present patterns sequence y1, y2, y3, y4, start process weight point v1, shown fig.']]\n",
      "\n",
      "[['total squared error (over patterns training set, ξ, containing m patterns) then ε = m x i=1 (di − n+1 x j=1 xijwj)2 want choose weights wj minimize squared error.', 'entire weight vector (in augmented, v, notation) adjusted according following rule v ←−v + ci(di −fi)yi where, before, yi i-th augmented pattern vector.', 'way ﬁnd set weights start arbitrary weight vector negative gradient ε function weights.', 'often, preferable use incremental procedure try tlu element, xi, ξ time, compute gradient single- pattern squared error, εi, appropriate adjustment weights, try member ξ.', 'ε quadratic wj, know global minimum, steepest descent procedure guaranteed ﬁnd minimum.', 'widrow-hoﬀprocedure makes adjustments weight vector when- dot product itself, yi•v, equal speciﬁed desired target', 'threshold logic units 43 squared error pattern, xi, label di (for desired output) is εi = (di − n+1 x j=1 xijwj)2 xij j-th component xi.', 'j-th component gradient single-pattern error is ∂εi ∂wj = −2(di − n+1 x j=1 xijwj)xij adjustment direction negative gradient change weight follows wj ←−wj + ci(di −fi)xij fi = pn+1 j=1 xijwj, ci governs size adjustment.']]\n",
      "\n",
      "[['finding weight values desired dot products corresponds solv- ing set linear equalities, widrow-hoﬀprocedure interpreted descent procedure attempts minimize mean-squared-error be- tween actual desired values dot product. (', 'described [hertz, krogh, & palmer, 1991, p. 160] . . .', '4.1.6 training tlu non-linearly-separable training sets training set linearly separable (perhaps noise inherently), desired ﬁnd “best” separating hy- perplane.', 'mean-squared-error criterion gives un- satisfactory results, however, prefers small errors large ones.', 'examples training curves tlu’s; performance training set; performance test set; cumulative number corrections.', 'typically, error-correction procedures non- linearly-separable training sets continue attempt correct inevitable errors, hyperplane settle acceptable place.', '4.2 linear machines natural generalization (two-category) tlu r-category classiﬁer structure, shown fig.', 'alternative, error correction continuous decrease zero value learning rate constant, c, result decreasing changes hyperplane.', 'consists simply storing (or “putting pocket”) set weights longest un- modiﬁed run successes far.', 'first, use widrow-hoﬀprocedure, (although converge zero error non-linearly separable problems) weight vector min- imizes mean-squared-error.'], ['duda [duda, 1966] suggested keeping track average value weight vector error correction average separating hyperplane performs reasonably non-linearly-separable problems.']]\n",
      "\n",
      "[['linear machines 45 familiar notation, ws x meant augmented vectors (with (n+1)-st component).', '4.9 shows character regions 2-dimensional space created linear machine r = 5.', 'r1 r3 r4 r5 x.w4 * x.wi & 4 r2 region figure 4.9 regions linear machine train linear machine, straightforward generalization 2-category error-correction rule.', 'y y argmax w1.x wr.x figure 4.8 linear machine diagram fig.', 'note r = 2, linear machine reduces tlu weight vector w = (w1 −w2).']]\n",
      "\n",
      "[['4.3 networks tlus 4.3.1 motivation examples layered networks classify correctly patterns non-linearly-separable training sets re- quires separating surfaces complex hyperplanes.', 'correction increases value u-th dot product decreases value v-th dot product.', 'tlus feedforward network arranged layers, elements layer j receiving inputs tlus layer j −1, network layered, feedforward', 'ﬁgure, weight values input lines tlu threshold value inside circle representing tlu.', 'single line 2-dimensional square separate vertices (1,1) (0,0) vertices (1,0) (0,1)—the function linearly separable im- plemented single tlu.', '2-category ﬁxed increment proce- dure, procedure guaranteed terminate, constant ci, exists weight vectors correct separations training set.', 'feedforward networks cycles; feedforward network tlu’s input depends (through zero intermediate tlus) tlu’s output. (', 'b. machine mistakenly classiﬁes category u pattern, xi, category v (u ̸= v), then wu ←−wu + cixi wv ←−wv −cixi weight vectors changed.', 'consider, example, 2- dimensional, parity function, f = x1x2 + x1 x2.']]\n",
      "\n",
      "[['networks tlus 47 f x1 x2 1.5 -0.5 0.5 1 1 -1 -1 1 1 figure 4.10 network parity function network.', 'x hidden units output units figure 4.11 layered, feedforward network implementing dnf functions two-layer networks deﬁned k-term dnf functions—they dnf functions having k terms.', 'some people count layers tlus include inputs layer also; network three-layer network.)', 'k-term dnf function implemented two-layer network k units hidden layer—to implement k terms—and output unit implement disjunction terms.']]\n",
      "\n",
      "[['x conjuncts disjunct feedforward, 2-layer network tlus disjunction terms conjunctions literals (terms) figure 4.12 two-layer network x2 x1 x3 f = x1x2 + x2x3 + x1x3 figure 4.13 planes implemented hidden units train two-layer network implements k-term dnf function, ﬁrst note output unit implements disjunction, weights ﬁnal layer ﬁxed.', 'discuss half-space intersections, half-space unions, np-hardness optimal versions, single-side-error-hypeplane methods, relation “aq” methods.']]\n",
      "\n",
      "[['leave reader think training procedure modiﬁed output tlu implemented function (or function).', '4.3.2 madalines two-category networks interesting example layered, feedforward network two-layer odd number hidden units, “vote-taking” tlu output unit.', 'ﬁrst layer partition feature space way, regardless subse- quent layers do, ﬁnal outputs consistent labeled training set.', 'hidden units voting incorrectly, change weight vectors ki dot products closest 0 error correction rule w ←−w + ci(di −fi)xi di desired response hidden unit (0 1) fi actual response (0 1). (', 'networks tlus 49 important comment layered networks adding additional layers compensate inadequate ﬁrst layer tlus.', 'is, perform error-correction hidden units correct vote majority voting correctly, change easiest change.', 'typically, response vote taking unit deﬁned response majority hidden units, output logics possible.', 'ridgway [ridgway, 1962] proposed following error-correction rule adjusting weights hidden units madaline • madaline correctly classiﬁes pattern, xi, corrections hidden units’ weight vectors, • madaline incorrectly classiﬁes pattern, xi, determine minimum number hidden units responses need changed (from 0 1 1 0—depending type error) order madaline correctly classify xi.', 'example problems set weight values exists given madaline structure classify members training set correctly, procedure fail ﬁnd them.', 'ﬁrst layer tlus partitions feature space dif- ferently labeled vectors region (that is, vectors yield set outputs ﬁrst-layer units).']]\n",
      "\n",
      "[['similarly, r-category training set linearly separable exists linear machine correctly classiﬁes members training set.', 'neural networks r-category madalines error-correcting output codes k hidden units (k > 1) two-layer network, responses correspond vertices k-dimensional hypercube.', 'wr wr arg max 1 r 1 n1 1 nr figure 4.14 piecewise linear machine', 'ordinary two-category madaline identiﬁes special points space, vertex consisting k 1’s vertex consisting k 0’s.', 'design r-category madaline identifying r vertices hidden-unit space classifying pattern according vertices hidden-unit response closest to.', '4.3.3 piecewise linear machines two-category training set linearly separable exists threshold func- tion correctly classiﬁes members training set.']]\n",
      "\n",
      "[['tlu network implements set 2k parallel hyperplanes, k number tlus receives inputs. (', 'reader consider n-dimensional parity function implemented cascade network having log2 n tlus.', 'unfortunately, example training sets separable given pwl machine structure error-correction training method fails ﬁnd solution.', 'method appear work situations [duda & fossum, 1966], al- [nilsson, 1965, page 89] observed “it probably eﬀective method training pwl machines having [weight vectors] bank.”', 'each k preceding tlus output 1 0; that’s 2k diﬀerent combinations—resulting 2k diﬀerent positions parallel hyperplanes.)', '4.3.4 cascade networks interesting class feedforward networks tlus ordered tlu receives inputs pattern components tlus lower ordering.', 'pattern classiﬁed incorrectly, subtract (a constant times) pattern vec- tor weight vector producing largest dot product (it incorrectly largest) add (a constant times) pattern vector weight vector correct bank weight vectors dot product locally largest bank. (', 'networks tlus 51 pwl machine groups weighted summing units r banks corre- sponding r categories.']]\n",
      "\n",
      "[['intuitively, looks weight-adjusting procedures network correct direction (relative error) making minimal changes.', 'network output unit, but, course, possible tlus output layer—each implementing diﬀerent function.', 'we assume “threshold weight” component associated weight vector; v notation instead include', 'neural networks l1 l2 l2 figure 4.16 planes implemented cascade network tlus cascade networks trained ﬁrst training l1 good job possible separating training patterns (perhaps pocket algorithm, example), training l2 (including weight l1 l2) good job possible separating training patterns, resulting network classiﬁes patterns training set satisfactorily.', '4.4 training feedforward networks back- propagation 4.4.1 notation general problem training network tlus diﬃcult.', 'input feature vector denoted x(0), ﬁnal output (of k-th layer tlu) f. tlu layer weight vector (connecting inputs) threshold; i-th tlu j-th layer weight vector denoted w(j) . (']]\n",
      "\n",
      "[['x(0) . . . . . . . . . . . .', 'desired response, di, i-th input vector, xi, training set, ξ, compute squared error entire training set be ε = x xi ϵ ξ (di −fi)2 fi actual response network input xi.', 'training feedforward networks backpropagation53 threshold component, chosen use familiar x,w notation, assuming vectors “augmented” appropriate.)', 'squared error single input vector, x, evoking output f desired output d is', 'wi(k-1) x(k-1) mj tlus m(k-1) tlus wli(j) wl(k) layer j-th layer (k-1)-th layer k-th layer . . .', 'vector w(j) components w(j) l,i l = 1, . . . ,', 'gradient descent squared error, adjust weight network proportional negative partial derivative ε respect weight.', 'denote weighted sum input i-th threshold unit j-th layer s(j) . (', 'f si(1) si(j) si(k-1) s(k) figure 4.17 k-layer network 4.4.2 backpropagation method gradient descent method, similar widrow hoﬀmethod, proposed authors training multi-layer, feedforward network.', 'before, deﬁne error function ﬁnal output network adjust weight network minimize error.']]\n",
      "\n",
      "[['vector partial derivative φ called gradient φ respect w denoted ∇wφ.', 'δ(j) tells sensitive squared error network output changes input threshold function.', 'ε’s dependence w(j) entirely s(j) , use chain rule write ∂ε ∂w(j) = ∂ε ∂s(j) ∂s(j) ∂w(j) s(j) = x(j−1)•w(j) , ∂s(j) ∂w(j) = x(j−1).', 'changing weight vectors directions negative gradient, fundamental rule weight changes network be w(j) ←w(j) + c(j) δ(j) x(j−1) c(j) learning rate constant weight vector. (', 'neural networks ε = (d −f)2 convenient partial derivatives ε respect weights groups corresponding weight vectors.', 'thus, ∂ε ∂w(j) = −2(d −f) ∂f ∂s(j) x(j−1) quantity (d−f) ∂f ∂s(j) plays important role calculations; shall denote δ(j) .', 'substituting yields ∂ε ∂w(j) = ∂ε ∂s(j) x(j−1) note ∂ε ∂s(j) = −2(d −f) ∂f ∂s(j) .', 'deﬁne partial derivative quantity φ, say, respect weight vector, w(j) , thus ∂φ ∂w(j) def = \" ∂φ ∂w(j) 1i , . . . ,']]\n",
      "\n",
      "[['sigmoid threshold function f (s) s f (s) = 1/[1 + e<s] figure 4.18 sigmoid function 1[russell & norvig 1995, page 595] attributes use idea [bryson & ho 1969].', 'small changes sums change f all, f change, changes abruptly 1 0 vice versa.', 'now, turn attention calculation δ(j) ’s. deﬁnition, have δ(j) = (d −f) ∂f ∂s(j) problem, however, attempting carry partial deriva- tives f respect s’s.', 'trick involves replacing threshold functions diﬀerentiable functions called sigmoids.1 output sigmoid function, superimposed threshold function, shown fig.', 'way diﬃculty proposed werbos [werbos, 1974] (perhaps independently) pursued researchers, example [rumelhart, hinton, & williams, 1986].', 'usually, sigmoid function f(s) = 1 1+e−s , s input f output.']]\n",
      "\n",
      "[['x(0) . . . . . . . . . . . .', 'wi(k-1) fi(k-1) si(k-1) f(k) s(k) x(k-1) mj sigmoids m(k-1) sigmoids wli(j) wl(k) bi(j) bi(1) bi(k-1) b(k) layer j-th layer (k-1)-th layer k-th layer . . .', 'output i-th sigmoid unit j-th layer denoted f (j) . (', 'figure 4.19 network sigmoid units 4.4.3 computing weight changes final layer ﬁrst calculate δ(k) order compute weight change ﬁnal sigmoid unit']]\n",
      "\n",
      "[['backpropagation weight adjustment single element ﬁnal layer written as w ←−w + c(d −f)f(1 −f)x written format, error-correction rule is w ←−w + c(d −f)x widrow-hoﬀrule is w ←−w + c(d −f)x diﬀerence (except fact f thresholded widrow- hoﬀ) f(1 −f) term presence sigmoid function.', 'f 0, f(1 −f) 0; f 1, f(1 −f) 0; f(1 −f) obtains maximum value 1/4 f 1/2 (that is, input sigmoid 0).', 'weight changes region “fuzz” surrounding hyperplane, changes direction correcting error, error-correction widrow-hoﬀrules.', 'pattern far away fuzzy hyperplane, f(1 −f) value close 0, backpropagation rule makes little change weight values regardless desired output. (', 'substituting gives us δ(k) = (d −f (k))f (k)(1 −f (k)) rewriting general rule weight vector changes, weight vector ﬁnal layer changed according rule w(k) ←w(k) + c(k)δ(k)x(k−1) δ(k) = (d −f (k))f (k)(1 −f (k)) interesting compare backpropagation error-correction rule widrow-hoﬀrule.', 'training feedforward networks backpropagation57 δ(k) = (d −f (k))∂f (k) ∂s(k) given sigmoid function using, f(s) = 1 1+e−s , ∂f ∂s = f(1 −f).']]\n",
      "\n",
      "[['so δ(j) = (d −f) ∂f ∂s(j) = (d −f) \" ∂f ∂s(j+1) 1 ∂s(j+1) 1 ∂s(j) + · · · + ∂f ∂s(j+1) l ∂s(j+1) l ∂s(j) + · · · + ∂f ∂s(j+1) mj+1 ∂s(j+1) mj+1 ∂s(j) # = mj+1 x l=1 (d −f) ∂f ∂s(j+1) l ∂s(j+1) l ∂s(j) = mj+1 x l=1 δ(j+1) l ∂s(j+1) l ∂s(j) remains compute ∂s(j+1) l ∂s(j) ’s. ﬁrst write s(j+1) l = x(j)•w(j+1) l = mj+1 x ν=1 f (j) ν w(j+1) νl then, weights depend s’s ∂s(j+1) l ∂s(j) = ∂ hpmj+1 ν=1 f (j) ν w(j+1) νl ∂s(j) = mj+1 x ν=1 w(j+1) νl ∂f (j) ν ∂s(j) now, note ∂f (j) ν ∂s(j) = 0 ν = i, case ∂f (j) ν ∂s(j) ν = f (j) ν (1 −f (j) ν ).', 'therefore ∂s(j+1) l ∂s(j) = w(j+1) il f (j) (1 −f (j) )', 'ﬁnal output, f, depends s(j) summed inputs sigmoids (j + 1)-th layer.', 'neural networks 4.4.4 computing changes weights intermediate layers expression δ’s, similarly compute change weight vectors network.']]\n",
      "\n",
      "[['calculations simply implemented “backpropagating” δ’s weights reverse direction (thus, backprop algorithm).', 'quantity δ(k) = (d −f)f(1 −f) controls overall sign weight adjustments network. (', 'fall early error-function valley deep (a local minimum), typically broad, soon', 'average eﬀect depends weights going sigmoid unit j-th layer (small weights produce little downstream eﬀect) eﬀects changes outputs (j + 1)-th layer sigmoid units ﬁnal output (as measured δ(j+1)’s).', 'having computed δ(j+1) layer j + 1, use equation compute δ(j) ’s. base case δ(k), computed δ(k) = (d −f (k))f (k)(1 −f (k)) use expression δ’s generic weight changing rule, namely w(j) ←w(j) + c(j) δ(j) x(j−1) rule appears complex, intuitively reasonable explanation.', 'it interesting note expression independent error function; error function explicitly aﬀects computation δ(k).)', 'adjustments diminish ﬁnal output, f, approaches 0 1, vanishing eﬀect f then.)', 'recursion equation δ’s shows, adjustments weights going sigmoid unit j-th layer proportional eﬀect adjustments sigmoid unit’s output (its f (j)(1−f (j)) factor).', 'training feedforward networks backpropagation59 use result expression δ(j) give δ(j) = f (j) (1 −f (j) ) mj+1 x l=1 δ(j+1) l w(j+1) il equation recursive δ’s. (', '1986, [hertz, krogh, & palmer, 1991]), quickprop, regulariza- tion methods] simulated annealing apply simulated annealing, value learning rate constant gradually decreased time.'], ['4.4.5 variations backprop [to written problem local minima, simulated annealing, momemtum (plaut, et al.,']]\n",
      "\n",
      "[['sharp left sharp right straight ahead centroid outputs steers vehicle figure 4.20 alvinn network network ﬁve hidden units ﬁrst layer 30 output units second layer; sigmoid units.', 'unit near array output units higher output units, van steered left; unit near array high output, van steered right. “', '4.4.6 application steering van neural network system called alvinn (autonomous land vehicle neural network) trained steer chevy van successfully ordinary roads highways speeds 55 mph [pomerleau, 1991, pomerleau, 1993].', 'process gets analogy annealing metallurgy, material’s temperature gradually decreased allowing crystalline structure reach minimal energy state.', '5 hidden units connected 960 inputs 30 output units connected hidden units . . .', 'likely deep valleys, end process (with small values learning rate constant), descend deepest point.']]\n",
      "\n",
      "[['4.5 synergies neural network knowledge-based methods written; discuss rule-generating procedures (such [towell & shavlik, 1992]) expert-provided rules aid neural net training vice-versa [towell, shavlik, & noordweier, 1990].', 'also, long, straight stretches road, network trained long time produce straight-ahead steering angles; training swamp earlier training follow curved road.', 'network trained incrementally backprop produce driver-speciﬁed steering angles response visual pattern occurs real time driving.', 'driver drives van, actual steering angles taken correct labels corresponding inputs.', 'synergies neural network knowledge-based methods61 units computed, van’s steering angle set corresponding value hard left hard right.', 'wouldn’t want try avoid problems instructing driver drive erratically occasionally, system learn mimic erratic behavior.', 'instead, original image shifted rotated software create 14 additional images vehicle appears situated diﬀerently relative road.', 'model tells system steering angle ought shifted images, given driver-speciﬁed steering angle original image, system constructs additional 14 labeled training patterns add encountered ordinary driver training.', 'first, driver usually driving well, network experience far-from-center vehicle positions and/or incorrect vehicle orien- tations.']]\n",
      "\n",
      "[]\n",
      "\n",
      "[['given pattern, x, want use statistical tech- niques determine category—that is, determine distribution drawn.', 'chapter 5 statistical learning 5.1 statistical decision theory 5.1.1 background general method suppose pattern vector, x, random variable probability distri- bution category 1 diﬀerent category 2. (', 'describe information loss function, λ(i | j), i, j = 1, 2.', 'given pattern, x, want decide category way minimizes expected value loss.', 'λ(i | j) represents loss incurred decide pattern category category j. assume λ(1 | 1) λ(2 | 2) 0.', 'speciﬁcally, suppose probability distributions (perhaps probability density functions), p(x | 1) p(x | 2).', 'developing decision method, necessary know relative serious- ness kinds mistakes made. (', 'techniques based idea minimizing ex- pected value quantity similar error function deriving weight-changing rules backprop.', 'given pattern, x, decide category i, expected value loss be lx(i) = λ(i | 1)p(1 | x) + λ(i | 2)p(2 | x) p(j | x) probability given pattern x, category j. decision rule decide x belongs category 1 lx(1) ≤lx(2), decide category 2 otherwise.']]\n",
      "\n",
      "[['generally, deﬁne k(i | j) λ(i | j)p(j), decision rule simply, decide category1 iﬀ k(1 | 2)p(x | 2) ≤k(2 | 1)p(x | 1) case, need compare (perhaps weighted) quantities p(x | i) = 1 2.', 'statistical learning use bayes’ rule expressions p(j | x) terms p(x | j), assume known (or estimatible) p(j | x) = p(x | j)p(j) p(x) p(j) (a priori) probability category j (one category probable other); p(x) (a priori) probability pattern x pattern asked classify.', 'performing substitutions given bayes’ rule, decision rule becomes decide category 1 iﬀ λ(1 | 1)p(x | 1)p(1) p(x) + λ(1 | 2)p(x | 2)p(2) p(x) ≤λ(2 | 1)p(x | 1)p(1) p(x) + λ(2 | 2)p(x | 2)p(2) p(x) fact λ(i | i) = 0, noticing p(x) common expressions, obtain, decide category 1 iﬀ λ(1 | 2)p(x | 2)p(2) ≤λ(2 | 1)p(x | 1)p(1) λ(1 | 2) = λ(2 | 1) p(1) = p(2), decision particu- larly simple decide category 1 iﬀ p(x | 2) ≤p(x | 1) p(x | j) called likelihood j respect x, simple decision rule implements called maximum-likelihood decision.']]\n",
      "\n",
      "[['statistical decision theory 65 5.1.2 gaussian (or normal) distributions multivariate (n-dimensional) gaussian distribution given proba- bility density function p(x) = 1 (2π)n/2|σ|1/2 e −(x−m)tς −1 (x−m) 2 n dimension column vector x, column vector m called mean vector, (x −m)t transpose vector (x −m), σ covariance matrix distribution (an n × n symmetric, positive deﬁnite matrix), σ−1 inverse covariance matrix, |σ| determinant covariance matrix.', 'general, formula exponent gaussian distribution positive deﬁnite quadratic form (that is, value positive); equi-probability contours hyper-ellipsoids n-dimensional space.', 'components covariance matrix given by σ2 ij = e[(xi −mi)(xj −mj)] particular, σ2 ii called variance xi.', 'covariance matrix diagonal, oﬀ-diagonal terms 0, major axes elliptical contours aligned coordinate axes.', 'formula appears complex, intuitive idea gaussian dis- tributions given n = 2.', 'suppose assume classes pattern vectors want distinguish distributed according gaussian distribution diﬀerent means covariance matrices.', 'is, class tends patterns clustered point n-dimensional space, class tends patterns clustered point.', 'three-dimensional plot distribution shown ﬁgure, contours equal probability shown bot- tom.']]\n",
      "\n",
      "[['statistical learning -5 0 5 -5 0 5 0 0.25 0.5 0.75 1 -5 0 5 -5 0 5 0 25 .5 75 1 -6 -4 -2 0 2 4 6 -6 -4 -2 0 2 4 6 x1 x2 p(x1,x2) 2 4 6 2 4 6 x1 x2 figure 5.1 two-dimensional gaussian distribution decide category 1 iﬀ 1 (2π)n/2|σ2|1/2 e−1/2(x−m2)tς −1 2 (x−m2) equal 1 (2π)n/2|σ1|1/2 e−1/2(x−m1)tς −1 1 (x−m1) category 1 patterns distributed mean covariance m1 σ1, respectively, category 2 patterns distributed mean covariance m2 σ2.']]\n",
      "\n",
      "[['statistical decision theory 67 -5 0 5 10 -5 0 5 10 0 0.25 0.5 0.75 1 -5 0 5 10 -5 0 5 10 0 25 .5 75 1 x1 x2 p(x1,x2) -5 -2.5 0 2.5 5 7.5 10 -5 -2.5 0 2.5 5 7.5 10 figure 5.2 sum gaussian distributions decide category 1 iﬀ (x −m1)tς−1 1 (x −m1) < (x −m2)tς−1 2 (x −m2) + b b, constant bias term, incorporates logarithms fractions preceding exponential, etc.', 'surface separates space parts, contains points assigned category 1 contains points assigned category 2.', 'covariance matrices category identical diagonal, σii equal other, contours equal probability distributions', 'quadratic forms multiplied represented terms components xi, decision rule involves quadric surface (a hyperquadric) n-dimensional space.']]\n",
      "\n",
      "[['caution sample covariance matrix singular training patterns happen lie subspace n-dimensional space—as certainly will, example, number training patterns n.) 5.1.3 conditionally independent binary components suppose vector x random variable having binary (0,1) components.', 'quadric forms (1/|σ|)(x−mi)t(x−mi), decision rule is decide category 1 iﬀ (x −m1)t(x −m1) < (x −m2)t(x −m2) multiplying yields x•x −2x•m1 + m1•m1 < x•x −2x•m2 + m2•m2 ﬁnally, decide category 1 iﬀ x•m1 ≥x•m2 + constant x•(m1 −m2) ≥constant constant depends lengths mean vectors.', 'parameters (mi, σi) probability distributions categories known, techniques estimating them, estimates decision rule.', 'example, suﬃcient training patterns, use sample means sample covariance matrices. (']]\n",
      "\n",
      "[['p(xn | 1) p(x1 | 2)p(x2 | 2) . . .', 'p(xn | 2) ≥p(2) p(1) iﬀ log p(x1 | 1) p(x1 | 2) + log p(x2 | 1) p(x2 | 2) + · · · + log p(xn | 1) p(xn | 2) + log p(1) p(2) ≥0 let deﬁne values components distribution speciﬁc values arguments, xi p(xi = 1 | 1) = pi p(xi = 0 | 1) = 1 −pi p(xi = 1 | 2) = qi p(xi = 0 | 2) = 1 −qi now, note xi assume values 1 0 log p(xi | 1) p(xi | 2) = xi log pi qi + (1 −xi) log (1 −pi) (1 −qi)', 'statistical decision theory 69 p(x | i) = p(x1 | i)p(x2 | i) · · · p(xn | i) = 1, 2 recall minimum-average-loss decision rule, decide category 1 iﬀ λ(1 | 2)p(x | 2)p(2) ≤λ(2 | 1)p(x | 1)p(1) assuming conditional independence components λ(1 | 2) = λ(2 | 1), obtain, decide category 1 iﬀ p(1)p(x1 | 1)p(x2 | 1) · · · p(xn | 1) ≥p(x1 | 2)p(x2 | 2) · · · p(xn | 2)p(2) iﬀ p(x1 | 1)p(x2 | 1) . . .']]\n",
      "\n",
      "[['relatively large values k decreases chance decision unduly inﬂuenced noisy training pattern close x. large values k reduce acuity method.', 'precisely, k-nearest-neighbor method assigns new pattern, x, category plurality k closest neighbors belong.', 'statistical learning = xi log pi(1 −qi) qi(1 −pi) + log (1 −pi) (1 −qi) substituting expressions decision rule yields decide category 1 iﬀ n x i=1 xi log pi(1 −qi) qi(1 −pi) + n x i=1 log (1 −pi) (1 −qi) + log p(1) p(2) ≥0 achieve decision tlu weight values follows wi = log pi(1 −qi) qi(1 −pi) = 1, . . . ,', 'n, wn+1 = log p(1) 1 −p(1) + n x i=1 log (1 −pi) (1 −qi) know pi, qi p(1), use sample labeled training patterns estimate parameters.', 'k-nearest-neighbor method thought estimating values probabilities classes given x. course denser points x, larger value k, better estimate.', 'given training set ξ m labeled patterns, nearest-neighbor procedure decides new pattern, x, belongs category closest neighbors ξ.']]\n",
      "\n",
      "[['also, distance calculations required ﬁnd nearest neighbors eﬃciently computed kd-tree methods [friedman, et al.,', 'theorem cover hart [cover & hart, 1967] relates performance 1-nearest-neighbor method performance minimum-probability- of-error classiﬁer.', 'mentioned earlier, minimum-probability-of-error clas- siﬁer assign new pattern x category maximized p(i)p(x | i), p(i) priori probability category i, p(x | i) probability (or probability density function) x given x belongs category i, categories = 1, . . . ,', 'distance measure modiﬁed scaling features spread attribute values dimension approximately same.', 'case, distance vectors qpn j=1 a2 j(x1j −x2j)2, aj scale factor dimension j. example nearest-neighbor decision problem shown fig.', 'k = 8 x (a pattern classified) 1 1 1 1 1 1 1 1 2 1 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 training pattern class training pattern patterns category 1 patterns category 2 patterns category 3 plurality category 1, decide x category 1 figure 5.3 8-nearest-neighbor decision [baum, 1994] theoretical analysis error rate function number training patterns case points randomly distributed surface unit sphere underlying function linearly separable.', 'nearest-neighbor methods 71 distance metric nearest-neighbor methods (for numerical at- tributes) simple euclidean distance.', 'nearest-neighbor methods memory intensive large number training patterns stored achieve good generalization.']]\n",
      "\n",
      "[['statistical learning probability density functions) probability error, εnn, 1-nearest- neighbor classiﬁer bounded by ε ≤εnn ≤ε \\x12 2 −ε r r −1 \\x13 ≤2ε r number categories. [']]\n",
      "\n",
      "[['chapter 6 decision trees 6.1 deﬁnitions decision tree (generally deﬁned) tree internal nodes tests (on input patterns) leaf nodes categories (of patterns).', 'dimensions decision trees diﬀer a. tests multivariate (testing features input once) univariate (testing features).', 'decision tree assigns class number (or output) input pattern ﬁltering pattern tests tree.', '6.1 outcomes; left-most assigns input pattern class 3, middle sends input pattern test t4, right-most assigns pattern class 1.', 'calls subsets patterns ﬁlter tip categories subsets patterns having label classes.', 'distinction, however, use words “category” “class” interchangeably refer quinlan calls “class.”', 'follow usual convention depicting leaf nodes class number.1 note discussing decision trees limited implementing boolean functions—they useful general, categorically valued functions.']]\n",
      "\n",
      "[['prominent id3 new version, c4.5 [quinlan, 1986, quinlan, 1993], cart [breiman, et al.,', 'classes binary inputs, tree implements boolean function, called boolean decision tree.', 'k-dl class boolean functions implemented multivariate decision tree having (highly unbalanced) form shown fig.', 'decision trees t1 t2 t3 t4 t4 t4 3 1 3 2 1 2 3 2 1 figure 6.1 decision tree d. classes two.', 'attribute value 0 input pattern, branch left; value 1, branch right.', 'dnf form implemented tree obtained tracing path leading tip node corresponding output value 1, forming conjunction tests path, taking disjunction conjunctions.', '6.2 supervised learning univariate decision trees systems learning decision trees proposed.']]\n",
      "\n",
      "[['entropy uncertainty remaining class pattern— knowing set, ξ, patterns deﬁned as h(ξ) = − x p(i|ξ) log2 p(i|ξ)', 'extension multiple-outcome tests straightforward computation- ally gives poor results entropy decreased having outcomes.', 'supervised learning univariate decision trees 75 x3 x2 x4 x1 1 0 1 1 0 0 0 1 x3x2 x3x2 x3x4 x3x4x1 x3x4x1 f = x3x2 + x3x4x1 1 0 0 1 0 figure 6.2 decision tree implementing dnf function 6.2.1 selecting type test usual, n features attributes.', '6.2.2 uncertainty reduction select tests main problem learning decision trees binary-attribute case selecting order tests.', 'attributes categorical, non-binary, tests formed dividing attribute values mutually exclusive exhaustive subsets.']]\n",
      "\n",
      "[['want select tests node travel decision tree, uncertainty class pattern less.', 'suppose ni patterns ξ ξi = 1, ..., k. (some ni 0.)', 'perform test, t, having k possible outcomes patterns ξ, create k subsets, ξ1, ξ2, . . . ,', 'estimate uncertainty is ˆh(ξ) = − x ˆp(i|ξ) log2 ˆp(i|ξ) simplicity, we’ll drop “hats” use sample statistics real probabilities.', 'let ˆp(i|ξ) number patterns ξ belonging class divided total number patterns ξ.', 'decision trees cq cq-1 ci 1 vn vn-1 vi v1 figure 6.3 decision tree implementing decision list p(i|ξ) probability pattern drawn random ξ belongs class i, summation classes.', 'knew t applied pattern ξ resulted j-th outcome (that is, knew pattern ξj), uncertainty class be h(ξj) = − x p(i|ξj) log2 p(i|ξj) reduction uncertainty (beyond knowing pattern ξ) be']]\n",
      "\n",
      "[['estimate ˆp(ξj) p(ξj) number patterns ξ outcome j divided total number patterns ξ.', 'supervised learning univariate decision trees 77 x3 = a, b, c, d {a, c} {b} x1 = e, b, d {e,b} {d} x4 = a, e, f, g {a, g} {e, f} x2 = a, g {a} {g} 1 2 1 1 2 {d} 2 figure 6.4 decision tree categorical attributes h(ξ) −h(ξj) course test t guaranteed produce reduction uncertainty don’t know result test j-th outcome.', 'average reduction uncertainty achieved test t (applied patterns ξ) then rt (ξ) = h(ξ) −e[ht (ξ)] important family decision tree learning algorithms selects root tree test gives maximum reduction uncertainty, applies criterion recursively termination condition met (which shall discuss detail later).', 'estimate average uncertainty ξj, by e[ht (ξ)] = x j p(ξj)h(ξj) ht (ξ) mean average uncertainty performing test t patterns ξ, p(ξj) probability test outcome j, sum taken 1 k. again, don’t know probabilities p(ξj), use sample values.']]\n",
      "\n",
      "[['x1 x2 x3 test x1 figure 6.5 patterns classiﬁed decision tree initial uncertainty set, ξ, containing points is h(ξ) = −(6/8) log2(6/8) −(2/8) log2(2/8) = 0.81 next, calculate uncertainty reduction perform x1 ﬁrst.', 'left- hand branch patterns belonging class 0 (we set ξl), right-hand-branch (ξr) patterns class.', 'suppose want use uncertainty-reduction method build decision tree classify following patterns pattern class (0, 0, 0) 0 (0, 0, 1) 0 (0, 1, 0) 0 (0, 1, 1) 0 (1, 0, 0) 0 (1, 0, 1) 1 (1, 1, 0) 0 (1, 1, 1) 1 single test, x1, x2, x3, performed ﬁrst?']]\n",
      "\n",
      "[['principle, given set labeled patterns, measure uncertainty reduc- tion test achieved possible threshold (there ﬁnite number thresholds diﬀerent test results ﬁnite number training patterns).', 'networks equivalent decision trees 79 hx1(ξl) = −(4/4) log2(4/4) −(0/4) log2(0/4) = 0 uncertainty right-hand branch is hx1(ξr) = −(2/4) log2(2/4) −(2/4) log2(2/4) = 1 half patterns “go left” half “go right” test x1.', 'thus, average uncertainty performing x1 test is 1/2hx1(ξl) + 1/2hx1(ξr) = 0.5 uncertainty reduction ξ achieved x1 is rx1(ξ) = 0.81 −0.5 = 0.31 similar calculations, test x3 achieves exactly uncertainty reduction, x2 achieves reduction whatsoever.', '6.2.3 non-binary attributes attributes non-binary, use uncertainty-reduction tech- nique select tests.', 'similarly, attribute categorical (with ﬁnite number categories), ﬁnite number mutually exclusive exhaustive subsets values attribute split.', 'suppose example value at- tribute real number test performed set threshold test number greater threshold.', '6.3 networks equivalent decision trees univariate boolean decision trees implementations dnf functions, equivalent two-layer, feedforward neural networks.', 'decision tree procedure creates implements boolean function f = x1x3. [']]\n",
      "\n",
      "[['6.7 linearly separable functions, im- plemented tlu, indicated l1, l2, l3, l4.', 'dif- ferent approaches training procedures discussed [brent, 1990], [john, 1995], (for special case) [marchand & golea, 1993].', 'decision-tree induction methods discussed chapter thought particular ways establish structure weight values networks.', 'x x1 x2 x3 x4 terms -1 +1 disjunction x3x2 x3x4x1 +1 -1 +1 f 1.5 0.5 x3 x2 x4 x1 1 0 1 1 0 0 0 1 x3x2 x3x2 x3x4 x3x4x1 x3x4x1 f = x3x2 + x3x4x1 1 0 0 1 0 figure 6.6 univariate decision tree equivalent network multivariate decision trees linearly separable functions node implemented feedforward networks—in case three-layer ones.', '6.4 overﬁtting evaluation 6.4.1 overﬁtting supervised learning, choose function ﬁt training set set hypotheses.', 'know priori function trying guess belongs small subset possible functions, then, incomplete set training samples, possible reduce subset functions consistent training set suﬃciently useful guesses value function inputs training set.', 'course, implemented network, features evaluated parallel input pattern, implemented decision tree features branch traveled input pattern need evaluated.']]\n",
      "\n",
      "[['however, bias, training set suﬃciently large compared size hypothesis space, consistent functions useful guesses, generalization performance poor.', 'but, comparing learning systems (for example, comparing diﬀerent decision trees) select performs best test set, comparison amounts “training test data.”', 'true, training test data enlarges training set, consequent ex- pected improvement generalization, danger overﬁtting comparing diﬀerent learning systems.', 'is, decision tree synthesized classify members training set correctly, perform poorly new patterns build decision tree.', '6.4.2 validation methods straightforward way estimate hypothesized function (such decision tree) performs test set test test set!', 'overfitting evaluation 81 l1 l2 l3 l4 1 0 1 1 0 0 0 1 1 0 0 1 0 x l1 l2 l3 l4 conjunctions l1l2 l1 l3 l4 < + + + disjunction < f figure 6.7 multivariate decision tree equivalent network larger training set, likely randomly selected consistent function appropriate outputs patterns seen.', 'decision tree suﬃcient size implement boolean function danger overﬁtting—especially training set small.']]\n",
      "\n",
      "[['procedure result errors accepting small number errors training set results fewer errors testing set.', 'cross-validation cross-validation, divide training set ξ k mutually exclusive exhaustive equal-sized subsets ξ1, . . . ,', 'subset, ξi, train union subsets, empirically determine error rate, εi, ξi. (', 'the error rate number classiﬁcation errors ξi divided number patterns ξi.)', 'decision trees split training set—using (say) two-thirds training estimating generalization performance.', 'estimate error rate expected new patterns classiﬁer trained patterns ξ average εi.', 'leave-one-out validation leave-one-out validation cross validation special case k equals number patterns ξ, ξi consists single pattern.', '6.4.3 avoiding overﬁtting decision trees near tips decision tree patterns node.', 'general rule lowest error-rate attainable sub-tree fully expanded tree 1/2 error rate fully expanded tree [weiss & kulikowski, 1991, page 126].', 'type validation is, course, expensive computationally, useful accurate estimate error rate classiﬁer needed.']]\n",
      "\n",
      "[['consider problem transmitting labels training set patterns, assuming receiver information ordered set patterns.', 'number bits transmission require depends technique encoding decision trees size tree.', 'tree small accurately classiﬁes patterns, economical transmit tree transmit labels directly.', '6.4.4 minimum-description length methods important tree-growing pruning technique based minimum- description-length (mdl) principle. (', 'general, number bits (or description length binary encoded message) t + d, t length message required transmit tree, d length message required transmit labels', 'overfitting evaluation 83 (from weiss, s., kulikowski, c., computer systems learn, morgan kaufmann, 1991) training errors validation errors 1 2 3 4 5 6 7 8 9 0.2 0.4 0.6 0.8 1.0 0 0 error rate number terminal nodes iris data decision tree figure 6.8 determining overﬁtting begins stopping growth decision tree, grow size prune away leaf nodes ancestors cross- validation accuracy longer increases.']]\n",
      "\n",
      "[['sense, tree associated smallest value t + d best economical tree.', '6.4.5 noise data noise data means inevitably accept number errors—depending noise level.', 'dealing noise, then, requires accepting errors leaf nodes fact small number patterns leaf nodes.', 'use description length measure quality tree ways a. growing tree, use reduction description length select tests (instead reduction uncertainty).', 'b. pruning tree grown zero error, prune away nodes (starting tips) achieve decrease description length.', 'dnf-form equivalent function implemented decision tree f = x1x2 + x1x2x3x4 + x1x3x4.', '6.5 problem replicated subtrees decision trees economical means implementing boolean functions.', 'quinlan rivest [quinlan & rivest, 1989] proposed techniques encoding decision trees lists exception labels calculating description length (t+d) trees labels.', 'dnf form non-minimal (in number disjunctions) equivalent f = x1x2 + x3x4.', 'need replication means takes longer learn tree subtrees replicated tree learned smaller training subset.'], ['attempt build decision graph instead tree [oliver, dowe, & wallace, 1992, kohavi, 1994].']]\n",
      "\n",
      "[['problem missing attributes 85 x1 x3 x2 1 0 x4 0 1 x3 0 x4 0 1 figure 6.9 decision tree subtree replication test x3x4, decision tree simpliﬁed, shown fig.', 'rule set processed, case rule “active” given pattern, care taken active rules conﬂict decision class pattern.', 'john, 1995] gives nice overview (with citations) learning linear discriminant trees presents method based “soft entropy.”', 'researchers proposed techniques learning decision trees tests node linearly separable functions. [', 'conjunct rule determined unnecessary elimination little eﬀect classiﬁcation accuracy—as determined chi-square test, example.', 'quinlan [quinlan, 1987] discusses methods reducing set rules sim- pler set 1) eliminating antecedent rule “unnecessary” conjuncts, 2) eliminating “unnecessary” rules.', 'method dealing replicated subtree problem involves ex- tracting propositional “rules” decision tree.', 'rules an- tecedents conjunctions lead leaf nodes, consequents class corresponding leaf node.', 'example rule tree repeating subtree example be x1 ∧¬x2 ∧x3 ∧x4 ⊃1.']]\n",
      "\n",
      "[['statlog project, [taylor, michie, & spiegalhalter, 1994] thorough comparisons machine learning algorithms diﬀerent types problems.', '6.7 comparisons experimenters compared decision-tree, neural-net, nearest- neighbor classiﬁers wide variety problems.', 'decision trees x1 x3 x2 1 0 x4 0 1 figure 6.10 decision graph 6.6 problem missing attributes added.']]\n",
      "\n",
      "[['and, general conclusions enable classiﬁer method best sorts classiﬁcation problems, [quinlan, 1994] provide intuition properties problems render ill suited decision trees, hand, backpropa- gation, other.']]\n",
      "\n",
      "[]\n",
      "\n",
      "[['far, seen (boolean) algebraic expressions, decision trees, neural networks, plus computational mechanisms techniques computing nearest neighbors.', 'chapter 7 inductive logic programming diﬀerent representational forms functions input vari- ables.', 'chapter, consider matter learning logic programs given set variable values logic program return t (the positive instances) set variable values return f (the negative instances).', 'unary function “true” returns t value argument t. (we think boolean functions arguments having values t f instead 0 1.)', 'example, boolean exclusive-or (odd parity) function variables computed following logic program parity(x,y) - true(x), ¬ true(y) - true(y), ¬ true(x) follow prolog syntax (see, example, [mueller & page, 1988]), convention write variables strings beginning lower-case letters predicates strings beginning upper-case letters.', 'similarly, logic program (whose ordinary application compute bindings variables) simply decide predicate value true (t) false (f).', 'subspecialty machine learning deals learning logic programs called inductive logic programming (ilp) [lavraˇc & dˇzeroski, 1994].']]\n",
      "\n",
      "[['following terminology introduced connection version spaces, program suﬃcient covers positive instances necessary cover neg- ative instances. (', 'logic program, π, returns t set arguments x, program covers arguments write covers(π, x).', 'example ilp problem, suppose trying induce func- tion nonstop(x,y), value t pairs cities connected non-stop air ﬂight f pairs cities.', 'depending exact set examples, induce program nonstop(x,y) - hub(x), hub(y) - satellite(x,y) - satellite(y,x) value t cities hub cities satellite other.', 'positive examples, (a,b), (a, a1), pairs; negative examples, (a1, a2), pairs.', 'learning problems, want induced program generalize well; is, presented arguments represented training set (but needed background knowledge), like function guess well.', 'that is, program implements suﬃcient condition training instance positive covers positive training instances;', '7.1 notation deﬁnitions evaluating logic programs ilp, implicitly append background facts program adopt usual convention program value t set inputs program interpreter returns t actually running program (with background facts appended) inputs; oth- erwise value f. given background facts, program return t input (a, a1), example.', 'hub(a) intended mean city denoted hub city, satellite(a1,a) intended mean city denoted a1 satellite city denoted a.) train- ing facts, want induce program nonstop(x,y), written terms background relations hub satellite, value t positive instances value f negative instances.', 'air-ﬂight problem, background information ground facts hub(a), hub(b), satellite(a1,a), plus others. (']]\n",
      "\n",
      "[['diﬀerent ways search consistent logic program are 1) start [ρ - ] specialize program consistent, 2) start [ρ - f ] generalize program consistent.', '< < < < < < < /1 necessary program /2 sufficient program /3 consistent program + + + + + + + + + + < < positive instance covered /2 /3 figure 7.1 suﬃcient, necessary, consistent programs version spaces, program suﬃcient necessary cover fewer examples specializing it.', 'discussing method starts [ρ - ], specializes program necessary (but longer suﬃcient), reachieves suﬃciency stages generalizing—ensuring stage program remains necessary (by specializing).', 'special logic program, certainly necessary, value f inputs, [ρ - f ].', 'general logic program, certainly suﬃcient, value t inputs, single clause body, [ρ - ], called fact prolog.', '7.2 generic ilp algorithm primary operators search consistent program special- ization generalization, discuss operations.', 'imperfect (noisy) training sets, relax criterion settle program covers fraction positive instances allowing cover fraction negative instances.']]\n",
      "\n",
      "[['c. add clause program analogously, ways logic program specialized a. replace variables program clause terms (a substitution).', 'special case, use here, clause c1 special clause c2 set literals body c2 subset c1.', 'ordering relation structure partially ordered clauses, called reﬁnement graph, similar version space.', 'inductive logic programming major ways logic program generalized a. replace terms program clause variables. (', 'add clause, add clause [ρ - ] specialize adding literals body.', 'clause c1 immediate successor clause c2 graph clause c1 obtained clause c2 adding literal body c2.', 'd. literal equates variable head clause variable term mentioned background knowledge. (', 'c. remove clause program presenting ilp learning method adds clauses program generalizing adds literals body clause special- izing.']]\n",
      "\n",
      "[['given training set, ξ argument sets known relation ρ ρ; ξ+ positive instances, ξ−are negative instances.', 'algorithm written follows generic ilp algorithm (adapted [lavraˇc & dˇzeroski, 1994, p. 60].)', 'ilp programs follow approach discussing (of specializing clauses adding literal) deﬁned methods computing possible literals add clause.', 'restrictions additional literals imposed, syntactic ones successors reﬁne- ment graph easily computed.', 'ready write simple generic algorithm inducing logic program, π inducing relation ρ.', 'the positive instances ξcur denoted ξ+ cur, negative ones ξ− cur.)', 'literals consider adding are hub(x) hub(y) hub(z) satellite(x,y) satellite(y,x) satellite(x,z) satellite(z,y) (x = y) (if recursive programs allowed, add literals nonstop(x,z) nonstop(z,y).)', 'generic ilp algorithm 93 e. literal (except arguments) head clause. (', 'inner loop constructing clause, c, necessary refers subset, ξcur, training instances. (']]\n",
      "\n",
      "[['inductive logic programming nonstop(x,y) - nonstop(x,y) - hub(x) nonstop(x,y) - satellite(x,y) nonstop(x,y) - (x = y) . . . . . . . . . . . .', 'initialize c = ρ −. repeat [the inner loop makes c necessary.]', 'cities a, b, c “hub” cities, know nonstop ﬂights hub cities (even shown portion route map).', 'the termination tests inner outer loops relaxed appro- priate case noisy instances.)', 'nonstop(x,y) - hub(x), hub(y) . . . . . . . . .']]\n",
      "\n",
      "[['ξ+ contains pairs {< a, b >, < a, c >, < b, c >, < b, >, < c, >, < c, b >, < a, a1 >, < a, a2 >, < a1, >, < a2, >, < b, b1 >, < b, b2 >, < b1, b >, < b2, b >, < c, c1 >, < c, c2 >, < c1, c >, < c2, c >} example, assume ξ−contains pairs cities shown fig.', 'are {< a, b1 >, < a, b2 >, < a, c1 >, < a, c2 >, < b, c1 >, < b, c2 >, < b, a1 >, < b, a2 >, < c, a1 >, < c, a2 >, < c, b1 >, < c, b2 >, < b1, >, < b2, >, < c1, >, < c2, >, < c1, b >, < c2, b >, < a1, b >, < a2, b >, < a1, c >, < a2, c >, < b1, c >, < b2, c >} cities shown map, training set necessarily exhaust cities.', 'example 95 cities “satellites” hubs, know nonstop ﬂights satellite city hub.', 'learning program given set positive instances, ξ+, pairs cities nonstop ﬂights set negative instances, ξ−, pairs cities nonstop ﬂights.', 'b c c1 c2 b1 b2 a1 a2 figure 7.3 airline route map want learning program induce program computing value relation nonstop.']]\n",
      "\n",
      "[['satellite {< a1, a, >, < a2, >, < b1, b >, < b2, b >, < c1, c >, < c2, c >} pairs cities mentioned map relation satellite.', 'inductive logic programming description relation extensional form—it explicitly names pairs relation pairs relation.', 'use notation satellite(x,y) express pair < x, y > relation satellite.', 'desire learn nonstop relation logic program terms background relations, hub satellite, given extensional form.', 'knowing predicate nonstop two-place predicate, inner loop algorithm initializes ﬁrst clause nonstop(x,y) - .', 'following positive instances ξ covered nonstop(x,y) - hub(x) {< a, b >, < a, c >, < b, c >, < b, >, < c, >, < c, b >, < a, a1 >, < a, a2 >, < b, b1 >, < b, b2 >, < c, c1 >, < c, c2 >} compute covering, interpret logic program nonstop(x,y) - hub(x) pairs cities ξ, pairs given background relation hub ground facts.', 'assume learning program following extensional deﬁnitions relations hub satellite hub {< >, < b >, < c >} cities mentioned map assumed relation hub.']]\n",
      "\n",
      "[['program contains clauses nonstop(x,y) - hub(x), hub(y) - satellite(x,y) program suﬃcient cover following positive instances {< a, a1 >, < a, a2 >, < b, b1 >, < b, b2 >, < c, c1 >, < c, c2 >}', 'positive instances covered clause {< a, a1 >, < a, a2 >, < a1, >, < a2, >, < b, b1 >, < b, b2 >, < b1, b >, < b2, b >, < c, c1 >, < c, c2 >, < c1, c >, < c2, c >} positive instances covered nonstop(x,y) - hub(x), hub(y) removed ξ form ξcur pass inner loop.', 'cover following positive instances ξcur {< a1, >, < a2, >, < b1, b >, < b2, b >, < c1, c >, < c2, c >} instances removed ξcur pass inner loop.', 'ξcur consists negative instances ξ plus positive instances (listed above) covered.', 'example 97 {< a, b1 >, < a, b2 >, < a, c1 >, < a, c2 >, < c, a1 >, < c, a2 >, < c, b1 >, < c, b2 >, < b, a1 >, < b, a2 >, < b, c1 >, < b, c2 >} thus, clause necessary literal added.', 'following positive instances covered nonstop(x,y) - hub(x), hub(y) {< a, b >, < a, c >, < b, c >, < b, >, < c, >, < c, b >} longer negative instances ξ covered clause nonstop(x,y) - hub(x), hub(y) necessary, terminate ﬁrst pass inner loop.', 'order attempt cover them, inner loop creates clause c, initially set nonstop(x,y) - .']]\n",
      "\n",
      "[['relation canfly satisﬁed following pairs postive instances {< b1, b >, < b1, b2 >, < b1, c >, < b1, c1 >, < b1, c2 >, < b, b1 >, < b2, b1 >, < c, b1 >, < c1, b1 >, < c2, b1 >, < b2, b >, < b2, c >, < b2, c1 >, < b2, c2 >, < b, b2 >, < c, b2 >, < c1, b2 >, < c2, b2 >, < b, c >, < b, c1 >, < b, c2 >, < c, b >, < c1, b >, < c2, b >, < c, c1 >, < c, c2 >, < c1, c >, < c2, c >, < c1, c2 >, < c2, c1 >}', 'again, b c hub cities, b1 b2 satellites b, c1 c2 satellites c. introduced new cities, b3 c3.', '7.4 inducing recursive programs induce recursive program, allow addition literal having predicate letter head clause.', 'note program applied (perhaps good generalization) cities be- sides partial map—so long evaluate relations hub satellite cities.', 'inductive logic programming pass inner loop, add clause nonstop(x,y) - satellite(y,x).', 'mechanisms ensure program terminate; sure new literal diﬀerent variables head literal.', 'clause necessary, program containing clauses suﬃcient, procedure terminates with nonstop(x,y) - hub(x), hub(y) - satellite(x,y) - satellite(y,x) clause necessary, program suﬃcient, pro- gram consistent instances training set.', 'example continues airline map, map somewhat simpler order reduce size extensional relations used.']]\n",
      "\n",
      "[['suﬃcient cover following positive instances {< b1, b2 >, < b1, c >, < b1, c1 >, < b1, c2 >, < b2, b1 >, < c, b1 >, < c1, b1 >, < c2, b1 >, < b2, c >, < b2, c1 >, < b2, c2 >, < c, b2 >, < c1, b2 >, < c2, b2 >, < b, c1 >,', 'inducing recursive programs 99 b c c1 c2 b1 b2 b3 c3 figure 7.4 airline route map closed-world assumption map, negative instances canfly be {< b3, b2 >, < b3, b >, < b3, b1 >, < b3, c >, < b3, c1 >, < b3, c2 >, < b3, c3 >, < b2, b3 >, < b, b3 >, < b1, b3 >, < c, b3 >, < c1, b3 >, < c2, b3 >, < c3, b3 >, < c3, b2 >, < c3, b >, < c3, b1 >, < c3, c >, < c3, c1 >, < c3, c2 >, < b2, c3 >, < b, c3 >, < b1, c3 >, < c, c3 >, < c1, c3 >, < c2, c3 >} induce canfly(x,y) extensionally deﬁned background relation nonstop given earlier (modiﬁed required reduced airline map) canfly (recursively).']]\n",
      "\n",
      "[['measure gives comparison quinlan’s based adding literal increases odds instance drawn random covered new clause positive instance odds adding literal.', 'inductive logic programming < b, c2 >, < c1, b >, < c2, b >, < c1, c2 >, < c2, c1 >} thus, add clause program.', 'interpreter attempts estab- lish nonstop(b3,z) z. background facts match, clause cover < b3, b >.', 'program suﬃcient consistent; is canfly(x,y) - nonstop(x,y) - nonstop(x,z), canfly(z,y) 7.5 choosing literals add ﬁrst practical ilp systems quinlan’s foil [quinlan, 1990].', 'foil, quinlan suggested candidate literals compared information-like measure—similar measures inducing decision trees.', 'is, p =(number positive instances covered clause)/(total number instances covered clause).', 'interpreter, clause canfly(x,y) - nonstop(x,z) covers positive instances covered ﬁrst clause, covers negative instances < b2, b3 >, < b, b3 >.', 'interpreter attempts establish nonstop(b1,z) z. nonstop(b1, b), example, background fact, interpreter returns t—which means instance < b1, b2 > covered.', 'inner loop, ﬁrst create clause canfly(x,y) - nonstop(x,z) introduces new variable z. digress brieﬂy describe program containing clause unbound variables body interpreted.', 'major problem involves deciding select literal add inner loop (from literals allowed).'], ['let p estimate probability instance drawn random covered clause adding literal positive instance.']]\n",
      "\n",
      "[['splitting single variable, split node involves asking mutually exclusive exhaustive subsets value variable belongs.', 'let pl denote probability instance drawn random instances covered new clause (with l added) positive.', 'reader refer [pazzani & kibler, 1992, lavraˇc & dˇzeroski, 1994, muggleton, 1991, muggleton, 1992].', 'quinlan discusses post-processing pruning methods presents experi- mental results method applied learning recursive relations lists, learning rules chess endgames card game eleusis, standard tasks mentioned machine learning literature.', 'relationships ilp decision tree induction101 selecting literal, l, add clause, instances previously covered covered; positive negative.', '7.6 relationships ilp decision tree induction generic ilp algorithm understood type decision tree induction.', 'example, node tested variable xi, xi values drawn {a, b, c, d, e, f}, possible split (among many) according value xi value {a, b, c} {d, e, f}.', 'ﬁnding literal high value λl, quinlan’s foil system restricts choice literals that a) contain variable used, b) place restrictions variables literal selected predicate letter literal induced (in order prevent inﬁnite recursion), c) survive pruning test based values λl literals selected far.', 'it turns value quinlan’s information theoretic measure increases monotonically λl, use instead.)', 'example, node tested variables xi xj, xi xj values drawn {a, b, c, d, e, f}, possible binary split'], ['specializing clause way fails cover negative instances previously covered covers positive instances previously covered result high value λl. (', 'is, deﬁne λl = ol/o, want literal gives high value λl.']]\n",
      "\n",
      "[['intensional deﬁnition terms logic program relation r head set clauses bodies involve background relations.', 'background relation r1 satisﬁed patterns; ﬁltered right (to relation r2), rest ﬁltered left (more happens later).', 'relation, r, distinguishes positive negative patterns ξ given terms following logic program r - r1, r2, r3', 'desire construct intensional deﬁnition r terms r1, . . . ,', 'inductive logic programming (among many) according < xi, xj > satisﬁed relation {< a, c >, < c, d >}. (', 'note subset method forming single- variable splits equivalently framed 1-ary relations—which usually called properties.)', 'actually, decision trees decision lists—a special case decision trees, refer trees discussions.)', 'broad outline, method inducing intensional version rela- tion r illustrated considering decision tree shown fig.', 'framework, ilp problem follows given training set, ξ, positively negatively labeled patterns components drawn set variables {x, y, z, . . .}.', 'right-going patterns ﬁltered sequence relational tests positively labeled patterns sat- isfy relation—in case r3.'], ['correspond clause created ﬁrst pass inner loop generic ilp algo- rithm.)', 'is, subset patterns satisfying relations, r1, r2, r3 contains positive instances ξ. (', 'rk, positively labeled patterns ξ satisﬁed r negatively labeled patterns are.', 'diagram, patterns ξ ﬁrst ﬁltered decision tree top- level node 1.', 'positively labeled patterns ξ form extensional deﬁnition relation, r. given background relations, r1, . . . ,', 'ξ2 ﬁltered top-level node 2 manner, node 2 satisﬁed positively labeled samples ξ2.', 'generic ilp algorithm understood decision tree induction, node decision tree sub-decision tree, sub- decision tree consists nodes binary splits variables background relations, ri.', 'example, ξ4 contains negatively labeled patterns union ξ1 ξ3 contains positively labeled patterns.']]\n",
      "\n",
      "[['setting problem, training set, ξ expressed set 2- dimensional vectors components x y. values components range cities {a, b, c, a1, a2, b1, b2, c1, c2} (for simplicity) allow patterns x y value.', 'relationships ilp decision tree induction103 r1 r2 r3 t t t f f f t f r4 r5 t t f f t f u u1 u2 = u < u1 u3 u4= u2 < u3 node 1 node 2 (only positive instances satisfy tests) (only positivel instances satisfy tests) (only negative instances) figure 7.5 decision tree ilp - r4, r5 apply sort decision-tree induction procedure problem generating logic program relation nonstop (refer fig.', 'values x y categorical, decision-tree induction diﬃcult task—involving need invent relations', 'before, relation, nonstop, contains following pairs cities, positive instances {< a, b >, < a, c >, < b, c >, < b, >, < c, >, < c, b >, < a, a1 >, < a, a2 >, < a1, >, < a2, >, < b, b1 >, < b, b2 >, < b1, b >, < b2, b >, < c, c1 >, < c, c2 >, < c1, c >, < c2, c >} pairs cities named map fig.']]\n",
      "\n",
      "[['select relations way select literals; available tests, selection based leads largest value λri.']]\n",
      "\n",
      "[['bibliographical historical remarks 105 hub(x) t f u node 1 (top level) {<a,b>, <a,c>, <b,c>, <b,a>, <c,a>, <c,b>} hub(y) t t f node 2 (top level) satellite(x,y) f t t {<a1,a>, <a2,a>, <b1,b>, <b2,b>, <c1,c>, <c2,c>} f {<a,a1>, <a,a2>,<b,b1>, <b,b2>, <c,c1>, <c,c2>} satellite(y,x) f f t node 3 (top level) t {only negative instances} (only positive instances) (only positive instances) (only positive instances) f figure 7.6 decision tree airline route problem']]\n",
      "\n",
      "[]\n",
      "\n",
      "[['8.1 notation assumptions pac learn- ing theory assume training set ξ n-dimensional vectors, xi, = 1, . . . ,', 'm, labeled (by 1 0) according target function, f, unknown learner.', 'gave intuitive arguments support claim seeing small fraction possible inputs (and values) guess correctly values subsequent inputs—if knew function trying guess belonged appropriately restricted subset functions.', 'in literature pac learning theory, target function usually called target concept denoted c, consistent previous notation continue denote f.) problem guess 107', 'chapter 8 computational learning theory chapter posed problem guessing function given set sample inputs values.', 'insight led theory probably approximately correct (pac) learning—initially developed leslie valiant [valiant, 1984].', 'is, given training set sample patterns adequate allow select function, consistent labeled samples, restricted set hypotheses high probability function select approximately correct (small probability error) subsequent samples drawn random according distribution labeled samples drawn.']]\n",
      "\n",
      "[['assume target function element set functions, c. assume hypothesis, h, element set, h, hypotheses, includes set, c, target functions.', 'suppose able ﬁnd h classiﬁes m randomly drawn training samples correctly; is, h consistent randomly selected training set, ξ.', 'training occasions, m randomly drawn training samples, h turn approximately correct (for given value ε), not.', 'general, h won’t identical f, strive value h(x) = value f(x) x’s.', 'ﬁnite number hypotheses hypothesis set (as hypothesis sets considered), produce consistent hypothesis set testing training data.', 'class, c, polynomially pac learnable terms h provided exists polynomial-time learning algorithm (polynomial number samples needed, m, dimension, n, 1/ε, 1/δ) pac-learns functions c terms h. initial work pac assumed h = c, later shown func- tions polynomially pac-learned assumption (assuming', 'general, learning algorithm pac-learns functions c terms h iﬀfor function fϵ c, outputs hypothesis hϵ h, probability (1 −δ), εh ≤ε.', 'h probably (except δ) approximately correct (pac) probability approximately correct greater 1−δ, δ conﬁdence parameter.', 'h approximately (except ε ) correct εh ≤ε, ε accuracy parameter.', 'shall m greater bound value depends ε δ, h guaranteed probably approximately correct.'], ['quantify notion, deﬁne error h, εh, probability x drawn randomly according p misclassiﬁed εh = x [xh(x)̸=f(x)] p(x) boldface symbols need smaller subscripts math environments.']]\n",
      "\n",
      "[['pac learning 109 p ̸= np)—but polynomially pac-learned h strict superset c!', 'error hi εhi= probability hi classify pattern error (that is, diﬀerently f classify it).', 'εhb > ε, probability hb (or bad hypothesis) classify pattern correctly (1 −ε).', 'prob[some h ϵ hb classiﬁes m patterns correctly] = p hb ϵ hb prob[hb classiﬁes m patterns correctly |hb ϵ hb] ≤k(1 −ε)m, k = |hb|.', 'c h identical, restrictive deﬁnition properly pac-learnable class class c exists algorithm polynomially pac-learns functions c terms c. 8.2 pac learning 8.2.1 fundamental theorem suppose learning algorithm selects h randomly consistent values f m training patterns.', 'then, probability exists hypothesis h consistent f members ξ error greater ε |h|e−εm.', 'is, prob[hb classiﬁes m patterns correctly |hb ϵ hb] ≤(1 −ε)m.', 'let h set hypotheses, ξ set m ≥1 training examples drawn independently according distribution p, f classiﬁcation function h, ε > 0.', 'probability error randomly selected h greater ε, h consis- tent values f(x) m instances x (drawn according arbitrary p), equal |h|e−εm, |h| number hypotheses h. state result theorem [blumer, et al.,']]\n",
      "\n",
      "[['taking natural logarithm sides yields ln |h| −εm ≤ln δ m ≥(1/ε)(ln |h| + ln(1/δ)) qed corollary important reasons.', 'computational learning theory is, prob[there bad hypothesis classiﬁes m patterns correctly] ≤k(1 −ε)m.', 'proof ﬁnd bound m guarantees prob[there hypothesis error > ε classiﬁes m patterns correctly] ≤δ.', 'qed corollary theorem is corollary 8.2 given m ≥(1/ε)(ln |h| + ln(1/δ)) independent samples, probability exists hypothesis h consistent f samples error greater ε δ.', 'clearly states select hypothesis consistent m samples assured probability (1 −δ) error ε.', 'possible point confusion bound given corollary upper bound value m needed guarantee polynomial probably ap- proximately correct learning.', 'k ≤|h| (1 −ε)m ≤e−εm, have prob[there bad hypothesis classiﬁes m patterns correctly] = prob[there hypothesis error > ε classiﬁes m patterns correctly] ≤|h|e−εm.']]\n",
      "\n",
      "[['example, consider following patterns, labeled 1 (from [dietterich, 1990]) (0, 1, 1, 0) (1, 1, 1, 0) (1, 1, 0, 0) processing ﬁrst pattern, h = x1x2x3x4; processing second pattern, h = x2x3x4; ﬁnally, pattern, h = x2x4.', 'n = 50, ε = 0.01 δ = 0.01, m ≥5, 961 guarantees pac learnability.', 'then, |h| = 3n, m ≥(1/ε)(ln(3n) + ln(1/δ)) ≥(1/ε)(1.1n + ln(1/δ)) note bound m increases polynomially n, 1/ε, 1/δ.', 'pac learning 111 8.2.2 examples terms let h set terms (conjunctions literals).', 'then, additional pattern, xi, labeled 1, delete h boolean variables appearing xi sign diﬀerent sign h. processing patterns labeled 1, check patterns labeled 0 sure assigned value 1 h. if, stage algorithm, patterns labeled 0 assigned 1 h, exists term consistently classiﬁes patterns ξ, exit failure.', 'order terms properly pac learnable, additionally ﬁnd time polynomial m n hypothesis h consistent set m patterns labeled value term.', 'initialize boolean function, h, conjunction n literals corresponding values n components x1. (', 'following procedure ﬁnding consistent hypothesis requires o(nm) steps (adapted [dietterich, 1990, page 268]) given training sequence, ξ, m examples.', 'components value 1 corresponding positive literals; components value 0 corresponding negative literals.)']]\n",
      "\n",
      "[['summary order class functions properly pac- learnable a. algorithm produces consistent hypothesis m n-dimensional samples time polynomial m n. b. sample size, m, needed ensure pac learnability polyno- mial (or better) (1/ε), (1/δ), n showing ln |h| polynomial better number dimensions.', 'np-hard dnf 22n polynomial (all boolean functions) (members class k-2nn two-layer, feedforward neural networks exactly k hidden units output unit.)', 'n = 50, ε = 0.01 δ = 0.01, m ≥173, 748 guarantees pac learnabil- ity.', 'computational learning theory m ≥(1/ε) \\x00n2 ln 2 + ln(1/δ) \\x01 again, note bound m increases polynomially n, 1/ε, 1/δ.', 'adapted [dietterich, 1990, pages 262 268] gives references proofs time complexities.)', 'linearly separable functions properly pac learnable, additionally ﬁnd time polynomial m n hypothesis h consistent set m labeled linearly separable patterns.', 'terms 3n polynomial yes k-term dnf 2o(kn) np-hard (k disjunctive terms) k-dnf 2o(nk) polynomial yes (a disjunction k-sized terms) k-cnf 2o(nk) polynomial yes (a conjunction k-sized clauses) k-dl 2o(nkk lg n) polynomial yes (decision lists k-sized terms) lin.', '8.2.3 properly pac-learnable classes properly pac-learnable classes functions given following table. (']]\n",
      "\n",
      "[['vapnik-chervonenkis dimension 113 hinted earlier, enlarging class hypotheses makes learning easier.', 'pac learning theory powerful analytic tool, (like complexity theory) deals mainly worst-case results.', 'subset, h, boolean functions able dichotomize arbitrary set, ξ, m boolean patterns 2m ways.', '8.3 vapnik-chervonenkis dimension 8.3.1 linear dichotomies consider set, h, functions, set, ξ, (unlabeled) patterns.', 'general (that is, non-boolean case), subset, h, functions dichotomize set, ξ, m patterns 2m ways, h shatters ξ.', 'ξ include 2n boolean patterns, example, 22n ways dichotomize them, (of course) set possible boolean functions dichotomizes ways.', 'fact class two- layer, feedforward neural networks polynomially pac learnable attack theory networks, successful applications. [', 'measure expressive power set hypotheses, relative ξ, ability arbitrary classiﬁcations patterns ξ.1 m patterns ξ, 2m diﬀerent ways divide patterns disjoint exhaustive subsets.', 'possible enlarging space hy- potheses makes ﬁnding consistent training examples easier.', '8.1, 14 dichotomies 1and, course, hypothesis drawn set arbitrary classiﬁcations set training patterns, little likelihood hypothesis generalize training set.'], ['interesting class functions k-2nn poly- nomially pac learnable hypotheses drawn k′-2nn k′ > k. (at time writing, matter undecided.)', 'sim- ilarly, linearly separable functions implemented tlus weight values restricted 0 1 properly pac learnable, unrestricted linearly separable functions are.', 'so, target function k-term-dnf, able ﬁnd hypothesis k-cnf probably approximately correct target function.']]\n",
      "\n",
      "[['m > n, set m points general position n-dimensional space subset (n+1) points lies (n−1)-dimensional hyperplane.', 'πl(m, n) = 2 n x i=0 c(m −1, i) m > n, = 2m m ≤n', 'm ≤n, set m points general position (m −2)-dimensional hyperplane contains set.', 'note inﬁnite number hyperplanes, are, nevertheless, ﬁnite number ways hyperplanes dichotomize ﬁnite number patterns.', '1 2 3 4 14 dichotomies 4 points 2 dimensions 5 6 7 figure 8.1 dichotomizing points dimensions number dichotomies achievable hyperplanes depends patterns disposed.', 'computational learning theory points dimensions (each separating line yields dichotomies depending points line classiﬁed 1 0). (', 'denote number linear dichotomies m points general position n-dimensional space expression πl(m, n).', 'thus, example, set m ≥4 points general position three-dimensional space lie (two-dimensional) plane.']]\n",
      "\n",
      "[['reason m = 2(n + 1) called capacity tlu [cover, 1965].', 'special separation found m < 2(n + 1) patterns—almost dichotomy patterns linearly separable.', 'm examples [can correctly classiﬁed by] net w weights (for m >> w), net fraction ε errors new examples chosen [uniform] distribution ε = w/m.”', 'patterns) (dimension) 1 2 3 4 5 1 2 2 2 2 2 2 4 4 4 4 4 3 6 8 8 8 8 4 8 14 16 16 16 5 10 22 30 32 32 6 12 32 52 62 64 7 14 44 84 114 126 8 16 58 128 198 240 note class linear dichotomies shatters m patterns m ≤n + 1.', 'bold-face entries table correspond highest values m linear dichotomies shatter m patterns n dimensions.', 'm > 2(n + 1), randomly selected dichotomy m points certainly linearly separable.', 'm < 2(n + 1), dichotomy m points certainly linearly separable.', '8.2 plot pλ(n+1),n versus λ n, λ = m/(n + 1).', 'analogous results generalizing abilities neural networks developed [baum & haussler, 1989] given intuitive experimen- tal justiﬁcation [baum, 1994, page 438] “the results indicate following heuristic rule holds.', 'number training patterns exceeds capacity, fact tlu separates training patterns according labels means terms tlu generalize new patterns.'], ['note large n (say n > 30) quickly pm,n falls 1 0 m goes 2(n + 1).', '8.3.2 capacity let pm,n = πl(m,n) 2m = probability randomly selected dichotomy (out 2m possible dichotomies m patterns n dimensions) linearly separable.', 'sure separation found forced training set generalizes well, case linearly separable functions separate m training patterns.']]\n",
      "\n",
      "[['in case class linearly separable functions, maximum number achieved m points general position.)', 'number dichotomies will, course, depend disposition m points n-dimensional space; πh(m, n) maximum possible arrangements m points. (', 'set ξ be, example, {0.5, 2.5, - 2.3, 3.14}, hypotheses set [1, 4.5].', 'general, let denote maximum number dichotomies set m n-dimensional patterns hypotheses h πh(m, n).', 'maximum number called vapnik-chervonenkis (vc) dimension denoted vcdim(h) [vapnik & chervonenkis, 1971].', 'computational learning theory 0 1 2 3 4 10 20 30 40 50 0 0.25 0.5 0.75 1 0 1 2 3 4 10 20 30 40 50 0 25 .5 75 1 ph(n + 1), n h n figure 8.2 probability random dichotomy linearly separable 8.3.3 general capacity result corollary 7.2 gave expression number training patterns suﬃcient guarantee required level generalization—assuming function guessing function belonging class known ﬁnite cardinality.', 'hypothesis label points 2.5 3.14 1 points - 2.3 0.5 0.', 'class, h, maximum value m πh(m, n) = 2m, is, h shatters m patterns.', 'example, let calculate vc dimension hypothesis space single intervals real line—used classify points real line.']]\n",
      "\n",
      "[['vapnik-chervonenkis dimension 117 set hypotheses (single intervals real line) arbitrarily classify points.', 'soon 2 training patterns real line provided know classiﬁcation function trying guess single interval, begin good generalization.', 'dichotomy vcdim(h) fewer patterns general position n dimensions achieved hypothesis h, vcdim(h) patterns training set order hypothesis consistent training set suﬃciently constrained imply good generalization.', 'baum, 1994, page 438] gives experimental evidence proposition “ . . .', '8.3.4 facts speculations vc dimen- sion • ﬁnite number, |h|, hypotheses h, then vcdim(h) ≤log(|h|) • vc dimension terms n dimensions n. • suppose generalize example hypothesis set single intervals real line.', 'multilayer [neural] nets vc dimension roughly equal total number [adjustable] weights.”', 'figure 8.3 dichotomizing points interval vc dimension useful measure expressive power hypothesis set.', 'hypothesis space consisting conjunctions tests (called axis-parallel hyper-rectangles) vc dimension bounded by n ≤vcdim ≤2n • seen, tlus n inputs vc dimension n + 1. • [']]\n",
      "\n",
      "[['computational learning theory 8.4 vc dimension pac learning theorems connect idea vc dimension pac learn- ing [blumer, et al.,', '1988] theorem 8.5 pac learning algorithm examine ω(1/ε lg(1/δ) + vcdim(h)) training patterns.', 'theorem gives lower (necessary) bound number training patterns required pac learning [ehrenfeucht, et al.,', 'theorem 8.4 set hypotheses, h, properly pac learnable if a. m ≥(1/ε) max [4 lg(2/δ), 8 vcdim lg(13/ε)], b. algorithm outputs hypothesis h ϵ h consistent training set polynomial (in m n) time.', 'second theorems improves bound number training patterns needed linearly separable functions linear n. previous example training patterns needed ensure pac learnability linearly separable function n = 50, ε = 0.01, δ = 0.01, obtained m ≥173, 748.', 'n = 50, ε = 0.01, δ = 0.01, m ≥16, 551 ensures pac learnability.']]\n",
      "\n",
      "[['encoding involves description point separately; other, shorter, encodings involve description clusters points point cluster described given cluster belongs to.', 'ﬁrst set (a) naturally partitionable classes, second (b) diﬃcult partition all, (c) problematic.', 'setting, assume want encode description set points, ξ, message minimal length.', 'partition separates ξ r mutually exclusive exhaustive subsets, ξ1, . . . ,', 'speciﬁc techniques described chapter explicitly use mdl principles, mdl method applied success.', 'stages • form r-way partition set ξ unlabeled training patterns (where value r, itself, need induced patterns).']]\n",
      "\n",
      "[['patterns features numeric, distance measure ordinary euclidean distance points n-dimensional space.', 'figure 9.1 unlabeled patterns divided mutually exclusive exhaustive subsets, ξ1, . . . ,', 'suppose r randomly chosen cluster seekers, c1, . . . ,', '9.2 clustering methods 9.2.1 method based euclidean distance unsupervised learning methods use measure similarity patterns order group clusters.']]\n",
      "\n",
      "[['pattern, xi, presented, ﬁnd cluster seeker, cj, closest xi closer xi cj ←−(1 −αj)cj + αjxi αj learning rate parameter j-th cluster seeker; determines far cj moved xi.', 'clustering methods 121 u11 u12 u21 u22 u23 u31 u32 u11 f u12 = u1 u21 f u22 f u23 = u2 u31 f u32 = u3 u1 f u2 f u3 = u figure 9.2 hierarchy clusters one-by-one.', 'intuitively, cluster seeker gets reasonably clustered set patterns (and cluster seeker located), converge center gravity cluster.', 'adjustment rule, cluster seeker center gravity (sample mean) set patterns far moved.', 'example, set αj = 1/(1 + mj) use rule mj ←−mj +1.']]\n",
      "\n",
      "[['like partition set patterns clusters sum sample variances (badnesses) clusters small.', 'course cluster pattern, sample variances zero, arrange measure badness partition increase number clusters.', 'unsupervised learning u u2 u11 u12 u31 u32 u21 u22 u23 u1 u3 figure 9.3 displaying hierarchy tree cluster seekers converged, classiﬁer implied now- labeled patterns ξ based voronoi partitioning space (based distances cluster seekers).', 'measure badness, v , cluster patterns, {xi}, computing sample variance deﬁned by v = (1/k) x (xi −m)2 m sample mean cluster, deﬁned be m = (1/k) x xi k number points cluster.']]\n",
      "\n",
      "[['hand, cluster seekers, ci, deﬁnes cluster sample variance larger δ, place new cluster seeker, cj, random location somewhat adjacent ci reset masses ci cj zero.', 'elaborations basic cluster-seeking procedure allow number clus- ter seekers vary depending distances depending sample variances clusters.', 'values parameters ε δ set depending relative weights given sample variances numbers clusters.', 'commonly technique compute standard deviation (i.e., square root variance) components entire training set normalize values components adjusted standard deviations equal.', 'clustering methods 123 c1 c2 c3 separating boundaries figure 9.4 minimum-distance classiﬁcation clusters number way somewhat similar principle minimal description length discussed earlier.', 'way badness par- tition ultimately decrease decreasing total sample variance comparatively little penalty additional cluster seeker.', 'example, distance, dij, cluster seekers, ci cj, falls threshold ε, replace single cluster seeker placed center gravity (taking account respective masses).', 'way decrease overall badness partition reducing number clusters compara- tively little penalty increased variance.']]\n",
      "\n",
      "[['described follows a. begin set unlabeled patterns ξ list, l, clusters.', 'assuming conditional independence pattern components, xi, quantity maximized is s(x, ci) = p(x1|ci)p(x2|ci) · · · p(xn|ci)p(ci) p(xj|ci) estimated sample statistics patterns clusters expression. (', 'decide clusters arbitrary pattern, x, assigned selecting ci probability, p(ci|x), largest, providing p(ci|x) larger ﬁxed threshold, δ.', 'unsupervised learning 9.2.2 method based probabilities suppose partition training set, ξ, r mutually exclusive exhaustive clusters, c1, . . . ,', 'is, cmax ←−cmax ∪{x} update sample statistics p(x1|cmax), p(x2|cmax), . . . ,', 'before, deﬁne sample mean cluster, ci, be mi = (1/ki) x xjϵ ci xj ki number patterns ci.', 'b) s(x, cmax) ≤δ, create new cluster, cnew = {x} add cnew l. 3.']]\n",
      "\n",
      "[['9.3 hierarchical clustering methods 9.3.1 method based euclidean distance suppose set, ξ, unlabeled training patterns.', 'shortest distance cluster vectors, ci cj, form new cluster, c, consisting union ci cj.', 'hierarchical clustering methods 125 d. sample statistics clusters changed entire iteration ξ, terminate clusters l; 2.', 'shortest distance pattern, xi, cluster vector, cj (representing cluster, cj), form new cluster, c, consisting union cj {xi}.', 'collect xi xj cluster, c, eliminate xi xj ξ replace cluster vector, c, equal average xi xj.', 'ternary tree formed instead searches points ξ triangle deﬁned patterns minimal area.', 'reduce number points ξ time, ultimately terminate tree clusters rooted cluster containing points original training set.', 'clusters organized hierarchically binary tree cluster 9 root, clusters 7 8 descendants root, on.', 'smallest distance pairs patterns, form new cluster, c, replace pair patterns ξ average.']]\n",
      "\n",
      "[['unsupervised learning 1 2 3 5 4 6 7 8 9 figure 9.5 agglommerative clustering 9.3.2 method based probabilities probabilistic quality measure partitions develop measure goodness partitioning based accurately guess pattern given partition in.', 'then, probability guess i-th component correctly is x j probability(guess vij)pi(vij|ck) = x j [pi(vij|ck)]2 average number (the n) components values guessed correctly method given sum probabilities components x x x j [pi(vij|ck)]2', 'before, compute sample statistics p(xi|ck) probability values component given class assigned partitioning.', 'suppose use following probabilistic guessing rule values components vector x given class k. guess xi = vij probability pi(vij|ck).', 'suppose given partitioning ξ r classes, c1, . . . ,', 'suppose component xi x values vij, index j steps domain component.']]\n",
      "\n",
      "[['finally, dividing number clusters produces ﬁnal z value partition, z(p1) = 3/2.', 'second partition, p2, gives following sample probabilities p1(v11 = 1|c1) = 1 p2(v21 = 1|c1) = 1/2 p3(v31 = 1|c1) = 1 summing values components (0 1) gives (1)2 + (0)2 = 1 component 1, (1/2)2 + (1/2)2 = 1/2 component 2, (1)2 + (0)2 = 1 component 3.', 'similar calculations yield z(p3) = 1 z(p4) = 3/4, method evaluating partitions favor placing patterns single cluster.', 'summing values components (0 1) gives (1/2)2 + (1/2)2 = 1/2.', 'hierarchical clustering methods 127 given partitioning r classes, goodness measure, g, parti- tioning average expression classes g = x k p(ck) x x j [pi(vij|ck)]2 p(ck) probability pattern class ck.', 'let’s evaluate z values follow- ing ones p1 = {a, b, c, d}, p2 = {{a, b}, {c, d}}, p3 = {{a, c}, {b, d}}, p4 = {{a}, {b}, {c}, {d}}.', 'order penalize measure having large number classes, divide r overall “quality” measure partitioning z = (1/r) x k p(ck) x x j [pi(vij|ck)]2 example use measure trivially simple clustering three-dimensional patterns shown fig.', 'finally, dividing number clusters produces ﬁnal z value partition, z(p2) = 1 1/4, high z(p1).', 'sample probabilities pi(vi1 = 1) pi(vi0 = 0) equal 1/2 components.']]\n",
      "\n",
      "[['method uses z values place patterns vari- ous nodes; sample statistics update z values pattern placed node.', 'arrange times dur- ing process non-empty node tree (besides successors) exactly successor.', 'algorithm follows a. start tree root node contains patterns ξ single successor node.', 'unsupervised learning x2 x3 x1 b c d figure 9.6 patterns 3-dimensional space iterative method hierarchical clustering evaluating partitionings m patterns selecting best computationally intractable.', 'general, successors node, η, labeled mutually exclusive exhaustive subsets pattern set labelling node η.', 'following iterative method based hi- erarchical clustering procedure called cobweb [fisher, 1987].', 'best host determined tentatively placing xi successors calculating resulting z value']]\n",
      "\n",
      "[['node merging happen nodes having parent merged overall increase quality resulting classiﬁcation performed successors parent.', 'ﬁrst, program at- tempted ﬁnd categories (we class 1 class 2) united states senators based votes (yes no) issues.', 'ﬁnal classiﬁcation tree order dependent, cobweb proce- dure incorporates node merging splitting.', 'issue class 1 class 2 toxic waste yes budget cuts yes sdi reduction yes contra aid yes line-item veto yes mx production yes', 'e. best host node, η, place xi η, generate successor node η, generate sibling node η, 2.', 'node splitting heuristic node splitting consider replacing best host group siblings host’s successors.', 'g. best host non-empty, non-singleton node, η, place xi η, set µ η, 4.', 'merging improves z value, new node containing union patterns merged nodes replaces merged nodes, nodes merged installed successors new node.', 'f. best host non-empty, singleton (tip) node, η, place xi η, create successor node η containing singleton pattern η, create successor node η containing xi, create successor node η, create successor nodes new non-empty successors η, 2.']]\n",
      "\n",
      "[['n0 soybean diseases n1 diaporthe stem canker n2 charcoal rot n3 n31 rhizoctonia rot n32 phytophthora rot figure 9.7 taxonomy induced soybean diseases 9.4 bibliographical historical remarks added.', 'unsupervised learning second experiment, program attempted classify soybean dis- eases based characteristics.']]\n",
      "\n",
      "[['multi-step prediction, expect prediction accuracy better better increases m. 10.2 supervised temporal-diﬀerence meth- ods training method naturally suggests use actual value z time m + 1 (once known) supervised learning procedure 131', 'chapter 10 temporal-diﬀerence learning 10.1 temporal patterns prediction prob- lems chapter, consider problems wish learn predict future value quantity, z, n-dimensional input pattern, x. problems, patterns occur temporal sequence, x1, x2, . . .,', 'components xi features values available time, t = i. distinguish kinds prediction problems.', 'one, desire predict value z time t = + 1 based input xi i. example, wish predict aspects tomorrow’s weather based set measurements today.', 'm. example, wish series predictions aspect weather new year’s day, based measurements taken day new year’s.', 'kind prediction problem, desire sequence predictions value z ﬁxed time, t = m + 1, based xi, = 1, . . . ,']]\n",
      "\n",
      "[['widrow-hoﬀrule results f(x, w) = x • w. then (∆w)i = c(z −fi)xi interesting form (∆w)i developed note (z −fi) = m x k=i (fk+1 −fk) deﬁne fm+1 = z. substituting formula (∆w)i yields (∆w)i = c(z −fi) ∂fi ∂w', 'assume prediction, f(x), depends vector modiﬁable weights, w. dependence explicit, write f(x, w).', 'su- pervised learning, consider procedures following type xi, prediction f(xi, w) computed compared z, learning rule (whatever is) computes change, (∆wi), w. then, taking account weight changes pattern sequence having predictions old weight vector, change w follows w ←−w + m x i=1 (∆w)i attempting minimize squared error z f(xi, w) gradient descent, weight-changing rule pattern is (∆w)i = c(z −fi) ∂fi ∂w c learning rate parameter, fi prediction z, f(xi, w), time t = i, ∂fi ∂w is, deﬁnition, vector partial derivatives ( ∂fi ∂w1 , . . . ,', 'is, seek learn function, f, f(xi) close possible z i. typically, need training set, ξ, consisting sequences.', 'method better supervised learning important problems base learning diﬀerence f(xi+1) f(xi) diﬀerence z f(xi).', '∂fi ∂wn ) wi individual components w. (the expression ∂fi ∂w written ∇wfi.)', 'reader recall equivalent expression (∆w)i deriving backpropagation formulas training multi-layer neural networks.']]\n",
      "\n",
      "[['λ < 1, degrees unsupervised learning, prediction function strives prediction like successive ones (whatever be).', 'here, λ term gives exponentially decreasing weight diﬀerences later time t = i. λ = 1, rule began—weighting diﬀerences equally, λ →0, weight (fi+1 −fi) diﬀerence.', 'intermediate values λ account diﬀerently weighted diﬀerences future pairs successive predictions.', 'case f(x, w) = x • w, temporal diﬀerence form widrow-hoﬀrule is (∆w)i = cxi m x k=i (fk+1 −fk) reason writing (∆w)i temporal-diﬀerence form permit interesting generalization follows (∆w)i = c ∂fi ∂w m x k=i λ(k−i)(fk+1 −fk) 0 < λ ≤1.', 'supervised temporal-difference methods 133 = c ∂fi ∂w m x k=i (fk+1 −fk) form, instead diﬀerence prediction value z, use diﬀerences successive predictions—thus phrase temporal-diﬀerence (td) learning.', 'shall soon unsupervised procedures result better learning supervised ones important class problems.', 'td(1) considered pure supervised learning procedure, sensitive ﬁnal value z provided teacher.', 'td(0), error diﬀerence successive predic- tions, td(1), error diﬀerence ﬁnally revealed value z prediction.', 'interesting compare extreme cases td(0) (∆w)i = c(fi+1 −fi) ∂fi ∂w td(1) (∆w)i = c(z −fi) ∂fi ∂w extremes handled learning mechanism; error term diﬀerent.']]\n",
      "\n",
      "[['write expression weight change rule takes account (∆w)i w ←−w + m x i=1 c ∂fi ∂w m x k=i λ(k−i)(fk+1 −fk) interchanging order summations yields w ←−w + m x k=1 c k x i=1 λ(k−i)(fk+1 −fk) ∂fi ∂w = w + m x k=1 c(fk+1 −fk) k x i=1 λ(k−i) ∂fi ∂w interchanging indices k ﬁnally yields w ←−w + m x i=1 c(fi+1 −fi) x k=1 λ(i−k) ∂fk ∂w if, earlier, want use expression form w ←−w+pm i=1(∆w)i, write (∆w)i = c(fi+1 −fi) x k=1 λ(i−k) ∂fk ∂w now, let ei = pi k=1 λ(i−k) ∂fk ∂w, develop computationally eﬃcient recurrence equation ei+1 follows ei+1 = i+1 x k=1 λ(i+1−k) ∂fk ∂w = ∂fi+1 ∂w + x k=1 λ(i+1−k) ∂fk ∂w', 'temporal-difference learning 10.3 incremental computation (∆w)i rewrite formula (∆w)i, (∆w)i = c ∂fi ∂w m x k=i λ(k−i)(fk+1 −fk) allow type incremental computation.']]\n",
      "\n",
      "[['equation computed incrementally, (∆w)i depends pair successive predictions [weighted] sum past values ∂fi ∂w.', 'case, sequences temporally presented patterns contain important information ignored conventional supervised method widrow-hoﬀrule.', 'similarly, xf sequence, equally likely sequence terminates z = 1 vector xe.', 'sutton [sutton, 1988, page 19] gives interesting example involving random walk, repeat here.', '10.1, sequences vectors, x, generated follows start vector xd; vector sequence equally likely adjacent vectors diagram.', 'vector xc (or xe), equally likely vectors adjacent xc (or xe).', 'xb sequence, equally likely sequence terminates z = 0 vector xc.', '10.4 experiment td methods td prediction methods [especially td(0)] suited situations patterns generated dynamic process.', 'experiment td methods 135 = ∂fi+1 ∂w + λei rewriting (∆w)i terms, obtain (∆w)i = c(fi+1 −fi)ei where e1 = ∂f1 ∂w e2 = ∂f2 ∂w + λe1 etc.', 'saves substantially memory, longer necessary individually remember past values ∂fi ∂w.”'], ['quoting sutton [sutton, 1988, page 15] (about diﬀerent equation, quote applies equally one) “. . .']]\n",
      "\n",
      "[['weight vector increments summed sequences presented, sum change weight vector pass sequences.', 'temporal-difference learning 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 z = 0 z = 1 xb xc xd xe xf typical sequences xdxcxdxexf 1 xdxcxbxcxdxexdxexf 1 xdxexdxcxb 0 figure 10.1 markov process random walk example markov process; transitions state state j occur probabilities depend j. given set sequences generated process training set, want able predict value z x test sequence.', 'experiments process, sutton linear predictor, f(x, w) = x • w. learning problem ﬁnd weight vector, w, minimizes mean-squared error z predicted value z. given ﬁve diﬀerent values x on, following predictions f(xb) = w1, f(xc) = w2, f(xd) = w3, f(xe) = w4, f(xf ) = w5, wi i-th component weight vector. (', 'process repeated (using training sequences) (quoting sutton) “the procedure longer produced signiﬁcant changes weight vector.', 'note values predictions limited 1 0—even z values—because minimizing mean-squared error.)', 'weight vector increments, (∆w)i, computed pattern presentation weight changes sequences presented.']]\n",
      "\n",
      "[['root-mean-squared diﬀerences best learned predictions (over c) optimal ones plotted fig.', 'answer widrow-hoﬀprocedure minimizes error training set; necessarily minimize error future experience. [', 'even though, ﬁxed, small c, weight vector converged vector, converge somewhat diﬀerent vector diﬀerent values c.) convergence, predictions ﬁnal weight vector com- pared optimal predictions transition probabilities.', '0.10 0.12 0.14 0.16 0.18 0.20 0.0 0.1 0.3 0.5 0.7 0.9 1.0 h error best c widrow-hoff td(1) td(0) (adapted sutton, p. 20, 1988) figure 10.2 prediction errors td(λ) notice widrow-hoﬀprocedure perform versions td(λ) λ < 1!', 'compute proba- bilities 1/6, 1/3, 1/2, 2/3, 5/6 xb, xc, xd, xe, xf , respectively.', 'known that, repeated presentations, widrow-hoﬀprocedure minimizes rms error predictions actual outcomes training set ([widrow & stearns, 1985]).', 'experiment td methods 137 ﬁnal value [for 100 diﬀerent training sets random sequences], independent initial value.” (']]\n",
      "\n",
      "[['temporal-difference learning matching future experience—those consistent maximum- likelihood estimate underlying markov process.”', 'dayan [dayan, 1992] extended result theorem 9.1 td(λ) arbitrary λ 0 1. (', 'sutton conjectures c approach 0 training progresses, variance predictions approach 0 also.', '10.6 intra-sequence weight updating standard weight updating rule td(λ) methods is w ←−w + m x i=1 c(fi+1 −fi) x k=1 λ(i−k) ∂fk ∂w weight update occurs entire sequence observed.', '10.5 theoretical results possible analyze performance linear-prediction td(λ) methods markov processes.', 'instead, modify rule that, pair predictions, fi+1 = f(xi+1, wi) fi = f(xi, wi).', 'theorem 10.1 (sutton, page 24, 1988) absorbing markov chain, linearly independent set observation vectors {xi} non- terminal states, exists ε > 0 positive c < ε initial weight vector, predictions linear td(0) (with weight updates sequence) converge expected value optimal (maximum likelihood) predictions true process.', 'method truly incremental (in analogy weight updating rules neural nets), desirable change weight vector pattern presentation.', 'fi = f(xi, wi−1), rule prediction diﬀerence, (fi+1 −fi), sensitive changes x changes w lead instabilities.', 'obvious extension is wi+1 ←−wi + c(fi+1 −fi) x k=1 λ(i−k) ∂fk ∂w fi+1 computed making weight change; is, fi+1 = f(xi+1, wi).'], ['expected values predictions converge, predictions converge vary expected values depending recent experience.']]\n",
      "\n",
      "[['linear td(0) method regarded technique training simple network consisting single dot product unit (and threshold sigmoid function).', 'b. = 1, ..., m, do (a) fi ←−xi • w (we compute fi anew time use value fi+1 previous time through.) (', 'weight changing rule i-th weight vector j-th layer weights form before, namely w(j) ←−w(j) + cδ(j) x(j−1) δ(j) given recursively by δ(j) = f (j) (1 −f (j) ) mj+1 x l=1 δ(j+1) l w(j+1) il w(j+1) il l-th component i-th weight vector (j +1)-th layer weights.', 'td(0) change network weights according expression wi+1 = wi + c(fi+1 −fi) ∂fi ∂w change standard backpropagation weight- changing rule diﬀerence term desired output output unit ﬁnal (k-th) layer, (d −f (k)), replaced diﬀerence term successive outputs, (fi+1 −fi).', 'b) fi+1 ←−xi+1 • w (c) di+1 ←−fi+1 −fi (d) w ←−w + c di+1xi (if fi computed changed weight vector, value closer fi+1 desired.)', 'intra-sequence weight updating 139 td(0) linear predictors, rule is wi+1 = wi + c(fi+1 −fi)xi rule implemented follows a. initialize weight vector, w, arbitrarily.', 'change direct eﬀect expression δ(k) becomes δ(k) = 2(f ′(k) −f (k))f (k)(1 −f (k)) f ′(k) f (k) successive outputs network.']]\n",
      "\n",
      "[['40 hidden units 2 x 24 cells 4 output units hidden output units sigmoids learning rate c = 0.1; initial weights chosen randomly <0.5 +0.5.', 'temporal-difference learning 10.7 example application td-gammon program called td-gammon [tesauro, 1992] learns play backgammon training neural network temporal-diﬀerence methods.', 'bar, board, moves 198 inputs 1 2 3 # > 3 . . .', 'p3 = pr(black wins) p4 = pr(black gammons) p1 = pr(white wins) p2 = pr(white gammons) estimated payoff d = p1 + 2p2 < p3 < 2p4 no.', 'network trained minimize error actual payoﬀand estimated payoﬀ, actual payoﬀis deﬁned df = p1 + 2p2 −p3 −2p4, pi actual probabilities outcomes deﬁned ﬁgure. . . .', 'estimated probabilities figure 10.3 td-gammon network td-gammon learned network select results best predicted payoﬀ. is, stage game ﬁnite set moves possible lead set, {x}, new board positions.']]\n",
      "\n",
      "[['bibliographical historical remarks 141 predicted payoﬀis selected white’s (and smallest black’s).', 'td(1), network trained that, t, output, dt, input xt tended expected ﬁnal payoﬀ, df, given input.', 'dt network’s estimate payoﬀ time t (before made), dt+1 estimate time t + 1 (after made), weight adjustment rule is ∆wt = c(dt+1 −dt) t x k=1 λt−k ∂dk ∂w wt vector weights network time t, ∂dk ∂w gradient dk weight space. (', 'made, network weights adjusted predicted payoﬀfrom original position closer resulting position.', 'td-gammon (with 40 hidden units, λ = 0.7, c = 0.1) won 66.2% 10,000 games sun microsystems gammontool 55% 10,000 games neural network trained expert moves.', 'commenting later version td- gammon, incorporating special features inputs, tesauro said “it appears strongest program seen author.”', 'special cases clear, recall td(0), network trained that, t, output, dt, input xt tended expected output, dt+1, input xt+1.', 'for layered, feedforward network, td-gammon, weight changes weight vectors layer expressed usual manner.)']]\n",
      "\n",
      "[]\n",
      "\n",
      "[['moment, assume mapping states vectors one-to-one, and, fact, use notation x refer state environment input vector.', 'maximize rewards, need able predict actions change inputs, particular, actions lead rewards.', 'formalize problem following way robot exists environment consisting set, s, states.', 'chapter 11 delayed-reinforcement learning 11.1 general problem imagine robot exists environment sense act.', 'assume robot’s sensory apparatus constructs input vector, x, environment, informs robot state environment in.', 'learner’s goal ﬁnd policy, π(x), maps input vectors actions way maximizes rewards accumulated time.', 'assume discrete time model; input vector time t = xi, action taken time ai, expected reward, ri, received t = depends action taken state, ri = r(xi, ai).']]\n",
      "\n",
      "[['robot receives input vector (x1, x2) telling cell in; capable actions, n, e, s, w moving robot cell up, right, down, left, respectively.', 'chapter describe methods learning optimal policies based reward values received learner.', 'let’s suppose robot lands goal cell gets reward, immediately transported random cell, quest reward continues.', 'example, input robot (1,3), robot chooses action w, input robot (1,3) receives reward −1.', 'way displaying policy grid-world robot arrow cell indicating direc- tion robot cell.', 'delayed-reinforcement learning xi ri learner environment (reward) (state) (action) ai figure 11.1 reinforcement learning 11.2 example “grid world,” shown fig.']]\n",
      "\n",
      "[['then, want maximize expected future reward deﬁne v π(x) as v π(x) = e \" ∞ x i=0 γirπ(x) # case, v π(x) value policy π input x.', 'markovian environments, example, probability action state xi lead state xj given transition probability p[xj|xi, a].', 'optimal policy maximizes v π(x) inputs, x. general, want consider case rewards, ri, random variables eﬀects actions environmental states random.', 'temporal discounting optimal policies 145 r g 1 2 3 4 5 6 7 1 2 3 4 5 6 7 8 figure 11.2 grid world 11.3 temporal discounting optimal poli- cies delayed reinforcement learning, assumes rewards distant future valuable immediate rewards.', 'suppose policy π(x) maps input vectors actions, let rπ(x) reward received i-th time step begins executing policy π starting state x. total reward accumulated time steps policy π beginning state x is v π(x) = ∞ x i=0 γirπ(x) reason temporal discount factor sum ﬁnite.']]\n",
      "\n",
      "[['famous “optimality equation” v π∗(x) = max \" r(x, a) + γ x x′ p[x′|x, a]v π∗(x′) # theory dynamic programming (dp) [bellman, 1957, ross, 1983] assures optimal policy, π∗, satisﬁes equation.', 'delayed-reinforcement learning r g 1 2 3 4 5 6 7 1 2 3 4 5 6 7 8 figure 11.3 optimal policy grid world action prescribed π taken state x leads state x′ (randomly according transition probabilities), write v π(x) terms v π(x′) follows v π(x) = r[x, π(x)] + γ x x′ p[x′|x, π(x)]v π(x′) (in summary) γ = discount factor, v π(x) = value state x policy π, r[x, π(x)] = expected immediate reward received execute action prescribed π state x, p[x′|x, π(x)] = probability environment transitions state x′ execute action prescribed π state x. words, value state x policy π expected value immediate reward received executing action recommended π plus average value (under π) states accessible x. optimal policy, π∗(and others!),']]\n",
      "\n",
      "[['11.4 q-learning watkins [watkins, 1989] proposed technique calls incremental dynamic programming.', 'q-learning 147 provides methods calculating v π∗(x) π∗, assuming know average rewards transition probabilities.', 'model actions, is, knew state, x, action a, state, x′ resulted, use method called value iteration ﬁnd optimal policy.', 'is, π∗(x) = arg max \" r(x, a) + γ x x′ p[x′|x, a]v π∗(x′) # but, course, assuming know average rewards transition probabilities, ﬁnd method eﬀectively learns them.', 'knew transition probabilities, average rewards, v π∗(x) x a, easy implement optimal policy.', 'value iteration works follows begin assigning, randomly, estimated value ˆv (x) state, x. i-th step process, suppose state xi (that is, input i-th step xi), estimated value state xi i-th step ˆvi(xi).', 'suppose subsequent state having highest estimated value x′ i. update estimated value, ˆvi(xi), state xi follows ˆvi(x) = (1 −ci) ˆvi−1(x) + ci h ri + γ ˆvi−1(x′ i) x = xi, = ˆvi−1(x) otherwise.', 'adjustment moves value ˆvi(xi) increment (depend- ing ci) closer h ri + γ ˆvi(x′ i) .', 'simply select maximizes r(x, a) + γ p x′ p[x′|x, a]v π∗(x′).', 'assuming ˆvi(x′ i) good estimate vi(x′ i), adjustment helps estimates consistent.'], ['providing 0 < ci < 1 visit state inﬁnitely often, process value iteration converge optimal values.', 'let a; π stand policy chooses action once, chooses actions according policy π.']]\n",
      "\n",
      "[['suppose action state x leads state x′. deﬁnitions q v , easy that qπ(x, a) = r(x, a) + γe[v π(x′)] r(x, a) average value immediate reward received execute action state x. optimal policy (and others), version optimality equation terms q values qπ∗(x, a) = max h r(x, a) + γe h qπ∗(x′, a) ii actions, a, states, x. now, optimal q values (for x), implement optimal policy simply selecting action maximized r(x, a) + γe \\x02 qπ∗(x′, a) \\x03 .', 'i-th episode, agent • observes current state xi, • selects [using method described below] performs action ai, • observes subsequent state x′ i, • receives immediate reward ri,', 'quote (with minor notational changes) [watkins & dayan, 1992, page 281] “in q-learning, agent’s experience consists sequence dis- tinct stages episodes.', 'is, π∗(x) = arg max h r(x, a) + γe h qπ∗(x′, a) ii watkins’ proposal amounts td(0) method learning q values.', 'delayed-reinforcement learning optimal value state x given by v π∗(x) = max qπ∗(x, a) equation holds optimal policy, π∗. optimal policy given by π∗(x) = arg max qπ∗(x, a) note action makes qπ(x, a) larger v π(x), improve π changing π(x) = a. making change basis powerful learning rule shall describe shortly.']]\n",
      "\n",
      "[['q learning strengthens usual td methods, however, td (applied reinforcement problems value iteration) requires one-step lookahead, model eﬀects actions, q learning not.', 'imagine q values predictions ultimate (inﬁnite horizon) total reward, learning procedure described exactly td(0) method learning predict q values.', 'q-learning 149 • adjusts qi−1 values learning factor ci, according to qi(x, a) = (1 −ci)qi−1(x, a) + ci[ri + γvi−1(x′ i)] x = xi = ai, = qi−1(x, a) otherwise, vi−1(x′) = max b [qi−1(x′, b)] best agent thinks state x′. . . .', 'convenient notation (proposed [schwartz, 1993]) representing change q value is q(x, a) β ←−r + γv (x′) q(x, a) new q value input x action a, r immediate reward action taken response input x, v (x′) maximum (over actions) q value state reached action taken state x, β fraction way new q value, q(x, a), adjusted equal r + γv (x′).', 'current q values, qi(x, a), agent selects action maximizes qi(x, a).', 'deﬁne ni(x, a) index (episode number) i-th time action tried state x. then, have', 'watkins dayan [watkins & dayan, 1992] prove that, certain con- ditions, q values computed learning procedure converge optimal ones (that is, ones optimal policy based).', 'q value adjusted closer (by determined ci) sum immediate reward plus discounted maximum (over actions) q values state entered.']]\n",
      "\n",
      "[['considered strong condition way states actions selected—however, stochastic con- ditions theorem, method guaranteed ﬁnd optimal policy weaker conditions.', 'sequence episodes forms basis learning include inﬁnite number episodes starting state action.', 'relationships q learning, dynamic programming, control described [barto, bradtke, & singh, 1994].', 'again, quote [watkins & dayan, 1992, page 281] “the important condition implicit convergence theorem . . .', 'note, however, episodes need form continuous sequence—that x′ episode need x episode.”', 'delayed-reinforcement learning theorem 11.1 (watkins dayan) markov problems states {x} actions {a}, given bounded rewards |rn| ≤r, learning rates 0 ≤cn < 1, ∞ x i=0 cni(x,a) = ∞, ∞ x i=0 \\x02 cni(x,a) \\x032 < ∞ x a, qn(x, a) →q∗ n(x, a) n →∞, x a, probability 1, q∗ n(x, a) corresponds q values optimal policy.', '11.5 discussion, limitations, extensions q-learning 11.5.1 illustrative example q-learning procedure requires maintain table q(x, a) values state-action pairs.', 'deﬁnition optimal q values state depends recursively expected values q values subsequent states (and expected values rewards), expected values explicitly computed procedure.', 'instead, values approximated iterative sampling actual stochastic mechanism produces successor states.']]\n",
      "\n",
      "[['maximum q value occurs = w, robot moves west cell (1,3)—receiving immediate reward.', 'q learning is, date, successful technique temporal credit assignment, related method, called bucket brigade algorithm, proposed [holland, 1986].', 'general problem associating rewards state-action pairs called temporal credit assignment problem—how credit reward apportioned actions leading it?', 'learning rate parameter c = 0.5 γ = 0.9, q value q((2, 3), w) adjusted 7 5.75.', 'q learning gradually reinforces actions con- tribute positive rewards increasing associated q values.', 'maximum q value cell (1,3) 5, learning mechanism attempts value q((2, 3), w) closer discounted value 5 plus immediate reward (which 0 case).', 'typically, example, rewards occur somewhat actions lead them— phrase delayed-reinforcement learning.', 'random walk happens stumble rewarding states q learning begin produce q values useful, and, then, q values work way outward rewarding states.', 'learning problems similar faced agent grid world thoroughly studied sutton proposed architecture, called dyna, solving [sutton, 1990].', 'agent’s model world obtained q learning actual world, planning accomplished q learning model world.'], ['imagine better better approximations optimal q values gradually propagate states producing rewards states agent fre- quently visits.', 'discussion, limitations, extensions q-learning151 x q(x, a) r(x, a) (2,3) w 7 0 (2,3) n 4 0 (2,3) e 3 0 (2,3) s 6 0 (1,3) w 4 -1 (1,3) n 5 0 (1,3) e 2 0 (1,3) s 4 0 suppose robot cell (2,3).']]\n",
      "\n",
      "[['mentioned, convergence theorem q learning require on-line learning; indeed, special precautions taken ensure on-line learning meets conditions theorem.', 'example, grid-world problem, imagine selecting action randomly ac- cording probability distribution actions (n, e, s, w).', 'then, generalized “goal” maximizing discounted future reward instead simply achieving particular condition.', 'on-line learning discovers good paths rewards, agent ﬁxate discover policy leads possibly greater long-term reward.', '11.5.2 random actions pattern presentation sequence patterns caused agent’s action response pattern, called on-line learning method.', 'methods, also, proposed dealing exploration, in- cluding making unvisited states intrinsically rewarding “interval estimate,” related uncertainty estimate state’s value [kaelbling, 1993].', 'example, ﬁrst ﬁnd action prescribed q values choose action probability 1/2, choose orthogonal actions probability 3/16 each, choose opposite action probability 1/8.', 'possibility presents interesting way generalize classical notion “goal” ai planning systems—even learning.', 'way force exploration perform occasional random actions (in- stead single action prescribed current q values).', 'goal maintenance, particular state, expressed terms reward earned agent state performed action transitioned state step.'], ['policy modiﬁed “simulated annealing” gradually increase probabil- ity action prescribed q values time goes on.', 'instead representing goal condition achieved, represent “goal struc- ture” set rewards given achieving conditions.', 'reinforcement learning phraseology, problem referred problem exploitation (of learned behavior) versus exploration (of possibly better behavior).']]\n",
      "\n",
      "[['optimum q values problem (whatever be) complex computed linear machine, layered neural network used.', 'wi q(a1, x) q(a2, x) q(ar, x) figure 11.4 net computes q values neural net agent r actions select from.', 'y y y trainable weights y wi r dot product units q(ai, x) = x .', 'q values (as function input pattern x action ai) computed dot products weight vectors (one action) input vector.', 'weight adjustments according td(0) procedure bring q value action selected closer sum immediate reward (if any) (discounted) maximum q value input pattern.', 'discussion, limitations, extensions q-learning153 11.5.3 generalizing inputs large problems impractical maintain table like grid-world example.']]\n",
      "\n",
      "[['possible reinstate markov framework (over x’s) x includes current sensory precepts information agent’s memory.', 'perceptual alias- ing, longer guarantee q learning result useful action policies, let optimal ones.', 'case, longer markov problem; is, x vector, given action, depend sequence previous ones immediately preceding one.', 'delayed-reinforcement learning interesting examples delayed-reinforcement training simulated actual robots requiring structural credit assignment reported [lin, 1992, mahadevan & connell, 1992].', 'input vector results agent’s perceptual apparatus (as assume does), reason suppose uniquely identiﬁes environmental state.', '11.5.5 scaling problems diﬃculties far prohibited wide application reinforcement learn- ing large problems. (', '11.5.4 partially observable states far, identiﬁed input vector, x, actual state envi- ronment.', 'use random actions • favor states visited recently • separate learning phase use phase • employ teacher guide exploration b. slow time convergence • combine learning prior knowledge; use estimates q values (rather random values) initially • use hierarchy actions; learn primitive actions ﬁrst freeze useful sequences macros learn use macros', 'the td-gammon program, mentioned chap- ter, probably unique terms success high-dimensional problem.)', 'researchers attempted deal problem variety methods including attempting model “hid- den” states internal memory [lin, 1993].']]\n",
      "\n",
      "[['bibliographical historical remarks 155 • employ teacher; use graded “lessons”—starting near rewards backing away, use examples good behavior [lin, 1992] • use eﬃcient computations; e.g. updates episode [moore & atkeson, 1993] c. large state spaces • use hand-coded features • use neural networks • use nearest-neighbor methods [moore, 1990] d. temporal discounting problems.', 'use learning method based average rewards [schwartz, 1993] e. “transfer” learning .', 'small γ learner greedy present rewards indiﬀerent future; large γ slows learning. •', 'reinforcement learning replaced “planner” uses action model produce plans achieve goals.', 'learned depends reward struc- ture; rewards change, learning start over. •', 'separate learning parts learn “action model” predicts actions change states (and constant prob- lems), learn “values” states reinforcement learn- ing diﬀerent set rewards.']]\n",
      "\n",
      "[]\n",
      "\n",
      "[['chapter 12 explanation-based learning 12.1 deductive learning learning methods studied far, typically training set ex- haust version space.', 'logic, deductive system conclusions logically follow set input facts, system sound.1 contrast inductive deductive systems logical setting, suppose set facts (the training set) includes following formulas {round(obj1), round(obj2), round(obj3), round(obj4), ball(obj1), ball(obj2), ball(obj3), ball(obj4)} learning system forms conclusion (∀x)[ball(x) ⊃round(x)] in- ductive.', 'sense, implicitly knew φ along, inherent knowing ∆. yet, φ obvious given ∆, 1logical reasoning systems sound, example non-monotonic reasoning, produce inductive conclusions logically follow input facts.', 'suppose logical proposition, φ, logically follows set facts, ∆. circumstances process deducing φ ∆results learning φ?']]\n",
      "\n",
      "[['strictly speaking, speed-up learning result system able decisions that, principle, learning took place.', 'surface, real diﬀerence experience-based hypotheses chess player makes constitutes good play kind learning studying far.', 'let suppose proof constructed depend given triangle right triangle; case learn general fact.', 'extreme case, chess player said learn chess optimal play inherent rules chess.', '12.2 domain theories types information present inductive methods studied information inherent training samples information domain implied “bias” (for example, hypothesis set choose functions).', 'typically, smaller hypothesis set (that is, priori information function sought), dependent information supplied training set (that is, fewer samples).', 'ebl, specialize parts domain theory explain particular ex- ample, generalize explanation produce element domain theory useful similar examples.', 'example, suppose given theorems geometry asked prove sum angles right triangle 180 degrees.']]\n",
      "\n",
      "[['or, loan oﬃcer bank, ask sorts things s/he looks making decision loan, encode knowledge set rules expert system, use expert system decisions.', 'knowledge loan oﬃcer originated set “policies” (the domain theory), application policies specialized eﬃcient experience special cases loans district.', 'example 159 domain theory example (x p) prove x p specialize explanation (proof) generalize new domain rule things \"like\" x p y like x complex proof process trivial proof y p figure 12.1 ebl process data people known good bad credit risks train classiﬁer decisions.']]\n",
      "\n",
      "[['sees(x, y) ∧habile(x) ⊃fixes(x, y) (a habile individual entity ﬁx entity.)', 'suppose given number facts num5, as robot(num5) r2d2(num5) age(num5, 5) manufacturer(num5, gr) . . .', 'explanation-based learning suppose domain theory logical sentences taken together, help deﬁne robot classiﬁed robust. (', 'example, let’s suppose domain theory includes sentences fixes(u, u) ⊃robust(u) (an individual ﬁx robust.)', 'having found proof particular robot, able derive new sentence use allows faster conclusion.']]\n",
      "\n",
      "[['example 161 fixes(u, u) => robust(u) robust(num5) fixes(num5, num5) sees(num5,num5) habile(num5) sees(x,y) & habile(x) => fixes(x,y) robot(w) => sees(w,w) robot(num5) r2d2(x) => habile(x) r2d2(num5) figure 12.2 proof tree told robust(num5) true, attempt ﬁnd proof assertion facts num5 domain theory.', 'fact, con- struct following rule explanation robot(num5) ∧r2d2(num5) ⊃robust(num5) explanation allowed prune attributes num5 irrelevant (at deciding robust(num5)).']]\n",
      "\n",
      "[['explanation-based learning examination proof shows proof structure, sentences domain theory, independently talking num5 individual.', 'apply rules proof forward direction, keeping track substitutions imposed general uniﬁers proof. (', 'clearly, certain assumptions, general rule easily con- clude robust individual original proof process was.', 'substitutions applied variables tip nodes root node yield general rule robot(r) ∧r2d2(r) ⊃robust(r).', 'sole role example instance ebl provide template proof help guide generalization process.', 'process num5 example generalized variable [dejong & mooney, 1986] identity elimination (the precise identity num5 turned irrelevant). (', '12.4 evaluable predicates domain theory includes number predicates occuring formula trying prove custom- arily describe individual.', 'example, replace robot(num5) robot(r) r2d2(num5) r2d2(s) redo proof—using explanation proof template.', 'basing generalization process examples helps insure learn rules matched distribution problems occur.', 'the generalization process described ex- ample based [dejong & mooney, 1986] diﬀers [mitchell, et al.,'], ['note occurrence sees(r, r) node tree forces uniﬁcation x y domain rule, sees(x, y)∧habile(y) ⊃fixes(x, y).', 'generalize proof process replaces constants tip nodes proof tree variables works upward—using uniﬁcation constrain values variables needed obtain proof.']]\n",
      "\n",
      "[['finding new rule corresponds ﬁnding simpler expression formula proved terms evaluable predicates.', 'evaluable predicates 163 robust(r) fixes(r, r) sees(r,r) habile(s) robot(r) r2d2(s) {r/w} {s/x} {r/x, r/y, r/s} {r/u} robot(w) => sees(w,w) r2d2(x) => habile(x) sees(x,y) & habile(x) => fixes(x,y) fixes(u, u) => robust(u) r2d2(r) applying {r/s} figure 12.3 generalized proof tree “extensional,” logical rules “intensional.”', 'logical rules serve connect data base predicates higher level abstractions described (if deﬁned) rules.', 'evaluable predicates correspond components input pattern vector; predicates domain theory correspond hidden units.', 'typically look truth values formulas containing intensional predicates; derived rules database.', 'domain theory useful connecting formulas want prove truth values “looked up” evaluated.', 'usage reﬂects fact predicates data base deﬁned extension—we explicitly list tuples sastisfying relation.']]\n",
      "\n",
      "[['12.4, consider problem ﬁnding plan robot room r1 fetch box, b1, going adjacent room, r2, pushing', 'considering examples (num5 num6), arises, want generalize rules like robot(u)∧[c3po(u)∨r2d2(u)] ⊃robust(u)?', 'explanation-based learning 12.5 general proofs examining domain theory example reveals alternative rule been robot(u) ∧c3po(u) ⊃robust(u).', '12.7.1 macro-operators planning automatic planning systems, eﬃciency enhanced chain- ing sequence operators macro-operators.', 'example [dejong & mooney, 1986] structural generalization (via disjunctive augmen- tation ).', 'seeing number similar examples, willing induce formula bionic(u) ⊃[c3po(u) ∨r2d2(u)] case rule disjunction replaced robot(u) ∧bionic(u) ⊃robust(u).', 'adding new rule decreases depth shortest proof increases number formulas domain theory.', '12.6 utility ebl known theorem proving complexity ﬁnding proof depends number formulas domain theory depth shortest proof.', 'exam- ple process creating macro-operators based techniques explored [fikes, et al.,', 'example, eﬃciency retrieved evaluable predicate, say, bionic(u) domain theory contained r2d2(x) ⊃bionic(x) c3po(x) ⊃ bionic(x).']]\n",
      "\n",
      "[['r1 r2 r3 d1 d2 b1 initial state inroom(robot, r1) inroom(b1,r2) connects(d1,r1,r2) connects(d1,r2,r1) . . .', 'figure 12.4 initial state robot problem construct plan set strips operators include gothru(d, r1, r2) preconditions inroom(robot, r1), connects(d, r1, r2) delete list inroom(robot, r1) add list inroom(robot, r2) pushthru(b, d, r1, r2) preconditions inroom(robot, r1), connects(d, r1, r2), inroom(b, r1) delete list inroom(robot, r1), inroom(b, r1) add list inroom(robot, r2), inroom(b, r2) backward-reasoning strips system produce plan shown fig.']]\n",
      "\n",
      "[['preconditions generalized plan are inroom(robot, r1) connects(d1, r1, r2) connects(d2, r2, r4) inroom(b, r4) inroom(b1,r1) pushthru(b1,d,r1,r1) inroom(robot, r1), connects(d, r1, r1), inroom(b1, r1) inroom(robot, r2), connects(d1, r2, r1), inroom(b1, r2) {r2/r1, d1/d} gothru(d2, r3, r2) inroom(robot, r3), connects(d2, r3, r2), connects(d1, r2, r1), inroom(b1, r2) {r1/r3, d1/d2} inroom(robot, r1), connects(d1, r1, r2), connects(d1, r2, r1), inroom(b1, r2) r1 r2 r3 d1 d2 gothru(d1,r1,r2) pushthru(b1,d1,r2,r1) b1 plan figure 12.5 plan robot problem related technique chains sequences operators form general ones chunking mechanism soar [laird, et al.,', 'explanation-based learning connects(d1, r1, r2) connects(d1, r2, r1) inroom(b1, r2) saving speciﬁc plan, valid speciﬁc constants mentions, useful saving general one.']]\n",
      "\n",
      "[['system called prodigy, minton proposed ebl learn eﬀective ways control search [minton, 1988].', 'meta theory includes statements control choice subgoal work on, oper- ator apply, etc.', 'prodigy strips-like system solves planning problems blocks-world, simple mobile robot world, job-shop scheduling.', 'applications 167 inroom(b1,r4) pushthru(b1,d2,r2,r4) inroom(robot, r2), connects(d1, r1, r2), connects(d2, r2, r4), inroom(b1, r4) gothru(d1, r1, r2) inroom(robot, r1), connects(d1, r1, r2), connects(d2, r2, r4), inroom(b1, r4) figure 12.6 generalized plan 12.7.2 learning search control knowledge use creating macro-operators, ebl methods improve eﬃciency planning way also.', 'producing plan, analyzes successful unsuccessful choices attempts explain terms domain theory.']]\n",
      "\n",
      "[['explanation-based learning (and (current −node node) (candidate −goal node (on x y)) (candidate −goal node (on y z))) (prefer goal (on y z) (on x y)) prodigy keeps statistics learned rules used, savings (in time ﬁnd plans), cost application.', 'minton [minton, 1990] shown overall advantage rules (as having rules hand-coded search control rules).']]\n",
      "\n",
      "[['computational learning theory natural learning systems, volume 1 constraints prospects, pp.', 'anderson, 1958] anderson, t. w., introduction multivariate statistical analysis, new york john wiley, 1958. [', 'baum, 1994] baum, e., “when k-nearest neighbor backpropagation accurate feasible-sized sets examples?”', 'innovative applications artiﬁcial intelligence, menlo park, ca aaai press, 1992. [', 'aha, 1991] aha, d., kibler, d., albert, m., “instance-based learning algorithms,” machine learning, 6, 37-66, 1991. [', 'bibliography [acorn & walden, 1992] acorn, t., walden, s., “smart support man- agement automated reasoning technology compaq customer ser- vice,” proc.', 'bellman, 1957] bellman, r. e., dynamic programming, princeton princeton university press, 1957. [', 'anderson & bower, 1973] anderson, j. r., bower, g. h., human asso- ciative memory, hillsdale, nj erlbaum, 1973. [', 'barto, bradtke, & singh, 1994] barto, a., bradtke, s., singh, s., “learn- ing act real-time dynamic programming,” appear ar- tiﬁcial intelligence, 1994. [', 'baum & haussler, 1989] baum, e, haussler, d., “what size net gives valid generalization?”'], ['bollinger & duﬃe, 1988] bollinger, j., duﬃe, n., computer control machines processes, reading, ma addison-wesley, 1988.']]\n",
      "\n",
      "[['reprinted shavlik, j. dietterich, t., readings machine learn- ing, san francisco morgan kaufmann, 1990, pp 452-467.', 'brent, 1990] brent, r. p., “fast training algorithms multi-layer neural nets,” numerical analysis project manuscript na-90-03, computer sci- ence department, stanford university, stanford, 94305, march 1990. [', 'dayan & sejnowski, 1994] dayan, p., sejnowski, t., “td(λ) converges probability 1,” machine learning, 14, pp.', '3-10), contract da 36-039 sc-78343, sri international, menlo park, ca, june 1962 september 1962. [', 'dasarathy, 1991] dasarathy, b. v., nearest neighbor pattern classiﬁcation techniques, ieee computer society press, 1991. [', 'cover & hart, 1967] cover, t., hart, p., “nearest neighbor pattern clas- siﬁcation,” ieee trans.', '1984] breiman, l., friedman, j., olshen, r., stone, c., classiﬁcation regression trees, monterey, ca wadsworth, 1984. [', 'reprinted shavlik, j. dietterich, t., readings machine learning, morgan kaufmann, san francisco, pp.', 'dayan, 1992] dayan, p., “the convergence td(λ) general λ,” machine learning, 8, 341-362, 1992. [', 'carbonell, 1983] carbonell, j., “learning analogy,” machine learning artiﬁcial intelligence approach, michalski, r., carbonell, j., mitchell, t., (eds.),'], ['cover, 1965] cover, t., “geometrical statistical properties systems linear inequalities applications pattern recognition,” ieee trans.', 'dejong & mooney, 1986] dejong, g., mooney, r., “explanation-based learning alternative view,” machine learning, 1145-176, 1986.']]\n",
      "\n",
      "[['1990] dietterich, t., hild, h., bakiri, g., “a compara- tive study id3 backpropagation english text-to-speech map- ping,” proc.', 'report prepared onr contract 3438(00), sri in- ternational, menlo park, ca, april 1966. [', 'evans & fisher, 1992] evans, b., fisher, d., process delay analyses decision-tree induction, tech.', 'efron, 1982] efron, b., jackknife, bootstrap resampling plans, philadelphia siam, 1982. [', 'etzioni, 1993] etzioni, o., “a structural theory explanation-based learn- ing,” artiﬁcial intelligence, 601, pp.', 'report cs92-06, department com- puter science, vanderbilt university, tn, 1992. [', 'fahlman & lebiere, 1990] fahlman, s., lebiere, c., “the cascade- correlation learning architecture,” touretzky, d., (ed.),', 'bibliography 171 [dietterich & bakiri, 1991] dietterich, t. g., bakiri, g., “error-correcting output codes general method improving multiclass induc- tive learning programs,” proc.', 'duda, 1966] duda, r. o., “training linear machine mislabeled patterns,” sri tech.', 'duda & fossum, 1966] duda, r. o., fossum, h., “pattern classiﬁcation iteratively determined linear piecewise linear discriminant functions,” ieee trans.']]\n",
      "\n",
      "[['1993] fayyad, u. m., weir, n., djorgovski, s., “skicat machine learning system automated cataloging large scale sky surveys,” proc.', '1972] fikes, r., hart, p., nilsson, n., “learning execut- ing generalized robot plans,” artiﬁcial intelligence, pp 251-288, 1972.', 'reprinted shavlik, j. dietterich, t., readings machine learn- ing, san francisco morgan kaufmann, 1990, pp 468-486. [', 'fisher, 1987] fisher, d., “knowledge acquisition incremental conceptual clustering,” machine learning, 2139-172, 1987.', 'for longer version paper see fayyad, u. djorgovski, g., weir, n., “automating analysis cataloging sky surveys,” fayyad, u., et al.(eds.),', 'gallant, 1986] gallant, s. i., “optimal linear discriminants,” eighth inter- national conf.', 'reprinted shavlik, j. dietterich, t., readings machine learning, san francisco morgan kaufmann, 1990, pp.', 'genesereth & nilsson, 1987] genesereth, m., nilsson, n., logical founda- tions artiﬁcial intelligence, san francisco morgan kaufmann, 1987. [', 'haussler, 1988] haussler, d., “quantifying inductive bias ai learning al- gorithms valiant’s learning framework,” artiﬁcial intelligence, 36177-221, 1988.', 'fu, 1994] fu, l., neural networks artiﬁcial intelligence, new york mcgraw-hill, 1994. ['], ['gluck & rumelhart, 1989] gluck, m. rumelhart, d., neuroscience connectionist theory, developments connectionist theory, hills- dale, nj erlbaum associates, 1989. [', '1977] friedman, j. h., bentley, j. l., finkel, r. a., “an algorithm finding best matches logarithmic expected time,” acm trans.', 'feigenbaum, 1961] feigenbaum, e. a., “the simulation verbal learning be- havior,” proceedings western joint computer conference, 19121- 132, 1961. [']]\n",
      "\n",
      "[['hirsh, 1994] hirsh, h., “generalizing version spaces,” machine learning, 17, 5-45, 1994. [', 'ma- chine learning artiﬁcial intelligence approach, volume 2, chapter 20, san francisco morgan kaufmann, 1986. [', 'ieee pwer engineering society summer meeting, san francisco, ca, 1987. [', 'koza, 1994] koza, j., genetic programming ii automatic discovery reusable programs, cambridge, ma mit press, 1994.', 'koza, 1992] koza, j., genetic programming programming comput- ers means natural selection, cambridge, ma mit press, 1992. [', 'bibliography 173 [haussler, 1990] haussler, d., “probably approximately correct learning,” proc.', 'kohavi, 1994] kohavi, r., “bottom-up induction oblivious read-once de- cision graphs,” proc.', 'holland, 1986] holland, j. h., “escaping brittleness; possibilities general-purpose learning algorithms applied parallel rule-based systems.”', 'hunt, marin, & stone, 1966] hunt, e., marin, j., stone, p., experiments induction, new york academic press, 1966. [', 'hertz, krogh, & palmer, 1991] hertz, j., krogh, a, palmer, r., introduc- tion theory neural computation, lecture notes, vol.'], ['hebb, 1949] hebb, d. o., organization behaviour, new york john wiley, 1949. [', 'holland, 1975] holland, j., adaptation natural artiﬁcial systems, ann arbor university michigan press, 1975. (', 'kolodner, 1993] kolodner, j., case-based reasoning, san francisco morgan kaufmann, 1993. [', 'kaelbling, 1993] kaelbling, l. p., learning embedded systems, cambridge, ma mit press, 1993. [']]\n",
      "\n",
      "[['computational learning theory natural learning systems, volume 1 constraints prospects, pp.', 'langley, 1996] langley, p., elements machine learning, san francisco morgan kaufmann, 1996. [', '1986] laird, j., rosenbloom, p., newell, a., “chunking soar anatomy general learning mechanism,” machine learn- ing, 1, pp.', 'mcculloch & pitts, 1943] mcculloch, w. s., pitts, w. h., “a logical cal- culus ideas immanent nervous activity,” bulletin mathe- matical biophysics, vol.', 'minton, 1988] minton, s., learning search control knowledge explanation-based approach, kluwer academic publishers, boston, ma, 1988.', 'maass & tur´an, 1994] maass, w., tur´an, g., “how fast thresh- old gate learn?,”', 'lin, 1992] lin, l., “self-improving reactive agents based reinforcement learning, planning, teaching,” machine learning, 8, 293-321, 1992. [', 'lavraˇc & dˇzeroski, 1994] lavraˇc, n., dˇzeroski, s., inductive logic pro- gramming, chichester, england ellis horwood, 1994. [', 'mahadevan & connell, 1992] mahadevan, s., connell, j., “automatic programming behavior-based robots reinforcement learn- ing,” artiﬁcial intelligence, 55, pp.', 'marchand & golea, 1993] marchand, m., golea, m., “on learning sim- ple neural concepts halfspace intersections neural decision lists,” network, 467-85, 1993. ['], ['michie, 1992] michie, d., “some directions machine intelligence,” unpub- lished manuscript, turing institute, glasgow, scotland, 1992. [', 'littlestone, 1988] littlestone, n., “learning quickly irrelevant at- tributes abound new linear-threshold algorithm,” machine learn- ing 2 285-318, 1988. [']]\n",
      "\n",
      "[['muggleton, 1991] muggleton, s., “inductive logic programming,” new gen- eration computing, 8, pp.', 'natarjan, 1991] natarajan, b., machine learning theoretical approach, san francisco morgan kaufmann, 1991.', 'bibliography 175 [minton, 1990] minton, s., “quantitative results concerning utility explanation-based learning,” artiﬁcial intelligence, 42, pp.', 'reprinted shavlik, j. dietterich, t., readings machine learning, san francisco morgan kaufmann, 1990, pp.', 'muggleton, 1992] muggleton, s., inductive logic programming, london aca- demic press, 1992. [', 'moore, 1992] moore, a., “fast, robust adaptive control learning forward models,” moody, j., hanson, s., lippman, r., (eds.),', 'moore & atkeson, 1993] moore, a., atkeson, c., “prioritized sweeping reinforcement learning data time,” machine learn- ing, 13, pp.', 'mueller & page, 1988] mueller, r. page, r., symbolic computing lisp prolog, new york john wiley & sons, 1988. [', 'muroga, 1971] muroga, s., threshold logic applications, new york wiley, 1971. [', '1994] moore, a. w., hill, d. j., johnson, m. p., “an em- pirical investigation brute force choose features, smoothers, function approximators,” hanson, s., judd, s., petsche, t., (eds.),'], ['advances neural information processing systems 4, san francisco morgan kaufmann, 1992. [']]\n",
      "\n",
      "[['pomerleau, 1993] pomerleau, d, neural network perception mobile robot guidance, boston kluwer academic publishers, 1993. [', 'quinlan, 1986] quinlan, j. ross, “induction decision trees,” machine learning, 181–106, 1986.', 'quinlan, 1987] quinlan, j. r., “generating production rules decision trees,” ijcai-87 proceedings tenth intl.', 'pomerleau, 1991] pomerleau, d., “rapidly adapting artiﬁcial neural net- works autonomous navigation,” lippmann, p., et al. (', '176 bibliography [nilsson, 1965] nilsson, n. j., “theoretical experimental investigations trainable pattern-classifying systems,” tech.', 'peterson, 1961] peterson, w., error correcting codes, new york john wiley, 1961. [', 'quinlan & rivest, 1989] quinlan, j. ross, rivest, ron, “inferring deci- sion trees minimum description length principle,” informa- tion computation, 80227–248, march, 1989. [', 'reprinted shavlik, j. dietterich, t., readings machine learning, san francisco morgan kaufmann, 1990, pp.', 'pazzani & kibler, 1992] pazzani, m., kibler, d., “the utility knowl- edge inductive learning,” machine learning, 9, 57-94, 1992. [', 'pagallo & haussler, 1990] pagallo, g. haussler, d., “boolean feature dis- covery empirical learning,” machine learning, vol.5, no.1, pp.'], ['radc-tr- 65-257, final report contract af30(602)-3448, rome air develop- ment center (now rome laboratories), griﬃss air force base, new york, september, 1965. [', 'quinlan, 1990] quinlan, j. r., “learning logical deﬁnitions relations,” machine learning, 5, 239-266, 1990.', 'nilsson, 1990] nilsson, n. j., mathematical foundations learning ma- chines, san francisco morgan kaufmann, 1990. (', 'oliver, dowe, & wallace, 1992] oliver, j., dowe, d., wallace, c., “infer- ring decision graphs minimum message length principle,” proc.', 'this book reprint learning machines foundations trainable pattern-classifying systems, new york mcgraw-hill, 1965.) [']]\n",
      "\n",
      "[['shavlik, mooney, & towell, 1991] shavlik, j., mooney, r., towell, g., “symbolic neural learning algorithms experimental compar- ison,” machine learning, 6, pp.', 'rivest, 1987] rivest, r. l., “learning decision lists,” machine learning, 2, 229-246, 1987. [', 'com- putational learning theory natural learning systems, volume 1 constraints prospects, pp.', 'quinlan, 1994] quinlan, j. r., “comparing connectionist symbolic learn- ing methods,” hanson, s., drastal, g., rivest, r., (eds.),', 'ross, 1983] ross, s., introduction stochastic dynamic programming, new york academic press, 1983. [', 'russell & norvig 1995] russell, s., norvig, p., artiﬁcial intelligence modern approach, englewood cliﬀs, nj prentice hall, 1995. [', 'samuel, 1959] samuel, a., “some studies machine learning game checkers,” ibm journal research development, 3211-229, july 1959. [', 'rosenblatt, 1958] rosenblatt, f., principles neurodynamics, washington spartan books, 1961. [', 'ridgway, 1962] ridgway, w. c., adaptive logic system generalizing properties, phd thesis, tech.', 'sejnowski, koch, & churchland, 1988] sejnowski, t., koch, c., church- land, p., “computational neuroscience,” science, 241 1299-1306, 1988. ['], ['schwartz, 1993] schwartz, a., “a reinforcement learning method max- imizing undiscounted rewards,” proc.', 'shavlik & dietterich, 1990] shavlik, j. dietterich, t., readings ma- chine learning, san francisco morgan kaufmann, 1990.', 'bibliography 177 [quinlan, 1993] quinlan, j. ross, c4.5 programs machine learning, san francisco morgan kaufmann, 1993. [', 'rissanen, 1978] rissanen, j., “modeling shortest data description,” auto- matica, 14465-471, 1978. [', 'rumelhart, hinton, & williams, 1986] rumelhart, d. e., hinton, g. e., williams, r. j., “learning internal representations error propa- gation,” rumelhart, d. e., mcclelland, j. l., (eds.)']]\n",
      "\n",
      "[['178 bibliography [sutton & barto, 1987] sutton, r. s., barto, a. g., “a temporal- diﬀerence model classical conditioning,” proceedings ninth annual conference cognitive science society, hillsdale, nj erl- baum, 1987. [', 'watkins & dayan, 1992] watkins, c. j. c. h., dayan, p., “technical note q-learning,” machine learning, 8, 279-292, 1992.', 'tesauro, 1992] tesauro, g., “practical issues temporal diﬀerence learn- ing,” machine learning, 8, nos.', 'sutton, 1990] sutton, r., “integrated architectures learning, planning, reacting based approximating dynamic programming,” proc.', 'unger, 1989] unger, s., essence logic circuits, englewood cliﬀs, nj prentice-hall, 1989. [', 'sutton, 1988] sutton, r. s., “learning predict methods temporal diﬀerences,” machine learning 3 9-44, 1988. [', 'towell & shavlik, 1992] towell g., shavlik, j., “interpretation artiﬁ- cial neural networks mapping knowledge-based neural networks rules,” moody, j., hanson, s., lippmann, r., (eds.),', 'taylor, michie, & spiegalhalter, 1994] taylor, c., michie, d., spiegal- halter, d., machine learning, neural statistical classiﬁcation, paramount publishing international. [', 'towell, shavlik, & noordweier, 1990] towell, g., shavlik, j., noordweier, m., “reﬁnement approximate domain theories knowledge-based artiﬁcial neural networks,” proc.', 'utgoﬀ, 1989] utgoﬀ, p., “incremental induction decision trees,” machine learning, 4161–186, nov.,'], ['vapnik & chervonenkis, 1971] vapnik, v., chervonenkis, a., “on uniform convergence relative frequencies, theory probability applications, vol.', 'various editors, 1989-1994] advances neural information processing sys- tems, vols 1 6, san francisco morgan kaufmann, 1989 -1994. [']]\n",
      "\n",
      "[['winder, 1962] winder, r., threshold logic, phd dissertation, princeton uni- versity, princeton, nj, 1962. [', 'bibliography 179 [watkins, 1989] watkins, c. j. c. h., learning delayed rewards, phd thesis, university cambridge, england, 1989. [', 'widrow, 1962] widrow, b., “generalization storage networks ada- line neurons,” yovits, jacobi, goldstein (eds.),', 'widrow & lehr, 1990] widrow, b., lehr, m. a., “30 years adaptive neural networks perceptron, madaline backpropagation,” proc.', 'widrow & stearns, 1985] widrow, b., stearns, s., adaptive signal pro- cessing, englewood cliﬀs, nj prentice-hall. [', 'werbos, 1974] werbos, p., regression new tools prediction analysis behavioral sciences, ph.d.', 'weiss & kulikowski, 1991] weiss, s., kulikowski, c., computer systems learn, san francisco morgan kaufmann, 1991. [']]\n",
      "\n",
      "[['supervised machine learning algorithms, provide labeled data, example, prediction stock market prices, unsupervised need labeled data, example, classification emails spam non-spam.', 'lesser variables parameters, variance reduced ● cross-validation methods like k-folds ● model parameters likely cause overfitting, techniques regularization like lasso penalize parameters steve nouri \\u200b \\u200bhttps//www.linkedin.com/in/stevenouri/', 'whereas, use regression analysis dealing continuous data, example predicting stock prices certain point time.', 'overfitting situation occurs model learns training set well, taking random fluctuations training data concepts.', 'examples include decision trees, k-nearest neighbors, topic models latent dirichlet analysis.', '100 machine learning s & answers steve nouri q1 explain difference supervised unsupervised machine learning?', 'involves cost term features involved objective function ● making simple model.']]\n",
      "\n",
      "[['easiest ways handle missing corrupted data drop rows columns replace entirely value.', 'useful methods pandas ● isnull() dropna() help find columns/rows missing data drop ● fillna() replace wrong values placeholder value q7 explain ensemble learning.', 'algorithms methods finding set parameters minimize loss function evaluating parameters data making adjustments.', 'split given data set different sections namely,’training set’ ‘test set’. ‘', \"stochastic gradient descent, you'll evaluate 1 training sample set parameters updating them.\", 'complex models prone overfitting (high variance) expressive close truth (low bias).', 'ensemble learning, base models like classifiers regressors generated combined better results.', 'predictive models tradeoff bias (how model fits data) variance (how model changes based changes inputs).', 'training set small, model right bias low variance work better likely overfit.']]\n",
      "\n",
      "[['models low bias high variance tend perform better work fine complex relationships.', 'fourier transform converts signal time frequency domain — it’s common way extract features audio signals time series sensor data.', 'l2 regularization tends spread error terms, l1 binary/sparse, variables assigned 1 0 weighting.', 'deep learning subset machine learning concerned neural networks use backpropagation certain principles neuroscience accurately model large sets unlabelled semi-structured data.', 'term ‘false positive,’ word ‘positive’ refers ‘yes’ row predicted value confusion matrix.', \"test set small, you'll unreliable estimation model performance (performance statistic high variance).\", 'fourier transform finds set cycle speeds, amplitudes, phases match time signal.']]\n",
      "\n",
      "[['applications supervised machine learning include ● email spam detection train model historical data consists emails categorized spam spam.', '● healthcare diagnosis providing images disease, model trained detect person suffering disease not.', 'generative model learn categories data discriminative model simply learn distinction different categories data.', 'case semi-supervised learning, training data contains small labeled data large unlabeled data.', 'supervised learning uses data completely labeled, unsupervised learning uses training data.', '● fraud detection training model identify suspicious patterns, detect instances possible fraud.', '● sentiment analysis refers process algorithms documents determine they’re positive, neutral, negative sentiment.', '● example, ecommerce website suggest items buy, based prior purchases made, spending habits, items wishlist, customers’ purchase habits, on.']]\n",
      "\n",
      "[['latent dirichlet allocation (lda) common method topic modeling, classifying documents subject matter.', 'lda generative model represents documents mixture topics probability distribution possible words. \"', 'algorithm assumes presence feature class related presence feature (absolute independence features), given class variable.', 'new features, principal components, sequentially maximize variance represented (i.e. principal component variance, second principal component second most, on).', 'weighted average precision recall model, results tending 1 best, tending 0 worst.']]\n",
      "\n",
      "[['● time email hit inbox, spam filter use statistical analysis algorithms like decision trees svm determine likely email spam ● likelihood high, label spam, email won’t hit inbox steve nouri \\u200b \\u200bhttps//www.linkedin.com/in/stevenouri/', 'building spam filter involves following process ● email spam filter fed thousands emails ● emails label ‘spam’ ‘not spam.’', 'simple restatement fundamental problem machine learning possibility overfitting training data carrying noise data test set, providing inaccurate generalizations.', 'fixed rule choose algorithm classification problem, follow guidelines ● accuracy concern, test different algorithms cross-validate ● training dataset small, use models low variance high bias ● training dataset large, use models high variance little bias q28 design email spam filter?', 'main methods avoid overfitting 1- model simpler reduce variance taking account fewer variables parameters, removing noise training data.', '3- use regularization techniques lasso penalize certain model parameters they’re likely cause overfitting.', 'use classification regression wanted results reflect belongingness data points dataset certain explicit categories (ex wanted know male female correlated male female names.)', '● supervised machine learning algorithm determine type emails marked spam based spam words like lottery, free offer, money, refund, etc.', 'classification produces discrete values dataset strict categories, regression gives continuous results allow better distinguish differences individual points.']]\n",
      "\n",
      "[['roc (receiver operating characteristic) performance plot binary classifiers true positive rate (y-axis) vs. false positive rate (x- axis).', 'split dataset training test sets, use cross-validation techniques segment dataset composite sets training test sets data.', '● based accuracy model, use algorithm highest accuracy testing models q29 evaluation approaches work gauge effectiveness machine learning model?', 'lot machine learning interview s type involve implementation machine learning models company’s problems.', \"auc area roc curve, it's common performance metric evaluating binary classification models.\", \"it's equivalent expected probability uniformly drawn random positive ranked uniformly drawn random negative.\", 'what’s important demonstrate understand nuances model measured choose right performance measures right situations.', 'you’ll research company industry in-depth, especially revenue drivers company has, types users company takes context industry it’s in.']]\n",
      "\n",
      "[['● precision = (true positive) / (true positive + false positive) recall ● recall ratio number events recall number total events.', 'begin leaf nodes popular pruning algorithm called reduced error pruning, which ● starting leaves, node replaced popular class ● prediction accuracy affected, change kept ● advantage simplicity speed steve nouri \\u200b \\u200bhttps//www.linkedin.com/in/stevenouri/', 'advantages\\u200b neural networks (specifically deep nns) led performance breakthroughs unstructured datasets images, audio, video.', \"example, want detect type cancer that's prevalent 1% population, build model achieves 99% accuracy simply classifying cancer-free.\", 'precision ● precision ratio events correctly recall total number events recall (mix correct wrong recalls).', 'decision tree builds classification (or regression) models tree structure, datasets broken ever-smaller subsets developing decision tree, literally tree-like way branches nodes.', '● recall = (true positive) / (true positive + false negative) q36 decision tree classification?', 'q33 area roc curve (auroc) better raw accuracy out-of-sample evaluation metric?']]\n",
      "\n",
      "[['mechanisms similar first, means order k-nearest neighbors work, need labeled data want classify unlabeled point (thus nearest neighbor part).', 'reduce dimensionality combining features feature engineering, removing collinear features, algorithmic dimensionality reduction.', 'here, it’s important remember while, model needs checked sure it’s working correctly.', 'k-means clustering requires set unlabeled points threshold algorithm unlabeled points gradually learn cluster groups computing mean distance different points.', 'spotify shopped amazon recognize recommendation system it’s information filtering system predicts user want hear based choice patterns provided user.', 'stages building machine learning model are ● model building choose suitable algorithm model train according requirement ● model testing\\u200b check accuracy model test data ● applying mode \\u200bmake required changes testing use final model real-time projects.', 'machine learning relates study, design, development algorithms computers capability learn explicitly programmed.', 'data mining defined process unstructured data tries extract knowledge unknown interesting patterns.']]\n",
      "\n",
      "[['pca (principal components analysis), kpca ( kernel-based principal component analysis) ica ( independent component analysis) important feature extraction techniques dimensionality reduction.', 'statistical learning techniques allow learning function predictor set observed data predictions unseen future data.', 'bias term measures closely average classifier produced learning algorithm matches target function.', 'different types techniques machine learning ● supervised learning ● unsupervised learning ● semi-supervised learning ● reinforcement learning ● transduction ● learning learn q45 given data set.', 'know, normal distribution, ~68% data lies 1 standard deviation mean (or mode, median), leaves ~32% data unaffected.', 'techniques provide guarantees performance learned predictor future unseen data based statistical assumption data generating process.']]\n",
      "\n",
      "[['conceptually, say, lasso regression (l1) variable selection parameter shrinkage, ridge regression parameter shrinkage end including coefficients model.', 'context confusion matrix, type error occurs classify value positive (1) actually negative (0).', 'hence, classifier run unseen sample, couldn’t find patterns returned predictions higher error.', 'type error committed null hypothesis true reject it, known ‘false positive’.', 'convex hull created, maximum margin hyperplane (mmh) perpendicular bisector convex hulls.', 'type ii error committed null hypothesis false accept it, known ‘false negative’.', 'quote islr’s authors hastie, tibshirani asserted that, presence variables medium / large sized effect, use lasso regression.', 'case linearly separable data, convex hull represents outer boundaries groups data points.', 'training error 0.00 means classifier mimicked training data patterns extent, available unseen data.']]\n",
      "\n",
      "[['example think chessboard, movement bishop rook calculated manhattan distance respective vertical & horizontal movements.', 'simple words, ordinary square(ols) method linear regression approximates parameters resulting minimum distance actual predicted values.', 'vif = variance model / variance model single independent variable calculate ratio independent variable.', 'maximum likelihood helps choosing values parameters maximizes likelihood parameters likely produce observed data.', 'q53 suggest treating categorical variable continuous variable result better predictive model?', 'ols maximum likelihood methods respective regression methods approximate unknown parameter (coefficient) value.']]\n",
      "\n",
      "[['● multi-class classification problems, svms require one-vs-rest method, scalable memory intensive.', 'couple drawbacks linear model ● linear model holds strong assumptions true application.', 'simplest example cross-validation split data groups training data testing data, use training data build model testing data test model.', 'couple reasons random forest better choice model support vector machine ● random forests allow determine feature importance.', 'assumes linear relationship, multivariate normality, little multicollinearity, auto-correlation, homoscedasticity ● linear model can’t discrete binary outcomes.']]\n",
      "\n",
      "[['large error gradients accumulate result large changes neural network weights training, called exploding gradient problem.', 'causality applies situations action, x, causes outcome, y, correlation relating action (x) action(y) x necessarily cause y. q67 exploding gradient problem backpropagation technique?', 'rotation pca important maximizes separation variance obtained components interpretation components easier.', 'explain kernel trick kernel way computing dot product vectors 𝐱x 𝐲y (possibly high dimensional) feature space, kernel functions called “generalized dot product” kernel trick method linear classifier solve non-linear problem transforming linearly inseparable data linearly separable ones higher dimension.', 'answer yes random forest ensemble method takes weak decision trees strong learner.', 'associative rule mining techniques discover patterns data like features (dimensions) occur features (dimensions) correlated.', 'marginalizationarginalisation summing probability random variable x given joint probability distribution x variables.']]\n",
      "\n",
      "[['learning rate compensates penalises hyperplanes making wrong moves expansion rate deals finding maximum separation area classes.', 'rsquared represents variance captured virtual linear regression line respect total variance captured dataset.', 'context data science aiml, pruning refers process reducing redundant branches decision tree.', 'handle outliers, cap threshold, use transformations reduce skewness data remove outliers anomalies errors.', 'pruning involves turning branches decision tree leaf nodes removing leaf nodes original branch.', 'discover outliers tools functions like box plot, scatter plot, z-score, iqr score etc.', 'goals model training identify signal ignore noise model given free rein minimize error, possibility suffering overfitting.', 'q72 linear regression line stop rotating finds optimal spot fitted data?', 'decision trees prone overfitting, pruning tree helps reduce size minimizes chances overfitting.']]\n",
      "\n",
      "[['data augmentation technique synthesizing new data modifying existing data way target changed, changed known way.', 'example, ocr, flips change text won’t beneficial; however, resizes small rotations help.', 'visualization ● univariate visualization ● bivariate visualization ● multivariate visualization missing value treatmen\\u200bt – replace missing values mean/median outlier detection – use boxplot identify distribution outliers, apply iqr set boundary iqr q78 data augmentation?', 'modifications images ● resize ● horizontal vertical flip ● rotate ● add noise ● deform ● modify colors problem needs customized data augmentation pipeline.', 'exploratory data analysis (eda) helps analysts understand data better forms foundation better models.', 'inductive logic programming (ilp) subfield machine learning uses logic programming representing background knowledge examples.', 'difference inductive machine learning deductive machine learning follows machine-learning model learns examples set observed instances draw generalized conclusion deductive learning model draws conclusion conclusion drawn.']]\n",
      "\n",
      "[['firstly, need clear picture data, constraints, problems heading different machine learning algorithms.', 'that, try optimize hyperparameters ways – grid search, random search, bayesian optimization.', 'important steps follow achieve good working model data collection, data preparation, choosing machine learning model, training model, model evaluation, parameter tuning lastly prediction.', 'mainly based artificial neural network data taken input technique makes intuitive decisions artificial neural network.', 'following step data categorization step, two-step process – categorization input categorization output.', 'q81 difference machine learning deep learning machine learning branch computer science method implement artificial intelligence.', 'secondly, understand type kind data plays primary role deciding algorithm use.']]\n",
      "\n",
      "[['train multi-layered neural network prime motivation backpropagation learn appropriate internal demonstrations.', 'easy, sure skip closely related artificial intelligence thereby, ai interview s. machine learning algorithms easy serialize.', 'convex function continuous function, value midpoint interval given domain numerical mean values ends interval.', \"bayesian networks referred 'belief networks' 'casual networks', represent graphical model probability relationship set variables.\", 'true positive rate machine learning percentage positives properly acknowledged, recall count results correctly identified relevant.', 'genetic programming software systems implement algorithm uses random mutation, fitness function, crossover, multiple generations evolution resolve user-defined task.']]\n",
      "\n",
      "[['surely, people going engage machine learning near future q93 define sampling.', 'entropy measure uncertainty associated random variable y. expected number bits required communicate value variable.', '● locates certain patterns data makes certain predictions provide answers matters.', 'decision boundary decision surface hypersurface divides underlying feature space subspaces, class.', 'bayesian networks relate variables (e.g., speech signals protein sequences) called dynamic bayesian networks.', 'bayesian logic program consists components ● logical contains set bayesian clauses, capture qualitative structure domain.', 'intents machine learning stated below, ● system gets information established computations well-founded decisions outputs.', 'navigation system considered examples machine learning calculate distance places optimization techniques.']]\n",
      "\n",
      "[['learning curve, training error cross-validating error plotted number training data points.', 'aim generative model generate new samples distribution new data instances, whereas, discriminative model highlights differences different kinds data instances.', 'references 1 springboard.com 2 \\u200bsimplilearn.com 3 geeksforgeeks.org 4 \\u200belitedatascience.com 5 analyticsvidhya.com 6 \\u200bguru99.com 7 \\u200bintellipaat.com 8 towardsdatascience.com 9 mygreatlearning.com 10 \\u200bmindmajix.com 11 toptal.com 12 \\u200bglassdoor.co.in 13 \\u200budacity.com 14 educba.com 15 \\u200banalyticsindiamag.com 16 \\u200bubuntupit.com 17 \\u200bjavatpoint.com 18 quora.com 19 hackr.io 20 kaggle.com steve nouri \\u200b \\u200bhttps//www.linkedin.com/in/stevenouri/', 'in-depth knowledge statistics, probability, data modelling, programming language, cs, application ml libraries algorithms, software design required successful machine learning engineer.', 'feature engineering process transforming raw data features better represent underlying problem predictive models, resulting improved model accuracy unseen data.']]\n",
      "\n",
      "[['a. count word document b. vector notation word c. speech tag d. basic dependency grammar e. answer e)all features text corpus.', 'a. lemmatization b. soundex c. cosine similarity d. n-grams answer a) lemmatization helps base form word, e.g. playing -> play, eating -> eat, etc.other options meant different purposes.', 'a. lemmatization b. euclidean distance c. cosine similarity d. n-grams answer b) c) distance word vectors computed cosine similarity euclidean distance.', 'created document term matrix input data 20k documents machine learning model.', 'e.g. cosine angle words “football” “cricket” closer 1 compared angle words “football” “new delhi” q3.']]\n",
      "\n",
      "[['latent dirichlet allocation a. 1 b. 2, 3 c. 1, 3 d. 1, 2, 3 answer d) q5.', 'dissimilarity words expressed cosine similarity values significantly higher 0.5 a. true b. false answer a) q7.', 'a. speech tagging b. skip gram n-gram extraction c. continuous bag words d. dependency parsing constituency parsing answer d) q6.', 'named entity help extract organization, time, date, city, etc..type entities given sentence, speech helps extract noun, verb, pronoun, adjective, etc..from given sentence tokens.', 'a. detecting objects image b. facial recognition c. speech biometric d. text summarization', 'text parsing techniques noun phrase detection, verb phrase detection, subject detection, object detection nlp.', 'following keyword normalization techniques nlp a. stemming b. speech c. named entity recognition d. lemmatization answer a) d) speech (pos) named entity recognition(ner) keyword normalization techniques.']]\n",
      "\n",
      "[['correct value product tf (term frequency) idf (inverse-document- frequency), term “hello” appears approximately one-third total documents?', 'nlp, process removing words like “and”, “is”, “a”, “an”, “the” sentence called a. stemming b. lemmatization c. stop word d. answer c) lemmatization, stop words a, an, the, etc.. removed.', 'steve nouri https//www.linkedin.com/in/stevenouri/ answer (d) a) b) computer vision use cases, c) speech use case.', 'nlp, algorithm decreases weight commonly words increases weight words collection documents a. term frequency (tf) b. inverse document frequency (idf) c. word2vec d. latent dirichlet allocation (lda) answer b) q11.', 'a. kt * log(3) b. t * log(3) / k c. k * log(3) / t d. log(3) / kt answer (c) formula tf k/t formula idf log(total docs / docs containing “data”) = log(1 / (⅓)) = log (3) correct choice klog(3)/t q10.', 'corpus n documents, randomly chosen document contains total t terms term “hello” appears k times.']]\n",
      "\n",
      "[['q14 identify odd a. nltk b. scikit learn c. spacy d. bert answer d) ones mentioned nlp libraries bert, word embedding q15 tf-idf helps establish?', '● idf obtained dividing total number documents number documents containing term taking logarithm quotient.', 'nlp, process converting sentence paragraph tokens referred stemming a. true b. false answer b) statement describes process tokenization stemming, false.', 'nlp, tokens converted numbers giving neural network a. true b. false answer a) nlp, words converted number feeding neural network.', 'tf-idf takes account number times word appears document offset number documents appear corpus.', 'a. frequently occurring word document b. important word document answer b) tf-idf helps establish important particular word context document corpus.']]\n",
      "\n",
      "[['q18 text mining, converting text tokens converting integer floating-point vectors a. countvectorizer b. tf-idf c. bag words d. ners answer a) countvectorizer helps above, applicable.', 'q16 nlp, process identifying people, organization given sentence, paragraph called a. stemming b. lemmatization c. stop word removal d. named entity recognition answer d) q17 following pre-processing technique nlp a. stemming lemmatization b. converting lowercase c. removing punctuations d. removal stop words e. sentiment analysis answer e) sentiment analysis pre-processing technique.']]\n",
      "\n",
      "[['d) answer c) bert (bidirectional encoder representations transformer) supports context modelling previous sentence context taken consideration.', 'bert model uses previous sentence arrive context.word2vec glove word embeddings, provide context.', 'following word embeddings custom trained specific subject nlp a. word2vec b. bert', 'steve nouri https//www.linkedin.com/in/stevenouri/ print(vector.toarray()) output [[1 1 1 1 2 1 1 1 1 1 1 1 1 1]] second section interview s covers advanced nlp techniques word2vec, glove word embeddings, advanced models gpt, elmo, bert, xlnet based s, explanations.', 'nlp, words represented vectors called neural word embeddings a. true b. false answer a) word2vec, glove based models build word embedding vectors multidimensional.', 'nlp, bidirectional context supported following embedding a. word2vec b. bert c. glove d. answer b) bert provides bidirectional context.']]\n",
      "\n",
      "[['word embeddings capture multiple dimensions data represented vectors a. true b. false answer a) q24.', 'following better choice address nlp use cases semantic similarity, reading comprehension, common sense reasoning a. elmo b. open ai’s gpt c. ulmfit answer b) open ai’s gpt able learn complex pattern data transformer models attention mechanism suited complex use cases semantic similarity, reading comprehensions, common sense reasoning.', 'language biases introduced historical data training word embeddings, example bias a. new delhi india, beijing china b. man computer, woman homemaker answer a) statement b) bias buckets woman homemaker, statement a) biased statement.', 'steve nouri https//www.linkedin.com/in/stevenouri/ c. glove d. answer b) bert allows transform learning existing pre-trained models custom trained given specific subject, unlike word2vec glove existing word embeddings used, transfer learning text possible.', 'nlp, word embedding vectors help establish distance tokens a. true b. false answer a) use cosine similarity establish distance vectors represented word embeddings q25.']]\n",
      "\n",
      "[['trains independent lstm language model left right right left shallowly concatenates a. gpt b. bert c. ulmfit d. elmo', 'following architecture trained faster needs training data a. lstm based language modelling b. transformer architecture answer b) transformer architectures supported gpt onwards faster train needed data training too.', 'a. glove b. word2vec c. elmo d. nltk answer c) emlo word embeddings supports word multiple embeddings, helps word different context captures context meaning word unlike glove word2vec.', 'q30 given token, input representation sum embedding token, segment position embedding a. elmo b. gpt c. bert d. ulmfit answer c) bert uses token, segment position embedding.', 'steve nouri https//www.linkedin.com/in/stevenouri/ c. open ai’s gpt d. ulmfit answer c) ulmfit lstm based language modeling architecture.']]\n",
      "\n",
      "[['● sentiment analysis ● language translation (english german, chinese english, etc..) ● document summarization ● answering ● sentence completion ● attribute extraction (key information extraction documents) ● chatbot interactions ● topic classification ● intent extraction ● grammar sentence correction ● image captioning ● document ranking ● natural language inference q35.', 'a. openai gpt b. elmo c. bert d. ulmfit answer c)bert transformer architecture models relationship word words sentence generate attention scores.', 'attention scores later weights weighted average words’ representations fed fully-connected network generate new representation.', 'uses unidirectional language model producing word embedding a. bert b. gpt c. elmo d. word2vec answer b) gpt unidirectional model word embedding produced training information flow left right.', 'transformer model pays attention important word sentence a. true b. false', 'steve nouri https//www.linkedin.com/in/stevenouri/ answer d) elmo tries train independent lstm language models (left right right left) concatenates results produce word embedding.']]\n",
      "\n",
      "[['series nlp model forms family algorithms wide range classification tasks including sentiment prediction, filtering spam, classifying documents more.', 'outperformed bert 20 tasks achieves state art results 18 tasks including sentiment analysis, answering, natural language inference, etc.', 'steve nouri https//www.linkedin.com/in/stevenouri/ answer a) attention mechanisms transformer model model relationship words provide weights important word.', 'compared discriminative models like logistic regression, naive bayes model takes lesser time train.', 'algorithm perfect use working multiple classes text classification data dynamic changes frequently.', 'permutation language models feature a. bert b. emmo c. gpt d. xlnet answer d) xlnet provides permutation-based language modelling key difference bert.', 'a. bert b. xlnet c. gpt-2 d. elmo answer b) xlnet given best accuracy models.', 'transformer xl uses relative positional embedding a. true b. false a) instead embedding having represent absolute position word, transformer xl uses embedding encode relative distance words.']]\n",
      "\n",
      "[['text summarization intends create summary given piece text outlines main points document.', 'toolkit contains powerful libraries work different ml techniques break understand human language.', 'information extraction context natural language processing refers technique extracting structured information automatically unstructured sources ascribe meaning it.', 'models information extraction includes ● tagger module ● relation extraction module ● fact extraction module ● entity extraction module', 'text summarization proved blessing machines summarise large volumes text time time-consuming.', 'difference nltk spacey follows ● nltk collection programs choose from, spacey contains best- suited algorithm problem toolkit ● nltk supports wider range languages compared spacey (spacey supports 7 languages) ● spacey object-oriented library, nltk string processing library ● spacey support word vectors nltk q43.', 'nltk natural language toolkit series libraries programs symbolic statistical natural language processing.', 'steve nouri https//www.linkedin.com/in/stevenouri/ dependency parsing, known syntactic parsing nlp process assigning syntactic structure sentence identifying dependency parses.', 'dependency parsing needs resolve ambiguities order effectively assign syntactic structure sentence.']]\n",
      "\n",
      "[['parts speech tagging better known pos tagging refers process identifying specific words document group speech, based context.', 'pos tagging known grammatical tagging involves understanding grammatical structures identifying respective component.', 'steve nouri https//www.linkedin.com/in/stevenouri/ ● sentiment analysis module ● network graph module ● document classification & language modeling module q44.', 'masked language models help learners understand deep representations downstream tasks taking output corrupt input.', 'entity recognition commonly known ner process identifying specific entities text document informative unique context.', 'best nlp tools open sources are ● spacy ● textblob ● textacy ● natural language toolkit ● retext ● nlp.js ● stanford nlp ● cogcompnlp q49.', 'model creates occurrence matrix documents sentences irrespective grammatical structure word order.']]\n",
      "\n",
      "[['fact, ner involves entity chunking extraction entities segmented categorise different predefined classes.', 'pragmatic analysis deals outside word knowledge, means knowledge external documents and/or queries.', 'pragmatics analysis focuses described reinterpreted actually meant, deriving aspects language require real-world knowledge.', 'n-gram nlp simply sequence n words, conclude sentences appeared frequently, example, let consider progression words ● new york (2 gram) ● golden compass (3 gram) ● hotel (4 gram) sequence, easily conclude sentence (a) appeared frequently sentences, sentence(c) seen often.', 'perplexity high low; low perplexity ethical inability deal complicated problem high perplexity terrible failure deal complicated high.', 'word \"perplexed\" means \"puzzled\" \"confused\", perplexity general means inability tackle complicated problem specified.']]\n",
      "\n",
      "[['stop words was, were, is, am, the, a, an, how, why, more.', 'in terms computational complexity, self-attention layers faster recurrent layers sequence length n smaller representation dimensionality d, case sentence representations state-of-the-art models machine translations, word-piece byte-pair representations.” —']]\n",
      "\n",
      "[['example understand unigrams, bigrams, trigrams, refer diagram q62 steps involved solving nlp problem?', 'steve nouri https//www.linkedin.com/in/stevenouri/ latent semantic indexing mathematical technique improve accuracy information retrieval process.', 'design lsi algorithms allows machines detect hidden (latent) correlation semantics (words).', '● b regular expressions, + b regular expression language {a, b}.', 'suppose, b regular expressions, following true them ● {ɛ} regular language, ɛ regular expression it.']]\n",
      "\n",
      "[['q64 case processing natural language, normally mentioned common terminology nlp binding language terminology properly.', 'key factors given below ● vectors weights google word vectors, length tf-idf, varieties documents, word vectors, tf-idf.', 'q65 explain briefly word2vec word2vec embeds words lower-dimensional vector space shallow neural network.', '● analysis syntactic way mainly helps maintaining ordering properly available words.', 'help identifying human it’s fictional real, kind reality identification organization, events geographic location etc.', '● classification text learning supervising, set train, set validation dev, set define test, feature individual text, lda.', 'result set word-vectors vectors close vector space similar meanings based context, word-vectors distant differing meanings.', '● analysis sentiment know features sentiment, entities available sentiment, sentiment common dictionary.', 'major components explained below ● extraction entity actually identifying extracting critical data available information help segmentation provided sentence identifying entity.', '● reading machine language extraction possible entity, linking individual entity, dbpedia, libraries like pikes fred.']]\n",
      "\n",
      "[['names suggest, additive attention weighted sum multiplicative attention weighted multiplier hidden weights.', 'output time step passed decoder, resulting loss information learned previous time steps.', 'training process, model learns weights attention mechanisms recognize relative importance time step.', 'use attention encoder-decoder networks, fixed-dimensional vector passed decoder function vectors outputted intermediary steps.', 'encoder-decoder structure deep learning model architecture responsible state art solutions, including machine translation.', 'preprocessing steps commonly nlp tasks ● case normalization convert input case (lowercase uppercase) way reducing text canonical form ● punctuation/stop word/white space/special characters removal don’t think words characters relevant, remove reduce feature space ● lemmatizing/stemming reduce words inflectional forms (i.e. walks → walk) trim vocabulary ● generalizing irrelevant information replace numbers <number> token names <name> token q68 encoder-decoder structure work language modelling?']]\n",
      "\n",
      "[['word embeddings trained large corpus (for instance, extensive web scrape billions words), model vocabulary include common misspellings design.', 'terms found model vocabulary mapped “closest” vocabulary term using ● edit distance strings ● phonetic distance word pronunciations ● keyword distance catch common typos q72 following models perform tweet classification regards context mentioned above?', 'a) naive bayes b) svm c) solution (c) since, given data tweets information, means target variable present.', 'steve nouri https//www.linkedin.com/in/stevenouri/ diving productionization aspect, ideal machine learning service have ● endpoint(s) business systems use inference ● feedback mechanism validating model predictions ● database store predictions ground truths feedback ● workflow orchestrator (upon signal) re-train load new model serving based records database + prior training data ● form model version control facilitate rollbacks case bad deployments ● post-production accuracy error monitoring q71 handle misspellings text input?']]\n",
      "\n",
      "[['selection number topics directly proportional size data, number topic terms directly proportional size data.', 'number topic terms directly proportional size data a) 0 b) 25 c) 50 d) 75 e) 100 solution (a) lda unsupervised learning model, lda latent dirichlet allocation, linear discriminant analysis.', 'converting words lowercase affect dimensionality data a) 1 b) 2 c) 3 d) 1 2 e) 2 3 f) 1, 2 3 solution (d) choices b correct stopword removal decrease number features matrix, normalization words reduce redundant features, and, converting words lowercase decrease dimensionality.', 'a) frequency count terms b) vector notation sentence c) speech tag d) dependency grammar e) solution (e) techniques purpose engineering features model.']]\n",
      "\n",
      "[['time complexity lstm seq_length*hidden² time complexity transfomer seq_length²*hidden hidden size seq_length(which normally case), transfomer faster lstm.', 'better explained fastai videos) q78 difference learning latent features svd getting embedding vectors deep network?', '● exploding gradient(solved gradient clipping) ● dying relu — learning activation 0 (solved parametric relu) ● mean variance activations 0 1.(partially solved subtracting 0.5 activation.', 'number parameters lstm model bias 4(𝑚h+h²+h) 𝑚 input vectors size h output vectors size a.k.a.', 'steve nouri https//www.linkedin.com/in/stevenouri/ q76 latent dirichlet allocation model text classification purposes, alpha beta hyperparameter represent- a) alpha number topics documents, beta number terms topics false b) alpha density terms generated topics, beta density topics generated terms false c) alpha number topics documents, beta number terms topics false d) alpha density topics generated documents, beta density terms generated topics true solution (d) option d correct q77 problem relu?', 'hidden stores information till time step cell state stores particular information needed future time step.']]\n",
      "\n",
      "[['batch normalisation allows set higher learning rates, increasing speed training reduces unstability initial starting weights.', 'batchnorm — compute mean var layer minibatch layernorm — compute mean var single sample layer independently q85 transformer block layernorm instead batchnorm?', 'q88 tell language model doesn’t use dropout albert v2 — throws light fact lot assumptions granted necessarily true.', 'hard sharing train task time update weights losses soft sharing train task time.', 'looking advantages layernorm, robust batch size works better works sample level batch level.', 'steve nouri https//www.linkedin.com/in/stevenouri/ learning rate warm-up learning rate schedule low (or lower) learning rate beginning training avoid divergence unreliable gradients beginning.', 'batchnorm computes mean variance layer minibatch layernorm computes mean variance sample layer independently.', 'not great s checks awareness) ● lm tuning task text ● weight dropout ● discriminative learning rates layers ● gradual unfreezing layers ● slanted triangular learning rate schedule followed explaining help.']]\n",
      "\n",
      "[['● gpt bidirectional concept masking ● bert adds sentence prediction task training segment embedding q91 differences bert albert v2?', '● weights residual layers initially scaled factor 1/√n n number residual layers.', '● serve gpu/tpu/fpga ● 16 bit quantisation served gpu fp16 support ● pruning reduce parameters ● knowledge distillation (to smaller transformer model simple neural network) ● hierarchical softmax/adaptive softmax ● cache results explained here.', 'steve nouri https//www.linkedin.com/in/stevenouri/ ● layer normalization moved input sub-block, similar residual unit type “building block” (differently original type “bottleneck”, batch normalization applied weight layers).', 'k results tf-idf similarity rank results ● semantic encoding + cosine similarity ● model trained ranking']]\n",
      "\n",
      "[['recurrent neural networks, additional loop node loop essentially includes time component network well.', 'like normalizing input helps improve logistic regression model, normalize activations hidden layers deep learning model well q100 backpropagation different rnn compared ann?', 'interviewee things transfer learning latest models need talk having neutral class good accuracy/f1 still, model classify positive negative.', 'interviewee talk create dataset training strategies like selection language model, language model fine-tuning datasets multi- task learning.', 'regular expression representation natural language form mathematical expressions containing character sequence.', 'interviewer asked fundamentals deep learning architectures, key topic improving deep learning model’s performance.', 'hand, regular grammar generator natural language, defining set defined rules syntax strings natural language follow.', 'steve nouri https//www.linkedin.com/in/stevenouri/ q96 k results tf-idf similarity rank results ● semantic encoding + cosine similarity ● model trained ranking q97 sentiment classifier?']]\n",
      "\n",
      "[['100 python interview s prepare 2019 updated aug 14,2019 379.7k views python certiõcation sought-after skill programming domain.', 'mymock interview service real tech jobs \\ue913 mock interview latest tech domains i.e java, ai, devops,etc interviewed leading tech experts real time assement report video recording python mock interview \\ue90b \\ue921 follow linkedin more steve nouri https//www.linkedin.com/in/stevenouri/', 'compiled list python interview s classiõed 7 sections, namely basic interview s oops interview s basic python programs python libraries interview s web scraping interview s data analysis interview s multiple choice s (mcq) moving ahead, recording python interview s instructor shared experience expertise help crack python interview python interview s answers | python training | edureka aayushi johari technophile likes writing diàerent technologies spreading knowledge.', '100+ s python programming basics help diàerent expertise levels reap maximum beneõt blog.']]\n",
      "\n",
      "[['python dynamically typed, means don’t need state types variables declare like that.', 'things like x=111 x=\"i\\'m string\" error python suited object orientated programming allows deõnition classes composition inheritance.', 'classes õrst class objects writing python code quick running slower compiled languages.', 'numpy package good example this, it’s quick lot number crunching isn’t actually python python õnds use spheres – web applications, automation, scientiõc modeling, big data applications more.', 'means that, unlike languages like c variants, python need compiled run.', 'fortunately ，python allows inclusion c based extensions bottlenecks optimized away are.', 'python interview s answers | python interview preparati python interview s answers | python interview preparati… \\ue921', 'syntax list_1 = [10, ‘chelsea’, 20] syntax tup_1 = (10, ‘chelsea’ , 20) q2.']]\n",
      "\n",
      "[['global variables variables declared outside function global space called global variables.', 'commonly built-in modules are os sys math random data time json q10.what local variables global variables python?', 'example a=2 def add() b=3 c=a+b print(c) add() output 5 try access local variable outside function add(), throw error.', 'int() – converts data type integer type öoat() – converts data type öoat type ord() – converts characters integer hex() – converts integers hexadecimal oct() – converts integer octal tuple() – function convert tuple.']]\n",
      "\n",
      "[['ans install python windows, follow steps install python link https//www.python.org/downloads/ this, install pc.', 'look location python installed pc following command command prompt cmd python.', 'but, arrays hold single data type elements lists hold data type elements.', \"example import array arr my_array=arr.array('i',[1,2,3,4]) my_list=[1,'abc',1.20] print(my_array) print(my_list) output array(‘i’, [1, 2, 3, 4]) [1, ‘abc’, 1.2] q16.\", 'class employee def __init__(self, name, age,salary) self.name = self.age = age self.salary = 20000 e1 = employee(\"xyz\", 23, 20000) # e1 instance class employee. #__', 'example def newfunc() print(\"hi, welcome edureka\") newfunc(); #calling function output hi, welcome edureka q17.what __init__?']]\n",
      "\n",
      "[['self variable init method refers newly created object methods, refers object method called.', 'continue allows skipping loop speciõc condition met control transferred beginning loop pass need block code syntactically, want skip execution.', \"ans consider example shown below random import shuffle x = ['keep', 'the', 'blue', 'flag', 'flying', 'high'] shuffle(x) print(x) output following code below. ['\", \"example import array arr my_array=arr.array('i',[1,2,3,4,5]) my_array[-1] output array(‘i’, [5, 4, 3, 2, 1]) [-1] reprints reversed copy ordered data structures array list.\"]]\n",
      "\n",
      "[['need hold ctrl key left click place want include # character type # once.', 'means xrange doesn’t actually generate static list run-time like range does.', 'diàerence range returns python list object x range returns xrange object.', 'example #comments python start like print(\"comments python start #\") output comments python start # q27.', 'uniform(a, b) chooses öoating point number deõned range [a,b).iyt returns öoating point number 3.', 'especially true memory sensitive system cell phone working with, range use memory create array integers, result memory error crash program.', 'ans pickle module accepts python object converts string representation dumps õle dump function, process called pickling.', 'means gigantic range you’d like generate list for, billion, xrange function use.']]\n",
      "\n",
      "[[\"dict={'country''india','capital''delhi','pm''modi'} print dict[country] india print dict[capital] delhi print dict[pm] modi q37.\", 'help() function help() function display documentation string facilitates help related modules, keywords, attributes, etc.', 'ans help() dir() functions accessible python interpreter viewing consolidated dump built-in functions.', 'is returns true 2 operands true (example “a” ‘a’) not returns inverse boolean value in checks element present sequence q34.', 'code divides 2 numbers \"\"\" x=8 y=4 z=x/y print(z) output 2.0 q33.', 'python exits, especially python modules having circular references objects objects referenced global namespaces de-allocated freed.']]\n",
      "\n",
      "[['index negative number starts ‘-1’ represents index sequence ‘-2’ penultimate index sequence carries forward like positive number.', 'negative index remove new-line spaces string allow string character given s[-1].', 'sub() – õnds substrings regex pattern matches replace diàerent string subn() – similar sub() returns new string no.', 'kwargs don’t know keyword arguments passed function, pass values dictionary keyword arguments.', 'ans built-in types python follows – integers floating-point complex numbers strings boolean built-in functions q45.', 'support (fairly) eþcient insertion, deletion, appending, concatenation, python’s list comprehensions easy construct manipulate.', 'numbers positive uses ‘0’ uses õrst index ‘1’ second index process goes like that.', 'syntax ternary operator given as [on_true] [expression] [on_false]x, y = 25, 50big = x x < y y example expression gets evaluated like x<y y, case x<y true value returned big=x incorrect big=y sent result.', 'ans use *args aren’t sure arguments going passed function, want pass stored list tuple arguments function. **']]\n",
      "\n",
      "[['deep copy makes execution program slower making certain copies object called.', 'certain limitations don’t support “vectorized” operations like elementwise addition multiplication, fact contain objects diàering types mean python store type information element, execute type dispatching code operating element.', 'references point original objects changes member class aàect original copy it.', 'numpy array faster lot built numpy, ffts, convolutions, fast searching, basic statistics, linear algebra, histograms, etc.', \"example a=arr.array('d', [1.1 , 2.1 ,3.1] ) a.append(3.4) print(a) a.extend([4.5,6.3,6.8]) print(a) a.insert(2,3.8) print(a) output array(‘d’, [1.1, 2.1, 3.1, 3.4]) array(‘d’, [1.1, 2.1, 3.1, 3.4, 4.5, 6.3, 6.8]) array(‘d’, [1.1, 2.1, 3.8, 3.1, 3.4, 4.5, 6.3, 6.8]) q47.\", \"example a=arr.array('d', [1.1, 2.2, 3.8, 3.1, 3.7, 1.2, 4.6]) print(a.pop()) print(a.pop(3)) a.remove(1.1) print(a) output 4.6 3.1 array(‘d’, [2.2, 3.8, 3.7, 1.2]) q48.\", 'ans shallow copy new instance type gets created keeps values copied new instance.']]\n",
      "\n",
      "[['python multi-threading package want multi-thread speed code up, it’s usually good idea use it.', 'import modules ways- example import array #importing original module import array arr # importing alias array import * #imports present array module oops interview s q55.', 'happens quickly human eye like threads executing parallel, taking turns cpu core.', 'multi-level inheritance – derived class d1 inherited base class base1, d2 inherited base2.', 'ans compiling linking allows new extensions compiled properly error linking passes compiled procedure.', 'class inheriting called super- class class inherited called derived / child class.']]\n",
      "\n",
      "[['so, instance, parent class method named abc child class method abc having parameters variables.', 'python lays concept preõxing variable, function method single double underscore imitate behavior protected private access speciõers.', 'consider example # m.py class myclass def f(self) print \"f()\" run monkey-patch testing like this import m def monkey_f(self) print \"monkey_f()\" m.myclass.f = monkey_f obj = m.myclass() obj.f() output below monkey_f() see, changes behavior f() myclass function deõned, monkey_f(), outside module m. q58.', 'ans python, term monkey patch refers dynamic modiõcations class module run-time.', 'example class employee def __init__(self, name) self.name = e1=employee(\"abc\") print(e1.name) output abc q57.']]\n",
      "\n",
      "[[\"def pyfunc(r) x range(r) print(' '*(r-x-1)+'*'*(2*x+1)) pyfunc(9) output * *** ***** ******* ********* *********** ************* *************** ***************** q67.\", 'def bs(a)&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# = list &nbsp; b=len(a)-1&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# minus 1 compare 2 adjacent values &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; x range(b) &nbsp; &nbsp; &nbsp; y range(b-x) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; a[y]>a[y+1] &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; a[y],a[y+1]=a[y+1],a[y] &nbsp; return a=[32,5,3,6,7,54,87] bs(a) output [3, 5, 6, 7, 32, 54, 87] q66.', 'class a &nbsp; pass obj=a() obj.name=\"xyz\" print(\"name = \",obj.name) output = xyz q64.']]\n",
      "\n",
      "[['a=int(input(\"enter number\"))&nbsp; &nbsp; &nbsp; a>1 &nbsp; x range(2,a) &nbsp; &nbsp; &nbsp; if(a%x)==0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; print(\"not prime\") &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; break &nbsp; else &nbsp; &nbsp; &nbsp; print(\"prime\") else &nbsp; print(\"not prime\") output enter number 3 prime q69.', 'a=input(\"enter sequence\") b=a[-1] a==b &nbsp; print(\"palindrome\") else &nbsp; print(\"not palindrome\") output enter sequence 323 palindrome q70.', '# enter number terms needed&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#0,1,1,2,3,5.... a=int(input(\"enter terms\")) f=0&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#first element series s=1&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#second element series a<=0 &nbsp; print(\"the requested series \",f) else &nbsp; print(f,s,end=\" \") &nbsp; x range(2,a) &nbsp; &nbsp; &nbsp; next=f+s&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; print(next,end=\" \") &nbsp; &nbsp; &nbsp; f=s &nbsp; &nbsp; &nbsp; s=next</pre> output enter terms 5 0 1 1 2 3 q68.', 'open(some_large_file) fh count = 0 text = fh.read() character text character.isupper() count += 1 try transform single line.']]\n",
      "\n",
      "[[\"a0 = dict(zip(('a','b','c','d','e'),(1,2,3,4,5))) a1 = range(10)a2 = sorted([i a1 a0]) a3 = sorted([a0[s] s a0]) a4 = [i a1 a3] a5 = {ii*i a1} a6 = [[i,i*i] a1] print(a0,a1,a2,a3,a4,a5,a6) ans following õnal outputs a0, a1, … a6 a0 = {'a' 1, 'c' 3, 'b' 2, 'e' 5, 'd' 4} # order vary a1 = range(0, 10) a2 = [] a3 = [1, 2, 3, 4, 5] a4 = [1, 2, 3, 4, 5] a5 = {0 0, 1 1, 2 4, 3 9, 4 16, 5 25, 6 36, 7 49, 8 64, 9 81} a6 = [[0, 0], [1, 1], [2, 4], [3, 9], [4, 16], [5, 25], [6, 36], [7, 49], [8, 64], [9, 81]] python libraries interview s q73.\", 'ans django flask map url’s addresses typed web browsers functions python.', 'django consists prewritten code, user need analyze flask gives users create code, therefore, making simpler understand code.', 'ans flask web microframework python based “werkzeug, jinja2 good intentions” bsd license.', 'ans following code sort list python list = [\"1\", \"4\", \"0\", \"6\", \"9\"] list = [int(i) list] list.sort() print (list) q72.', 'flask simpler compared django but, flask lot meaning need specify details, django lot need work.']]\n",
      "\n",
      "[[\"add following lines code setting.py õle databases = { 'default' { 'engine' 'django.db.backends.sqlite3', 'name' os.path.join(base_dir, 'db.sqlite3'), } } q78.\", 'figure python interview s – django architecture developer provides model, view template maps url django magic serve user.', 'ans use command edit mysite/setting.py, normal python module module level representing django settings.', 'case sqlite database, case, database õle computer, absolute path, including õle õle.', 'database server— postgresql, mysql, oracle, mssql—and want use sqlite, use database’s administration tools create new database django project.', 'case database choice diàerent following keys database ‘default’ item match database connection settings.', 'template contains variables replaced values template evaluated tags (% tag %) control logic template.', 'ans use write view django django.http import httpresponse import datetime def current_datetime(request) = datetime.datetime.now() html = \"<html><body>it %s</body></html> % return httpresponse(html) returns current date time, html document q79.', 'django uses sqlite default; easy django users won’t require type installation.']]\n",
      "\n",
      "[['example, check google webcache age edureka.co you’d use following url http//webcache.googleusercontent.com/search?q=cacheedureka.co q84.', 'ans use following code save image locally url address import urllib.request urllib.request.urlretrieve(\"url\", \"local-filename.jpg\") q83.', 'figure python interview s – django framework data stored client side.', 'ans use following url format http//webcache.googleusercontent.com/search?q=cacheurlgoeshere sure replace “urlgoeshere” proper web address page site cache want retrieve time for.', 'django abstracts process sending receiving cookies, placing session id cookie client side, storing related data server side.', 'proxy models use model, want modify python level behavior model, changing model’s õelds.', 'abstract base classes style want parent’s class hold information don’t want type child model.']]\n",
      "\n",
      "[['ans calculate percentiles following code import numpy np = np.array([1,2,3,4,5]) p = np.percentile(a, 50) #returns 50th percentile, e.g. median print(p) output 3 q89.', 'ideal world, numpy contain array data type basic operations indexing, sorting, reshaping, basic elementwise functions, et cetera.', 'ans indices n maximum values numpy array code import numpy np arr = np.array([1, 3, 2, 4, 5]) print(arr.argsort()[-3][-1]) output [ 4 3 1 ] q88.', 'bs4 import beautifulsoup import requests import sys url = \\'http//www.imdb.com/chart/top\\' response = requests.get(url) soup = beautifulsoup(response.text) tr = soup.findchildren(\"tr\") tr = iter(tr) next(tr) movie tr title = movie.find(\\'td\\', {\\'class\\' \\'titlecolumn\\'} ).find(\\'a\\').contents[0] year = movie.find(\\'td\\', {\\'class\\' \\'titlecolumn\\'} ).find(\\'span\\', {\\'class\\' \\'secondaryinfo\\'}).contents[0] rating = movie.find(\\'td\\', {\\'class\\' \\'ratingcolumn imdbrating\\'} ).find(\\'strong\\').contents[0] row = title + \\' - \\' + year + \\' \\' + \\' \\' + rating print(row) code help scrap data imdb’s 250 list data analysis – python interview s q85.', 'ans map function executes function given õrst argument elements iterable given second argument.']]\n",
      "\n",
      "[['however, numpy’s important goals compatibility, numpy tries retain features supported predecessors.', 'a) / b) // c) % d) mentioned b) // operands integer python chops fraction gives round oà value, accurate answer use öoor division.', 'a) abc = 1,000,000 b) b c = 1000 2000 3000 c) a,b,c = 1000, 2000, 3000 d) a_b_c = 1,000,000 b) b c = 1000 2000 3000 spaces allowed variable names.', '2.5 answer, use öoor division //. so, 5//2 = 2.5 q93.', 'matplotlib provides basic 3d plotting mplot3d subpackage, mayavi provides wide range high-quality 3d visualization features, utilizing powerful vtk engine.', 'a) indicate private variables class b) confuse interpreter c) indicate global variables d) slow execution a) indicate private variable class python concept private variables, leading underscores indicate variables accessed outside class.', 'a) 31 characters b) 63 characters c) 79 characters d) d) identiõers length.', 'multiple correct answers possible) a) d = {} b) d = {“john”40, “peter”45} c) d = {40”john”, 45”peter”} d) d = (40”john”, 45”50”) b, c & d. dictionaries created specifying keys values.', 'ans like 2d plotting, 3d graphics scope numpy scipy, 2d case, packages exist integrate numpy.']]\n",
      "\n",
      "[['use libraries like pandas, numpy, matplotlib, scipy, scikit, pyspark master concepts like python machine learning, scripts, sequence, web scraping big data analytics leveraging apache spark.', 'open õle cscores.txt writing, use a) outõle = open(“cscores.txt”, “r”) b) outõle = open(“cscores.txt”, “w”) c) outõle = open(õle = “cscores.txt”, “r”) d) outõle = open(õle = “cscores.txt”, “o”) b) location contains double slashes ( ) w indicate õle written to.', '= 1 raise \"someerror\" else print(\"someerror occured\") \"someerror\" print (\"someerror occured\") a) someerror occured b) someerror occured c) invalid code d) c) invalid code new exception class inherit baseexception.', 'a) error b) c) 25 d) 2 c) 25 index -1 corresponds index list.', 'f = range (5) open(\"data.txt\", \"w\") f > 2 break print f.closed a) true b) false c) d) error a) true statement open õle guarantees õle object closed block exits.', 'a) b) exception occurs c) exception occurs d) exception occurs block c) exception occurs executed exception occurs.', 'wish learn python gain expertise quantitative analysis, data mining, presentation data numbers transforming career data scientist role, check interactive, live-online python certiõcation training.']]\n",
      "\n",
      "[['python inbuilt garbage collector, recycle unused memory frees memory makes available heap space.', 'converts source code written programmer intermediate language, translated machine language executed.', '1 / 7 read click >>>>> ai related cheatsheets tutorials place https//www.linkedin.com/pulse/all-cheatsheets-one-place-vipul-patel/', 'benefits pythons simple easy, portable, extensible, build-in data structure open source.', 'pickle module accepts python object converts string representation dumps file dump function, process called pickling.']]\n",
      "\n",
      "[['mutable immutable types pythons built types mutable built-in types list sets dictionaries immutable built-in types 2 / 7', 'pychecker static analysis tool detects bugs python source code warns style complexity bug.']]\n",
      "\n",
      "[['python documentation string known docstring, way documenting python 3 / 7', 'supports sharing setups, automation testing, shutdown code tests, aggregation tests collections etc.', 'mechanism select range items sequence types like list, tuple, strings etc.', 'pass means, no-operation python statement, words place holder compound statement, blank left written there.']]\n",
      "\n",
      "[[\"local variables variable assigned new value function's body, it's assumed local.\", 'xrange returns xrange object range returns list, uses memory matter range size is.']]\n",
      "\n",
      "[['generate random numbers python, need import command import random random.random() returns random floating point number range [0,1) 31) explain access module written python c?', 'python require explicit memory management interpreter allocates memory new variables free automatically provide easy readability use square brackets easy-to-learn beginners having built-in data types saves programming time effort declaring variables 34) mention use split function python?', 'access module written python c following method, module = =pyimport_importmodule(\"\"); 32) mention use // operator python?', 'floor divisionoperator , dividing operands result quotient showing digits decimal point.', \"http//career.guru99.com/ python script executable unix, need things, script file's mode executable line begin # ( #!\", 'python comprises huge standard library internet platforms like email, html, etc.']]\n",
      "\n",
      "[['features include flask wtf integration wtforms secure form csrf token global csrf protection internationalization integration recaptcha supporting file upload works flask uploads 38) explain common way flask script work?', 'flask web micro framework python based “werkzeug, jinja 2 good intentions” bsd licensed.', 'common way flask script work import path application path python file 39) explain access sessions flask?']]\n",
      "\n",
      "[['mvc perfect fit flask, pattern mvc consider following example powered tcpdf (www.tcpdf.org) 7 / 7']]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for page in tqdm(pages_and_metadata, total=len(pages_and_metadata)):\n",
    "    print(page[\"sentence_chunk\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedc481bbe1a825b",
   "metadata": {},
   "source": [
    "## 2.8 Converting sentence_chunks into sentence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f450c90f591d29ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:42:25.651367Z",
     "start_time": "2025-02-10T16:42:21.601242Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer(model_name_or_path=EMBEDDING_MODEL).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8820fa929ef41f04",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-02-10T16:42:47.633183Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Page 0\n",
      "Processing Page 1\n",
      "Processing Page 2\n",
      "Processing Page 3\n",
      "Processing Page 4\n",
      "Processing Page 5\n",
      "Processing Page 6\n",
      "Processing Page 7\n",
      "Processing Page 8\n",
      "Processing Page 9\n",
      "Processing Page 10\n",
      "Processing Page 11\n",
      "Processing Page 12\n",
      "Processing Page 13\n",
      "Processing Page 14\n",
      "Processing Page 15\n",
      "Processing Page 16\n",
      "Processing Page 17\n",
      "Processing Page 18\n",
      "Processing Page 19\n",
      "Processing Page 20\n",
      "Processing Page 21\n",
      "Processing Page 22\n",
      "Processing Page 23\n",
      "Processing Page 24\n",
      "Processing Page 25\n",
      "Processing Page 26\n",
      "Processing Page 27\n",
      "Processing Page 28\n",
      "Processing Page 29\n",
      "Processing Page 30\n",
      "Processing Page 31\n",
      "Processing Page 32\n",
      "Processing Page 33\n",
      "Processing Page 34\n",
      "Processing Page 35\n",
      "Processing Page 36\n",
      "Processing Page 37\n",
      "Processing Page 38\n",
      "Processing Page 39\n",
      "Processing Page 40\n",
      "Processing Page 41\n",
      "Processing Page 42\n",
      "Processing Page 43\n",
      "Processing Page 44\n",
      "Processing Page 45\n",
      "Processing Page 46\n",
      "Processing Page 47\n",
      "Processing Page 48\n",
      "Processing Page 49\n",
      "Processing Page 50\n",
      "Processing Page 51\n",
      "Processing Page 52\n",
      "Processing Page 53\n",
      "Processing Page 54\n",
      "Processing Page 55\n",
      "Processing Page 56\n",
      "Processing Page 57\n",
      "Processing Page 58\n",
      "Processing Page 59\n",
      "Processing Page 60\n",
      "Processing Page 61\n",
      "Processing Page 62\n",
      "Processing Page 63\n",
      "Processing Page 64\n",
      "Processing Page 65\n",
      "Processing Page 66\n",
      "Processing Page 67\n",
      "Processing Page 68\n",
      "Processing Page 69\n",
      "Processing Page 70\n",
      "Processing Page 71\n",
      "Processing Page 72\n",
      "Processing Page 73\n",
      "Processing Page 74\n",
      "Processing Page 75\n",
      "Processing Page 76\n",
      "Processing Page 77\n",
      "Processing Page 78\n",
      "Processing Page 79\n",
      "Processing Page 80\n",
      "Processing Page 81\n",
      "Processing Page 82\n",
      "Processing Page 83\n",
      "Processing Page 84\n",
      "Processing Page 85\n",
      "Processing Page 86\n",
      "Processing Page 87\n",
      "Processing Page 88\n",
      "Processing Page 89\n",
      "Processing Page 90\n",
      "Processing Page 91\n",
      "Processing Page 92\n",
      "Processing Page 93\n",
      "Processing Page 94\n",
      "Processing Page 95\n",
      "Processing Page 96\n",
      "Processing Page 97\n",
      "Processing Page 98\n",
      "Processing Page 99\n",
      "Processing Page 100\n",
      "Processing Page 101\n",
      "Processing Page 102\n",
      "Processing Page 103\n",
      "Processing Page 104\n",
      "Processing Page 105\n",
      "Processing Page 106\n",
      "Processing Page 107\n",
      "Processing Page 108\n",
      "Processing Page 109\n",
      "Processing Page 110\n",
      "Processing Page 111\n",
      "Processing Page 112\n",
      "Processing Page 113\n",
      "Processing Page 114\n",
      "Processing Page 115\n",
      "Processing Page 116\n",
      "Processing Page 117\n",
      "Processing Page 118\n",
      "Processing Page 119\n",
      "Processing Page 120\n",
      "Processing Page 121\n",
      "Processing Page 122\n",
      "Processing Page 123\n",
      "Processing Page 124\n",
      "Processing Page 125\n",
      "Processing Page 126\n",
      "Processing Page 127\n",
      "Processing Page 128\n",
      "Processing Page 129\n",
      "Processing Page 130\n",
      "Processing Page 131\n",
      "Processing Page 132\n",
      "Processing Page 133\n",
      "Processing Page 134\n",
      "Processing Page 135\n",
      "Processing Page 136\n",
      "Processing Page 137\n",
      "Processing Page 138\n",
      "Processing Page 139\n",
      "Processing Page 140\n",
      "Processing Page 141\n",
      "Processing Page 142\n",
      "Processing Page 143\n",
      "Processing Page 144\n",
      "Processing Page 145\n",
      "Processing Page 146\n",
      "Processing Page 147\n",
      "Processing Page 148\n",
      "Processing Page 149\n",
      "Processing Page 150\n",
      "Processing Page 151\n",
      "Processing Page 152\n",
      "Processing Page 153\n",
      "Processing Page 154\n",
      "Processing Page 155\n",
      "Processing Page 156\n",
      "Processing Page 157\n",
      "Processing Page 158\n",
      "Processing Page 159\n",
      "Processing Page 160\n",
      "Processing Page 161\n",
      "Processing Page 162\n",
      "Processing Page 163\n",
      "Processing Page 164\n",
      "Processing Page 165\n",
      "Processing Page 166\n",
      "Processing Page 167\n",
      "Processing Page 168\n",
      "Processing Page 169\n",
      "Processing Page 170\n",
      "Processing Page 171\n",
      "Processing Page 172\n",
      "Processing Page 173\n",
      "Processing Page 174\n",
      "Processing Page 175\n",
      "Processing Page 176\n",
      "Processing Page 177\n",
      "Processing Page 178\n",
      "Processing Page 179\n",
      "Processing Page 180\n",
      "Processing Page 181\n",
      "Processing Page 182\n",
      "Processing Page 183\n",
      "Processing Page 184\n",
      "Processing Page 185\n",
      "Processing Page 186\n",
      "Processing Page 187\n",
      "Processing Page 188\n",
      "Processing Page 189\n",
      "Processing Page 190\n",
      "Processing Page 191\n",
      "Processing Page 192\n",
      "Processing Page 193\n",
      "Processing Page 194\n",
      "Processing Page 195\n",
      "Processing Page 196\n",
      "Processing Page 197\n",
      "Processing Page 198\n",
      "Processing Page 199\n",
      "Processing Page 200\n",
      "Processing Page 201\n",
      "Processing Page 202\n",
      "Processing Page 203\n",
      "Processing Page 204\n",
      "Processing Page 205\n",
      "Processing Page 206\n",
      "Processing Page 207\n",
      "Processing Page 208\n",
      "Processing Page 209\n",
      "Processing Page 210\n",
      "Processing Page 211\n",
      "Processing Page 212\n",
      "Processing Page 213\n",
      "Processing Page 214\n",
      "Processing Page 215\n",
      "Processing Page 216\n",
      "Processing Page 217\n",
      "Processing Page 218\n",
      "Processing Page 219\n",
      "Processing Page 220\n",
      "Processing Page 221\n",
      "Processing Page 222\n",
      "Processing Page 223\n",
      "Processing Page 224\n",
      "Processing Page 225\n",
      "Processing Page 226\n",
      "Processing Page 227\n",
      "Processing Page 228\n",
      "Processing Page 229\n",
      "Processing Page 230\n",
      "Processing Page 231\n",
      "Processing Page 232\n",
      "Processing Page 233\n",
      "Processing Page 234\n",
      "Processing Page 235\n",
      "Processing Page 236\n",
      "Processing Page 237\n",
      "Processing Page 238\n",
      "Processing Page 239\n",
      "Processing Page 240\n",
      "Processing Page 241\n",
      "Processing Page 242\n",
      "Processing Page 243\n",
      "Processing Page 244\n",
      "Processing Page 245\n",
      "Processing Page 246\n",
      "Processing Page 247\n",
      "Processing Page 248\n",
      "Processing Page 249\n",
      "Processing Page 250\n",
      "Processing Page 251\n",
      "Processing Page 252\n",
      "Processing Page 253\n",
      "Processing Page 254\n",
      "Processing Page 255\n",
      "Processing Page 256\n",
      "Processing Page 257\n",
      "Processing Page 258\n",
      "Processing Page 259\n",
      "Processing Page 260\n",
      "Processing Page 261\n",
      "Processing Page 262\n",
      "Processing Page 263\n",
      "Processing Page 264\n",
      "Processing Page 265\n",
      "Processing Page 266\n",
      "Processing Page 267\n",
      "Processing Page 268\n",
      "Processing Page 269\n",
      "Processing Page 270\n",
      "Processing Page 271\n",
      "Processing Page 272\n",
      "Processing Page 273\n",
      "Processing Page 274\n",
      "Processing Page 275\n",
      "Processing Page 276\n",
      "Processing Page 277\n",
      "Processing Page 278\n",
      "Processing Page 279\n",
      "Processing Page 280\n",
      "Processing Page 281\n",
      "Processing Page 282\n",
      "Processing Page 283\n",
      "Processing Page 284\n",
      "Processing Page 285\n",
      "Processing Page 286\n",
      "Processing Page 287\n",
      "Processing Page 288\n",
      "Processing Page 289\n",
      "Processing Page 290\n",
      "Processing Page 291\n",
      "Processing Page 292\n",
      "Processing Page 293\n",
      "Processing Page 294\n",
      "Processing Page 295\n",
      "Processing Page 296\n",
      "Processing Page 297\n",
      "Processing Page 298\n",
      "Processing Page 299\n",
      "Processing Page 300\n",
      "Processing Page 301\n",
      "Processing Page 302\n",
      "Processing Page 303\n",
      "Processing Page 304\n",
      "Processing Page 305\n",
      "Processing Page 306\n",
      "Processing Page 307\n",
      "Processing Page 308\n",
      "Processing Page 309\n",
      "Processing Page 310\n",
      "Processing Page 311\n",
      "Processing Page 312\n",
      "Processing Page 313\n",
      "Processing Page 314\n",
      "Processing Page 315\n",
      "Processing Page 316\n",
      "Processing Page 317\n",
      "Processing Page 318\n",
      "Processing Page 319\n",
      "Processing Page 320\n",
      "Processing Page 321\n",
      "Processing Page 322\n",
      "Processing Page 323\n",
      "Processing Page 324\n",
      "Processing Page 325\n",
      "Processing Page 326\n",
      "Processing Page 327\n",
      "Processing Page 328\n",
      "Processing Page 329\n",
      "Processing Page 330\n",
      "Processing Page 331\n",
      "Processing Page 332\n",
      "Processing Page 333\n",
      "Processing Page 334\n",
      "Processing Page 335\n",
      "Processing Page 336\n",
      "Processing Page 337\n",
      "Processing Page 338\n",
      "Processing Page 339\n",
      "Processing Page 340\n",
      "Processing Page 341\n",
      "Processing Page 342\n",
      "Processing Page 343\n",
      "Processing Page 344\n",
      "Processing Page 345\n",
      "Processing Page 346\n",
      "Processing Page 347\n",
      "Processing Page 348\n",
      "Processing Page 349\n",
      "Processing Page 350\n",
      "Processing Page 351\n",
      "Processing Page 352\n",
      "Processing Page 353\n",
      "Processing Page 354\n",
      "Processing Page 355\n",
      "Processing Page 356\n",
      "Processing Page 357\n",
      "Processing Page 358\n",
      "Processing Page 359\n",
      "Processing Page 360\n",
      "Processing Page 361\n",
      "Processing Page 362\n",
      "Processing Page 363\n",
      "Processing Page 364\n",
      "Processing Page 365\n",
      "Processing Page 366\n",
      "Processing Page 367\n",
      "Processing Page 368\n",
      "Processing Page 369\n",
      "Processing Page 370\n",
      "Processing Page 371\n",
      "Processing Page 372\n",
      "Processing Page 373\n",
      "Processing Page 374\n",
      "Processing Page 375\n",
      "Processing Page 376\n",
      "Processing Page 377\n",
      "Processing Page 378\n",
      "Processing Page 379\n",
      "Processing Page 380\n",
      "Processing Page 381\n",
      "Processing Page 382\n",
      "Processing Page 383\n",
      "Processing Page 384\n",
      "Processing Page 385\n",
      "Processing Page 386\n",
      "Processing Page 387\n",
      "Processing Page 388\n",
      "Processing Page 389\n",
      "Processing Page 390\n",
      "Processing Page 391\n",
      "Processing Page 392\n",
      "Processing Page 393\n",
      "Processing Page 394\n",
      "Processing Page 395\n",
      "Processing Page 396\n",
      "Processing Page 397\n",
      "Processing Page 398\n",
      "Processing Page 399\n",
      "Processing Page 400\n",
      "Processing Page 401\n",
      "Processing Page 402\n",
      "Processing Page 403\n",
      "Processing Page 404\n",
      "Processing Page 405\n",
      "Processing Page 406\n",
      "Processing Page 407\n",
      "Processing Page 408\n",
      "Processing Page 409\n",
      "Processing Page 410\n",
      "Processing Page 411\n",
      "Processing Page 412\n",
      "Processing Page 413\n",
      "Processing Page 414\n",
      "Processing Page 415\n",
      "Processing Page 416\n",
      "Processing Page 417\n",
      "Processing Page 418\n",
      "Processing Page 419\n",
      "Processing Page 420\n",
      "Processing Page 421\n",
      "Processing Page 422\n",
      "Processing Page 423\n",
      "Processing Page 424\n",
      "Processing Page 425\n",
      "Processing Page 426\n",
      "Processing Page 427\n",
      "Processing Page 428\n",
      "Processing Page 429\n",
      "Processing Page 430\n",
      "Processing Page 431\n",
      "Processing Page 432\n",
      "Processing Page 433\n",
      "Processing Page 434\n",
      "Processing Page 435\n",
      "Processing Page 436\n",
      "Processing Page 437\n",
      "Processing Page 438\n",
      "Processing Page 439\n",
      "Processing Page 440\n",
      "Processing Page 441\n",
      "Processing Page 442\n",
      "Processing Page 443\n",
      "Processing Page 444\n",
      "Processing Page 445\n",
      "Processing Page 446\n",
      "Processing Page 447\n",
      "Processing Page 448\n",
      "Processing Page 449\n",
      "Processing Page 450\n",
      "Processing Page 451\n",
      "Processing Page 452\n",
      "Processing Page 453\n",
      "Processing Page 454\n",
      "Processing Page 455\n",
      "Processing Page 456\n",
      "Processing Page 457\n",
      "Processing Page 458\n",
      "Processing Page 459\n",
      "Processing Page 460\n",
      "Processing Page 461\n",
      "Processing Page 462\n",
      "Processing Page 463\n",
      "Processing Page 464\n",
      "Processing Page 465\n",
      "Processing Page 466\n",
      "Processing Page 467\n",
      "Processing Page 468\n",
      "Processing Page 469\n",
      "Processing Page 470\n",
      "Processing Page 471\n",
      "Processing Page 472\n",
      "Processing Page 473\n",
      "Processing Page 474\n",
      "Processing Page 475\n",
      "Processing Page 476\n",
      "Processing Page 477\n",
      "Processing Page 478\n",
      "Processing Page 479\n",
      "Processing Page 480\n",
      "Processing Page 481\n",
      "Processing Page 482\n",
      "Processing Page 483\n",
      "Processing Page 484\n",
      "Processing Page 485\n",
      "Processing Page 486\n",
      "Processing Page 487\n",
      "Processing Page 488\n",
      "Processing Page 489\n",
      "Processing Page 490\n",
      "Processing Page 491\n",
      "Processing Page 492\n",
      "Processing Page 493\n",
      "Processing Page 494\n",
      "Processing Page 495\n",
      "Processing Page 496\n",
      "Processing Page 497\n",
      "Processing Page 498\n",
      "Processing Page 499\n",
      "Processing Page 500\n",
      "Processing Page 501\n",
      "Processing Page 502\n",
      "Processing Page 503\n",
      "Processing Page 504\n",
      "Processing Page 505\n",
      "Processing Page 506\n",
      "Processing Page 507\n",
      "Processing Page 508\n",
      "Processing Page 509\n",
      "Processing Page 510\n",
      "Processing Page 511\n",
      "Processing Page 512\n",
      "Processing Page 513\n",
      "Processing Page 514\n",
      "Processing Page 515\n",
      "Processing Page 516\n",
      "Processing Page 517\n",
      "Processing Page 518\n",
      "Processing Page 519\n",
      "Processing Page 520\n",
      "Processing Page 521\n",
      "Processing Page 522\n",
      "Processing Page 523\n",
      "Processing Page 524\n",
      "Processing Page 525\n",
      "Processing Page 526\n",
      "Processing Page 527\n",
      "Processing Page 528\n",
      "Processing Page 529\n",
      "Processing Page 530\n",
      "Processing Page 531\n",
      "Processing Page 532\n",
      "Processing Page 533\n",
      "Processing Page 534\n",
      "Processing Page 535\n",
      "Processing Page 536\n",
      "Processing Page 537\n",
      "Processing Page 538\n",
      "Processing Page 539\n",
      "Processing Page 540\n",
      "Processing Page 541\n",
      "Processing Page 542\n",
      "Processing Page 543\n",
      "Processing Page 544\n",
      "Processing Page 545\n",
      "Processing Page 546\n",
      "Processing Page 547\n",
      "Processing Page 548\n",
      "Processing Page 549\n",
      "Processing Page 550\n",
      "Processing Page 551\n",
      "Processing Page 552\n",
      "Processing Page 553\n",
      "Processing Page 554\n",
      "Processing Page 555\n",
      "Processing Page 556\n",
      "Processing Page 557\n",
      "Processing Page 558\n",
      "Processing Page 559\n",
      "Processing Page 560\n",
      "Processing Page 561\n",
      "Processing Page 562\n",
      "Processing Page 563\n",
      "Processing Page 564\n",
      "Processing Page 565\n",
      "Processing Page 566\n",
      "Processing Page 567\n",
      "Processing Page 568\n",
      "Processing Page 569\n",
      "Processing Page 570\n",
      "Processing Page 571\n",
      "Processing Page 572\n",
      "Processing Page 573\n",
      "Processing Page 574\n",
      "Processing Page 575\n",
      "Processing Page 576\n",
      "Processing Page 577\n",
      "Processing Page 578\n",
      "Processing Page 579\n",
      "Processing Page 580\n",
      "Processing Page 581\n",
      "Processing Page 582\n",
      "Processing Page 583\n",
      "Processing Page 584\n",
      "Processing Page 585\n",
      "Processing Page 586\n",
      "Processing Page 587\n",
      "Processing Page 588\n",
      "Processing Page 589\n",
      "Processing Page 590\n",
      "Processing Page 591\n",
      "Processing Page 592\n",
      "Processing Page 593\n",
      "Processing Page 594\n",
      "Processing Page 595\n",
      "Processing Page 596\n",
      "Processing Page 597\n",
      "Processing Page 598\n",
      "Processing Page 599\n",
      "Processing Page 600\n",
      "Processing Page 601\n",
      "Processing Page 602\n",
      "Processing Page 603\n",
      "Processing Page 604\n",
      "Processing Page 605\n",
      "Processing Page 606\n",
      "Processing Page 607\n",
      "Processing Page 608\n",
      "Processing Page 609\n",
      "Processing Page 610\n",
      "Processing Page 611\n",
      "Processing Page 612\n",
      "Processing Page 613\n",
      "Processing Page 614\n",
      "Processing Page 615\n",
      "Processing Page 616\n",
      "Processing Page 617\n",
      "Processing Page 618\n",
      "Processing Page 619\n",
      "Processing Page 620\n",
      "Processing Page 621\n",
      "Processing Page 622\n",
      "Processing Page 623\n",
      "Processing Page 624\n",
      "Processing Page 625\n",
      "Processing Page 626\n",
      "Processing Page 627\n",
      "Processing Page 628\n",
      "Processing Page 629\n",
      "Processing Page 630\n",
      "Processing Page 631\n",
      "Processing Page 632\n",
      "Processing Page 633\n",
      "Processing Page 634\n",
      "Processing Page 635\n",
      "Processing Page 636\n",
      "Processing Page 637\n",
      "Processing Page 638\n",
      "Processing Page 639\n",
      "Processing Page 640\n"
     ]
    }
   ],
   "source": [
    "for page in pages_and_metadata:\n",
    "    embeddings = list()\n",
    "    print(f\"Processing Page {page['page_number']}\")\n",
    "    for sentence in page[\"sentences\"]:\n",
    "        # sentence = sentence.to(device)\n",
    "        embedding = embedding_model.encode(sentence, batch_size=512, convert_to_tensor=True, show_progress_bar=False, device=device)\n",
    "        embedding = np.stack(embedding.tolist(), axis=0)\n",
    "        embedding = torch.tensor(embedding)\n",
    "        embedding = embedding.type(torch.float32)\n",
    "        embeddings.append(embedding)\n",
    "    sentence_embeddings = [np.array(embedding) for embedding in embeddings]\n",
    "    pages_and_metadata[page[\"page_number\"]][\"embeddings\"] = sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3567e35e0f369ed6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T03:59:45.693927Z",
     "start_time": "2025-02-11T03:59:45.300342Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca46b08f4fbb4a9091893d02f392fbea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/641 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 768)\n",
      "(5, 768)\n",
      "(10, 768)\n",
      "(6, 768)\n",
      "(9, 768)\n",
      "(9, 768)\n",
      "(13, 768)\n",
      "(11, 768)\n",
      "(9, 768)\n",
      "(7, 768)\n",
      "(8, 768)\n",
      "(8, 768)\n",
      "(8, 768)\n",
      "(6, 768)\n",
      "(11, 768)\n",
      "(8, 768)\n",
      "(8, 768)\n",
      "(11, 768)\n",
      "(6, 768)\n",
      "(5, 768)\n",
      "(8, 768)\n",
      "(9, 768)\n",
      "(12, 768)\n",
      "(5, 768)\n",
      "(7, 768)\n",
      "(9, 768)\n",
      "(8, 768)\n",
      "(13, 768)\n",
      "(7, 768)\n",
      "(12, 768)\n",
      "(2, 768)\n",
      "(6, 768)\n",
      "(3, 768)\n",
      "(5, 768)\n",
      "(2, 768)\n",
      "(2, 768)\n",
      "(2, 768)\n",
      "(2, 768)\n",
      "(4, 768)\n",
      "(3, 768)\n",
      "(1, 768)\n",
      "(3, 768)\n",
      "(3, 768)\n",
      "(3, 768)\n",
      "(5, 768)\n",
      "(3, 768)\n",
      "(3, 768)\n",
      "(1, 768)\n",
      "(4, 768)\n",
      "(3, 768)\n",
      "(3, 768)\n",
      "(5, 768)\n",
      "(3, 768)\n",
      "(3, 768)\n",
      "(1, 768)\n",
      "(2, 768)\n",
      "(3, 768)\n",
      "(2, 768)\n",
      "(5, 768)\n",
      "(0,)\n",
      "(5, 768)\n",
      "(6, 768)\n",
      "(6, 768)\n",
      "(5, 768)\n",
      "(5, 768)\n",
      "(4, 768)\n",
      "(5, 768)\n",
      "(4, 768)\n",
      "(6, 768)\n",
      "(6, 768)\n",
      "(4, 768)\n",
      "(6, 768)\n",
      "(7, 768)\n",
      "(7, 768)\n",
      "(6, 768)\n",
      "(5, 768)\n",
      "(3, 768)\n",
      "(4, 768)\n",
      "(7, 768)\n",
      "(7, 768)\n",
      "(3, 768)\n",
      "(5, 768)\n",
      "(2, 768)\n",
      "(2, 768)\n",
      "(2, 768)\n",
      "(2, 768)\n",
      "(4, 768)\n",
      "(3, 768)\n",
      "(1, 768)\n",
      "(3, 768)\n",
      "(3, 768)\n",
      "(3, 768)\n",
      "(5, 768)\n",
      "(3, 768)\n",
      "(3, 768)\n",
      "(1, 768)\n",
      "(4, 768)\n",
      "(3, 768)\n",
      "(3, 768)\n",
      "(5, 768)\n",
      "(3, 768)\n",
      "(3, 768)\n",
      "(1, 768)\n",
      "(2, 768)\n",
      "(3, 768)\n",
      "(2, 768)\n",
      "(5, 768)\n",
      "(0,)\n",
      "(5, 768)\n",
      "(6, 768)\n",
      "(6, 768)\n",
      "(5, 768)\n",
      "(5, 768)\n",
      "(4, 768)\n",
      "(5, 768)\n",
      "(4, 768)\n",
      "(6, 768)\n",
      "(6, 768)\n",
      "(4, 768)\n",
      "(6, 768)\n",
      "(7, 768)\n",
      "(7, 768)\n",
      "(6, 768)\n",
      "(5, 768)\n",
      "(3, 768)\n",
      "(4, 768)\n",
      "(7, 768)\n",
      "(7, 768)\n",
      "(4, 768)\n",
      "(6, 768)\n",
      "(9, 768)\n",
      "(8, 768)\n",
      "(10, 768)\n",
      "(12, 768)\n",
      "(13, 768)\n",
      "(9, 768)\n",
      "(5, 768)\n",
      "(7, 768)\n",
      "(7, 768)\n",
      "(5, 768)\n",
      "(0,)\n",
      "(0,)\n",
      "(1, 768)\n",
      "(0,)\n",
      "(0,)\n",
      "(2, 768)\n",
      "(1, 768)\n",
      "(1, 768)\n",
      "(1, 768)\n",
      "(0,)\n",
      "(0,)\n",
      "(1, 768)\n",
      "(0,)\n",
      "(0,)\n",
      "(1, 768)\n",
      "(2, 768)\n",
      "(0,)\n",
      "(0,)\n",
      "(0,)\n",
      "(0,)\n",
      "(0,)\n",
      "(0,)\n",
      "(0,)\n",
      "(0,)\n",
      "(0,)\n",
      "(0,)\n",
      "(0,)\n",
      "(0,)\n",
      "(0,)\n",
      "(1, 768)\n",
      "(1, 768)\n",
      "(0,)\n",
      "(1, 768)\n",
      "(2, 768)\n",
      "(0,)\n",
      "(1, 768)\n",
      "(0,)\n",
      "(2, 768)\n",
      "(2, 768)\n",
      "(0,)\n",
      "(0,)\n",
      "(0,)\n",
      "(0,)\n",
      "(2, 768)\n",
      "(1, 768)\n",
      "(2, 768)\n",
      "(4, 768)\n",
      "(2, 768)\n",
      "(0,)\n",
      "(10, 768)\n",
      "(6, 768)\n",
      "(8, 768)\n",
      "(5, 768)\n",
      "(6, 768)\n",
      "(6, 768)\n",
      "(8, 768)\n",
      "(1, 768)\n",
      "(7, 768)\n",
      "(2, 768)\n",
      "(12, 768)\n",
      "(6, 768)\n",
      "(5, 768)\n",
      "(1, 768)\n",
      "(7, 768)\n",
      "(5, 768)\n",
      "(8, 768)\n",
      "(2, 768)\n",
      "(3, 768)\n",
      "(3, 768)\n",
      "(8, 768)\n",
      "(7, 768)\n",
      "(4, 768)\n",
      "(4, 768)\n",
      "(5, 768)\n",
      "(2, 768)\n",
      "(6, 768)\n",
      "(10, 768)\n",
      "(9, 768)\n",
      "(5, 768)\n",
      "(2, 768)\n",
      "(3, 768)\n",
      "(5, 768)\n",
      "(7, 768)\n",
      "(6, 768)\n",
      "(5, 768)\n",
      "(3, 768)\n",
      "(7, 768)\n",
      "(5, 768)\n",
      "(2, 768)\n",
      "(3, 768)\n",
      "(5, 768)\n",
      "(5, 768)\n",
      "(4, 768)\n",
      "(3, 768)\n",
      "(5, 768)\n",
      "(5, 768)\n",
      "(5, 768)\n",
      "(3, 768)\n",
      "(3, 768)\n",
      "(5, 768)\n",
      "(2, 768)\n",
      "(1, 768)\n",
      "(3, 768)\n",
      "(4, 768)\n",
      "(4, 768)\n",
      "(2, 768)\n",
      "(4, 768)\n",
      "(4, 768)\n",
      "(2, 768)\n",
      "(4, 768)\n",
      "(5, 768)\n",
      "(7, 768)\n",
      "(3, 768)\n",
      "(4, 768)\n",
      "(7, 768)\n",
      "(3, 768)\n",
      "(7, 768)\n",
      "(7, 768)\n",
      "(9, 768)\n",
      "(8, 768)\n",
      "(13, 768)\n",
      "(7, 768)\n",
      "(12, 768)\n",
      "(2, 768)\n",
      "(6, 768)\n",
      "(8, 768)\n",
      "(7, 768)\n",
      "(7, 768)\n",
      "(6, 768)\n",
      "(1, 768)\n",
      "(10, 768)\n",
      "(6, 768)\n",
      "(6, 768)\n",
      "(3, 768)\n",
      "(6, 768)\n",
      "(6, 768)\n",
      "(5, 768)\n",
      "(2, 768)\n",
      "(2, 768)\n",
      "(0,)\n",
      "(1, 768)\n",
      "(10, 768)\n",
      "(6, 768)\n",
      "(8, 768)\n",
      "(5, 768)\n",
      "(6, 768)\n",
      "(6, 768)\n",
      "(8, 768)\n",
      "(1, 768)\n",
      "(2, 768)\n",
      "(11, 768)\n",
      "(2, 768)\n",
      "(3, 768)\n",
      "(3, 768)\n",
      "(4, 768)\n",
      "(4, 768)\n",
      "(9, 768)\n",
      "(10, 768)\n",
      "(4, 768)\n",
      "(6, 768)\n",
      "(6, 768)\n",
      "(7, 768)\n",
      "(6, 768)\n",
      "(5, 768)\n",
      "(7, 768)\n",
      "(4, 768)\n",
      "(5, 768)\n",
      "(8, 768)\n",
      "(6, 768)\n",
      "(6, 768)\n",
      "(3, 768)\n",
      "(7, 768)\n",
      "(2, 768)\n",
      "(4, 768)\n",
      "(2, 768)\n",
      "(3, 768)\n",
      "(3, 768)\n",
      "(5, 768)\n",
      "(2, 768)\n",
      "(1, 768)\n",
      "(5, 768)\n",
      "(4, 768)\n",
      "(5, 768)\n",
      "(3, 768)\n",
      "(0,)\n",
      "(0,)\n",
      "(5, 768)\n",
      "(3, 768)\n",
      "(6, 768)\n",
      "(4, 768)\n",
      "(6, 768)\n",
      "(10, 768)\n",
      "(5, 768)\n",
      "(8, 768)\n",
      "(10, 768)\n",
      "(11, 768)\n",
      "(3, 768)\n",
      "(3, 768)\n",
      "(8, 768)\n",
      "(5, 768)\n",
      "(7, 768)\n",
      "(1, 768)\n",
      "(4, 768)\n",
      "(1, 768)\n",
      "(3, 768)\n",
      "(9, 768)\n",
      "(5, 768)\n",
      "(1, 768)\n",
      "(3, 768)\n",
      "(9, 768)\n",
      "(7, 768)\n",
      "(10, 768)\n",
      "(6, 768)\n",
      "(7, 768)\n",
      "(6, 768)\n",
      "(10, 768)\n",
      "(4, 768)\n",
      "(2, 768)\n",
      "(3, 768)\n",
      "(3, 768)\n",
      "(2, 768)\n",
      "(4, 768)\n",
      "(5, 768)\n",
      "(7, 768)\n",
      "(4, 768)\n",
      "(4, 768)\n",
      "(2, 768)\n",
      "(4, 768)\n",
      "(1, 768)\n",
      "(1, 768)\n",
      "(3, 768)\n",
      "(5, 768)\n",
      "(8, 768)\n",
      "(8, 768)\n",
      "(4, 768)\n",
      "(1, 768)\n",
      "(5, 768)\n",
      "(3, 768)\n",
      "(3, 768)\n",
      "(4, 768)\n",
      "(7, 768)\n",
      "(7, 768)\n",
      "(6, 768)\n",
      "(2, 768)\n",
      "(1, 768)\n",
      "(0,)\n",
      "(27, 768)\n",
      "(31, 768)\n",
      "(34, 768)\n",
      "(27, 768)\n",
      "(10, 768)\n",
      "(0,)\n",
      "(3, 768)\n",
      "(9, 768)\n",
      "(3, 768)\n",
      "(10, 768)\n",
      "(12, 768)\n",
      "(8, 768)\n",
      "(7, 768)\n",
      "(6, 768)\n",
      "(8, 768)\n",
      "(13, 768)\n",
      "(8, 768)\n",
      "(6, 768)\n",
      "(7, 768)\n",
      "(5, 768)\n",
      "(0,)\n",
      "(6, 768)\n",
      "(5, 768)\n",
      "(6, 768)\n",
      "(14, 768)\n",
      "(6, 768)\n",
      "(2, 768)\n",
      "(4, 768)\n",
      "(11, 768)\n",
      "(12, 768)\n",
      "(2, 768)\n",
      "(1, 768)\n",
      "(0,)\n",
      "(7, 768)\n",
      "(5, 768)\n",
      "(4, 768)\n",
      "(5, 768)\n",
      "(7, 768)\n",
      "(8, 768)\n",
      "(11, 768)\n",
      "(10, 768)\n",
      "(10, 768)\n",
      "(9, 768)\n",
      "(7, 768)\n",
      "(4, 768)\n",
      "(5, 768)\n",
      "(11, 768)\n",
      "(4, 768)\n",
      "(8, 768)\n",
      "(8, 768)\n",
      "(11, 768)\n",
      "(5, 768)\n",
      "(9, 768)\n",
      "(4, 768)\n",
      "(2, 768)\n",
      "(10, 768)\n",
      "(6, 768)\n",
      "(8, 768)\n",
      "(6, 768)\n",
      "(10, 768)\n",
      "(8, 768)\n",
      "(6, 768)\n",
      "(4, 768)\n",
      "(6, 768)\n",
      "(4, 768)\n",
      "(11, 768)\n",
      "(6, 768)\n",
      "(9, 768)\n",
      "(0,)\n",
      "(9, 768)\n",
      "(3, 768)\n",
      "(8, 768)\n",
      "(1, 768)\n",
      "(4, 768)\n",
      "(4, 768)\n",
      "(3, 768)\n",
      "(6, 768)\n",
      "(8, 768)\n",
      "(1, 768)\n",
      "(7, 768)\n",
      "(7, 768)\n",
      "(5, 768)\n",
      "(7, 768)\n",
      "(4, 768)\n",
      "(3, 768)\n",
      "(8, 768)\n",
      "(7, 768)\n",
      "(7, 768)\n",
      "(10, 768)\n",
      "(6, 768)\n",
      "(11, 768)\n",
      "(9, 768)\n",
      "(3, 768)\n",
      "(1, 768)\n",
      "(0,)\n",
      "(7, 768)\n",
      "(10, 768)\n",
      "(7, 768)\n",
      "(8, 768)\n",
      "(9, 768)\n",
      "(5, 768)\n",
      "(5, 768)\n",
      "(7, 768)\n",
      "(7, 768)\n",
      "(8, 768)\n",
      "(2, 768)\n",
      "(11, 768)\n",
      "(12, 768)\n",
      "(18, 768)\n",
      "(4, 768)\n",
      "(1, 768)\n",
      "(1, 768)\n",
      "(0,)\n",
      "(7, 768)\n",
      "(11, 768)\n",
      "(9, 768)\n",
      "(7, 768)\n",
      "(9, 768)\n",
      "(8, 768)\n",
      "(13, 768)\n",
      "(8, 768)\n",
      "(13, 768)\n",
      "(9, 768)\n",
      "(8, 768)\n",
      "(6, 768)\n",
      "(6, 768)\n",
      "(4, 768)\n",
      "(5, 768)\n",
      "(4, 768)\n",
      "(8, 768)\n",
      "(7, 768)\n",
      "(9, 768)\n",
      "(6, 768)\n",
      "(9, 768)\n",
      "(7, 768)\n",
      "(9, 768)\n",
      "(2, 768)\n",
      "(6, 768)\n",
      "(7, 768)\n",
      "(9, 768)\n",
      "(2, 768)\n",
      "(11, 768)\n",
      "(6, 768)\n",
      "(7, 768)\n",
      "(11, 768)\n",
      "(7, 768)\n",
      "(6, 768)\n",
      "(8, 768)\n",
      "(0,)\n",
      "(7, 768)\n",
      "(6, 768)\n",
      "(5, 768)\n",
      "(2, 768)\n",
      "(12, 768)\n",
      "(5, 768)\n",
      "(8, 768)\n",
      "(9, 768)\n",
      "(12, 768)\n",
      "(13, 768)\n",
      "(6, 768)\n",
      "(10, 768)\n",
      "(6, 768)\n",
      "(0,)\n",
      "(4, 768)\n",
      "(8, 768)\n",
      "(3, 768)\n",
      "(5, 768)\n",
      "(2, 768)\n",
      "(12, 768)\n",
      "(7, 768)\n",
      "(10, 768)\n",
      "(2, 768)\n",
      "(2, 768)\n",
      "(5, 768)\n",
      "(2, 768)\n",
      "(11, 768)\n",
      "(12, 768)\n",
      "(10, 768)\n",
      "(13, 768)\n",
      "(14, 768)\n",
      "(12, 768)\n",
      "(11, 768)\n",
      "(15, 768)\n",
      "(15, 768)\n",
      "(12, 768)\n",
      "(7, 768)\n",
      "(7, 768)\n",
      "(9, 768)\n",
      "(7, 768)\n",
      "(8, 768)\n",
      "(5, 768)\n",
      "(9, 768)\n",
      "(8, 768)\n",
      "(8, 768)\n",
      "(8, 768)\n",
      "(6, 768)\n",
      "(9, 768)\n",
      "(6, 768)\n",
      "(5, 768)\n",
      "(7, 768)\n",
      "(9, 768)\n",
      "(7, 768)\n",
      "(7, 768)\n",
      "(6, 768)\n",
      "(8, 768)\n",
      "(5, 768)\n",
      "(5, 768)\n",
      "(7, 768)\n",
      "(6, 768)\n",
      "(6, 768)\n",
      "(2, 768)\n",
      "(6, 768)\n",
      "(5, 768)\n",
      "(5, 768)\n",
      "(6, 768)\n",
      "(8, 768)\n",
      "(9, 768)\n",
      "(7, 768)\n",
      "(6, 768)\n",
      "(2, 768)\n",
      "(5, 768)\n",
      "(10, 768)\n",
      "(6, 768)\n",
      "(4, 768)\n",
      "(4, 768)\n",
      "(6, 768)\n",
      "(8, 768)\n",
      "(5, 768)\n",
      "(8, 768)\n",
      "(4, 768)\n",
      "(8, 768)\n",
      "(4, 768)\n",
      "(6, 768)\n",
      "(4, 768)\n",
      "(8, 768)\n",
      "(6, 768)\n",
      "(9, 768)\n",
      "(7, 768)\n",
      "(6, 768)\n",
      "(5, 768)\n",
      "(3, 768)\n",
      "(4, 768)\n",
      "(6, 768)\n",
      "(9, 768)\n",
      "(7, 768)\n",
      "(5, 768)\n",
      "(9, 768)\n",
      "(7, 768)\n",
      "(5, 768)\n",
      "(2, 768)\n",
      "(4, 768)\n",
      "(2, 768)\n",
      "(6, 768)\n",
      "(3, 768)\n",
      "(1, 768)\n"
     ]
    }
   ],
   "source": [
    "for page in tqdm(pages_and_metadata, total=len(pages_and_metadata)):\n",
    "    print(np.array(page[\"embeddings\"]).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8360bc8d64e45d79",
   "metadata": {},
   "source": [
    "## 2.9 Checking the metadata present for use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5023c654f155fd09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_number\n",
      "raw_text\n",
      "number_of_characters\n",
      "number_of_tokens\n",
      "number_of_words\n",
      "formatted_text\n",
      "sentences\n",
      "number_of_sentences\n",
      "sentence_chunk\n",
      "embeddings\n"
     ]
    }
   ],
   "source": [
    "for key in pages_and_metadata[0].keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bd19f71651b712",
   "metadata": {},
   "source": [
    "# 3. FETCHING SIMILAR CONTENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184fd946401c91fd",
   "metadata": {},
   "source": [
    "## 3.1 Getting the data embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8c81dcf0f0426027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee060ff7d9e844d68130b11283b224d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/641 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pages_and_metadata_embeddings = []\n",
    "\n",
    "for page in tqdm(pages_and_metadata, total=len(pages_and_metadata)):\n",
    "    page_embeddings = []\n",
    "    for chunk_embedding in pages_and_metadata[page[\"page_number\"]][\"embeddings\"]:\n",
    "        if isinstance(chunk_embedding, torch.Tensor):\n",
    "            chunk_embedding = chunk_embedding.tolist()\n",
    "        page_embeddings.append(chunk_embedding)\n",
    "    pages_and_metadata_embeddings.append(page_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78792dfaf1127f6",
   "metadata": {},
   "source": [
    "## 3.2 Converting each embedding into the same dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "59618f1fd119517a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pages_and_metadata_embeddings:\n",
    "    embedding_dim = len(pages_and_metadata_embeddings[0][0])\n",
    "    pages_and_metadata_embeddings = [\n",
    "            [np.pad(chunk, (0, max(0, embedding_dim - len(chunk))), mode='constant')[:embedding_dim]\n",
    "             for chunk in page]\n",
    "            for page in pages_and_metadata_embeddings\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2300c8743f62a23a",
   "metadata": {},
   "source": [
    "## 3.3 Flattening the nested list of embeddings and the sentence to fetch by index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4e07bdbecbd85c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_embeddings = [chunk for page in pages_and_metadata_embeddings for chunk in page]\n",
    "flat_data = [sentence for page in pages_and_metadata for sentence in page[\"sentences\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9a9684b1287496",
   "metadata": {},
   "source": [
    "## 3.4 Saving the flattened embeddings and the flattened data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4533ad448b48e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(flat_embeddings)\n",
    "df.to_csv(\"embeddings_v3.csv\", index=False)\n",
    "\n",
    "df = pd.DataFrame(flat_data)\n",
    "df.to_csv(\"data_v3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9c5f925a05048b",
   "metadata": {},
   "source": [
    "## 3.5 Loading the flattened embeddings and flattened data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "80f723dee61c4c4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:38:13.460239Z",
     "start_time": "2025-02-09T11:38:13.128443Z"
    }
   },
   "outputs": [],
   "source": [
    "flat_embeddings = pd.read_csv(\"embeddings_v3.csv\").to_numpy()\n",
    "flat_data = pd.read_csv(\"data_v3.csv\")[\"0\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d04b8ed2-21c5-4bd2-92e2-8d016e4c7079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the flat_embeddings is 3531\n",
      "The size of the flat_data is 3531\n"
     ]
    }
   ],
   "source": [
    "print(f\"The size of the flat_embeddings is {len(flat_embeddings)}\")\n",
    "print(f\"The size of the flat_data is {len(flat_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd66e23aff7a901",
   "metadata": {},
   "source": [
    "## 3.6 Converting embeddings to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f3f43ce0368e6c95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:38:14.661823Z",
     "start_time": "2025-02-09T11:38:14.644537Z"
    }
   },
   "outputs": [],
   "source": [
    "pages_and_metadata_embeddings = np.array(flat_embeddings, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61118042f2f3a0c1",
   "metadata": {},
   "source": [
    "## 3.7 Converting the numpy array embeddings to torch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "476ae4b96e92b843",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:38:16.402973Z",
     "start_time": "2025-02-09T11:38:15.539250Z"
    }
   },
   "outputs": [],
   "source": [
    "pages_and_metadata_embeddings = torch.tensor(pages_and_metadata_embeddings, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7691ac5f8be5636",
   "metadata": {},
   "source": [
    "## 3.8 Getting the similarity score by query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6745dc59ef034840",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:38:24.424800Z",
     "start_time": "2025-02-09T11:38:17.617912Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "query_embeddings = embedding_model.encode(QUERY, convert_to_tensor=True).to(device)\n",
    "dot_score = util.dot_score(query_embeddings, pages_and_metadata_embeddings)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bea1268563c3129",
   "metadata": {},
   "source": [
    "## 3.9 Getting the top k similar scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7dea591cca2af654",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:38:27.505935Z",
     "start_time": "2025-02-09T11:38:27.441640Z"
    }
   },
   "outputs": [],
   "source": [
    "top_scores, top_indices = torch.topk(dot_score, k=K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "87a0a2111e42c0cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:38:28.290866Z",
     "start_time": "2025-02-09T11:38:28.283472Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top scores: tensor([0.7078, 0.6232, 0.6208, 0.6178, 0.6178, 0.6054, 0.5918, 0.5918, 0.5917,\n",
      "        0.5829, 0.5771, 0.5741, 0.5741, 0.5692, 0.5692, 0.5648, 0.5648, 0.5575,\n",
      "        0.5546, 0.5524, 0.5505, 0.5386, 0.5386, 0.5329, 0.5311, 0.5233, 0.5233,\n",
      "        0.5229, 0.5227, 0.5187, 0.5187, 0.5187, 0.5140, 0.5137, 0.5114, 0.5081,\n",
      "        0.5041, 0.5022, 0.4989, 0.4967, 0.4951, 0.4950, 0.4946, 0.4920, 0.4920,\n",
      "        0.4920, 0.4920, 0.4919, 0.4918, 0.4915], device='cuda:0')\n",
      "Top indices: tensor([  60,  895, 3149,  766, 1234, 2405,   33,  896, 3221,  191, 3114,  763,\n",
      "        1231,  400,  587,  352,  539, 2380,  183, 3219,   44,  403,  590, 3125,\n",
      "        1694,  447,  634,  193, 1745,  108,  142,  926, 2383,  867,  144, 3112,\n",
      "        1079,   68, 1753, 2778, 3116, 1798,   64,   30,  889,  321,  508, 3229,\n",
      "         893,  103], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(f\"Top scores: {top_scores}\")\n",
    "print(f\"Top indices: {top_indices}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a93ccde99a8e5fd",
   "metadata": {},
   "source": [
    "## 3.10 Getting the top k content based on the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "412b8aad1ac4bc28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:38:31.127772Z",
     "start_time": "2025-02-09T11:38:31.124080Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data from page 60\n",
      "Fetching data from page 895\n",
      "Fetching data from page 3149\n",
      "Fetching data from page 766\n",
      "Fetching data from page 1234\n",
      "Fetching data from page 2405\n",
      "Fetching data from page 33\n",
      "Fetching data from page 896\n",
      "Fetching data from page 3221\n",
      "Fetching data from page 191\n",
      "Fetching data from page 3114\n",
      "Fetching data from page 763\n",
      "Fetching data from page 1231\n",
      "Fetching data from page 400\n",
      "Fetching data from page 587\n",
      "Fetching data from page 352\n",
      "Fetching data from page 539\n",
      "Fetching data from page 2380\n",
      "Fetching data from page 183\n",
      "Fetching data from page 3219\n",
      "Fetching data from page 44\n",
      "Fetching data from page 403\n",
      "Fetching data from page 590\n",
      "Fetching data from page 3125\n",
      "Fetching data from page 1694\n",
      "Fetching data from page 447\n",
      "Fetching data from page 634\n",
      "Fetching data from page 193\n",
      "Fetching data from page 1745\n",
      "Fetching data from page 108\n",
      "Fetching data from page 142\n",
      "Fetching data from page 926\n",
      "Fetching data from page 2383\n",
      "Fetching data from page 867\n",
      "Fetching data from page 144\n",
      "Fetching data from page 3112\n",
      "Fetching data from page 1079\n",
      "Fetching data from page 68\n",
      "Fetching data from page 1753\n",
      "Fetching data from page 2778\n",
      "Fetching data from page 3116\n",
      "Fetching data from page 1798\n",
      "Fetching data from page 64\n",
      "Fetching data from page 30\n",
      "Fetching data from page 889\n",
      "Fetching data from page 321\n",
      "Fetching data from page 508\n",
      "Fetching data from page 3229\n",
      "Fetching data from page 893\n",
      "Fetching data from page 103\n"
     ]
    }
   ],
   "source": [
    "context = list()\n",
    "for index in top_indices:\n",
    "    print(f\"Fetching data from page {index}\")\n",
    "    context.append(flat_data[index.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "777d6213b100c8ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:38:31.789353Z",
     "start_time": "2025-02-09T11:38:31.783885Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['overfitting comes light data associated complexity, means associated parameters relative number observations.', 'follow steve nouri ai data science posts https//lnkd.in/gzu463x overfitting, statistical model describes random error noise instead underlying relationship.', 'simple restatement fundamental problem machine learning possibility overfitting training data carrying noise data test set, providing inaccurate generalizations.', 'lot data overﬁtting avoided, overﬁtting happens relatively small dataset, try learn it.', 'lot data overﬁtting avoided, overﬁtting happens relatively small dataset, try learn it.', 'overfitting evaluation 83 (from weiss, s., kulikowski, c., computer systems learn, morgan kaufmann, 1991) training errors validation errors 1 2 3 4 5 6 7 8 9 0.2 0.4 0.6 0.8 1.0 0 0 error rate number terminal nodes iris data decision tree figure 6.8 determining overﬁtting begins stopping growth decision tree, grow size prune away leaf nodes ancestors cross- validation accuracy longer increases.', 'underfitting occurs statistical model machine learning algorithm capture underlying trend data.', 'underfitting occurs statistical model machine learning algorithm capture underlying trend data.', 'decision trees prone overfitting, pruning tree helps reduce size minimizes chances overfitting.', 'poor predictive performance – overfitting underfitting yield poor predictive performance, way different.', 'overfitting situation occurs model learns training set well, taking random fluctuations training data concepts.', 'machine learning, statistical model describes random error noise instead underlying relationship ‘overﬁtting’ occurs.', 'machine learning, statistical model describes random error noise instead underlying relationship ‘overﬁtting’ occurs.', 'quora) simple restatement fundamental problem machine learning possibility overfitting training data carrying noise data test set, providing inaccu- rate generalizations.', 'quora) simple restatement fundamental problem machine learning possibility overfitting training data carrying noise data test set, providing inaccu- rate generalizations.', 'reading bias-variance tradeoff (wikipedia) bias error erroneous overly simplistic assumptions learning algorithm you’re using.', 'reading bias-variance tradeoff (wikipedia) bias error erroneous overly simplistic assumptions learning algorithm you’re using.', '6.4 overﬁtting evaluation 6.4.1 overﬁtting supervised learning, choose function ﬁt training set set hypotheses.', 'validation set training set parameter selection avoiding overfitting machine learning model developed.', 'goals model training identify signal ignore noise model given free rein minimize error, possibility suffering overfitting.', 'prediction rate provides low prediction training error test error leads high business problem, error rate training set high error rate test set high, conclude overfitting model.', 'typically reduce overfit- ting models model robust (unlikely influ- enced small changes training data).', 'typically reduce overfit- ting models model robust (unlikely influ- enced small changes training data).', 'predictive models tradeoff bias (how model fits data) variance (how model changes based changes inputs).', 'terms machine learning, algorithm aims understand example particular concept generalizations form concepts training examples.', 'reading waymo tech machine learning interview s like test knowledge different machine learning methods, inven- tiveness don’t know answer.', 'reading waymo tech machine learning interview s like test knowledge different machine learning methods, inven- tiveness don’t know answer.', 'following differences overfitting underfitting definition – statistical model suffering overfitting describes random error noise place underlying relationship.', '9 1.3 learning requires bias . . . . . . . . . . . . . . . . . . . . . . . .', 'validation set considered training set parameter selection avoid overfitting model built.', 'validation set considered training set parameter selection avoid overfitting model built.', 'validation set considered training set parameter selection avoid overfitting model built.', 'however, bias, training set suﬃciently large compared size hypothesis space, consistent functions useful guesses, generalization performance poor.', 'support vector machine algorithm low bias high variance, trade-off changed increasing c parameter influences number violations margin allowed training data increases bias decreases variance.', 'prediction rate high inconsistency training error test error leads ta high business problem, error rate training set low error rate ithe n test set high, conclude overfitting model.', 'lesser variables parameters, variance reduced ● cross-validation methods like k-folds ● model parameters likely cause overfitting, techniques regularization like lasso penalize parameters steve nouri \\u200b \\u200bhttps//www.linkedin.com/in/stevenouri/', 'goal cross-validation term data set test model training phase (i.e. validation data set) limit problems like overfitting gain insight model generalize independent data set.', 'machine learning statistics, common task undergo fit model set training data.', '1 1.1.2 wellsprings machine learning . . . . . . . . . . . . . .', 'answer widrow-hoﬀprocedure minimizes error training set; necessarily minimize error future experience. [', '100 machine learning s & answers steve nouri q1 explain difference supervised unsupervised machine learning?', '81 6.4.3 avoiding overﬁtting decision trees . . . . . . . . . . .', 'overfitting, statistical model help letting know random noise errors instead underlying relationship.', 'statistics machine learning, common tasks fit model set training data, able reliable predictions general untrained data.', 'statistics machine learning, common tasks fit model set training data, able reliable predictions general untrained data.', 'regular- ization techniques like lasso help penalize model parameters likely lead overfitting.', 'regular- ization techniques like lasso help penalize model parameters likely lead overfitting.', 'firstly, need clear picture data, constraints, problems heading different machine learning algorithms.', 'combat overfitting underfitting, resample data estimate model accuracy (k-fold cross-validation) having validation dataset evaluate model.', 'goal cross-validation term data set test model training phase (i.e. validation data set) order limit problems like overfitting insight model generalize independent data set.']\n"
     ]
    }
   ],
   "source": [
    "print(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8877aa473dfa93b2",
   "metadata": {},
   "source": [
    "# 4. Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8fe840bd679626",
   "metadata": {},
   "source": [
    "## 4.1 Login to HuggingFace CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a7d432027efb7be0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:38:36.750390Z",
     "start_time": "2025-02-09T11:38:36.738802Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb7390a6907b46e9be6d8b116faccdfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75492f83d44a73d",
   "metadata": {},
   "source": [
    "## 4.2 Loading the LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e064d9014c81b7b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:38:47.266956Z",
     "start_time": "2025-02-09T11:38:37.962878Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "966fa720d4c04e799474719c7562e481",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=LLM_MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=False,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d6e6ed3c6d3949",
   "metadata": {},
   "source": [
    "## 4.3 Augmenting the prompt for instructing the LLM in a better way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1042c149cef8b04a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:38:48.422916Z",
     "start_time": "2025-02-09T11:38:47.273510Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL)\n",
    "context = \"\\n -\".join(context)\n",
    "base_prompt = f'''Bases on the following context items, please answer the query\n",
    "Context Items:\n",
    "{context}\n",
    "Query:\n",
    "{QUERY}\n",
    "Answer:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a9329f053bc5df59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:38:51.178431Z",
     "start_time": "2025-02-09T11:38:51.173961Z"
    }
   },
   "outputs": [],
   "source": [
    "base_prompt = base_prompt.format(context=context, query=QUERY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ae612c14837c95",
   "metadata": {},
   "source": [
    "## 4.4 Creating the dialogue template for the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a6eadc67098c5cc4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:38:55.854374Z",
     "start_time": "2025-02-09T11:38:55.847614Z"
    }
   },
   "outputs": [],
   "source": [
    "dialogue_template = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": base_prompt,\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafb0016557c0272",
   "metadata": {},
   "source": [
    "## 4.5 Applying the prompt to the dialogue template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "277dde89685a2245",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:38:57.127131Z",
     "start_time": "2025-02-09T11:38:57.105583Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
    "                                       tokenize=False,\n",
    "                                       add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c2a05e96737f1",
   "metadata": {},
   "source": [
    "## 4.6 Providing the prompt and retrieving the answer from the LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9b57262112415464",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:42:29.646183Z",
     "start_time": "2025-02-09T11:42:27.227166Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL)\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**input_ids, temperature=TEMPERATURE, do_sample=True, max_new_tokens=MAX_NEW_TOKENS)\n",
    "output_text = tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f7816da98e6a7a3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:42:29.658075Z",
     "start_time": "2025-02-09T11:42:29.653897Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><bos><start_of_turn>user\n",
      "Bases on the following context items, please answer the query\n",
      "Context Items:\n",
      "overfitting comes light data associated complexity, means associated parameters relative number observations.\n",
      " -follow steve nouri ai data science posts https//lnkd.in/gzu463x overfitting, statistical model describes random error noise instead underlying relationship.\n",
      " -simple restatement fundamental problem machine learning possibility overfitting training data carrying noise data test set, providing inaccurate generalizations.\n",
      " -lot data overﬁtting avoided, overﬁtting happens relatively small dataset, try learn it.\n",
      " -lot data overﬁtting avoided, overﬁtting happens relatively small dataset, try learn it.\n",
      " -overfitting evaluation 83 (from weiss, s., kulikowski, c., computer systems learn, morgan kaufmann, 1991) training errors validation errors 1 2 3 4 5 6 7 8 9 0.2 0.4 0.6 0.8 1.0 0 0 error rate number terminal nodes iris data decision tree figure 6.8 determining overﬁtting begins stopping growth decision tree, grow size prune away leaf nodes ancestors cross- validation accuracy longer increases.\n",
      " -underfitting occurs statistical model machine learning algorithm capture underlying trend data.\n",
      " -underfitting occurs statistical model machine learning algorithm capture underlying trend data.\n",
      " -decision trees prone overfitting, pruning tree helps reduce size minimizes chances overfitting.\n",
      " -poor predictive performance – overfitting underfitting yield poor predictive performance, way different.\n",
      " -overfitting situation occurs model learns training set well, taking random fluctuations training data concepts.\n",
      " -machine learning, statistical model describes random error noise instead underlying relationship ‘overﬁtting’ occurs.\n",
      " -machine learning, statistical model describes random error noise instead underlying relationship ‘overﬁtting’ occurs.\n",
      " -quora) simple restatement fundamental problem machine learning possibility overfitting training data carrying noise data test set, providing inaccu- rate generalizations.\n",
      " -quora) simple restatement fundamental problem machine learning possibility overfitting training data carrying noise data test set, providing inaccu- rate generalizations.\n",
      " -reading bias-variance tradeoff (wikipedia) bias error erroneous overly simplistic assumptions learning algorithm you’re using.\n",
      " -reading bias-variance tradeoff (wikipedia) bias error erroneous overly simplistic assumptions learning algorithm you’re using.\n",
      " -6.4 overﬁtting evaluation 6.4.1 overﬁtting supervised learning, choose function ﬁt training set set hypotheses.\n",
      " -validation set training set parameter selection avoiding overfitting machine learning model developed.\n",
      " -goals model training identify signal ignore noise model given free rein minimize error, possibility suffering overfitting.\n",
      " -prediction rate provides low prediction training error test error leads high business problem, error rate training set high error rate test set high, conclude overfitting model.\n",
      " -typically reduce overfit- ting models model robust (unlikely influ- enced small changes training data).\n",
      " -typically reduce overfit- ting models model robust (unlikely influ- enced small changes training data).\n",
      " -predictive models tradeoff bias (how model fits data) variance (how model changes based changes inputs).\n",
      " -terms machine learning, algorithm aims understand example particular concept generalizations form concepts training examples.\n",
      " -reading waymo tech machine learning interview s like test knowledge different machine learning methods, inven- tiveness don’t know answer.\n",
      " -reading waymo tech machine learning interview s like test knowledge different machine learning methods, inven- tiveness don’t know answer.\n",
      " -following differences overfitting underfitting definition – statistical model suffering overfitting describes random error noise place underlying relationship.\n",
      " -9 1.3 learning requires bias . . . . . . . . . . . . . . . . . . . . . . . .\n",
      " -validation set considered training set parameter selection avoid overfitting model built.\n",
      " -validation set considered training set parameter selection avoid overfitting model built.\n",
      " -validation set considered training set parameter selection avoid overfitting model built.\n",
      " -however, bias, training set suﬃciently large compared size hypothesis space, consistent functions useful guesses, generalization performance poor.\n",
      " -support vector machine algorithm low bias high variance, trade-off changed increasing c parameter influences number violations margin allowed training data increases bias decreases variance.\n",
      " -prediction rate high inconsistency training error test error leads ta high business problem, error rate training set low error rate ithe n test set high, conclude overfitting model.\n",
      " -lesser variables parameters, variance reduced ● cross-validation methods like k-folds ● model parameters likely cause overfitting, techniques regularization like lasso penalize parameters steve nouri ​ ​https//www.linkedin.com/in/stevenouri/\n",
      " -goal cross-validation term data set test model training phase (i.e. validation data set) limit problems like overfitting gain insight model generalize independent data set.\n",
      " -machine learning statistics, common task undergo fit model set training data.\n",
      " -1 1.1.2 wellsprings machine learning . . . . . . . . . . . . . .\n",
      " -answer widrow-hoﬀprocedure minimizes error training set; necessarily minimize error future experience. [\n",
      " -100 machine learning s & answers steve nouri q1 explain difference supervised unsupervised machine learning?\n",
      " -81 6.4.3 avoiding overﬁtting decision trees . . . . . . . . . . .\n",
      " -overfitting, statistical model help letting know random noise errors instead underlying relationship.\n",
      " -statistics machine learning, common tasks fit model set training data, able reliable predictions general untrained data.\n",
      " -statistics machine learning, common tasks fit model set training data, able reliable predictions general untrained data.\n",
      " -regular- ization techniques like lasso help penalize model parameters likely lead overfitting.\n",
      " -regular- ization techniques like lasso help penalize model parameters likely lead overfitting.\n",
      " -firstly, need clear picture data, constraints, problems heading different machine learning algorithms.\n",
      " -combat overfitting underfitting, resample data estimate model accuracy (k-fold cross-validation) having validation dataset evaluate model.\n",
      " -goal cross-validation term data set test model training phase (i.e. validation data set) order limit problems like overfitting insight model generalize independent data set.\n",
      "Query:\n",
      "What is overfitting in machine learning? Explain in 200 words\n",
      "Answer:<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Sure, here's a 200-word explanation of overfitting in machine learning:\n",
      "\n",
      "Overfitting is a phenomenon where a machine learning model becomes too closely fit to the training data and fails to generalize well to new, unseen data. This results in poor predictive performance, even on data similar to the training set.\n",
      "\n",
      "Overfitting occurs when a model is trained on a relatively small dataset and is unable to generalize its knowledge to a larger, unseen dataset. This can happen when the model has too few parameters to capture the underlying relationship between the features and the target variable. As a result, the model makes random errors that are not representative of the true underlying relationship.\n",
      "\n",
      "Overfitting can be addressed by increasing the size of the training dataset, using regularization techniques, or by using cross-validation to select the optimal model parameters.<eos>\n"
     ]
    }
   ],
   "source": [
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b739bcea21ecd35a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:42:29.775358Z",
     "start_time": "2025-02-09T11:42:29.772028Z"
    }
   },
   "outputs": [],
   "source": [
    "idx = output_text.find(\"Answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e8a743149d967b4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:42:29.840167Z",
     "start_time": "2025-02-09T11:42:29.836521Z"
    }
   },
   "outputs": [],
   "source": [
    "answer = output_text[idx+7:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3acff9162acb0b8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:42:29.974904Z",
     "start_time": "2025-02-09T11:42:29.970946Z"
    }
   },
   "outputs": [],
   "source": [
    "answer = answer.replace(\"**\", \"\")\n",
    "answer = answer.replace(\"<start_of_turn>model\",\"\")\n",
    "answer = re.sub(\"<.*?>\", \"\", answer)\n",
    "# answer = answer[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ea26bd933ddca3e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:42:37.796573Z",
     "start_time": "2025-02-09T11:42:37.787565Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cleaned answer is: \n",
      "\n",
      "Sure, here's a 200-word explanation of overfitting in machine learning:\n",
      "\n",
      "Overfitting is a phenomenon where a machine learning model becomes too closely fit to the training data and fails to generalize well to new, unseen data. This results in poor predictive performance, even on data similar to the training set.\n",
      "\n",
      "Overfitting occurs when a model is trained on a relatively small dataset and is unable to generalize its knowledge to a larger, unseen dataset. This can happen when the model has too few parameters to capture the underlying relationship between the features and the target variable. As a result, the model makes random errors that are not representative of the true underlying relationship.\n",
      "\n",
      "Overfitting can be addressed by increasing the size of the training dataset, using regularization techniques, or by using cross-validation to select the optimal model parameters.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The cleaned answer is: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687ee293-bfa9-4715-a7d7-77644fec5bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
