{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. IMPORTING LIBRARIES, FUNCTION AND DEFINING GLOBAL VARIABLES",
   "id": "1f2d85b35e1e945f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1.1 Importing Libraries",
   "id": "7e3e95c360721358"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:38:04.113988Z",
     "start_time": "2025-02-09T11:38:00.074547Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Library needed to load the pdf\n",
    "import fitz\n",
    "\n",
    "# Library needed to process the text using regular expressions\n",
    "import re\n",
    "\n",
    "# Library needed to display or process the data in forms of dataframes\n",
    "import pandas as pd\n",
    "\n",
    "# Library needed to handle the operations in deep learning\n",
    "import torch\n",
    "\n",
    "# Library needed to convert the data into arrays for faster processing\n",
    "import numpy as np\n",
    "\n",
    "# Library to handle operating system related operations\n",
    "import os\n",
    "\n",
    "from rich.jupyter import display"
   ],
   "id": "e1be9ae60d3bc106",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1.2 Importing Functions",
   "id": "b8b18b422d1d839a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:38:09.421236Z",
     "start_time": "2025-02-09T11:38:04.113988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# (OPTIONAL) Function to beautify the waiting process with a loading bar\n",
    "from tqdm.auto import tqdm as tqdm\n",
    "\n",
    "# Function to process the text in English\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# Function to convert paragraphs to sentences\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Function to provide utility services to process the text such as tokenization, sentencizer\n",
    "from sentence_transformers import util\n",
    "\n",
    "# Functions for loading the LLM model\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# Function for fetching the paths to pdfs\n",
    "from glob import glob"
   ],
   "id": "7d138f8cdda61885",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1.3 Defining Global Variables",
   "id": "d989d11d53db12e8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:38:09.583345Z",
     "start_time": "2025-02-09T11:38:09.577578Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Global variable consisting of all the stop words\n",
    "from spacy.lang.en import STOP_WORDS\n",
    "\n",
    "# Global variable telling about the number of sentences in each chunk stored in the dictionary\n",
    "SENTENCE_CHUNKS = 10\n",
    "\n",
    "# Global variable storing the name of the model that is used for the embedding\n",
    "EMBEDDING_MODEL = 'all-mpnet-base-v2'\n",
    "\n",
    "# Global variable storing the names of the pdfs that are to be loaded to be fed into the RAG model\n",
    "PDF_PATHS = list()\n",
    "\n",
    "# Global variable storing the integer telling to fetch the top k similar records for further processing\n",
    "K = 5\n",
    "\n",
    "# Global variable storing the name of the LLM model that will be used for augmenting the similar data\n",
    "LLM_MODEL = 'google/gemma-2b-it'\n",
    "\n",
    "# (FOR TESTING) Global variable storing the query that user wants to ask\n",
    "QUERY = \"What is machine learning?\"\n",
    "\n",
    "# Setting up the device agnostic code\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Global variable for setting the temperature of the LLM model i.e how much data should LLM generate\n",
    "TEMPERATURE = 0.6\n",
    "\n",
    "## Global variable defining the length of tokens that the LLM has to generate\n",
    "MAX_NEW_TOKENS = 256"
   ],
   "id": "5d9391505ec14fa5",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2. DATA ACQUISITION",
   "id": "cd579bf352b46f64"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2.1 Getting the paths to all the pdfs in the `Dataset` folder",
   "id": "d9ac37ccdc34c608"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "PDF_PATHS = glob('.\\\\Dataset\\\\*.pdf')",
   "id": "144a542f97a7fd0d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for idx, path in tqdm(enumerate(PDF_PATHS), total=len(PDF_PATHS)):\n",
    "    print(f\"{idx+1}. {path[10:-4]}\")"
   ],
   "id": "26c7124cd29610ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2.2 Opening all the documents",
   "id": "85dd0b9bf343f79b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "documents = list()\n",
    "for path in tqdm(PDF_PATHS, total=len(PDF_PATHS)):\n",
    "    doc = fitz.open(path)\n",
    "    documents.append(doc)"
   ],
   "id": "84de47a6ce3e068a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for doc in tqdm(documents, total=len(documents)):\n",
    "    print(doc)"
   ],
   "id": "1fee0989615356e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2.3 Getting the text from all the documents",
   "id": "81e1e996c74b7c79"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pages = dict()\n",
    "for doc in tqdm(documents, total=len(documents)):\n",
    "    for page_number, page in tqdm(enumerate(doc), total=len(doc)):\n",
    "        page_number = len(pages)\n",
    "        pages[page_number] = page.get_text()"
   ],
   "id": "f9f67dd2a9810cc0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for page_number, page in tqdm(pages.items(), total=len(documents)):\n",
    "    print(f\"{page_number}. {pages[page_number]}\")\n",
    "    print()"
   ],
   "id": "826c83e4f4dbda7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2.4 Getting the metadata of each page",
   "id": "21bb2c1ba808daed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pages_and_metadata = list()\n",
    "for page_number, page in tqdm(pages.items(), total=len(pages)):\n",
    "    metadata = dict()\n",
    "    metadata['page_number'] = page_number\n",
    "    metadata['raw_text'] = page\n",
    "    metadata['number_of_characters'] = len(page)\n",
    "    metadata['number_of_tokens'] = len(page)/4\n",
    "    metadata['number_of_words'] = len(page.split())\n",
    "    pages_and_metadata.append(metadata)"
   ],
   "id": "7753b375ca7e8f31",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df = pd.DataFrame(pages_and_metadata)",
   "id": "fdf53efcf6db14a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.describe().round(2)",
   "id": "4fda5bbeb07d871b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2.4 Preprocessing the `raw_text` from metadata",
   "id": "ceb4a2f984a78d18"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def convert_to_lowercase(text):\n",
    "    new_text = text.lower()\n",
    "    return new_text"
   ],
   "id": "b8103461a031b8c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def remove_stopwords(text):\n",
    "    new_text = []\n",
    "    for word in text.split():\n",
    "        if word not in STOP_WORDS:\n",
    "            new_text.append(word)\n",
    "    return \" \".join(new_text)"
   ],
   "id": "522849a9bcea61aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def remove_html_tags(text):\n",
    "    new_text = re.sub(r\"<!--.*?-->\", \"\", text)\n",
    "    return new_text"
   ],
   "id": "9c2f5226953fb7ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def remove_newlines(text):\n",
    "    new_text = re.sub(r\"\\n+\", \" \", text)\n",
    "    return new_text"
   ],
   "id": "e1d03d45cc290e34",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def remove_multiple_spaces(text):\n",
    "    new_text = text.replace(\"  \", \" \")\n",
    "    return new_text"
   ],
   "id": "be43849bb32f99de",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def remove_comments(text):\n",
    "    new_text = re.sub(r\"<!--.*?-->\", \"\", text)\n",
    "    return new_text"
   ],
   "id": "c3d5cde9e9b9d9ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def preprocess_text(text):\n",
    "    text = convert_to_lowercase(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = remove_html_tags(text)\n",
    "    text = remove_newlines(text)\n",
    "    text = remove_multiple_spaces(text)\n",
    "    text = remove_comments(text)\n",
    "    return text"
   ],
   "id": "ce97b989463a7c01",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for page in tqdm(pages_and_metadata, total=len(pages_and_metadata)):\n",
    "    page[\"formatted_text\"] = preprocess_text(page[\"raw_text\"])"
   ],
   "id": "2241967daa9316b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for page in tqdm(pages_and_metadata, total=len(pages_and_metadata)):\n",
    "    print(page[\"formatted_text\"])\n",
    "    print()"
   ],
   "id": "d92a1fdb3f006122",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2.5 Converting the paragraphs to sentences",
   "id": "26d1c3bd529dadde"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "nlp = English()\n",
    "nlp.add_pipe('sentencizer')\n",
    "for page in tqdm(pages_and_metadata, total=len(pages_and_metadata)):\n",
    "    sentences = nlp(page[\"formatted_text\"]).sents\n",
    "    sentences = [str(sentence) for sentence in sentences]\n",
    "    pages_and_metadata[page[\"page_number\"]][\"sentences\"] = sentences"
   ],
   "id": "3247973ed0de93c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for page in tqdm(pages_and_metadata, total=len(pages_and_metadata)):\n",
    "    print(page[\"sentences\"])\n",
    "    print()"
   ],
   "id": "3d1e7e6e7b96e2e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2.6 Update the metadata",
   "id": "3244522b7b680f87"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for page in tqdm(pages_and_metadata, total=len(pages_and_metadata)):\n",
    "    pages_and_metadata[page[\"page_number\"]][\"number_of_sentences\"] = len(page[\"sentences\"])"
   ],
   "id": "8943b0ff4b313750",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for key in pages_and_metadata[0].keys():\n",
    "    print(key)"
   ],
   "id": "b9813d9d02986948",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2.7 Converting sentences to sentence_chunks",
   "id": "6b2d4bf7c69a5ea0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for page in tqdm(pages_and_metadata, total=len(pages_and_metadata)):\n",
    "    sentences = pages_and_metadata[page[\"page_number\"]][\"sentences\"]\n",
    "    sentence_chunk = [sentences[i : i+SENTENCE_CHUNKS] for i in range(0, len(sentences), SENTENCE_CHUNKS)]\n",
    "    page[\"sentence_chunk\"] = sentence_chunk"
   ],
   "id": "22ed0b39d46cf55e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for page in tqdm(pages_and_metadata, total=len(pages_and_metadata)):\n",
    "    print(page[\"sentence_chunk\"])\n",
    "    print()"
   ],
   "id": "5b03d9f534a696c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2.8 Converting sentence_chunks into sentence embeddings",
   "id": "eedc481bbe1a825b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "embedding_model = SentenceTransformer(model_name_or_path=EMBEDDING_MODEL).to(device)",
   "id": "f450c90f591d29ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for page in pages_and_metadata:\n",
    "    print(f\"Processing page {page['page_number']}\")\n",
    "    embeddings = list()\n",
    "    for sentence in page[\"sentences\"]:\n",
    "        # sentence = sentence.to(device)\n",
    "        embedding = embedding_model.encode(sentence, batch_size=32, convert_to_tensor=True, show_progress_bar=True)\n",
    "        embedding = np.stack(embedding.tolist(), axis=0)\n",
    "        embedding = torch.tensor(embedding)\n",
    "        embedding = embedding.type(torch.float32)\n",
    "        embeddings.append(embedding)\n",
    "    sentence_embeddings = [np.array(embedding) for embedding in embeddings]\n",
    "    pages_and_metadata[page[\"page_number\"]][\"embeddings\"] = sentence_embeddings"
   ],
   "id": "8820fa929ef41f04",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for page in tqdm(pages_and_metadata, total=len(pages_and_metadata)):\n",
    "    print(np.array(page[\"embeddings\"]).shape)"
   ],
   "id": "3567e35e0f369ed6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2.9 Checking the metadata present for use",
   "id": "8360bc8d64e45d79"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for key in pages_and_metadata[0].keys():\n",
    "    print(key)"
   ],
   "id": "5023c654f155fd09",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. FETCHING SIMILAR CONTENT",
   "id": "59bd19f71651b712"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3.1 Getting the data embeddings",
   "id": "184fd946401c91fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pages_and_metadata_embeddings = []\n",
    "\n",
    "for page in tqdm(pages_and_metadata, total=len(pages_and_metadata)):\n",
    "    page_embeddings = []\n",
    "    for chunk_embedding in pages_and_metadata[page[\"page_number\"]][\"embeddings\"]:\n",
    "        if isinstance(chunk_embedding, torch.Tensor):\n",
    "            chunk_embedding = chunk_embedding.tolist()\n",
    "        page_embeddings.append(chunk_embedding)\n",
    "    pages_and_metadata_embeddings.append(page_embeddings)"
   ],
   "id": "8c81dcf0f0426027",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3.2 Converting each embedding into the same dimensions",
   "id": "78792dfaf1127f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if pages_and_metadata_embeddings:\n",
    "    embedding_dim = len(pages_and_metadata_embeddings[0][0])\n",
    "    pages_and_metadata_embeddings = [\n",
    "            [np.pad(chunk, (0, max(0, embedding_dim - len(chunk))), mode='constant')[:embedding_dim]\n",
    "             for chunk in page]\n",
    "            for page in pages_and_metadata_embeddings\n",
    "        ]"
   ],
   "id": "59618f1fd119517a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3.3 Flattening the nested list of embeddings and the sentence to fetch by index",
   "id": "2300c8743f62a23a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "flat_embeddings = [chunk for page in pages_and_metadata_embeddings for chunk in page]\n",
    "flat_data = [sentence for page in pages_and_metadata for sentence in page[\"sentences\"]]"
   ],
   "id": "4e07bdbecbd85c9d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3.4 Saving the flattened embeddings and the flattened data",
   "id": "fd9a9684b1287496"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = pd.DataFrame(flat_embeddings)\n",
    "df.to_csv(\"embeddings.csv\", index=False)\n",
    "\n",
    "df = pd.DataFrame(flat_data)\n",
    "df.to_csv(\"data.csv\", index=False)"
   ],
   "id": "4533ad448b48e20",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3.5 Loading the flattened embeddings and flattened data",
   "id": "6e9c5f925a05048b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:38:13.460239Z",
     "start_time": "2025-02-09T11:38:13.128443Z"
    }
   },
   "cell_type": "code",
   "source": [
    "flat_embeddings = pd.read_csv(\"embeddings.csv\").to_numpy()\n",
    "flat_data = pd.read_csv(\"data.csv\")[\"0\"].tolist()"
   ],
   "id": "80f723dee61c4c4b",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3.6 Converting embeddings to numpy array",
   "id": "1dd66e23aff7a901"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:38:14.661823Z",
     "start_time": "2025-02-09T11:38:14.644537Z"
    }
   },
   "cell_type": "code",
   "source": "pages_and_metadata_embeddings = np.array(flat_embeddings, dtype=np.float32)",
   "id": "f3f43ce0368e6c95",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3.7 Converting the numpy array embeddings to torch tensors",
   "id": "61118042f2f3a0c1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:38:16.402973Z",
     "start_time": "2025-02-09T11:38:15.539250Z"
    }
   },
   "cell_type": "code",
   "source": "pages_and_metadata_embeddings = torch.tensor(pages_and_metadata_embeddings, dtype=torch.float32).to(device)",
   "id": "476ae4b96e92b843",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3.8 Getting the similarity score by query",
   "id": "c7691ac5f8be5636"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:38:24.424800Z",
     "start_time": "2025-02-09T11:38:17.617912Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embedding_model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "query_embeddings = embedding_model.encode(QUERY, convert_to_tensor=True).to(device)\n",
    "dot_score = util.dot_score(query_embeddings, pages_and_metadata_embeddings)[0]"
   ],
   "id": "6745dc59ef034840",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:38:25.403335Z",
     "start_time": "2025-02-09T11:38:25.396515Z"
    }
   },
   "cell_type": "code",
   "source": "print(dot_score)",
   "id": "a19329fec15147ef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3611, 0.1300, 0.2697,  ..., 0.1486, 0.0943, 0.0550], device='cuda:0')\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3.9 Getting the top k similar scores",
   "id": "9bea1268563c3129"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:38:27.505935Z",
     "start_time": "2025-02-09T11:38:27.441640Z"
    }
   },
   "cell_type": "code",
   "source": "top_scores, top_indices = torch.topk(dot_score, k=K)",
   "id": "7dea591cca2af654",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:38:28.290866Z",
     "start_time": "2025-02-09T11:38:28.283472Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Top scores: {top_scores}\")\n",
    "print(f\"Top indices: {top_indices}\")"
   ],
   "id": "87a0a2111e42c0cb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top scores: tensor([0.7429, 0.6610, 0.6148, 0.6127, 0.6017], device='cuda:0')\n",
      "Top indices: tensor([ 872,   15, 2471,  532,  412], device='cuda:0')\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3.10 Getting the top k content based on the scores",
   "id": "1a93ccde99a8e5fd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:38:31.127772Z",
     "start_time": "2025-02-09T11:38:31.124080Z"
    }
   },
   "cell_type": "code",
   "source": [
    "context = list()\n",
    "for index in top_indices:\n",
    "    print(f\"Fetching data from page {index}\")\n",
    "    context.append(flat_data[index.item()])"
   ],
   "id": "412b8aad1ac4bc28",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data from page 872\n",
      "Fetching data from page 15\n",
      "Fetching data from page 2471\n",
      "Fetching data from page 532\n",
      "Fetching data from page 412\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:38:31.789353Z",
     "start_time": "2025-02-09T11:38:31.783885Z"
    }
   },
   "cell_type": "code",
   "source": "print(context)",
   "id": "777d6213b100c8ef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['machine learning?', 'it’s smart algorithms making decisions based available data.', 'machine learning relates study, design, development algorithms computers capability learn explicitly programmed.', 'reading: waymo tech machine learning interview questions like test knowledge different machine learning methods, inven- tiveness don’t know answer.', 'machine learning interview questions look details.']\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4. Augmentation",
   "id": "8877aa473dfa93b2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4.1 Login to HuggingFace CLI",
   "id": "1a8fe840bd679626"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:38:36.750390Z",
     "start_time": "2025-02-09T11:38:36.738802Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ],
   "id": "a7d432027efb7be0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0398db18e6b6413c93807c422545603f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4.2 Loading the LLM model",
   "id": "75492f83d44a73d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:38:47.266956Z",
     "start_time": "2025-02-09T11:38:37.962878Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=LLM_MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=False,\n",
    ").to(device)"
   ],
   "id": "e064d9014c81b7b4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "41fdb16df53a46408afd57b1638e6698"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4.3 Augmenting the prompt for instructing the LLM in a better way",
   "id": "31d6e6ed3c6d3949"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:38:48.422916Z",
     "start_time": "2025-02-09T11:38:47.273510Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL)\n",
    "context = \"\\n -\".join(context)\n",
    "base_prompt = f'''Bases on the following context items, please answer the query\n",
    "Context Items:\n",
    "{context}\n",
    "Query:\n",
    "{QUERY}\n",
    "Answer:'''"
   ],
   "id": "1042c149cef8b04a",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:38:51.178431Z",
     "start_time": "2025-02-09T11:38:51.173961Z"
    }
   },
   "cell_type": "code",
   "source": "base_prompt = base_prompt.format(context=context, query=QUERY)",
   "id": "a9329f053bc5df59",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4.4 Creating the dialogue template for the LLM",
   "id": "d7ae612c14837c95"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:38:55.854374Z",
     "start_time": "2025-02-09T11:38:55.847614Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dialogue_template = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": base_prompt,\n",
    "}]"
   ],
   "id": "a6eadc67098c5cc4",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4.5 Applying the prompt to the dialogue template",
   "id": "bafb0016557c0272"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:38:57.127131Z",
     "start_time": "2025-02-09T11:38:57.105583Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
    "                                       tokenize=False,\n",
    "                                       add_generation_prompt=True)"
   ],
   "id": "277dde89685a2245",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4.6 Providing the prompt and retrieving the answer from the LLM model",
   "id": "a3c2a05e96737f1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:42:29.646183Z",
     "start_time": "2025-02-09T11:42:27.227166Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL)\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**input_ids, temperature=TEMPERATURE, do_sample=True, max_new_tokens=MAX_NEW_TOKENS)\n",
    "output_text = tokenizer.decode(outputs[0])"
   ],
   "id": "9b57262112415464",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:42:29.658075Z",
     "start_time": "2025-02-09T11:42:29.653897Z"
    }
   },
   "cell_type": "code",
   "source": "print(output_text)",
   "id": "f7816da98e6a7a3c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><bos><start_of_turn>user\n",
      "Bases on the following context items, please answer the query\n",
      "Context Items:\n",
      "machine learning?\n",
      " -it’s smart algorithms making decisions based available data.\n",
      " -machine learning relates study, design, development algorithms computers capability learn explicitly programmed.\n",
      " -reading: waymo tech machine learning interview questions like test knowledge different machine learning methods, inven- tiveness don’t know answer.\n",
      " -machine learning interview questions look details.\n",
      "Query:\n",
      "What is machine learning?\n",
      "Answer:<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Sure, here's the answer to the query:\n",
      "\n",
      "According to the context items, machine learning is a field of study that involves algorithms that can learn from data to make decisions and solve problems.<eos>\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:42:29.775358Z",
     "start_time": "2025-02-09T11:42:29.772028Z"
    }
   },
   "cell_type": "code",
   "source": "idx = output_text.find(\"Answer\")",
   "id": "b739bcea21ecd35a",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:42:29.840167Z",
     "start_time": "2025-02-09T11:42:29.836521Z"
    }
   },
   "cell_type": "code",
   "source": "answer = output_text[idx+7:]",
   "id": "e8a743149d967b4d",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:42:29.974904Z",
     "start_time": "2025-02-09T11:42:29.970946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "answer = answer.replace(\"**\", \"\")\n",
    "answer = answer.replace(\"<start_of_turn>model\",\"\")\n",
    "answer = re.sub(\"<.*?>\", \"\", answer)\n",
    "# answer = answer[]"
   ],
   "id": "3acff9162acb0b8c",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:42:37.796573Z",
     "start_time": "2025-02-09T11:42:37.787565Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"The cleaned answer is: {answer}\")",
   "id": "ea26bd933ddca3e4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cleaned answer is: \n",
      "\n",
      "Sure, here's the answer to the query:\n",
      "\n",
      "According to the context items, machine learning is a field of study that involves algorithms that can learn from data to make decisions and solve problems.\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e23afe595bdb2557",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
