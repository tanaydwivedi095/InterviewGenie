{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f2d85b35e1e945f",
   "metadata": {},
   "source": [
    "# 1. IMPORTING LIBRARIES, FUNCTION AND DEFINING GLOBAL VARIABLES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3e95c360721358",
   "metadata": {},
   "source": [
    "## 1.1 Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1be9ae60d3bc106",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:38:29.057193Z",
     "start_time": "2025-02-10T16:38:24.848738Z"
    }
   },
   "outputs": [],
   "source": [
    "# Library needed to load the pdf\n",
    "import fitz\n",
    "\n",
    "# Library needed to process the text using regular expressions\n",
    "import re\n",
    "\n",
    "# Library needed to display or process the data in forms of dataframes\n",
    "import pandas as pd\n",
    "\n",
    "# Library needed to handle the operations in deep learning\n",
    "import torch\n",
    "\n",
    "# Library needed to convert the data into arrays for faster processing\n",
    "import numpy as np\n",
    "\n",
    "# Library to handle operating system related operations\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b18b422d1d839a",
   "metadata": {},
   "source": [
    "## 1.2 Importing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d138f8cdda61885",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:39:37.411274Z",
     "start_time": "2025-02-10T16:39:32.176313Z"
    }
   },
   "outputs": [],
   "source": [
    "# (OPTIONAL) Function to beautify the waiting process with a loading bar\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "# Function to process the text in English\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# Function to convert paragraphs to sentences\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Function to provide utility services to process the text such as tokenization, sentencizer\n",
    "from sentence_transformers import util\n",
    "\n",
    "# Functions for loading the LLM model\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "# Function for fetching the paths to pdfs\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d989d11d53db12e8",
   "metadata": {},
   "source": [
    "## 1.3 Defining Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d9391505ec14fa5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:39:42.151208Z",
     "start_time": "2025-02-10T16:39:42.144311Z"
    }
   },
   "outputs": [],
   "source": [
    "# Global variable consisting of all the stop words\n",
    "from spacy.lang.en import STOP_WORDS\n",
    "\n",
    "# Global variable telling about the number of sentences in each chunk stored in the dictionary\n",
    "SENTENCE_CHUNKS = 10\n",
    "\n",
    "# Global variable storing the name of the model that is used for the embedding\n",
    "EMBEDDING_MODEL = 'all-MiniLM-L12-v2'\n",
    "\n",
    "# Global variable storing the names of the pdfs that are to be loaded to be fed into the RAG model\n",
    "PDF_PATHS = list()\n",
    "\n",
    "# Global variable storing the integer telling to fetch the top k similar records for further processing\n",
    "K = 10\n",
    "\n",
    "# Global variable storing the name of the LLM model that will be used for augmenting the similar data\n",
    "LLM_MODEL = 'google/gemma-2b-it'\n",
    "\n",
    "# (FOR TESTING) Global variable storing the query that user wants to ask\n",
    "# QUERY = \"What are some good practices in machine learning?\"\n",
    "# QUERY = \"What is formula for sigmoid function? The value for s in the function is 5. Can you compute the answer for the sigmoid function.\"\n",
    "QUERY = \"What is Tanh activation function.\"\n",
    "\n",
    "# Setting up the device agnostic code\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Global variable for setting the temperature of the LLM model i.e how much data should LLM generate\n",
    "TEMPERATURE = 0.9\n",
    "\n",
    "## Global variable defining the length of tokens that the LLM has to generate\n",
    "MAX_NEW_TOKENS = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd579bf352b46f64",
   "metadata": {},
   "source": [
    "# 2. DATA ACQUISITION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ac37ccdc34c608",
   "metadata": {},
   "source": [
    "## 2.1 Getting the paths to all the pdfs in the `Dataset` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144a542f97a7fd0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:39:44.681217Z",
     "start_time": "2025-02-10T16:39:44.671241Z"
    }
   },
   "outputs": [],
   "source": [
    "PDF_PATHS = glob('.\\\\Dataset\\\\*.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c7124cd29610ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:39:46.384156Z",
     "start_time": "2025-02-10T16:39:46.366334Z"
    }
   },
   "outputs": [],
   "source": [
    "for idx, path in tqdm(enumerate(PDF_PATHS), total=len(PDF_PATHS)):\n",
    "    print(f\"{idx+1}. {path[10:-4]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dd0b9bf343f79b",
   "metadata": {},
   "source": [
    "## 2.2 Opening all the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84de47a6ce3e068a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:39:48.621570Z",
     "start_time": "2025-02-10T16:39:48.539419Z"
    }
   },
   "outputs": [],
   "source": [
    "documents = list()\n",
    "for path in tqdm(PDF_PATHS, total=len(PDF_PATHS)):\n",
    "    doc = fitz.open(path)\n",
    "    documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fee0989615356e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:39:49.354274Z",
     "start_time": "2025-02-10T16:39:49.339845Z"
    }
   },
   "outputs": [],
   "source": [
    "for doc in tqdm(documents, total=len(documents)):\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e1e996c74b7c79",
   "metadata": {},
   "source": [
    "## 2.3 Getting the text from all the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f67dd2a9810cc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:39:55.297068Z",
     "start_time": "2025-02-10T16:39:54.009306Z"
    }
   },
   "outputs": [],
   "source": [
    "pages = dict()\n",
    "for doc in tqdm(documents, total=len(documents)):\n",
    "    for page_number, page in enumerate(doc):\n",
    "        if(page_number<15): \n",
    "            continue\n",
    "        page_number = len(pages)\n",
    "        pages[page_number] = page.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826c83e4f4dbda7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:40:00.931923Z",
     "start_time": "2025-02-10T16:40:00.896512Z"
    }
   },
   "outputs": [],
   "source": [
    "for page_number, page in tqdm(pages.items(), total=len(documents)):\n",
    "    print(f\"{page_number}. {pages[page_number]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bb2c1ba808daed",
   "metadata": {},
   "source": [
    "## 2.4 Getting the metadata of each page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7753b375ca7e8f31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:40:03.757120Z",
     "start_time": "2025-02-10T16:40:03.738343Z"
    }
   },
   "outputs": [],
   "source": [
    "pages_and_metadata = list()\n",
    "for page_number, page in tqdm(pages.items(), total=len(pages)):\n",
    "    metadata = dict()\n",
    "    metadata['page_number'] = page_number\n",
    "    metadata['raw_text'] = page\n",
    "    metadata['number_of_characters'] = len(page)\n",
    "    metadata['number_of_tokens'] = len(page)/4\n",
    "    metadata['number_of_words'] = len(page.split())\n",
    "    pages_and_metadata.append(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf53efcf6db14a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:40:04.656908Z",
     "start_time": "2025-02-10T16:40:04.643066Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pages_and_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fda5bbeb07d871b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:40:06.939020Z",
     "start_time": "2025-02-10T16:40:06.915569Z"
    }
   },
   "outputs": [],
   "source": [
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6f59c1ce38342",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:40:08.516641Z",
     "start_time": "2025-02-10T16:40:08.511511Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"The number of tokens this model is being trained on are: {df[\"number_of_tokens\"].sum()/1000000:.2f} million tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea25bc7daf4f49bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:40:09.361819Z",
     "start_time": "2025-02-10T16:40:09.356518Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"The number of pages in the database are: {len(pages_and_metadata)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb4a2f984a78d18",
   "metadata": {},
   "source": [
    "## 2.4 Preprocessing the `raw_text` from metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8103461a031b8c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:41:53.870665Z",
     "start_time": "2025-02-10T16:41:53.856349Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_to_lowercase(text):\n",
    "    new_text = text.lower()\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522849a9bcea61aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:41:54.463014Z",
     "start_time": "2025-02-10T16:41:54.459723Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    new_text = []\n",
    "    for word in text.split():\n",
    "        if word not in STOP_WORDS:\n",
    "            new_text.append(word)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2f5226953fb7ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:41:54.848133Z",
     "start_time": "2025-02-10T16:41:54.842801Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "    new_text = re.sub(r\"<!--.*?-->\", \"\", text)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d03d45cc290e34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:41:55.205639Z",
     "start_time": "2025-02-10T16:41:55.202443Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_newlines(text):\n",
    "    new_text = re.sub(r\"\\n+\", \" \", text)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be43849bb32f99de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:41:55.666352Z",
     "start_time": "2025-02-10T16:41:55.662230Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_multiple_spaces(text):\n",
    "    new_text = text.replace(\"  \", \" \")\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d5cde9e9b9d9ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:41:56.144254Z",
     "start_time": "2025-02-10T16:41:56.139212Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_comments(text):\n",
    "    new_text = re.sub(r\"<!--.*?-->\", \"\", text)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed85a9aa-2a62-472e-8f54-b32283309103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unnecessary_text(text):\n",
    "    new_text = text.replace(\"answer:\",\"\").replace(\"question\", \"\").replace(\":\",\"\").replace(\"  \",\" \")\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce97b989463a7c01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:41:56.936299Z",
     "start_time": "2025-02-10T16:41:56.922968Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = convert_to_lowercase(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = remove_html_tags(text)\n",
    "    text = remove_newlines(text)\n",
    "    text = remove_multiple_spaces(text)\n",
    "    text = remove_comments(text)\n",
    "    text = remove_unnecessary_text(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2241967daa9316b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:41:57.858803Z",
     "start_time": "2025-02-10T16:41:57.814634Z"
    }
   },
   "outputs": [],
   "source": [
    "for page in tqdm(pages_and_metadata, total=len(pages_and_metadata)):\n",
    "    page[\"formatted_text\"] = preprocess_text(page[\"raw_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92a1fdb3f006122",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:41:59.743703Z",
     "start_time": "2025-02-10T16:41:59.720627Z"
    }
   },
   "outputs": [],
   "source": [
    "for page in tqdm(pages_and_metadata, total=len(pages_and_metadata)):\n",
    "    print(page[\"formatted_text\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d1c3bd529dadde",
   "metadata": {},
   "source": [
    "## 2.5 Converting the paragraphs to sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3247973ed0de93c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:42:03.889149Z",
     "start_time": "2025-02-10T16:42:03.168549Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = English()\n",
    "nlp.add_pipe('sentencizer')\n",
    "for page in tqdm(pages_and_metadata, total=len(pages_and_metadata)):\n",
    "    sentences = nlp(page[\"formatted_text\"]).sents\n",
    "    sentences = list(set([str(sentence).strip() for sentence in sentences if len(str(sentence).split())>10]))\n",
    "    pages_and_metadata[page[\"page_number\"]][\"sentences\"] = sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1e7e6e7b96e2e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:42:05.307900Z",
     "start_time": "2025-02-10T16:42:05.277901Z"
    }
   },
   "outputs": [],
   "source": [
    "for page in tqdm(pages_and_metadata, total=len(pages_and_metadata)):\n",
    "    print(page[\"sentences\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3244522b7b680f87",
   "metadata": {},
   "source": [
    "## 2.6 Update the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8943b0ff4b313750",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:42:07.860161Z",
     "start_time": "2025-02-10T16:42:07.846917Z"
    }
   },
   "outputs": [],
   "source": [
    "for page in tqdm(pages_and_metadata, total=len(pages_and_metadata)):\n",
    "    pages_and_metadata[page[\"page_number\"]][\"number_of_sentences\"] = len(page[\"sentences\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9813d9d02986948",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:42:08.918920Z",
     "start_time": "2025-02-10T16:42:08.912408Z"
    }
   },
   "outputs": [],
   "source": [
    "for key in pages_and_metadata[0].keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2d4bf7c69a5ea0",
   "metadata": {},
   "source": [
    "## 2.7 Converting sentences to sentence_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ed0b39d46cf55e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:42:11.689414Z",
     "start_time": "2025-02-10T16:42:11.669184Z"
    }
   },
   "outputs": [],
   "source": [
    "for page in tqdm(pages_and_metadata, total=len(pages_and_metadata)):\n",
    "    sentences = pages_and_metadata[page[\"page_number\"]][\"sentences\"]\n",
    "    sentence_chunk = [sentences[i : i+SENTENCE_CHUNKS] for i in range(0, len(sentences), SENTENCE_CHUNKS)]\n",
    "    page[\"sentence_chunk\"] = sentence_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b03d9f534a696c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:42:12.995127Z",
     "start_time": "2025-02-10T16:42:12.968943Z"
    }
   },
   "outputs": [],
   "source": [
    "for page in tqdm(pages_and_metadata, total=len(pages_and_metadata)):\n",
    "    print(page[\"sentence_chunk\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedc481bbe1a825b",
   "metadata": {},
   "source": [
    "## 2.8 Converting sentences into sentence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f450c90f591d29ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T16:42:25.651367Z",
     "start_time": "2025-02-10T16:42:21.601242Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer(model_name_or_path=EMBEDDING_MODEL).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8820fa929ef41f04",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-02-10T16:42:47.633183Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "for page in tqdm(pages_and_metadata):\n",
    "    embeddings = list()\n",
    "    # print(f\"Processing Page {page['page_number']}\")\n",
    "    for sentence in page[\"sentences\"]:\n",
    "        # sentence = sentence.to(device)\n",
    "        embedding = embedding_model.encode(sentence, batch_size=1024, convert_to_tensor=True, show_progress_bar=False, device=device)\n",
    "        embedding = np.stack(embedding.tolist(), axis=0)\n",
    "        embedding = torch.tensor(embedding)\n",
    "        embedding = embedding.type(torch.float32)\n",
    "        embeddings.append(embedding)\n",
    "    sentence_embeddings = [np.array(embedding) for embedding in embeddings]\n",
    "    pages_and_metadata[page[\"page_number\"]][\"embeddings\"] = sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3567e35e0f369ed6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T03:59:45.693927Z",
     "start_time": "2025-02-11T03:59:45.300342Z"
    }
   },
   "outputs": [],
   "source": [
    "for page in tqdm(pages_and_metadata, total=len(pages_and_metadata)):\n",
    "    print(np.array(page[\"embeddings\"]).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8360bc8d64e45d79",
   "metadata": {},
   "source": [
    "## 2.9 Checking the metadata present for use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5023c654f155fd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in pages_and_metadata[0].keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bd19f71651b712",
   "metadata": {},
   "source": [
    "# 3. FETCHING SIMILAR CONTENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184fd946401c91fd",
   "metadata": {},
   "source": [
    "## 3.1 Getting the data embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c81dcf0f0426027",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_and_metadata_embeddings = []\n",
    "\n",
    "for page in tqdm(pages_and_metadata, total=len(pages_and_metadata)):\n",
    "    page_embeddings = []\n",
    "    for chunk_embedding in pages_and_metadata[page[\"page_number\"]][\"embeddings\"]:\n",
    "        if isinstance(chunk_embedding, torch.Tensor):\n",
    "            chunk_embedding = chunk_embedding.tolist()\n",
    "        page_embeddings.append(chunk_embedding)\n",
    "    pages_and_metadata_embeddings.append(page_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78792dfaf1127f6",
   "metadata": {},
   "source": [
    "## 3.2 Converting each embedding into the same dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59618f1fd119517a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pages_and_metadata_embeddings:\n",
    "    embedding_dim = len(pages_and_metadata_embeddings[0][0])\n",
    "    pages_and_metadata_embeddings = [\n",
    "            [np.pad(chunk, (0, max(0, embedding_dim - len(chunk))), mode='constant')[:embedding_dim]\n",
    "             for chunk in page]\n",
    "            for page in pages_and_metadata_embeddings\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2300c8743f62a23a",
   "metadata": {},
   "source": [
    "## 3.3 Flattening the nested list of embeddings and the sentence to fetch by index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e07bdbecbd85c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_embeddings = [chunk for page in pages_and_metadata_embeddings for chunk in page]\n",
    "flat_data = [sentence for page in pages_and_metadata for sentence in page[\"sentences\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9a9684b1287496",
   "metadata": {},
   "source": [
    "## 3.4 Saving the flattened embeddings and the flattened data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4533ad448b48e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(flat_embeddings)\n",
    "df.to_csv(\"embeddings_v5.csv\", index=False)\n",
    "\n",
    "df = pd.DataFrame(flat_data)\n",
    "df.to_csv(\"data_v5.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9c5f925a05048b",
   "metadata": {},
   "source": [
    "## 3.5 Loading the flattened embeddings and flattened data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80f723dee61c4c4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:38:13.460239Z",
     "start_time": "2025-02-09T11:38:13.128443Z"
    }
   },
   "outputs": [],
   "source": [
    "flat_embeddings = pd.read_csv(\"embeddings_v5.csv\").to_numpy()\n",
    "flat_data = pd.read_csv(\"data_v5.csv\")[\"0\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d04b8ed2-21c5-4bd2-92e2-8d016e4c7079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the flat_embeddings is 92230\n",
      "The size of the flat_data is 92230\n"
     ]
    }
   ],
   "source": [
    "print(f\"The size of the flat_embeddings is {len(flat_embeddings)}\")\n",
    "print(f\"The size of the flat_data is {len(flat_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd66e23aff7a901",
   "metadata": {},
   "source": [
    "## 3.6 Converting embeddings to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3f43ce0368e6c95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:38:14.661823Z",
     "start_time": "2025-02-09T11:38:14.644537Z"
    }
   },
   "outputs": [],
   "source": [
    "pages_and_metadata_embeddings = np.array(flat_embeddings, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61118042f2f3a0c1",
   "metadata": {},
   "source": [
    "## 3.7 Converting the numpy array embeddings to torch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "476ae4b96e92b843",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:38:16.402973Z",
     "start_time": "2025-02-09T11:38:15.539250Z"
    }
   },
   "outputs": [],
   "source": [
    "pages_and_metadata_embeddings = torch.tensor(pages_and_metadata_embeddings, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7691ac5f8be5636",
   "metadata": {},
   "source": [
    "## 3.8 Getting the similarity score by query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6745dc59ef034840",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:38:24.424800Z",
     "start_time": "2025-02-09T11:38:17.617912Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "query_embeddings = embedding_model.encode(QUERY, convert_to_tensor=True).to(device)\n",
    "dot_score = util.dot_score(query_embeddings, pages_and_metadata_embeddings)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bea1268563c3129",
   "metadata": {},
   "source": [
    "## 3.9 Getting the top k similar scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dea591cca2af654",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:38:27.505935Z",
     "start_time": "2025-02-09T11:38:27.441640Z"
    }
   },
   "outputs": [],
   "source": [
    "top_scores, top_indices = torch.topk(dot_score, k=K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87a0a2111e42c0cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:38:28.290866Z",
     "start_time": "2025-02-09T11:38:28.283472Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top scores: tensor([0.7715, 0.7452, 0.6983, 0.6900, 0.6900, 0.6748, 0.6615, 0.6551, 0.6482,\n",
      "        0.6480], device='cuda:0')\n",
      "Top indices: tensor([10490, 83082, 87417, 43686, 31166,  4333, 84253, 87936, 10046, 18902],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(f\"Top scores: {top_scores}\")\n",
    "print(f\"Top indices: {top_indices}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a93ccde99a8e5fd",
   "metadata": {},
   "source": [
    "## 3.10 Getting the top k content based on the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "412b8aad1ac4bc28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:38:31.127772Z",
     "start_time": "2025-02-09T11:38:31.124080Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data from page 10490\n",
      "Fetching data from page 83082\n",
      "Fetching data from page 87417\n",
      "Fetching data from page 43686\n",
      "Fetching data from page 31166\n",
      "Fetching data from page 4333\n",
      "Fetching data from page 84253\n",
      "Fetching data from page 87936\n",
      "Fetching data from page 10046\n",
      "Fetching data from page 18902\n"
     ]
    }
   ],
   "source": [
    "context = list()\n",
    "for index in top_indices:\n",
    "    print(f\"Fetching data from page {index}\")\n",
    "    context.append(flat_data[index.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "777d6213b100c8ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:38:31.789353Z",
     "start_time": "2025-02-09T11:38:31.783885Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['problem • tanh activation function times earlier book i’ve mentioned arguments tanh function better activation function sigmoid function.', 'hyperbolic tangent (tanh) \\ue000e hyperbolic tangent tanh.x/ d e2x', 'furthermore, existence symmetries particular property ‘tanh’ function applies wide range activation functions (k˙urkov´a kainen, 1994).', 'tanh similar identity function near , training deep neural 0 network ˆy = w\\ue03etanh(u \\ue03etanh(v \\ue03ex)) resembles training linear model ˆy = w\\ue03eu\\ue03ev \\ue03ex long activations network kept small.', 'tanh similar identity function near , training deep neural 0 network ˆy = w\\ue03etanh(u \\ue03etanh(v \\ue03ex)) resembles training linear model ˆy = w\\ue03eu\\ue03ev \\ue03ex long activations network kept small.', 'logistic function f(a) = 1 1 + exp(−a) (5.17) tanh activation function f(a) = tanh(a) = exp(a) −exp(−a) exp(a) + exp(−a) . (', 'update candidate z computed linear combination xj hj', '5.191) exists equivalent network, computes exactly func- tion, hidden unit activation functions given tanh(a) tanh func- tion deﬁned (5.59).', 'simplest variation tanh (pronounced “tanch”) neuron, replaces sigmoid function hyperbolic tangent function.', 'range tanh activation function (−1, 1) like sigmoid, range bounded, unlike sigmoid, includes negative values.']\n"
     ]
    }
   ],
   "source": [
    "print(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8877aa473dfa93b2",
   "metadata": {},
   "source": [
    "# 4. Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8fe840bd679626",
   "metadata": {},
   "source": [
    "## 4.1 Login to HuggingFace CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7d432027efb7be0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:38:36.750390Z",
     "start_time": "2025-02-09T11:38:36.738802Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e71e6fa2df784f0ebe0cb9e51baac30b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75492f83d44a73d",
   "metadata": {},
   "source": [
    "## 4.2 Loading the LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e064d9014c81b7b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:38:47.266956Z",
     "start_time": "2025-02-09T11:38:37.962878Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0720c7607a75425680238d7557f911ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OSError",
     "evalue": "Insufficient system resources exist to complete the requested service. (os error 1450)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m      2\u001b[0m     pretrained_model_name_or_path\u001b[38;5;241m=\u001b[39mLLM_MODEL,\n\u001b[0;32m      3\u001b[0m     torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16,\n\u001b[0;32m      4\u001b[0m     low_cpu_mem_usage\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m      5\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    565\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    566\u001b[0m     )\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    570\u001b[0m )\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\transformers\\modeling_utils.py:4245\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   4235\u001b[0m         load_contexts\u001b[38;5;241m.\u001b[39mappend(tp_device)\n\u001b[0;32m   4237\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(load_contexts):\n\u001b[0;32m   4238\u001b[0m         (\n\u001b[0;32m   4239\u001b[0m             model,\n\u001b[0;32m   4240\u001b[0m             missing_keys,\n\u001b[0;32m   4241\u001b[0m             unexpected_keys,\n\u001b[0;32m   4242\u001b[0m             mismatched_keys,\n\u001b[0;32m   4243\u001b[0m             offload_index,\n\u001b[0;32m   4244\u001b[0m             error_msgs,\n\u001b[1;32m-> 4245\u001b[0m         ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_load_pretrained_model(\n\u001b[0;32m   4246\u001b[0m             model,\n\u001b[0;32m   4247\u001b[0m             state_dict,\n\u001b[0;32m   4248\u001b[0m             loaded_state_dict_keys,  \u001b[38;5;66;03m# XXX: rename?\u001b[39;00m\n\u001b[0;32m   4249\u001b[0m             resolved_archive_file,\n\u001b[0;32m   4250\u001b[0m             pretrained_model_name_or_path,\n\u001b[0;32m   4251\u001b[0m             ignore_mismatched_sizes\u001b[38;5;241m=\u001b[39mignore_mismatched_sizes,\n\u001b[0;32m   4252\u001b[0m             sharded_metadata\u001b[38;5;241m=\u001b[39msharded_metadata,\n\u001b[0;32m   4253\u001b[0m             _fast_init\u001b[38;5;241m=\u001b[39m_fast_init,\n\u001b[0;32m   4254\u001b[0m             low_cpu_mem_usage\u001b[38;5;241m=\u001b[39mlow_cpu_mem_usage,\n\u001b[0;32m   4255\u001b[0m             device_map\u001b[38;5;241m=\u001b[39mdevice_map,\n\u001b[0;32m   4256\u001b[0m             offload_folder\u001b[38;5;241m=\u001b[39moffload_folder,\n\u001b[0;32m   4257\u001b[0m             offload_state_dict\u001b[38;5;241m=\u001b[39moffload_state_dict,\n\u001b[0;32m   4258\u001b[0m             dtype\u001b[38;5;241m=\u001b[39mtorch_dtype,\n\u001b[0;32m   4259\u001b[0m             hf_quantizer\u001b[38;5;241m=\u001b[39mhf_quantizer,\n\u001b[0;32m   4260\u001b[0m             keep_in_fp32_modules\u001b[38;5;241m=\u001b[39mkeep_in_fp32_modules,\n\u001b[0;32m   4261\u001b[0m             gguf_path\u001b[38;5;241m=\u001b[39mgguf_path,\n\u001b[0;32m   4262\u001b[0m             weights_only\u001b[38;5;241m=\u001b[39mweights_only,\n\u001b[0;32m   4263\u001b[0m         )\n\u001b[0;32m   4265\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[0;32m   4266\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\transformers\\modeling_utils.py:4791\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[1;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path, weights_only)\u001b[0m\n\u001b[0;32m   4784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   4785\u001b[0m     device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   4786\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   4787\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m hf_quantizer\u001b[38;5;241m.\u001b[39mquantization_config\u001b[38;5;241m.\u001b[39mquant_method \u001b[38;5;241m==\u001b[39m QuantizationMethod\u001b[38;5;241m.\u001b[39mTORCHAO\n\u001b[0;32m   4788\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m hf_quantizer\u001b[38;5;241m.\u001b[39mquantization_config\u001b[38;5;241m.\u001b[39mquant_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint4_weight_only\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4789\u001b[0m ):\n\u001b[0;32m   4790\u001b[0m     map_location \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice([d \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m device_map\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;28;01mif\u001b[39;00m d \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m]][\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m-> 4791\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m load_state_dict(\n\u001b[0;32m   4792\u001b[0m     shard_file, is_quantized\u001b[38;5;241m=\u001b[39mis_quantized, map_location\u001b[38;5;241m=\u001b[39mmap_location, weights_only\u001b[38;5;241m=\u001b[39mweights_only\n\u001b[0;32m   4793\u001b[0m )\n\u001b[0;32m   4795\u001b[0m \u001b[38;5;66;03m# Mistmatched keys contains tuples key/shape1/shape2 of weights in the checkpoint that have a shape not\u001b[39;00m\n\u001b[0;32m   4796\u001b[0m \u001b[38;5;66;03m# matching the weights in the model.\u001b[39;00m\n\u001b[0;32m   4797\u001b[0m mismatched_keys \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m _find_mismatched_keys(\n\u001b[0;32m   4798\u001b[0m     state_dict,\n\u001b[0;32m   4799\u001b[0m     model_state_dict,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4804\u001b[0m     ignore_mismatched_sizes,\n\u001b[0;32m   4805\u001b[0m )\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\transformers\\modeling_utils.py:504\u001b[0m, in \u001b[0;36mload_state_dict\u001b[1;34m(checkpoint_file, is_quantized, map_location, weights_only)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;124;03mReads a PyTorch checkpoint file, returning properly formatted errors if they arise.\u001b[39;00m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m checkpoint_file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.safetensors\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m is_safetensors_available():\n\u001b[0;32m    503\u001b[0m     \u001b[38;5;66;03m# Check format of the archive\u001b[39;00m\n\u001b[1;32m--> 504\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m safe_open(checkpoint_file, framework\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    505\u001b[0m         metadata \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mmetadata()\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m metadata\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmlx\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "\u001b[1;31mOSError\u001b[0m: Insufficient system resources exist to complete the requested service. (os error 1450)"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=LLM_MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=False,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d6e6ed3c6d3949",
   "metadata": {},
   "source": [
    "## 4.3 Augmenting the prompt for instructing the LLM in a better way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1042c149cef8b04a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:38:48.422916Z",
     "start_time": "2025-02-09T11:38:47.273510Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL)\n",
    "context = \"\\n -\".join(context)\n",
    "base_prompt = f'''Based on the following context items, please answer the query\n",
    "Context Items:\n",
    "{context}\n",
    "Query:\n",
    "{QUERY}\n",
    "Answer:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9329f053bc5df59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:38:51.178431Z",
     "start_time": "2025-02-09T11:38:51.173961Z"
    }
   },
   "outputs": [],
   "source": [
    "base_prompt = base_prompt.format(context=context, query=QUERY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ae612c14837c95",
   "metadata": {},
   "source": [
    "## 4.4 Creating the dialogue template for the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6eadc67098c5cc4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:38:55.854374Z",
     "start_time": "2025-02-09T11:38:55.847614Z"
    }
   },
   "outputs": [],
   "source": [
    "dialogue_template = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": base_prompt,\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafb0016557c0272",
   "metadata": {},
   "source": [
    "## 4.5 Applying the prompt to the dialogue template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277dde89685a2245",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:38:57.127131Z",
     "start_time": "2025-02-09T11:38:57.105583Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
    "                                       tokenize=False,\n",
    "                                       add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c2a05e96737f1",
   "metadata": {},
   "source": [
    "## 4.6 Providing the prompt and retrieving the answer from the LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b57262112415464",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:42:29.646183Z",
     "start_time": "2025-02-09T11:42:27.227166Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL)\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**input_ids, temperature=TEMPERATURE, do_sample=True, max_new_tokens=MAX_NEW_TOKENS)\n",
    "output_text = tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7816da98e6a7a3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:42:29.658075Z",
     "start_time": "2025-02-09T11:42:29.653897Z"
    }
   },
   "outputs": [],
   "source": [
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b739bcea21ecd35a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:42:29.775358Z",
     "start_time": "2025-02-09T11:42:29.772028Z"
    }
   },
   "outputs": [],
   "source": [
    "idx = output_text.find(\"Answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a743149d967b4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:42:29.840167Z",
     "start_time": "2025-02-09T11:42:29.836521Z"
    }
   },
   "outputs": [],
   "source": [
    "answer = output_text[idx+7:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acff9162acb0b8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:42:29.974904Z",
     "start_time": "2025-02-09T11:42:29.970946Z"
    }
   },
   "outputs": [],
   "source": [
    "answer = answer.replace(\"**\", \"\")\n",
    "answer = answer.replace(\"<start_of_turn>model\",\"\")\n",
    "answer = re.sub(\"<.*?>\", \"\", answer)\n",
    "# answer = answer[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea26bd933ddca3e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T11:42:37.796573Z",
     "start_time": "2025-02-09T11:42:37.787565Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"The cleaned answer is: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2155f2e4-0953-4c2f-a6ea-093383f35bf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ecfdab-9045-4f4e-ae11-8c95d5bc420c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
